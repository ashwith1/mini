{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: torch in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: torch in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.30.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers numpy pandas scikit-learn PyPDF2 spacy nltk\n",
    "%pip install transformers\n",
    "%pip install transformers datasets torch scikit-learn\n",
    "%pip install transformers[torch]\n",
    "%pip install accelerate\n",
    "%pip install torch\n",
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to pdf files\n",
    "data_path = 'C:\\\\Users\\\\asha4\\\\OneDrive\\\\Desktop\\\\mini\\\\mini\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list to store pdf data\n",
    "pdf_as_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\15_pdf_Neubuerger_Brosch_engl.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\16_pdf_kip-kurz-englisch.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\190521_Sprachförderung_DINlang_RZ_EN_web.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\190927_Broschüre_Bahnstadt_final_englisch.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\230131_HeidelbergCARD_Flyer_2023_EN_RZ_ES (1).pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\9781800208421-THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\about us.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc4.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc5.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc6.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\acc7.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ad.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ad1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ad2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ad3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ADSA Application.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ADSA page.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ai.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ai1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ai2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\Applied Data Science and Analytics Master of Science (M.Sc.)_Curriculum_01.04.2023 (1).pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\bt.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\bt1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\computer science.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\core principle.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\cs.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\csfees.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\csfees2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\cube.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\dmt.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\dmt1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\dmt2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\dmt3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\download.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\DT.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\education.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\Erlebnisangebote_E.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\EZProxy-FAQs-E.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\food.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\Fundamentals-of-Data-Engineering-by-Joe-Reis-Matt-Housley.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\gaa.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\gbl.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\gbl1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\HD-B84_SRH_HS_HD_Factsheet_BEng_Electrical_Engineering_EN_2023_11_Web_300dpi.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\health city_E_ES.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\health.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ibe.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ibe1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ibe2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ibe3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\ibuddy.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\iml.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\iml1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\instructorinfo.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\international office.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\it.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\it1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\it2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\it3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\library.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\mentally.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\mt.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\mt1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\mt2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\mt3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\parent.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof10.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof2.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof4.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof5.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof6.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof7.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof8.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\prof9.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\psy.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\residences.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\scc3.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\schools.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\sig.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\sports.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\SRH-HS-HD-Tuition-fees-EEA-students-2023.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\SRH-HS-HD-Tuition-fees-non-EEA-students-2023.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\srh.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\srh1.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\Stadtfuehrungen_E.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\student.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\studies.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\study.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\Swati.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\unterkuenfte_arrangements_A4_broschuere_en_ES_web (1).pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\values.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\we.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\wt.pdf\n",
      "Attempting to open: C:\\Users\\asha4\\OneDrive\\Desktop\\mini\\mini\\data\\wt1.pdf\n",
      "Heidelberg –\n",
      "your new home\n",
      "www.heidelberg.deVouchers for \n",
      "museum and \n",
      "public library \n",
      "included “I lost my heart \n",
      "in Heidelberg.”\n",
      "Name of a 1927 musical by  \n",
      "Fred Raymond (music)  \n",
      "and Fritz Löhner-Beda  \n",
      "and Ernst Neubach (words)\n",
      "Welcome to Heidelberg!\n",
      "Dear new citizen,\n",
      "I am pleased that you have chosen our city as your new home. Heidelberg is \n",
      "known for its scenic beauty, its internationality and diversity. \n",
      "Living in Heidelberg means having a home: the city derives its high quality of life \n",
      "from the attractive mix of living and working, research and culture. As a \n",
      "cosmopolitan city and “Rainbow City”, Heidelberg welcomes all people with open \n",
      "arms, regardless of their origin or sexual identity. \n",
      "We want you to feel at home here – and to that end, we support you in all areas \n",
      "of life: Families with children have access to an excellent childcare network. \n",
      "Schoolchildren can develop their talents to the full here, because Heidelberg is \n",
      "the best school location in Germany according to several studies. Low-income \n",
      "families receive targeted support in the form of a wide range of free services, \n",
      "and we also support trainees and students on their way, for example by subsidizing \n",
      "local public transport tickets and with the training house. \n",
      "Eleven senior citizen centers are available as contact points for all older citizens. \n",
      "A wide range of cultural, leisure and sporting activities, from theaters and half-\n",
      "marathons to festivals with top reputations, leaves nothing to be desired. Climate protection plays a central role in Heidelberg: for example, we promote \n",
      "the purchase of photovoltaic systems and electric cars, support climate-friendly \n",
      "renovations and are proud of our forests and green spaces.\n",
      "We are also proud of our many scientific and research institutions that take their \n",
      "ideas and solutions for a better future from Heidelberg to the world. First and \n",
      "foremost is Ruperto Carola, our university of excellence, which attracts many \n",
      "young people to study in our city every year thanks to its international appeal.\n",
      "We wish you a good start in our beautiful city and are happy to answer any \n",
      "questions you may have.\n",
      "Your\n",
      "Prof. Dr. Eckart Würzner\n",
      "Mayor Content\n",
      "A good place to live  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "First things first  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "Heidelberg in figures  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n",
      "Unique 15 times over  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "International city of culture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n",
      "Top marks for school and childcare  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "Leading the way in climate protection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n",
      "Transport for everyone  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n",
      "World-class research  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n",
      "A safe city  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n",
      "Homes for everyone  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n",
      "Embracing diversity  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "Volunteers – a vital asset for Heidelberg  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n",
      "Getting the public involved  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n",
      "Partners around the world  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n",
      "Imprint  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n",
      "A good place to live  | 9 8 | A good place to live\n",
      "A good place to live\n",
      "Heidelberg is considered one of the most beautiful \n",
      "cities in Germany. Every year, around 12 million \n",
      "people visit the world-famous city alongside the \n",
      "river Neckar, to see the interplay between the castle, \n",
      "the old town and the river amid the hills of the \n",
      "Odenwald.\n",
      "The about 160,000 citizens of Heidelberg appreciate \n",
      "Heidelberg‘s high quality of life. Year after year, \n",
      "almost all respondents in the Heidelberg survey \n",
      "state that they love living in their city. This is hardly \n",
      "surprising, given the city’s many strengths.A good place to live  | 11 10 | A good place to live\n",
      "Family-friendly \n",
      " \n",
      "Heidelberg supports families with offers \n",
      "that are among the best in Germany: \n",
      "exemplary childcare, a choice of out -\n",
      "standing schools and holiday programs, \n",
      "fun playgrounds, child-friendly traffic \n",
      "planning and much more.\n",
      "City of science\n",
      " \n",
      "Heidelberg University is one of the best \n",
      "in the world. Together with the Uni -\n",
      "versity Hospital, respected research \n",
      "institutes like the German Cancer Re -\n",
      "search Center, numerous research-  \n",
      "related companies, and other higher \n",
      "education establishments, it has earned \n",
      "Heidelberg a worldwide reputation as \n",
      "a major center for science.Cosmopolitan\n",
      " \n",
      "An estimated 56,000 people with a \n",
      "migration background are at home in \n",
      "Heidelberg. They come from around \n",
      "180 countries and enrich the vibrant \n",
      "community.\n",
      "Good for recreation \n",
      " \n",
      "Because of its scenic location, its fa -\n",
      "vorable climatic conditions and its urban \n",
      "flair Heidelberg has one of the highest \n",
      "leisure values in Germany.\n",
      "Strong for business\n",
      "The city provides jobs for around \n",
      "120,000 people. Of these, around two-\n",
      "thirds are employed in the knowledge- \n",
      "intensive sector.Dynamic urban \n",
      "development\n",
      " \n",
      "Housing is a top priority in Heidelberg. \n",
      "The Bahnstadt is currently being devel -\n",
      "oped as a pioneering new district. It \n",
      "will provide living space for around \n",
      "6,800 people and up to 6,000 jobs.\n",
      "On four former sites of the US Army, \n",
      "future quarters are growing with sev -\n",
      "eral thousand apartments, space for \n",
      "innovative companies and high-quality \n",
      "sports, cultural and leisure facilities.\n",
      "Cultural mecca\n",
      " \n",
      "Heidelberg impresses with its wide \n",
      "range of cultural offerings in the areas \n",
      "of music, theater, dance, visual arts and \n",
      "film. Heidelberg is also the first and so \n",
      "far only German-speaking city of liter -\n",
      "ature in the UNESCO network of “Cre -\n",
      "ative Cities“.\n",
      "Voluntary work\n",
      " \n",
      "Heidelberg has a diverse and well-  \n",
      "structured range of volunteers. Their \n",
      "commitment is invaluable for success -\n",
      "ful coexistence and social cohesion.Involving people\n",
      " \n",
      "Heidelberg‘s citizens are systematically \n",
      "involved into municipal planning at an \n",
      "early stage. They are thus important \n",
      "sources of ideas for the administration \n",
      "and the city council. The city‘s project \n",
      "list, which provides information on \n",
      "planned projects, also enables early \n",
      "participation.\n",
      "A safe city\n",
      " \n",
      "Heidelberg is safe – thanks to a part -\n",
      "nership with the state police and a \n",
      "broad-based municipal public order \n",
      "service.\n",
      "Eco-friendly\n",
      "Heidelberg is an international pioneer \n",
      "in climate protection. As early as 2015, \n",
      "the United Nations named the city a \n",
      "“Global Green City“. With the climate \n",
      "protection action plan, adopted in 2019, \n",
      "Heidelberg aims to become climate-  \n",
      "neutral by 2030.First things first  | 13 12 | First things first \n",
      "All services of Heidelberg City Administration can be accessed at the adminis -\n",
      "trative offices, including: registering your new place of residence, applying for \n",
      "a passport, ID or parking permit, changing the place of registration of your car, \n",
      "applying for a police clearance certificate (Führungszeugnis) or International \n",
      "Driving Permit, picking up yellow recycling bags or the latest waste collection \n",
      "calendar, applying for a “Heidelberg-Pass“ / “Heidelberg-Pass+“, requesting \n",
      "childCare vouchers (Betreuungsgutscheine) or “housing eligibility vouchers“ \n",
      "(Wohnberechtigungsscheine), purchasing vouchers for the women’s night \n",
      "taxi (Frauen-nachttaxischeine), and much more. The addresses and open -\n",
      "ing times of the administrative offices can be found at www.heidelberg.de/\n",
      "administration.\n",
      "A wide range of citizens’ service information from A for “Abfallkalender“ \n",
      "(waste collection calendar) to Z for “Zulassungsformular“ (car registration \n",
      "form) is also available 24 hours a day on the internet at www.heidelberg.de/\n",
      "buergerservice .Homepage\n",
      "At www.heidelberg.de  you will find \n",
      "a comprehensive range of services \n",
      "and information: From an online city \n",
      "map to a calendar of events to the \n",
      "latest news from the administration \n",
      "and the municipal council.\n",
      "City Gazette\n",
      "Free of charge for every household: \n",
      "On Wednesdays, the Heidelberg City \n",
      "Gazette lands in your mailbox.\n",
      "Social media\n",
      "  heidelberg.de\n",
      "  heidelberg_de\n",
      "  heidelberg_de\n",
      "  StadtHeidelberg\n",
      "All posts of the municipal social media \n",
      "channels at a glance at \n",
      "www.stage.heidelberg.deMeinHeidelberg-App\n",
      "The MeinHeidelberg app provides cur -\n",
      "rent service information and tips. The \n",
      "app can be downloaded free of charge \n",
      "at www.digitales.heidelberg.de/app.\n",
      "Important emergency numbers\n",
      "Police: \n",
      "Dial 110 to reach the nearest police \n",
      "station.\n",
      "Fire and rescue services: \n",
      "Under the European emergency \n",
      "number 112 you can reach the fire de -\n",
      "partment and the rescue service for \n",
      "life- threatening cases of illness.Citizen Service\n",
      "The Heidelberg “citizen’s service“ \n",
      "(Bürgerservice) team are on hand \n",
      "from  8.00 am – 6.00 pm Monday to \n",
      "Friday to answer questions about \n",
      "the services of the City of Heidelberg \n",
      "and other Federal and state authori -\n",
      "ties. They can be reached on 06221 \n",
      "58-10580 or 115, the central number \n",
      "for information on any German \n",
      "public authority.  The citizens’ service \n",
      "can also connect you to other services \n",
      "as necessary.Citizen service in your local area\n",
      "Alternatively, you can access citizens’ \n",
      "services in person from the various \n",
      "“administrative offices“ (Bürgerämter) \n",
      "around the city:\n",
      " –Bürgeramt Altstadt (Old Town)\n",
      " – Bürgeramt Boxberg/Emmertsgrund\n",
      " –Bürgeramt Handschuhsheim\n",
      " –Bürgeramt Kirchheim\n",
      " –Bürgeramt Mitte  \n",
      "(“Central“ administrative office,  \n",
      "covering the districts of Bahnstadt, \n",
      "Bergheim, Südstadt and Weststadt)\n",
      " –Bürgeramt Neuenheim\n",
      " –Bürgeramt Pfaffengrund\n",
      " –Bürgeramt Rohrbach\n",
      " –Bürgeramt Wieblingen\n",
      " – Bürgeramt Ziegelhausen/  \n",
      "SchlierbachFirst things first \n",
      "The city of Heidelberg offers you information and \n",
      "assistance for a quick orientation in your new home. 12 Mio.  \n",
      "guests per year\n",
      " 120.000  \n",
      "workplaces\n",
      "Approximately 87 % employed  \n",
      "in the service sector65 %   \n",
      "of employees subject to \n",
      "social insurance contribu -\n",
      "tions work in the knowl -\n",
      "edge-intensive service \n",
      "sector and in industrial \n",
      "high technology\n",
      "38.000  \n",
      "students \n",
      "of which around 29,000 at \n",
      "Heidelberg University\n",
      "Heidelberg4you\n",
      "More than 200 locations \n",
      "are covered by the free \n",
      "public WLAN network Heidelberg in figures | 15 14 | Heidelberg in figures\n",
      "Heidelberg in figures\n",
      "160.000   \n",
      "Inhabitants\n",
      "Heidelberg is the “youngest“ \n",
      "city in Germany. About 39 % of \n",
      "the Heidelberg residents are \n",
      "younger than 30 years, around \n",
      "16 % older than 65.34 %  \n",
      "university graduates  \n",
      "of all employees subject \n",
      "to social insurance \n",
      "contributions. \n",
      "By comparison, the national  \n",
      "average is 18.5 %.\n",
      "109 km² \n",
      "area\n",
      "30 % \n",
      "populated \n",
      "area 70 % \n",
      "green space16 | Unique 15 times over Unique 15 times over  | 17\n",
      "Unique 15 \n",
      "times over\n",
      "Heidelberg has 15 districts, each \n",
      "with its own distinct identity: The sheer variety of the city’s districts \n",
      "is what makes Heidelberg such an \n",
      "attractive and vibrant place to live. \n",
      "Residents can make their own mark on \n",
      "their local district by joining one of the \n",
      "many district associations and other \n",
      "groups. \n",
      "With the Patrick-Henry-Village  \n",
      " Heidelberg‘s 16th district is currently under development. It is planned to \n",
      "create 5,000 jobs and housing for up \n",
      "to 10,000 people.\n",
      "For further information, please visit \n",
      "the section “Living in Heidelberg” at \n",
      "www.heidelberg.de . The weekly market in front of \n",
      "Tiefburg Castle in the Hand -\n",
      "schuhsheim district is a popular \n",
      "weekend meeting place and an \n",
      "experience for all the senses \n",
      "with the regional produce on \n",
      "offer there.\n",
      " –Altstadt (Old Town)\n",
      " –Bahnstadt\n",
      " –Bergheim\n",
      " –Boxberg\n",
      " –Emmertsgrund –Handschuhsheim\n",
      " –Kirchheim\n",
      " –Neuenheim\n",
      " –Pfaffengrund\n",
      " –Rohrbach –Schlierbach\n",
      " –Südstadt\n",
      " –Weststadt\n",
      " –Wieblingen\n",
      " –ZiegelhausenInternational city of culture  | 19\n",
      "International city of culture\n",
      "If you have been living in Heidelberg for some time \n",
      "already, you will know that it lives up to its reputa -\n",
      "tion. As well as the beautiful surrounding scenery \n",
      "of the Odenwald hills, and the lovely river Neckar, \n",
      "the city offers a wide range of cultural and leisure \n",
      "facilities.    Heidelberg is a city that knows how to \n",
      "continue its great literary tradition in a \n",
      "contemporary way. Heidelberg‘s tradi -\n",
      "tion as the seat of the oldest university \n",
      "in Germany with important philoso -\n",
      "phers, theologians and literary figures, \n",
      "is combined with Heidelberg‘s lively \n",
      "literary scene and above-average num -\n",
      "ber of libraries, bookstores, publishers \n",
      "and translators, publishing houses and \n",
      "translators. For this reason, in December \n",
      "2014 it became the first and only Ger -\n",
      "man city to be included in the world‘s \n",
      "largest cultural network, UNESCO‘s \n",
      "Creative Cities  Network, as a City  \n",
      "of Literature: www.heidelberg.de/  \n",
      "cityofliterature.\n",
      "With its extensive collections, the \n",
      "Kurpfälzisches Museum , founded \n",
      "in 1879, offers an astounding impres -\n",
      "sion of the former Electoral Palatinate and its capital Heidelberg. Learn more \n",
      "about the exhibitions and collections \n",
      "at www.museum-heidelberg.de.  As a \n",
      "new citizen, you can use the enclosed \n",
      "family voucher on your first visit to \n",
      "the Kurpfälzische Museum. You can \n",
      "redeem the enclosed family voucher \n",
      "for two adults and two children (see \n",
      "page 49).\n",
      "The Mark Twain Center for Transatlantic \n",
      "Relations in Südstadt is a cultural center \n",
      "and place of exchange on the history \n",
      "and future issues of German-American \n",
      "relations. At a historically significant \n",
      "location in the former commandant‘s of -\n",
      "fice, where the heart of the U.S. Army‘s \n",
      "headquarters in Europe beat, a multi -\n",
      "media exhibition is on display where \n",
      "visitors can discover various facets \n",
      "of German-American coexistence in \n",
      "Heidelberg with the help of digital \n",
      "aids and in a playful way. \n",
      "For more information, visit www.mark-\n",
      "twain-center.com.\n",
      "Culture and leisure\n",
      "Music, film, theater, dance, literature \n",
      "and art of all eras and styles, enter -\n",
      "tainment and celebrations, sports and \n",
      "recreation – Heidelberg offers a wide \n",
      "range of leisure activities.\n",
      "The Theater und OrchesterHeidel-  \n",
      "berg  i s  a  fi v e -d i v i s i o n  t h e a t e r  w i t h   \n",
      "opera, concert, drama and dance, as well as its own ensemble for children‘s \n",
      "and youth theater, whose capacity uti -\n",
      "lization is among the best in Germany. \n",
      "The theater‘s various festivals, such \n",
      "as the Heidelberger Stückemarkt, the \n",
      "Dance Biennale and the Heidelberg  \n",
      "Castle Festival contribute to this. You \n",
      "can find out more about the theater \n",
      "program at www.theater.heidelberg.de .18 | International city of culture\n",
      "Old masters and modern artists: the Kurpfälzische \n",
      "Museum offers an exciting variety. An overview of shopping opportu -\n",
      "nities in Heidelberg is provided by \n",
      "the online platform www.vielmehr.\n",
      "heidelberg.de . There you will also find \n",
      "information on the citywide “Danke -\n",
      "Schein“ voucher.\n",
      "If you want to stay up to date and well \n",
      "entertained, the municipal library is \n",
      "at your disposal with a wide range of \n",
      "literature and media. With the library \n",
      "card, you can borrow a comprehen -\n",
      "sive selection of books, magazines, \n",
      "CDs, DVDs and electronic media. As \n",
      "a new citizen, you will receive the card \n",
      "free of charge for three months upon \n",
      "presentation of the enclosed voucher (see page 45): www.heidelberg.de/\n",
      "stadtbuecherei . \n",
      "Would you like to enjoy nature in your \n",
      "free time, learn something or visit \n",
      "famous sights?\n",
      "There are numerous opportunities for \n",
      "all of these. Among the special attrac -\n",
      "tions are the world-famous castle  \n",
      "and the funicular to the Königstuhl. \n",
      "In the Heidelberg Zoo  – the largest \n",
      "zoo in the Metropolitan Region – you \n",
      "can discover more than about 2,700 \n",
      "animals from all over the world. Face \n",
      "to face with a tiger or lion, discover a \n",
      "red panda high up in the branches, \n",
      "watch elephant bulls bathing – all this Friends of contemporary art can ex -\n",
      "pect contemporary works in the hall \n",
      "of the Kunstverein  directly next to \n",
      "the Kurpfälzische Museum. For more \n",
      "information, visit www.hdkv.de .\n",
      "With the Prinzhorn Collection  and the \n",
      "Hassbecker Collection , Heidelberg \n",
      "has two internationally outstanding \n",
      "collections of works of outsider art with \n",
      "a focus on “Art and Madness“ and  \n",
      "“Naive Painting” established through \n",
      "the work of the two Heidelberg  \n",
      "museums Sammlung Prinzhorn and \n",
      "Haus Cajeth. \n",
      "Art and culture dealing with the les -\n",
      "bian, gay, bisexual, trans*, inter and \n",
      "queer (lsbtiq+ for short) community \n",
      "is offered annually by the Heidelberg \n",
      "Queer Festival . When it was founded \n",
      "in 2009, it was the first festival for queer \n",
      "culture. Today it is the largest and most \n",
      "renowned of its kind in Germany and \n",
      "one of the flagships of the “Rainbow \n",
      "City“ Heidelberg.\n",
      "Heidelberg also has a large number of in -\n",
      "dependent institutions such as Halle02, \n",
      "Kulturfenster, Deutsch-Amer -\n",
      "ikanisches Institut, Interkul -\n",
      "turelles Zentrum, Unterwegs-  \n",
      "Theater and Kulturhaus Karlstor -\n",
      "bahnhof . A list of all cultural institu -\n",
      "tions, sports facilities and other leisure \n",
      "activities can be found under Life > \n",
      "Leisure at www.heidelberg.de . \n",
      "The many festivals also have a special \n",
      "appeal, including the classical music festivals “Heidelberger Früh -\n",
      "ling“  and “Enjoy Jazz“ or the \n",
      "renowned “International Film \n",
      "Festival Heidelberg-Mann -\n",
      "heim“ .\n",
      "“Metropolink“  is also an inte -\n",
      "gral part of the Heidelberg Sum -\n",
      "mer. For years, this nationally \n",
      "renowned festival of urban art \n",
      "has been actively contributing to \n",
      "the city‘s development and gives \n",
      "Heidelberg a modern face.\n",
      "With “Metropolink‘s Commis -\n",
      "sary“  – Germany‘s largest indoor \n",
      "gallery for street art – there is \n",
      "now also a year-round venue for \n",
      "urban art, culture, exchange and \n",
      "encounters.\n",
      "At the municipal music and sing -\n",
      "ing school, around 4,400 students re -\n",
      "ceive instruction in singing, orchestra, \n",
      "choir, band and theory: \n",
      "www.musikschule.heidelberg.de .\n",
      "Shopping Location Heidelberg\n",
      "Heidelberg is one of the most attrac -\n",
      "tive shopping cities in Germany. There \n",
      "are more than 500 retail stores and \n",
      "cafés in the city center – from the old \n",
      "town to Bergheim and Neuenheim. \n",
      "The approximately 1.4-kilometer-long \n",
      "Hauptstrasse alone is home to more \n",
      "than 250 stores and restaurants.\n",
      "The variety of small owner-operated \n",
      "stores, the many side streets and the \n",
      "high quality of stay create a special \n",
      "shopping experience in the Old Town. 20 | International city of culture\n",
      "The former US area Patrick-Henry-Village is the perfect setting for the popular Metropolink festival. International city of culture  | 21International city of culture | 23\n",
      "field, for beach volleyball courts, a \n",
      "large children‘s playground as well as \n",
      "a skate facility, and there are oppor -\n",
      "tunities for the trend sport slackline. \n",
      "Recreation and relaxation can be found \n",
      "in any season in one of Heidelberg‘s \n",
      "five swimming pools , the more than \n",
      "120 sports clubs and over 70 sports \n",
      "facilities and halls, as well as the  “alla \n",
      "hopp!” facility  in Kirchheim.\n",
      "In the Heidelberg Innovation Park \n",
      "(hip)  on Speyerer Straße, the new \n",
      "SNP dome  provides additional space \n",
      "for club and school sports. The MLP \n",
      "Academics Heidelberg basketball team \n",
      "and the Rhein-Neckar Löwen handball team also play there. Major sporting \n",
      "events such as the HeidelbergMan \n",
      "triathlon, the SAS half-marathon, \n",
      "international rugby tournaments, \n",
      "the rowing regatta and the Dragon \n",
      "Boat Cup  attract recreational and pro -\n",
      "fessional athletes and many spectators. \n",
      "Other events, such as the Family Sports \n",
      "Day, round off the program.\n",
      "The “Explo Heidelberg”  on the grounds \n",
      "of the zoo offers fascinating scientific \n",
      "experiments to participate in. The \n",
      "House of Astronomy  on the König -\n",
      "stuhl attracts children and adults alike \n",
      "to research into stars, galaxies, planets \n",
      "and black holes.and more is possible. Around 500,000 \n",
      "visitors a year also experience seals, \n",
      "monkeys and exotic birds. The Zoo \n",
      "School offers year-round events for \n",
      "children, teens and adults, such as \n",
      "zoo lessons, zoo vacations and tours \n",
      "with a zoo ranger. More info at www.\n",
      "zoo-heidelberg.de . Tours and chil -\n",
      "dren‘s birthday parties can be booked \n",
      "at www.zooschule-heidelberg.de .\n",
      "In the city forest , there are 250 kilo -\n",
      "meters of hiking trails, numerous rest \n",
      "and lookout points, themed and ad -\n",
      "venture trails, the Königstuhl walk -\n",
      "ing course, designated mountain bike \n",
      "trails, the arboretums, the “Pferchel“ \n",
      "forest adventure area for children \n",
      "and much more to discover. The city \n",
      "forest is outstandingly sustainable: \n",
      "Heidelberg has had the globally rec -\n",
      "ognized PEFC seal of approval for its \n",
      "forest management since 2001. In \n",
      "2015, Heidelberg was the first city in Germany to receive the “Recreational \n",
      "Forest“ certificate for the high rec -\n",
      "reational quality of its entire forest \n",
      "in accordance with PEFC standards. \n",
      "In 2018, Heidelberg was allowed to \n",
      "call itself “PEFC Forest Capital.“ This \n",
      "was followed in 2021 by the PEFC \n",
      "designation “Spa and Healing Forest“ \n",
      "– likewise as the first German city. The \n",
      "environmental education program \n",
      "“Natürlich Heidelberg”  lets visitors \n",
      "experience nature and the forests \n",
      "around the city in a new way.\n",
      "From March to December, guided tours, \n",
      "excursions and hands-on activities for \n",
      "children, young people and adults take \n",
      "place. The offers, including barbecue \n",
      "huts, can be booked directly at www.\n",
      "natuerlich.heidelberg.de .\n",
      "The riverbank at the Neckar  is also \n",
      "particularly popular as a play, sports, \n",
      "sunbathing and recreational area. In \n",
      "addition, a part is used as a soccer 22 | International city of culture\n",
      "The river banks at the Neckar are a popular spot to meet and relax. Get out of the daily grind and into the fun. In Heidelberg, you can swim in five swimming pools. Whether \n",
      "it‘s a sports pool in the middle of the city, an outdoor pool with a wide-wave slide, or an indoor pool with \n",
      "a sauna – everyone will find their personal favorite pool here.Top marks for school and childcare  | 25 24 | Top marks for school and childcare  \n",
      "Top marks for school  \n",
      "and childcare \n",
      "Heidelberg is an extremely family-friendly city. It \n",
      "offers the best childcare for very young children of \n",
      "any city in the state of Baden-Württemberg, and its \n",
      "schools are outstanding.Education\n",
      "Education has a long tradition in Heidel-  \n",
      "berg, which is lived and further devel -\n",
      "oped. The spectrum extends far be -\n",
      "yond the excellent range of courses \n",
      "offered by the university and colleges. \n",
      "It includes services for all generations. \n",
      "The city claims the top position na -\n",
      "tionwide in early childhood education \n",
      "and is a nationwide pioneer in school-\n",
      "based learning. For more information, \n",
      "visit www.heidelberg.de/bildung .\n",
      "Heidelberg aims to give all its stu -\n",
      "dents the best school-leaving qual -\n",
      "ification possible. For example, ad -ditional systematically structured \n",
      "language support is provided at all \n",
      "Heidelberg elementary schools and \n",
      "the Marie-Marcks School.\n",
      "The Heidelberg School Support \n",
      "System (HÜS) provides support for \n",
      "lower-achieving students, while the \n",
      "Hector Children‘s Academy and the \n",
      "Hector Seminar are aimed at the \n",
      "highly gifted. \n",
      "Of course, this also includes students \n",
      "with disabilities. Heidelberg has devel -\n",
      "oped viable support systems when it \n",
      "comes to inclusion.Top marks for school and childcare  | 27\n",
      "e. V. , which unites more than 50 youth \n",
      "associations, supports their concerns.\n",
      "With the “Feierbad” , the city has cre -\n",
      "ated another flexible offer for young \n",
      "people in Heidelberg. On an event area \n",
      "in the Tiergartenbad, young people can \n",
      "celebrate together with free admission.\n",
      "Social\n",
      "All people in Heidelberg should be \n",
      "able to live independently and in a self-  \n",
      "determined manner. For this reason, \n",
      "the city offers a wide range of programs \n",
      "that give citizens on low incomes the \n",
      "chance to participate in social life.\n",
      "For example, the Heidelberg Pass  and \n",
      "the Heidelberg Pass+  provide free or \n",
      "reduced-price admission to numerous cultural, sporting or other activities and \n",
      "free childcare in daycare centers from \n",
      "birth to school entry. Both variants of \n",
      "the Heidelberg Pass can be applied \n",
      "for at the citizens‘ registration offices.\n",
      "A social ticket  is also available for \n",
      "Heidelberg Pass holders. This subsi -\n",
      "dized season ticket allows citizens to use \n",
      "buses and trains at low prices. This is \n",
      "available at the RNV customer center at \n",
      "the central station, and further informa -\n",
      "tion is available at www.rnv-online.de.\n",
      "The city supports its citizens direct -\n",
      "ly with subsidy programs for rented \n",
      "apartments and property and makes \n",
      "a contribution to affordable hous -\n",
      "ing. More at www.heidelberg.de/  \n",
      "foerderprogramm\n",
      "Numerous advice centers in Heidelberg \n",
      "offer help in all situations.\n",
      "www.heidelberg.de/beratung\n",
      "Senior citizens\n",
      "Heidelberg is a city in which it is easy \n",
      "to grow old. The pages at www.  \n",
      "heidelberg.de/senioren  provide an \n",
      "overview of the wide range of informa -\n",
      "tion, advice and services on offer and \n",
      "the many ways of becoming and stay -\n",
      "ing active. In eleven Heidelberg dis -\n",
      "tricts, senior centers  are a central \n",
      "point of contact for elderly people. \n",
      "They offer a leisure program that Family\n",
      "Heidelberg offers a wide range of op -\n",
      "tions for balancing family and career: \n",
      " –Even for the youngest children \n",
      "from the age of one, there are flexi -\n",
      "ble opening hours, even all day, and \n",
      "excellently qualified specialist staff  \n",
      "in the daycare centers.\n",
      " –Elementary schools offer lunch, \n",
      "homework, afternoon and vacation \n",
      "care.\n",
      " –The secondary school has the right \n",
      "profile for the next generation and \n",
      "is easy to reach. Every educational \n",
      "qualification can be obtained at ei -\n",
      "ther an all-day or a half-day school.Heidelberg is a statewide pioneer in \n",
      "early childhood care and quality  \n",
      "assurance. 80 percent of elementary \n",
      "school children take advantage of the \n",
      "extensive after-school care options.\n",
      "There are children‘s and youth centers \n",
      "in Heidelberg in practically every district \n",
      "and run by all providers. The municipal \n",
      "“Haus der Jugend”  (House of Youth) \n",
      "in Römerstraße offers a cross-district \n",
      "service. There is plenty of room here \n",
      "for sports, art, dance and music, both \n",
      "during school hours and holiday time. \n",
      "For more information, visit www.haus -\n",
      "derjugend-hd.de . The youth associa -\n",
      "tions of the two large churches, for \n",
      "example, organize camps, events and \n",
      "open meetings. The Stadtjugendring 26 | Top marks for school and childcare  \n",
      "Place to be: the strongly fre -\n",
      "quented skate park is located \n",
      "right next to the riverside.   Top marks for school and childcare  | 29 28 | Top marks for school and childcare\n",
      "ranges from memory training to hikes \n",
      "and lunches. The city places a lot of \n",
      "emphasis on promoting the health of \n",
      "its older citizens and designs individual \n",
      "exercise programs with many partners.\n",
      "With the Academy for the Elderly , \n",
      "Heidelberg also has a renowned inde -\n",
      "pendent educational institution for \n",
      "people over 60 that offers courses \n",
      "ranging from astronomy to learning \n",
      "Chinese to creative writing. For more \n",
      "information, visit www.akademie-fuer -\n",
      " aeltere.de .\n",
      "The city of Heidelberg aims to enable \n",
      "older people to live independently in \n",
      "their own homes for as long as possi -\n",
      "ble. For example, numerous outpa -\n",
      "tient social services offer house -\n",
      "keeping or nursing services. The \n",
      "Heidelberg Care Support Point  \n",
      "provides advice and arranges outpa -\n",
      "tient services (phone 06221 58-49000, pflegestuetzpunkt@heidelberg.de). \n",
      "Information is also available at www.\n",
      "heidelberg.de/pflege . Structural chang -\n",
      "es can also help people stay at home.\n",
      "Information on financing conversions, \n",
      "for example with the help of the city‘s \n",
      "“Barrier-free Lifetime Homes“ subsidy \n",
      "program, is available from the City of \n",
      "Heidelberg‘s housing counseling ser -\n",
      "vice (phone 06221 58-25300, wohn -\n",
      "beratung@heidelberg.de). \n",
      "Those in need of care and their rela -\n",
      "tives can use a traffic light system at \n",
      "www.heidelberg.de/pflegeplatz , \n",
      "showing whether or which nursing \n",
      "homes in Heidelberg currently have \n",
      "places available. A contact form ena -\n",
      "bles initial contact with the relevant \n",
      "facilities.\n",
      "The city‘s projects aim to empower older people, enable seniors to \n",
      "live as full and independent a life as possible in old age, and prevent \n",
      "loneliness.Leading the way in climate protection  | 31 30 | Leading the way in climate protection \n",
      "Heidelberg is committed to lasting environmental, \n",
      "nature, and climate protection and plays a pioneering \n",
      "role in this area internationally. The framework for \n",
      "this is provided by the 30-point Climate Protection \n",
      "Action Plan, which covers all areas of life.\n",
      "Leading the way in \n",
      "climate protection Climate Protection Action Plan\n",
      "Heidelberg has a pioneering role in \n",
      "environmental and climate protection, \n",
      "which the city intends to expand fur -\n",
      "ther. The goal: Heidelberg wants to \n",
      "be climate-neutral by 2050 at the lat -\n",
      "est. This means that the city wants to \n",
      "reduce CO2 emissions by 95 percent \n",
      "and cut the municipality‘s energy re -\n",
      "quirements by half. On the way to \n",
      "achieving this, Heidelberg launched a \n",
      "major climate protection action plan \n",
      "in November 2019 with the first 30 \n",
      "concrete proposals.\n",
      "This action plan sets out targets and \n",
      "priorities within the “Master Plan \n",
      "100 % Climate Protection“  (www.\n",
      "heidelberg.de/masterplan100).\n",
      "The proposals affect all areas of life,  \n",
      "from construction and housing, nu -\n",
      "trition and consumption to nature-  \n",
      "oriented urban design and mobility \n",
      "(#hd4climate).\n",
      "The city can and wants to achieve \n",
      "these changes only together with its \n",
      "citizens and with stakeholders from \n",
      "business and society. All new citizens \n",
      "are cordially invited to use their move \n",
      "here as a starting signal for (more) \n",
      "commitment to climate protection.\n",
      "Energy consulting and funding \n",
      "programs\n",
      "Interested parties can easily deter -\n",
      "mine their personal CO2 footprint \n",
      "using the Heidelberg CO2 Mirror at www.heidelberg.de/co2spiegel . Here \n",
      "you can find individual suggestions on \n",
      "how to save carbon dioxide.\n",
      "The subsidy program for the rational use \n",
      "of energy promotes thermal insulation of \n",
      "the house and ventilation with heat re -\n",
      "covery. There are also support programs \n",
      "for sustainable water management and \n",
      "environmentally friendly vehicles. Infor -\n",
      "mation is available at www.heidelberg.\n",
      "de/foerderprogramm .\n",
      "The city‘s free energy advice hotline \n",
      "provides citizens with expert advice \n",
      "on all aspects of energy conservation \n",
      "and climate protection: 06221 58-18141 \n",
      "and klimasuchtschutz@heidelberg.\n",
      "de. It is supported by the Heidelberg-  \n",
      "Rhein-Neckar-Kreis gGmbH climate \n",
      "protection and energy consulting \n",
      "agency . For further information, visit \n",
      "www.kliba-heidelberg.de .\n",
      "Anyone interested in generating electric -\n",
      "ity from solar energy on their own roof \n",
      "can get free personal advice – whether \n",
      "as an owner, landlord, tenant or compa -\n",
      "ny (information on the solar campaign \n",
      "at www.heidelberg.de/sonnenstrom ).\n",
      "The installation of photovoltaic systems \n",
      "on roofs and facades is also being pro -\n",
      "moted to encourage private individu -\n",
      "als, housing associations, businesses \n",
      "and farmers to exploit the solar power \n",
      "potential in Heidelberg. For more in -\n",
      "formation, visit www.heidelberg.de/\n",
      "foerderprogramm.Leading the way in climate protection  | 33\n",
      "Nature and landscape\n",
      "When it comes to preserving biodiver -\n",
      "sity, ecological aspects play the leading \n",
      "role in Heidelberg. For example, as part \n",
      "of the species protection plan, mainte -\n",
      "nance measures are carried out in eco -\n",
      "logically valuable areas of the district \n",
      "to preserve the habitat of rare animal \n",
      "and plant species. In addition, with \n",
      "the help of the program for biotope \n",
      "networking, connecting structures in \n",
      "the form of meadows are created in \n",
      "agriculturally used areas. Animals can \n",
      "use these as migration corridors. More \n",
      "information on these and other con -\n",
      "servation activities is available at www.\n",
      "heidelberg.de/naturundlandschaft .Sustainable consumption\n",
      "With projects relating to the topic of \n",
      "“sustainable consumption“, the City \n",
      "of Heidelberg would like to encour -\n",
      "age citizens to become aware of their \n",
      "responsibility as consumers and to \n",
      "contribute to sustainable development \n",
      "in their everyday purchasing decisions. \n",
      "Among other things, Heidelberg has \n",
      "been designated a Fair Trade Town . \n",
      "Since 2021, on the initiative of the city, \n",
      "producers of regional food from Hei -\n",
      "delberg and the region have been \n",
      "advertising their products with the new \n",
      "“Genial regional”  label. In addition \n",
      "to its commitment to fair trade and \n",
      "regional products, the city also wants \n",
      "to promote the use of organic foods. \n",
      "As part of the “Bio in Heidelberg“  \n",
      "campaign, various projects are being \n",
      "implemented to this end together with \n",
      "educational institutions as well as fa -\n",
      "cilities along the value chain – from \n",
      "farmers to retailers to restaurants. \n",
      "Two shopping guides with tips on sus -\n",
      "tainable purchasing and consump -\n",
      "tion alternatives are available at www.  \n",
      "heidelberg.de/bio .Passive house construction: Bahn -\n",
      "stadt\n",
      "With Bahnstadt, Heidelberg has \n",
      "launched an outstanding climate pro -\n",
      "tection project that serves as a global \n",
      "role model. Passive house construction \n",
      "is the standard for the entire district - so \n",
      "far, this is unique in Germany. Heidel -\n",
      "berg aims to become a climate-neutral \n",
      "city by 2050 at the latest. www.heidel -\n",
      "berg-bahnstadt.de\n",
      "Sustainable energy\n",
      "With its 2020/2030 energy concept, \n",
      "Stadtwerke Heidelberg is expanding \n",
      "a climate-friendly energy supply. The \n",
      "wood-fired combined heat and power \n",
      "plant, which supplies the Bahnstadt \n",
      "district with heat and electricity, was a \n",
      "first building block. Further plants and \n",
      "an energy and future storage facility are being built in the Pfaffengrund energy \n",
      "park. www.swhd.de/energiekonzeption\n",
      "Education for sustainable devel -\n",
      "opment\n",
      "The Education for Sustainable Develop -\n",
      "ment program teaches children about \n",
      "the impact of their own actions on \n",
      "the future. Numerous projects ensure \n",
      "that ESD topics are integrated into \n",
      "everyday school life: In the “E-Team \n",
      "Project” , for example, students work \n",
      "to protect the climate. Mobility projects \n",
      "motivate children to travel in a healthy \n",
      "and environmentally friendly way. At \n",
      "the popular “Natürlich Heidelberg”  \n",
      "series of events, children and adults \n",
      "learn how important nature conser -\n",
      "vation and environmental protection \n",
      "are. Information is available at www.\n",
      "heidelberg.de/bne .32 | Leading the way in climate protection\n",
      "The Bahnstadt district is characterized by passive house construction and is an international lighthouse \n",
      "project.\n",
      "Little people, big impact: It is never too early to \n",
      "be a climate activist. Transport for everyone  | 35\n",
      " 34 | Transport for everyone\n",
      "Transport for everyone\n",
      "Environmentally conscious mobility is important \n",
      "to the city. Heidelberg is therefore promoting \n",
      "the expansion of local public transport and the \n",
      "cycling network.With the Heidelberg Mobility Network , the street -\n",
      "car network is being comprehensively expanded, \n",
      "including the reconstruction of the central station  \n",
      "stop and the new streetcar to Bahnstadt.\n",
      " \n",
      "Since January 2019, Heidelberg has been the first \n",
      "municipality in the region to use electric buses in the \n",
      "city center.\n",
      "Local public transport\n",
      "Just under a third of all Heidelberg \n",
      "residents primarily use public trans -\n",
      "portation. With the Heidelberg Mobility \n",
      "Network, the streetcar network has \n",
      "been extensively expanded in recent \n",
      "years, including the construction of a \n",
      "new streetcar route through Bahnstadt \n",
      "and the modernization of the central \n",
      "station stop. For more information, visit \n",
      "www.heidelberg-mobinetz.de.\n",
      "For public transport timetable informa -\n",
      "tion, visit www.vrn.de . Since 2016, the \n",
      "city of Heidelberg has been promoting \n",
      "the switch from car to public transport \n",
      "with an annual ticket for local public \n",
      "transport for citizens who voluntarily \n",
      "deregister their vehicle.\n",
      "Cycle paths\n",
      "33 percent of Heidelberg residents reg -\n",
      "ularly use bicycles in the city center. \n",
      "Awarded the title of “Bicycle-Friendly \n",
      "Municipality”  by the state, Heidelberg is a model municipality for the Rad -\n",
      "KULTUR initiative of the Baden-Würt -\n",
      "temberg Ministry of Transport and In -\n",
      "frastructure. The city wants to further \n",
      "increase its attractiveness for cyclists \n",
      "and launched the bike offensive – a \n",
      "whole bundle of measures to improve \n",
      "cycling www.heidelberg.de/fahrrad -\n",
      "freundlich .\n",
      "Good connections to highways and \n",
      "air traffic\n",
      "Even distant destinations can be \n",
      "reached quickly and easily: By car, \n",
      "you can be on the Autobahn 5 in \n",
      "just a few minutes. Within just 50 \n",
      "minutes, you can reach Frankfurt \n",
      "Airport and Stuttgart in 90 minutes. \n",
      "With the long-distance train service, \n",
      "the journey is even faster. In addition, \n",
      "long-distance buses serve numerous \n",
      "destinations throughout Germany.World-class research  | 37\n",
      "World-class research\n",
      "Heidelberg is one of the most important centers of \n",
      "science worldwide. Heidelberg is one of the world‘s top \n",
      "locations for up-and-coming industries \n",
      "such as biotechnology and organic elec -\n",
      "tronics. Excellent cutting-edge research \n",
      "clusters and renowned research insti -\n",
      "tutes such as the European Laboratory \n",
      "for Molecular Biology (EMBL),  the Ger -\n",
      "man Cancer Research Center (DKFZ) \n",
      "and three Max Planck Institutes \n",
      "contribute to Heidelberg‘s outstanding \n",
      "worldwide reputation as science hub.\n",
      "As Heidelberg Mannheim Health & \n",
      "Life Science Alliance , the two medical \n",
      "faculties and university hospitals as well \n",
      "as the renowned research institutions \n",
      "in the region form a unique alliance \n",
      "in Germany. The goal: to make the \n",
      "Rhine-Neckar region an international \n",
      "center of excellence in life sciences, \n",
      "medical technology and health care \n",
      "industry. This should also raise the \n",
      "already first-class patient care onto a \n",
      "new level.\n",
      "Since 1901, 57 personalities whose lives \n",
      "are connected to Heidelberg, have re -\n",
      "ceived the Nobel Prize. The most recent \n",
      "one, the Nobel Prize of Chemistry, was \n",
      "awarded to Professor Dr. Stefan W. Hell \n",
      "for his development of super-resolution \n",
      "microscopy. \n",
      "34 percent of all those who work and \n",
      "pay statutory social security contribu -\n",
      "tions in the city hold a degree.Both Heidelberg and the Rhine-Neckar \n",
      "metropolitan region are characterized \n",
      "by close cooperation between science \n",
      "and business. This is reflected in a large \n",
      "number of interdisciplinary research \n",
      "centers such as the Heidelberg Tech -\n",
      "nology Park, the “Industry on Campus“ \n",
      "projects between Heidelberg University \n",
      "and different companies or in the office \n",
      "and laboratory buildings SkyLabs and \n",
      "SkyAngle in the new Bahnstadt district. \n",
      "The exceptional role of science in Hei -\n",
      "delberg also benefits other aspects \n",
      "of Heidelberg life: enabling daycare \n",
      "centers and schools to offer special ac -\n",
      "tivities, creating exciting local jobs and \n",
      "profiting those working in the creative \n",
      "industries. The city wants to leverage \n",
      "this effect further: The International \n",
      "Building Exhibition Heidelberg (IBA) \n",
      "is dedicated to building and exploring \n",
      "the knowledge city of the future with \n",
      "its motto “Knowledge | creates | City“.\n",
      "With the new  “Heidelberg Congress \n",
      "Center”  in the Bahnstadt district, con -\n",
      "gresses in science and business will \n",
      "find a new home. This will enable the \n",
      "“Stadthalle” in the old town to concen -\n",
      "trate on its strengths as a concert and \n",
      "cultural center again. The “Stadthalle”  \n",
      "is currently undergoing extensive ren -\n",
      "ovation.Founded in 1386, the University of \n",
      "Heidelberg  is the oldest university in \n",
      "Germany. Its leading role in the academic \n",
      "scene in Germany and abroad is reflect -\n",
      "ed in its classification and promotion \n",
      "by central and state government as an \n",
      "“university of excellence”. According to the \n",
      "Shanghai ranking, Ruperto Carola is reg -\n",
      "ularly the best-ranked German university and counts among the world’s Top 50. \n",
      "38,000 students take advantage of the \n",
      "courses offered by the various universi -\n",
      "ties, cooperative education institutes and \n",
      "other higher-education establishments in \n",
      "Heidelberg – including the Heidelberg \n",
      "University of Education (PH Heidel -\n",
      "berg), the private SRH University and \n",
      "the University of Jewish Studies .36 | World-class research\n",
      "The European Molecular Biology Laboratory (EMBL), one of the world‘s best-known biological research \n",
      "laboratories, has its headquarters in Heidelberg.A safe city\n",
      "Fighting fires, saving lives, ensuring traffic safety, \n",
      "settling disputes – there are many aspects to making \n",
      "sure that Heidelberg is a safe place to be around.Youth fire brigade and volunteer \n",
      "fire department\n",
      "In addition to the professional fire \n",
      "department with its more than 116 \n",
      "full-time firefighters, more than 400 \n",
      "citizens of the city of Heidelberg are \n",
      "involved on a voluntary basis in eight \n",
      "departments of the volunteer fire \n",
      "department. The children‘s and youth \n",
      "fire brigade has around 220 members, \n",
      "around a third of whom are girls. In \n",
      "addition to firefighting lessons, they \n",
      "meet for joint activities.\n",
      "Working together\n",
      "For a safe Heidelberg and for an op -\n",
      "timal response when disaster strikes, \n",
      "the city administration, police and res -\n",
      "cue services work closely together. \n",
      "The “municipal public order service” \n",
      "(Kommunaler Ordnungsdienst), for \n",
      "example, makes sure that bars and \n",
      "shops in the old town close on time and \n",
      "responds to disturbances of the peace. \n",
      "The “municipal enforcement service” \n",
      "(Gemeindevollzugsdienst) monitors \n",
      "parking and adherence to the rules of the road. To protect against flooding, \n",
      "the city’s civil engineering office works \n",
      "not only with the fire department and \n",
      "the municipal public order service but \n",
      "also with the relief organiziation DLRG \n",
      "as well as the civil protection organ -\n",
      "ization Technisches Hilfswerk and, if \n",
      "necessary, the Red Cross, the Malteser \n",
      "emergency service and the Army.\n",
      "Coping with disasters\n",
      "Tips are available in the “Disasters \n",
      "Alarm”  guidebook, which is available \n",
      "free of charge and can be download -\n",
      "ed from www.bbk.bund.de . Official \n",
      "warnings can be received directly on \n",
      "your smartphone via the warning app \n",
      "NINA. The app also provides warnings \n",
      "of severe weather or flood situations as \n",
      "well as advice on the correct reaction \n",
      "to such weather events. It is available \n",
      "free of charge for Android and iOS.\n",
      "Always think about the people around \n",
      "you: Inform them in case of an emer -\n",
      "gency situation and support them if \n",
      "they need help.The Heidelberg Fire Department\n",
      "Around the clock, the men and women \n",
      "of the Heidelberg Fire Department are \n",
      "on duty 24 hours a day to provide quick \n",
      "and effective help in emergencies. They \n",
      "are called out 2,500 times a year to fight \n",
      "fires, rescue citizens from road accidents \n",
      "and, increasingly, help people affected \n",
      "by extreme weather situations. On aver -\n",
      "age, they save between 20 to 25 people \n",
      "a year out of life-threatening situations.\n",
      "Doing this job properly requires inten -\n",
      "sive training, including annual training \n",
      "sessions in the fire station’s special gas-mask training facility, as well as state-\n",
      "of-the-art equipment. Three turntable \n",
      "ladders help rescue people from higher \n",
      "floors.\n",
      "In the districts of Pfaffengrund, Wieblin -\n",
      "gen and Ziegelhausen, new fire stations \n",
      "have been built. In October 2020, the \n",
      "new integrated control room started \n",
      "operations, coordination all calls to the \n",
      "emergency number 112 in Heidelberg \n",
      "and the Rhein-Neckar-region.\n",
      "It is the operations center for non-police \n",
      "emergency response for Heidelberg and \n",
      "the Rhine-Neckar district.38 | A safe city\n",
      "More than 600 members of the DLRG Heidelberg are committed to \n",
      "improving safety on and along the Neckar River.\n",
      "A safe city  | 39Homes for everyone  | 41\n",
      "Fighting discrimination\n",
      "It is important to the City of Heidelberg \n",
      "that all of its residents live together \n",
      "harmoniously.  The Office for Equal \n",
      "Opportunity  is the point of contact \n",
      "for anyone who wants to take action \n",
      "against discrimination or who has ex -\n",
      "perienced it themselves – whether on \n",
      "basis of their place of origin, a disa -\n",
      "bility, age, sexual identity or gender. \n",
      "Further information as well as advice \n",
      "and support are available by calling \n",
      "06221 58-15550, by e-mailing antidiskri -\n",
      "minierung@heidelberg.de and looking \n",
      "online at www.heidelberg.de/antidis -\n",
      "crimination .Help in cases of domestic violence\n",
      "The Heidelberg Intervention Model \n",
      "against violence in relationships \n",
      "(HIM) supports those affected by \n",
      "domestic violence to escape the cycle of \n",
      "violence. Women and children can turn \n",
      "to the Women‘s Intervention Center \n",
      "for help and counseling. The Men‘s \n",
      "Intervention Center and the men‘s \n",
      "hotline offer counseling and therapy \n",
      "for perpetrators of violence as well as \n",
      "support and counseling for men who \n",
      "have experienced violence themselves. \n",
      "Further information is available at www.\n",
      "heidelberg.de/him .40 | A safe city\n",
      "Homes for everyone\n",
      "Since 2012, just under 3,900 new apartments have \n",
      "been built in Heidelberg. This has been made pos -\n",
      "sible by the construction of new neighborhoods \n",
      "such as Bahnstadt as well as the reuse of former US  \n",
      "military sites.\n",
      "Dynamic growth\n",
      "Heidelberg is a popular place to live \n",
      "and work. However, this popularity \n",
      "puts a strain on the housing market. To \n",
      "create sufficient living space for people \n",
      "of all income groups, the city council \n",
      "unanimously adopted the Housing \n",
      "action program. An annual 800 new apartments are to be built in the com -\n",
      "ing years. In addition, the city runs \n",
      "two subsidy schemes for tenants and \n",
      "buyers and builds special family-friend -\n",
      "ly apartments in the new district of \n",
      "Bahnstadt. Moreover, the development \n",
      "of affordable housing on the former US \n",
      "military sites is a top priority.Bahnstadt is particularly popular with families and is Heidelberg‘s district with the most children.housing. The first residents moved here \n",
      "in the summer of 2016. The plans for \n",
      "former US hospital also consist mainly \n",
      "of residential buildings. On the site of \n",
      "Patton Barracks, the Heidelberg In -\n",
      "novation Park is being developed and \n",
      "Patrick-Henry-Village is to become a \n",
      "model city district of the future for up \n",
      "to 15,000 people. For further informa -\n",
      "tion, visit www.heidelberg.de/english/\n",
      "home/develop/conversion.\n",
      "IBA Heidelberg:  \n",
      "“Knowledge | creates | city”\n",
      "The Heidelberg International Building \n",
      "Exhibition (IBA) “Knowledge | Creates |  \n",
      "City” illuminates urban planning in the \n",
      "knowledge city of the future over a \n",
      "period of ten years until 2022.The IBA covers locations throughout \n",
      "the whole city and many areas of life. \n",
      "One of the projects, which involves a \n",
      "substantial public consultation element, \n",
      "is the development of a concept for \n",
      "the further use of Heidelberg‘s largest \n",
      "conversion area Patrick Henry Village. \n",
      "For further information, visit www.iba.\n",
      "heidelberg.de/english .Bahnstadt: A lively mix of housing, \n",
      "science and business\n",
      "The new district of Bahnstadt has model \n",
      "character: it is the largest passive house \n",
      "settlement in the world. With a vibrant \n",
      "mix of housing, science and commerce, \n",
      "the district creates new spaces to work \n",
      "and live in. Ultimately, Bahnstadt will be \n",
      "home for around 6,800 people and pro -\n",
      "vide jobs for 6,000, mainly in research \n",
      "and science-based companies.\n",
      "With a total area of 116 hectares, the \n",
      "Bahnstadt is larger than the entire old \n",
      "town of Heidelberg and one of the \n",
      "largest urban development projects \n",
      "in Germany.\n",
      "In addition to green and open spaces, \n",
      "streets and watercourses as well as \n",
      "kindergartens, an elementary school, a \n",
      "community center, a cinema and a local \n",
      "shopping center have been created. \n",
      "The new Europaplatz  will connect the \n",
      "district with the central station. The new conference center is also being \n",
      "built here. For further information, visit \n",
      "www.heidelberg-bahnstadt.de/en .\n",
      "The former US Army sites: A once-\n",
      "in-a-lifetime opportunity\n",
      "The withdrawal of the U.S. Army repre -\n",
      "sents a unique opportunity for Heidel -\n",
      "berg. 180 hectares are now free for new \n",
      "developments: for affordable housing, \n",
      "cultural and recreational offers, inno -\n",
      "vative scientific institutions, attractive \n",
      "business premises and much more. \n",
      "There are five former US sites to be \n",
      "redeveloped: Mark-Twain Village  /\n",
      "Campbell Barracks (43.3 hectares) in \n",
      "Südstadt, US Hospital (9.3 hectares) in \n",
      "Rohrbach as well as Patton Barracks \n",
      "(14.8 hectares), Patrick-Henry-Village \n",
      "(97.2 hectares) and the Airfield (15.6 \n",
      "hectares) in Kirchheim.\n",
      "The site in the southern part of the \n",
      "city is primarily used for affordable 42 | Homes for everyone Homes for everyone  | 43\n",
      "Future in a nutshell: the Heidelberg Innovation Parc (hip) on the premises of the former Patton Barracks. Patrick Henry Village (PHV) is to be further developed into a sustainable 16th district and become a \n",
      "model location for the use of digital technologies, innovative mobility concepts and climate-neutral \n",
      "energy supply. Embracing diversity  | 45 44 | Embracing diversity\n",
      "As a Rainbow City , Heidelberg is taking decisive steps to strengthen \n",
      "LGBTIQ+. Since 2016, the members of the “Sexual and Gender \n",
      "Diversity Round Table” have been advising the city administration. \n",
      "Around 15 local LGBTIQ+ initiatives contribute their expertise here \n",
      "in order to work out fundamental and current problems of LGBTIQ+ \n",
      "and to counteract exclusion and disadvantage through appropriate \n",
      "recommendations. With projects such as the Trans* Action Weeks \n",
      "Rhine-Neckar,  the cities of Heidelberg and Mannheim offer a plat -\n",
      "form to raise awareness and educate about trans* persons and the \n",
      "diversity of gender identities and their forms of expression. \n",
      "With the Intercultural Center , Heidelberg is creating a place for \n",
      "cultural participation that promotes cohesion in the city‘s society \n",
      "and the integration of people with a history of migration through \n",
      "various participation formats and an intercultural program. With the \n",
      "“Interreligious Dialogue” , the city facilitates a successful exchange \n",
      "of religions in Heidelberg. In order to ensure that the interests of \n",
      "foreign residents are also heard, the Migration Advisory Council  \n",
      "has had an advisory function in the city council for more than 20 \n",
      "years. Further information can be found at www.heidelberg.de/\n",
      "integration .\n",
      " \n",
      "The City of Heidelberg is intensively committed to the equal partici -\n",
      "pation and self-determination of people with disabilities. This includes \n",
      "barrier-free communication as well as comprehensive counseling and \n",
      "care services, the “Wohnberatung” (housing counseling)  as a \n",
      "specialist office for barrier-free planning, building, and living, or the \n",
      "improvement of mobility, for example through the wheelchair cab. \n",
      "The municipal representative for the disabled is the primary contact \n",
      "person for concerns of people with disabilities and their relatives \n",
      "and also advises the administration on matters of inclusion. Contact: \n",
      "behindertenbeauftragte@heidelberg.de, 06221 58-15590. Further \n",
      "information at www.heidelberg.de/inklusion . Embracing diversity\n",
      "Heidelberg is diverse: people from 180 nations, \n",
      "with different gender and sexual identities, old and \n",
      "young, people with and without disabilities live here. \n",
      "With numerous offers, measures and projects, the \n",
      "city is working with many partners to ensure that all \n",
      "people here have the same opportunities and can \n",
      "feel at home in Heidelberg.  Getting the public involved  | 47 46 | Volunteers – a vital asset for Heidelberg\n",
      "Heidelberg values its volunteers: with \n",
      "the Civic Engagement Coordination \n",
      "Office  (Koordinierungsstelle Bürgeren -\n",
      "gagement) as part of the mayor’s of -\n",
      "fice, there is a central contact point for \n",
      "voluntary work in the city hall. Once a \n",
      "year, it awards the honorary plaque \n",
      "(Ehrenamtsmedaille)  for outstanding \n",
      "civic commitment.The volunteer agency  (Freiwilligen-  \n",
      "Agentur) of the “Der PARITÄTISCHE” \n",
      "charity is an important municipal part -\n",
      "ner and an important contact for citi -\n",
      "zens who would like to get involved. \n",
      "An online database of volunteering \n",
      "opportunities can be found at www.\n",
      "freiwilligenagentur-heidelberg.de .The City of Heidelberg wants citizens to be \n",
      "involved from as early a stage as possible in \n",
      "decision-making on municipal projects, for \n",
      "example the redevelopment of the former \n",
      "US Army sites and the master plan for the \n",
      "Neuenheimer Feld area. To ensure the smooth \n",
      "running of this process, the city has drawn \n",
      "up “guidelines for public participation”  \n",
      "(Leitlinien für mitgestaltende Bürgerbeteiligung) \n",
      "in consultation with the public. \n",
      "The people of Heidelberg also have access \n",
      "to a “planning list”  (Vorhabenliste) that is \n",
      "updated regularly with details of current and \n",
      "forthcoming municipal projects. As well as \n",
      "indicating the stage a project has reached, the \n",
      "list also includes information on project costs \n",
      "and names relevant contacts. In this way, it \n",
      "facilitates dialogue between the city and its \n",
      "residents form an early stage of a project. \n",
      "You can direct any questions you have about \n",
      "public participation in Heidelberg to the mu -\n",
      "nicipal “coordination point” (Koordinierungss -\n",
      "telle). For more information, visit www.heidel -\n",
      "berg.de/involvement . Volunteers – a vital \n",
      "asset for Heidelberg\n",
      "For more than 15 years, the city administration has \n",
      "been supporting voluntary work.  Getting the \n",
      "public involved\n",
      "Getting the public even more \n",
      "involved in municipal decision-  \n",
      "making is very important to \n",
      "the City of Heidelberg. \n",
      "The Civic Engagement \n",
      "Coordination Office is \n",
      "happy to answer any \n",
      "questions about vol -\n",
      "untary work in Heidel -\n",
      "berg. Please visit www.\n",
      "heidelberg.de/buerger-  \n",
      "engagement  for further \n",
      "information.48 | Partners around the world\n",
      "CAMBRIDGE MONTPELLIER  \n",
      "BAUTZEN\n",
      "REHOVOTKUMAMOTOHANGZHOU\n",
      "SIMFEROPOLPALO ALTOPartners around the world\n",
      "Heidelberg entertains partnerships with eight cities \n",
      "on three continents. \n",
      "Contribution towards international \n",
      "understanding\n",
      "As an international city with almost \n",
      "12 million guests a year and about \n",
      "45,000 citizens with a migrant back -\n",
      "ground, Heidelberg has eight partner -\n",
      "ships: with Palo Alto  in the US, with \n",
      "Hangzhou  in China, with Montpellier  \n",
      "in France, with Cambridge  in Great \n",
      "Britain, Rehovot  in Israel, Bautzen in \n",
      "Germany, Simferopol  on the Crime -\n",
      "an peninsula and with Kumamoto  \n",
      "in Japan.Palo Alto and Hangzhou are Heidel -\n",
      "berg‘s youngest partner cities. Palo \n",
      "Alto in the US state of California is \n",
      "considered the “capital” of Silicon \n",
      "Valley. Hangzhou is the location of \n",
      "major scientific and educational in -\n",
      "stitutions and is particularly strong \n",
      "in the high-tech fields of biomedicine, \n",
      "informatics and digital new media.\n",
      "In addition, there are friendly relations \n",
      "with the cities of Mostar (Bosnia-Her -\n",
      "zegovina), Jelenia Góra (Poland) and \n",
      "Heidelberg (South Africa/Gauteng \n",
      "Province).\n",
      "Granting free entry* \n",
      "to the Kurpfälzisches Museum \n",
      "*valid for 2 adults and 2 children\n",
      "Can be exchanged for a library card  \n",
      "for the Public Library (Stadtbücherei)  \n",
      "valid for 3 months\n",
      "Voucher\n",
      "VoucherDiscover the past\n",
      "The unique collections at Kurpfälzisches Museum,  \n",
      "from art, handicrafts and archaeological finds   \n",
      "to fascinating insights into Heidelberg’s history, take  \n",
      "visitors on a thrilling journey of discovery through  \n",
      "the millennia. We look forward to your visit!\n",
      "Reading, listening, watching\n",
      "Free library card valid for 3 months \n",
      "At our library you will find: 230,000 books, more than 300 international magazines \n",
      "and newspapers, CDs and DVDs, superb premises, help and advice, 46 opening \n",
      "hours per week, a 24-hour digital library service, a mobile library, events, digital \n",
      "workstations, a library café, user information and parking area. \n",
      "Heidelberg Public Library\n",
      "Poststraße 15\n",
      "69115 Heidelberg\n",
      "Phone 06221  58-36100  \n",
      "stadtbuecherei@heidelberg.de\n",
      "www.stadtbuecherei.heidelberg.deOpening times\n",
      "Tuesday to Friday 10.00 am – 8.00 pm \n",
      "Saturdays 10.00 am – 4.00 pm  \n",
      " \n",
      "Nearest public transport\n",
      "Stadtbücherei, RömerstraßeKurpfälzisches Museum Heidelberg\n",
      "Hauptstraße 97\n",
      "69117 Heidelberg\n",
      "Phone 06221 58-34020  \n",
      "kurpfaelzischesmuseum@heidelberg.de\n",
      "www.museum.heidelberg.deOpening times\n",
      "Tuesday to Sunday 10.00 am – 6.00 pm \n",
      "Mondays: Closed  \n",
      " \n",
      "Nearest public transport\n",
      "Universitätsplatz, PeterskircheImprint\n",
      "City of Heidelberg\n",
      "Marktplatz 10\n",
      "69117 Heidelberg\n",
      "Text  \n",
      "Public Relations Office\n",
      "Design  \n",
      "City of Heidelberg, \n",
      "Brand Communication\n",
      "Photos  \n",
      "Tobias Dittmer: Cover and  \n",
      "Pages 8, 28, 30, 34, 46\n",
      "Julian Beekmann: Pages 3, 4, 6\n",
      "Steffen Diemer: Pages 16, 18, 22, 41, \n",
      "50 (o.)\n",
      "Tobias Schwerdt: Page 19 (u.)\n",
      "DNA Collective: Page 21\n",
      "Sven Ehlers / Stadtwerke HD: Page 23\n",
      "Envato.com: Page 24\n",
      "Philipp Rothe: Page 26\n",
      "Christian Buck: Pages 32, 33, 42, 49 (u.)\n",
      "Kinga Lubowiecka / EMBL: Page 36\n",
      "Lukas Rübenacker: Page 38\n",
      "Lukas Schütz / DLRG HD: Page 39\n",
      "KCAP: Page 43\n",
      "Uwe Anspach: Page 44\n",
      "Gattner / KMH: Page 49 (o.) \n",
      "Copyright © 2020 City of Heidelberg.  \n",
      "All right reserved. Reprints (including \n",
      "excerpts) only with the explicit per -\n",
      "mission of the City of Heidelberg.\n",
      "Edition\n",
      "1st Edition, June 2022Links\n",
      "Heidelberg City Administration / \n",
      "City of Heidelberg website\n",
      "www.heidelberg.de\n",
      "Online ‘citizens’ service’\n",
      "www.heidelberg.de/buergerservice  \n",
      "Calendar of events\n",
      "www.heidelberg.de/veranstaltungen\n",
      "Heidelberg-App\n",
      "www.meinheidelberg.de\n",
      "City of Heidelberg on social media\n",
      "www.facebook.de/heidelberg.de\n",
      "www.twitter.com/heidelberg_de\n",
      "www.instagram.com/heidelberg_de\n",
      "www.youtube.com/StadtHeidelbergCity of Heidelberg\n",
      "Marktplatz 10\n",
      "69117 Heidelberg\n",
      "Phone  06221  58-10580  \n",
      "Fax 06221  58-10900  \n",
      "stadt@heidelberg.de\n",
      "www.heidelberg.de\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "City of Heidelberg Municipal Integration Plan  \n",
      "Summary  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " City of Heidelberg Municipal Integration Plan - Summary  Page 2 \n",
      " Foreword by Dr. Eckart Würzner , Oberbürgermeister (Lord Mayor)  \n",
      " \n",
      "Heidelberg is the first local authority in Germany to carry out a representative surve y on the current \n",
      "situation of migrants in the city. We wanted to find out more about the circumstances, points of \n",
      "view and attitudes of a group of people which makes up more than a quarter of our resident urban \n",
      "population.  \n",
      "We’re proud of the findings from  this survey as most of those questioned do not sense that there \n",
      "are any serious problems with integration. They also afford us a complex insight into the current \n",
      "situation of migrants in our city. We have conducted a targeted evaluation of the results and  are \n",
      "concentrating our resources on those areas where there is a particular need for action.  \n",
      "We want to be more effective in capturing migrants’ many and varied talents and potential for the \n",
      "local community and providing them with the prerequisites for li ving as members of our multi -ethnic \n",
      "German society who are involved in it and make an active contribution to it – without forgetting \n",
      "their cultural roots.  \n",
      " \n",
      " \n",
      "Foreword by Wolfgang Erichson, Bürgermeister für Integration, Chancengleichheit und \n",
      "Bürgerdienste, (Deputy Mayor for Integration, Equal Opportunities and Citizens’ Services ): \n",
      "“The world in one city”  \n",
      " \n",
      "The philosopher, Karl Jaspers, who worked in Heidelberg coined the phrase “Heidelberg - a \n",
      "spiritual life form” and chose the image of “The world in one cit y” for Heidelberg. He wanted to \n",
      "make it clear to us that the image of a city is characterised not just by its architecture and \n",
      "monuments but in particular by its people. For us, this means investigating and scrutinising the \n",
      "measures currently carried out i n the area of integration.  \n",
      " \n",
      "With the Heidelberg Municipal Integration Plan, we want to spell out that Heidelbergers with and \n",
      "without a history of migration have the same interests in common. There is no clash of interests \n",
      "between indigenous people and immi grants. In fact, they have a common interest in the successful \n",
      "inclusion of people with a history of immigration and in their full involvement in the opportunities \n",
      "available in education, employment and for their own advancement. This requires everyone - \n",
      "indigenous people as well as immigrants - to get involved in everyday life in the city’s districts and \n",
      "clubs, at school and in the workplace.  \n",
      " \n",
      "This doesn’t mean that we’re leaving it as a vague statement of intent - we have agreed targets \n",
      "and measures and set priorities in a number of areas. The key thing is that we concentrate our \n",
      "efforts particularly on the section of immigrants who suffer from significant integration needs. Our \n",
      "aim is to ensure that our endeavours are targeted at meeting these needs. To this end, we must \n",
      "actively confront obstacles to integration such as insufficient German language skills, \n",
      "unemployment and dropping out of educational and professional training.  \n",
      " \n",
      "We will monitor developments on a regular and structured basis and carry out  progress checks. I \n",
      "would particularly like to thank all those people who have provided critical support and been \n",
      "actively involved in the process of drafting this plan and have thus contributed to its success.  \n",
      " \n",
      " \n",
      "Foreword  Michael Mwa AlliMadi, Vorsitzende r des Ausländerrats/Migrationsrats ( Chair of the \n",
      "Foreig ners’ Council/Migrants’ Council ) \n",
      " \n",
      "“If you open your door, you’ll have a big house” - so goes an African saying. In a figurative sense, \n",
      "Heidelberg is a big house in many ways. The city can put one reaso n for its “bigness” down to the \n",
      "way in which for hundreds of years it has opened its door to strangers – not just to tourists but also \n",
      "to those who make a lasting contribution. Its current reputation, its academic excellence and its \n",
      "wealth would be inconce ivable without migration.  \n",
      " \n",
      "The Municipal Integration Plan now underway is an ambitious attempt to realise a consistent City of Heidelberg Municipal Integration Plan - Summary  Page 3 \n",
      " integration policy by means of a coherent steering instrument. Measurable targets and \n",
      "transparency will ensure that there is no regressi on through complacency.  \n",
      " \n",
      "The Foreigners’ Council/Migrants’ Council is a central interface for reaching the people – and is an \n",
      "interface for the City’s beneficent measures as well. However, more structures are needed beyond \n",
      "that, such as strengthening migr ant community organisations and creating a central intercultural \n",
      "centre as a meeting place and advice centre.  \n",
      " \n",
      "The Foreigners’ Council/Migrants’ Council welcomes the Municipal Integration Plan and will provide \n",
      "critical and constructive support in its imple mentation in the interests of the people with a migration \n",
      "background who live here.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " City of Heidelberg Municipal Integration Plan - Summary  Page 4 \n",
      " 1. The City of Heidelberg’s potential and strategy  \n",
      " \n",
      "There are two ways of looking at Heidelberg as a cosmopolitan and multicultural city of science \n",
      "where two -thirds of its migrants have university entrance qualifications: there is the approach \n",
      "based on potential and the perspective which looks at shortcomings.  The City of Heidelberg is \n",
      "pursuing the strategy of continuing to translate proven approaches into sustai nable structures.  \n",
      "In this task, its actions are guided by the following statements:  \n",
      " \n",
      "We promote the participation of all citizens.  \n",
      "We act to prevent exclusion.  \n",
      "We address disadvantaged target groups.  \n",
      "We choose interculturally sensitive pathways.  \n",
      "We alr eady live out our commitment to tolerance.  \n",
      "Heidelberg is a home for all.  \n",
      "We strengthen involvement in communal life.  \n",
      "We monitor changes and measure our successes.  \n",
      "We support people who seek asylum here.  \n",
      " \n",
      " \n",
      "2. The process so far  \n",
      " \n",
      "Integration is embedded a s a guiding principle in the 2015 Heidelberg City Development Plan \n",
      "(known as STEP 2015) which since 1997 has provided the City of Heidelberg with guidelines and \n",
      "targets for responsible local authority policy. The essential process steps taken by the City w hen \n",
      "drafting the current Municipal Integration Plan were:  \n",
      " \n",
      "1. Stock -take of the ongoing projects and activities which are available in Heidelberg specifically \n",
      "for people with a migration background.  \n",
      "2. Development of recommendations for action in selected area s by committed groups.  \n",
      "3. Survey on the current situation of people with a migration background in Heidelberg.  \n",
      "  \n",
      " \n",
      "3. The current situation of migrants in Heidelberg  \n",
      " \n",
      "In order to find out more about the people who come from different cultural groups, the City of \n",
      "Heidelberg arranged to have a representative survey carried out by the Sinus Sociovision Institute. \n",
      "The population covered by this investigation were all those people who had a migration \n",
      "background and were resident in Heidelberg from the age of 18. The  main results of the survey \n",
      "were:  \n",
      " The proportion of migrants from highly industrialised countries is disproportionately high in \n",
      "Heidelberg compared to the national average  \n",
      " Heidelberg’s migrants are significantly younger with 61 per cent between 30 and 60 years of \n",
      "age and there are more people in employment and fewer retired people among them than in the \n",
      "control group of Heidelberg’s overall population  \n",
      " Compared to the urban population overall, there is a higher proportion of lower net household \n",
      "incomes amon g migrants but also a higher proportion of higher income earners - hence, the \n",
      "range of incomes is wider  \n",
      " Two-thirds of Heidelberg’s migrants have qualified for university entrance or passed exams at a \n",
      "similar level; a willingness to work hard and a desire f or social advancement feature prominently \n",
      "in the migrant community  \n",
      " City of Heidelberg Municipal Integration Plan - Summary  Page 5 \n",
      " It is clear that the precarious background rooted in tradition to which nationally 47 per cent of \n",
      "migrants belong is considerably smaller in Heidelberg at 17 per cent. On the other hand, \n",
      "economically and socially elevated backgrounds are much more prevalent than for the national \n",
      "average. In particular, an intellectual and cosmopolitan background is absolutely dominant among \n",
      "Heidelberg’s migrants at 48 per cent.  \n",
      "The results of the survey are  available online at the City of Heidelberg website: \n",
      "http://www.heidelberg.de/servlet/PB/menu/1196309/index.html  \n",
      " \n",
      " \n",
      "4. The aim of the City of Heidelberg: successful integration  \n",
      " \n",
      "Integration requires both a willingness to accept by the majority society and a willingness to \n",
      "integrate on the part of the migrants. It is a two -sided process. Only when we regard the values of \n",
      "the constitution as a common foundation will we be able to cre ate equality of opportunity for all \n",
      "members of society.  \n",
      " \n",
      "As shown in the Sinus survey, because the composition of the various migrant groups in \n",
      "Heidelberg is becoming increasingly complex, work based around target groups will be a key factor \n",
      "in successfull y carrying out effective integration work. The Municipal Integration Plan provides a \n",
      "new opportunity to honour the obligations which Heidelberg undertook back in 2007 with the \n",
      "signing of the EU Charter for Equality of Women and Men in Local Life.  \n",
      " \n",
      " \n",
      "5. Area s for action by the City of Heidelberg in respect of integration  \n",
      " \n",
      "In line with the nature of integration policy as an across -the-board task, the integration measures \n",
      "are widely distributed across the various departments of the City of Heidelberg. Six areas  for action \n",
      "which relate to integration were selected.  \n",
      " \n",
      " \n",
      "5.1 Education and language support  \n",
      " \n",
      "General data/facts and figures:   \n",
      "The level of education in the City of Heidelberg is high. However, the school -leaving qualifications \n",
      "of pupils with a migration ba ckground do not correspond to the City’s impressive statistics: only 21 \n",
      "per cent qualified for university entrance  (Abitur ), while 18 per cent gained the secondary school -\n",
      "leaving qualification leading to HE/FE (Realschulabschluss ) and 44 per cent gained th e basic \n",
      "school -leaving qualification  (Hauptschulabschluss ). 17 per cent left school without any school -\n",
      "leaving qualifications. The figure for German pupils was just 3.3 per cent.  \n",
      " \n",
      "The City of Heidelberg’s strengths:  \n",
      " Early years education, language support  and musical education  \n",
      " “Kommunale Bildungslandschaft ” (\"Municipal Education Landscape\") (greater networking \n",
      "between local authorities and schools)  \n",
      " “Heidelberger Unterstützungssystem Schule ” (“Heidelberg School Support System”) \n",
      "(extracurricular service for under -performing children)  \n",
      " “Schulkinder helfen Schulkindern ” (“Schoolchildren Help Schoolchildren”) (migrants who have \n",
      "succeeded in education act as learning buddies to pupils)  \n",
      " General education services (including the “ Zweite Heimat ” (\"Second Home \")) programme run \n",
      "by the Heidelberg Adult Education Centre, education vouchers and the wide range of foreign \n",
      "language literature available at the Public  Library)  \n",
      " \n",
      "Aims:    \n",
      " Organise “integration through education” as a two -sided process by the majority society and City of Heidelberg Municipal Integration Plan - Summary  Page 6 \n",
      " migrants whereby both sides are included in the education and support services. \n",
      "(Implementation planned for 2011)  \n",
      " Extend the successful “ Kinder lernen Deutsch ” language support programme for the 2010/2011 \n",
      "academic year to year groups 3 and 4.  \n",
      " Gain the supp ort of Heidelberg’s schools in improving educational opportunities and \n",
      "establishing fairness in education. (Implemented since Autumn 2010)  \n",
      " Provide greater availability of cultural education activities, such as those of the Music and \n",
      "Singing School, to fami lies with a history of migration and in particular those on low incomes. \n",
      "(Implemented in 2011)  \n",
      " Provide support for adult migrants to enable them to get involved in general education and thus \n",
      "impart the idea of life -long learning (i.e. supplementary German language courses, foreign \n",
      "language guided tours in museums and the Public  Library - implemented in 2011).  \n",
      " Ensure recognition of the family language of children with a migration background as a resource \n",
      "and as providing potential and support for multilingua lism among these children and young \n",
      "people in order to maintain and extend language diversity. (Implemented since 2010)  \n",
      " Buddy  and mentoring projects  \n",
      " \n",
      " \n",
      "5.2 Training and the job market  \n",
      " \n",
      "General data/facts and figures:  \n",
      "Also i n Heidelberg, foreigners are considera bly disadvantaged compared to Germans in respect of \n",
      "access to employment and are more likely to be affected by long -term unemployment. The number \n",
      "of unemployed foreigners has increased since the end of 2008 and currently stands at 10.5 per \n",
      "cent (as of 30.0 4.2010). Furthermore, only 7.8 per cent of apprentices in Heidelberg do not have \n",
      "German nationality.  \n",
      " \n",
      "The City of Heidelberg’s strengths:   \n",
      " Personalised support plans through skills analysis at compulsory  secondary schools and special \n",
      "schools.  \n",
      " Heidelberg’s  \"Jugendberufshelfer\"  (\"youth careers advisers\") supervise pupils at compulsory \n",
      "secondary schools and pupils who receive support from Grade 8  and upwards.  \n",
      " Support is provided for foreign youngsters in par ticular through the “Azubi -Fonds ” project.  \n",
      " Specialis t language training at the University of Education  for young people with a migration \n",
      "background from the Grade 7 and upwards .  \n",
      " The “ Treff miteinander ” (\"Come Together\") project provides services which support personal \n",
      "development and educational and profes sional integration.  \n",
      " The pilot programme “Talent e für die  Metropol region – Kooperatives Übergangsmanagement \n",
      "Schule und Beruf (KÜM )\" (\"Talent for the Metropolitan Region – School -to-Job Transfer \n",
      "Management Cooperation”) aims to increase the number of pupils transferring from school to \n",
      "training.  \n",
      " Companies whose owners have a migration background are recruited as training providers \n",
      "through the “ Ausbildungsverbund Heidelberg\"  (“Heidelberg Training Alliance\") . \n",
      " \n",
      "Aims:  \n",
      " Increase the training capacity of (foreign) co mpanies and vocational training opportunities for \n",
      "disadvantaged young people (with a migration background). (Implemented 01.11.2009 – \n",
      "31.10.2010)  \n",
      " Provide individual support for under -performing young people from compulsory secondary \n",
      "schools, special school s and vocational training colleges when entering work. (Implemented \n",
      "since 2010)  \n",
      " Make use of “local heroes”: demonstrate opportunities for the professional and social integration \n",
      "of migrants. (To be implemented in the long term)  \n",
      " Open up prospects for establ ishing new businesses for Heidelbergers with a history of migration City of Heidelberg Municipal Integration Plan - Summary  Page 7 \n",
      " who are trained in Germany. (Implemented from 2011 onwards)  \n",
      " Ease shortage of skilled workers and integrate skilled people with a history of migration and \n",
      "foreign school -leaving qualificati ons into the job market. (Implemented from 2011 onwards)  \n",
      " \n",
      " \n",
      "5.3 Health and welfare systems  \n",
      " \n",
      "The City of Heidelberg’s strengths:  \n",
      " Heidelberg joined Germany’s “Healthy Cities” network in 1991.  \n",
      " The “ Trink dich fit und schlau ” (“Drink Yourself Fit and Smart”) projec t has made drinking water \n",
      "an integral part of the school day at Heidelberg’s primary and special schools.  \n",
      " The Sarah Wiener Foundation project - “Für gesunde Kinder und was Vernünftiges zu e ssen” \n",
      "(“Healthy Children and Eating Properly”) – aims to correct th e poor nutrition practices of the fast \n",
      "food generation.  \n",
      " “HEIKE – Keiner fällt durchs Netz“ (HEIKE = Heidelberger Kinderschutz Engag ement) is the \n",
      "name of a new collaborative project between the city and the university to strengthen child \n",
      "protection.  \n",
      " Parents  (to-be) can call at the “ Frühe Hilfen ” (\"Early Assistance)  drop-in centre if they are \n",
      "worried about the demands of parenting or need help or advice.  \n",
      " Since 1993, the “Migration und Gesundheit\" (\" Migration and Health \") Working Group has \n",
      "provided the coordin ation for improving health care for migrants.  \n",
      " \n",
      "Aims:  \n",
      " Enable  trustful  contact with doctors in medical dialogues, also  in the mother tongue. (To be \n",
      "implemented in the medium term)  \n",
      " Produce a leaflet for migrants to improve their knowledge and involvement in t he German health \n",
      "and education systems. (To be implemented in the medium term)  \n",
      " Improve the health care of all migrants through a survey of foreign language skills among \n",
      "doctors (specialising in general medicine, psychiatry and psychotherapy) as well as the  \n",
      "respective authorised specialist personnel. (Implemented in 2011)  \n",
      " \n",
      " \n",
      "5.4 Neighbourhoods and voluntary work  \n",
      " \n",
      "General data/facts and figures:  \n",
      "The places where foreign inhabitants live are distributed unevenly across the city. They are \n",
      "concentrated in the Ber gheim, Altstadt, Emmertsgrund, Boxberg and Rohrbach districts. Nearly a \n",
      "third of today’s migrants in Heidelberg arrived between 2000 and 2008 and they come from \n",
      "Eastern Europe and the USA in particular. The satisfaction among Heidelbergers with a migration  \n",
      "background with where they live is generally high at 84 per cent; however, in the 30 –44 age group \n",
      "an above -average number tend to be dissatisfied with their current housing arrangements.  \n",
      "The additional demand for housing in Heidelberg is estimated to be a round 8,000 homes by 2020.  \n",
      " \n",
      "The City of Heidelberg’s strengths:  \n",
      " The City of Heidelberg advocates adequate housing provision, socially responsible housing \n",
      "development and a social infrastructure.  \n",
      " The District Management Team in Emmertsgrund wants to develop  measures together with the \n",
      "people to upgrade the housing and living conditions in Emmertsgrund.  \n",
      " The Women & the Future Workshops are extending dialogue right across cultural boundaries.  \n",
      " A series of information events for senior citizens in Turkish informs  migrants of stimulating \n",
      "activities and out -patient and in -patient services for older people in Heidelberg.  \n",
      " The City’s senior citizens’ centres are meeting places for senior citizens of different nationalities.  \n",
      " The “ Älter werden in der neuen Heimat\"  (“Agei ng in a New Homeland”) Network organises \n",
      "various joint events for migrants and Germans every year.  City of Heidelberg Municipal Integration Plan - Summary  Page 8 \n",
      " \n",
      " Since 1997, the FreiwilligenBörse Heidelberg  (Heidelberg Volunteers’ Exchange)  has operated \n",
      "as a local drop -in centre to promote volunteering in Heidelberg.  \n",
      " \n",
      "Aims:  \n",
      " Actively support migrants’ involvement in volunteering as a result of the approval of the \n",
      "Volunteering Concept. (To be implemented in the short term)  \n",
      " Increase identification with the districts  of the city and encourage sharing between various \n",
      "popul ation groups. (Implemented in 2011)  \n",
      " Provide more support for migrant community organisations through the provision of additional \n",
      "specialist advice and education services in the medium term and an Intercultural Centre in the \n",
      "long term.  \n",
      " Mediation between th e expectations and principles of social co -existence of the different \n",
      "population groups, including steering services, sponsorship schemes and neighbourhood \n",
      "events.  (Implemented in 2011)  \n",
      " \n",
      "5.5 Sport  \n",
      " \n",
      "The City of Heidelberg’s strengths:  \n",
      " The \"Integration durch  Dialog und Bewegung\"  (“Integration through Dialogue and Movement”) \n",
      "Project organises special sports activities for women and men and for children and young \n",
      "people.  \n",
      " The “ Kick dich schlau ” (\"Play It Smart”) Project aims to support the development of social skills \n",
      "as well as academic performance.  \n",
      " The “Sport fest der Kulturen ” (“Sports from All Cultures”) event provides for insights into other \n",
      "cultures.  \n",
      " In Heidelberg, many sports clubs are actively involved in integration.  \n",
      " \n",
      "Aims:  \n",
      " Create greater transparency in  existing activities. (Implemented in 2011)  \n",
      " Provide support for club membership for children and young people from low income families \n",
      "through the Heidelberg Pass +. (Since Autumn 2010)  \n",
      " Increase intercultural skills in sport: provide support for participat ion of migrants and in particular \n",
      "migrants in sports clubs. (Implemented from 2011 onwards)  \n",
      " \n",
      "5.6 Culture  \n",
      " \n",
      "General data/facts and figures:  \n",
      "The leisure and cultural landscape in Heidelberg is very diverse with some 70 to 100 or more \n",
      "events per  day – ranging from  exhibitions,  world music and visual arts right through to dance \n",
      "theatre. Heidelberg’s support for culture amounts to a sum of € 130 per inhabitant and therefore \n",
      "leads comparable German local authorities.  \n",
      " \n",
      "The City of Heidelberg’s strengths:  \n",
      " Intercultural activities at the German -American Institute and at the Eine-Welt-Zentrum  (One \n",
      "World Centre)  integration takes place here through sharing and mutual interest.  \n",
      "  Heidelberg’s educational institutions, such as the Music and Singing School, Public  Library, \n",
      "Office for Cultural Affairs, Kurpfälzisches Museum and many others provide access to cultural \n",
      "life. \n",
      " The numerous projects of the City of Heidelberg’s ZWINGER3 Children’s and Youth Theatre \n",
      "make a drama out of life for the target group of adolescent migrants.  \n",
      " Since 1995, the Karlstorbahnhof has provided a wide range of activities as a socio -cultural \n",
      "centre and in the intercultural sector.  \n",
      " City of Heidelberg Municipal Integration Plan - Summary  Page 9 \n",
      " Aims:  \n",
      " Provide more networking of existing cultural institutions, awareness -raising and training for \n",
      "participants in cultural work on intercultural issues. (Implemented since 2010)  \n",
      " Monitor ongoing projects and implement findings compiled during the course of the projects. \n",
      "(Implemented since the end of 2010)  \n",
      " Develop information specific to the target groups about cultural and othe r activities in order to \n",
      "extend participation in cultural life. (To be implemented in the medium term)  \n",
      " \n",
      " \n",
      "6. Over -arching issues of integration work  \n",
      " \n",
      "6.1 Intercultural openness  \n",
      "Local government is one of the first points of contact in Heidelberg for many mi grants and it strives \n",
      "to develop an intercultural orientation. This is supported by a series of professional development \n",
      "training courses on intercultural skills. In addition to local government, the “InfoCafé International” \n",
      "or ICI also offers a range of i nformation to newly arrived foreign students.  \n",
      " \n",
      "6.2 PR work  \n",
      "Integral to our PR work are information and briefings on integration activities and promoting \n",
      "Heidelberg’s image as a cosmopolitan and tolerant city. The City of Heidelberg promotes \n",
      "integration, op enness and a willingness to be accommodating among the public. Reports on \n",
      "integration issues and the organisation of events help to make integration come alive in everyday \n",
      "life. \n",
      " \n",
      "6.3 Heidelberg: City of Business and City of Science  \n",
      "Heidelberg is well -serve d as a business location by its very high proportion of service industries, \n",
      "mainly characterised by science and research, minimal dependence on economic cycles, stable \n",
      "employment, reputation and image as well as its high proportion of small businesses. Thi s is also \n",
      "to the benefit of people with a history of immigration, as the city is able to offer them excellent \n",
      "opportunities for their training and careers.  \n",
      " \n",
      "The \"Prognos Zukunftsatlas Branchen 2009\"  confirmed the City of Heidelberg as having excellent \n",
      "future potential, in particular in the areas of health science and corporate and research services. \n",
      "The city has large pioneering projects in the form of the new Bahnstadt District Development and \n",
      "Campus II.  \n",
      " \n",
      "The Ruprecht -Karl University was founded in 1386 and is Germany’s oldest university; thus, it has \n",
      "an international reputation and is the largest employer in the city. Other higher education \n",
      "institutions in Heidelberg are the University of Education  with its Intercultural Centre of Excellence, \n",
      "SRH Private  University Heidelberg and the Hochschule für Jüdische Studien (College of Jewish \n",
      "Studies ).  \n",
      " \n",
      "Heidelberg is also the headquarters of the Academy of Sciences & Humanities and numerous \n",
      "major international research institutions, such as the European Molecular  Biology Laboratory \n",
      "(EMBL), the National Centre for Tumour Diseases (NCT), the world’s most modern Ion -Beam \n",
      "Therapy Centre (HIT), the German Cancer Research Centre (DKFZ) and four Max Planck \n",
      "Institutes - for Comparative Public Law and International Law, As tronomy, Nuclear Physics and \n",
      "Medical Research.  \n",
      " \n",
      "Furthermore, the Heidelberg Technology Park is home to more than 80 companies and scientific \n",
      "institutions and around 1,400 employees and provides a centre of innovation which is recognised \n",
      "worldwide.  \n",
      " \n",
      "6.4 Interfaith dialogue  City of Heidelberg Municipal Integration Plan - Summary  Page 10 \n",
      " Taking part in interfaith dialogue in Heidelberg are the two Christian churches as well as the Jewish \n",
      "community, the Turkish Islamic Culture Association and the Deputy Mayor responsible for \n",
      "integration and equal opportunities. It aims to be a platform for an equal, respectful but also critical \n",
      "exchange of views as well as encounter and cooperation in everyday life. As well as the Africa \n",
      "Service, the Latino Prayer Circle, the Persian Bible Study Hour, the French language Études \n",
      "Bibliques an d the Franciscan Children’s Drum Group, the work of the “Migration skirche ” (Immigrant \n",
      "Church ) Project of the Chapel Community outreach ministry should also be highlighted. As well as \n",
      "tasks such as the honorary management, pastoral care and theological supp ort of migrant groups, \n",
      "a main focus is the strengthening and networking of women’s groups.  \n",
      " \n",
      "6.5 Educational sponsorship projects  \n",
      "The regional service centre of the “ Aktion zusammen wachsen ” of the Paritätischer \n",
      "Wohlfahrtsverband  currently supports 16 exi sting and newly established sponsorship projects of \n",
      "various associations, clubs, foundations and institutions in Heidelberg which provide \n",
      "comprehensive assistance - from authority sponsors and learning support right through to \n",
      "integration support.  \n",
      " \n",
      "6.6 Principle demands of the Foreigners’ Council/Migrants’ Council  \n",
      " \n",
      " Intercultural centre/meeting house  \n",
      "In order to improve the coordination of integration measures, an intercultural centre is to be \n",
      "established providing a central point for information, specialis t advice, educational services and an \n",
      "information and communication portal for people with a migration background and migrant \n",
      "community organisations.  \n",
      " \n",
      " Specialist advice and educational services for migrant community organisations  \n",
      "The migrant community or ganisations are to be included in the City’s integration work to a greater \n",
      "extent than before through advice, support and a range of training. A specialist advice centre can \n",
      "bring greater professionalism to the work of the organisations.  \n",
      " \n",
      " Education and la nguage support  \n",
      "The language support services of the migrant community organisations are to be regarded as \n",
      "supporting multilingualism in a globally oriented society. Additional resources are to be made \n",
      "available for language support services in order to be able to provide services above B1 level. \n",
      "There needs to be a greater focus on vocational content here.  \n",
      " \n",
      " \n",
      "7. Steering the integration work  \n",
      " \n",
      "7.1 The steering process at the City Administration  \n",
      "As an across -the-board task, integration requires all the bodies  involved to work together. The \n",
      "implementation of the agreed integration needs to be steered centrally. The steering body is \n",
      "comprised of the local council, the Committee for Integration and Equal Opportunities, the \n",
      "Foreigners’ Council/Migrants’ Council an d the Lord Mayor as the Leader of the Council and is \n",
      "supported at departmental level by the Department of Family, Social Welfare and Culture and the \n",
      "Department of Integration, Equal Opportunities and Citizens’ Services. The implementation of the \n",
      "integratio n measures is also the responsibility of the respective Heads of Office and of our \n",
      "collaborative partners. The sharing of experience which is coordinated by the Bürgeramt (Local \n",
      "Administration Office ) 1 and the joint development of measures are being exten ded through a \n",
      "steering group which has yet to be set up.  \n",
      " \n",
      "7.2 Networking of local actors  \n",
      "                                                 \n",
      "1 As of 1st May 2011, the Equal Opportunities Offi ce has been responsible for this.  City of Heidelberg Municipal Integration Plan - Summary  Page 11 \n",
      " In practice, the success of integration activities depends on how these different measures, projects \n",
      "and approaches are linked together. The City of Heidelberg will i nvestigate where greater \n",
      "coordination will be required in future. Increased transparency between the actors will be a key aim \n",
      "in respect of reducing parallel structures and consolidating resources for specific purposes.  \n",
      " \n",
      "7.3 Impact monitoring  \n",
      "In view of the fact that the quality requirements for projects are growing at the same time as \n",
      "resources are becoming scarcer, integration activities need to be designed to be especially \n",
      "efficient. Central to this will be the demands for concrete results. Good impact  monitoring requires \n",
      "a willingness to discuss the work on the part of the experts involved, and as a result more precise \n",
      "knowledge about the results and impact over the course of the project can be incorporated into the \n",
      "further planning and implementation,  thereby contributing to improving the steering and the \n",
      "prospects for success. This monitoring system will be set up by the end of 2011 with the \n",
      "participation of the Foreigners’ Council/Migrants’ Council and in transparent cooperation with the \n",
      "offices invo lved. \n",
      " \n",
      " \n",
      "8. Acknowledgements - and looking ahead  \n",
      "The City of Heidelberg considers it to be of great importance to take this opportunity to express its \n",
      "thanks to all those who have supported the development of the Municipal Integration Plan.  \n",
      "  \n",
      "1. Language and education  \n",
      "   Subgroup : Children, School and Family   \n",
      "-  Spokeswoman : Mrs Prof. Dr. Ingrid Dietrich, Pädagogische Hochschule, Interkulturelles \n",
      "Kompetenzzentrum (University of Education, Intercultural Centre of Excellence ) \n",
      "-  Mrs Hülya Amhari, Auslände rrat/Migrationsrat  (Foreigners' Council/Migration Council )  \n",
      "-  Mrs Dr. Orietta Angelucci von Bogdandy, HIPPY  \n",
      "-  Mr Nicolas Apfel -Totaro, Jugendgemei nderat (Youth Council ) \n",
      "-  Mrs Yvonne Bedbur, Pädagogische Hoc hschule, Interkulturelles Kompetenzzentrum (University \n",
      "of Education, Intercultural Centre of Excellence ) \n",
      "-  Herr Malte Burmester, Jugendgemeinderat  (City Youth Council )  \n",
      "-  Mr Giuseppe Cibella  \n",
      "-  Mrs Ulrike Duchrow, Asylarbeitskreis  (Asylum Working Group )  \n",
      "-  Mrs Renate Emer, Kinder -und Jugendamt  (Office of Children and Youth )  \n",
      "-  Mrs Birgit Fliedner, Kinder -und Jugendamt (Office of Children and Youth )  \n",
      "-  Mrs Anja Kegler, Kinder - und Jugendamt (Office of Children and Youth ) \n",
      "-  Mrs Renate Kneise, Bezirksbeirätin (Member of the District Advisory Council ) \n",
      "-  Mrs Dr. Marianne Laurig, HIPPY  \n",
      "-  Mrs Catherine Mechler -Dupouey, Interku ltureller Elternverein, Ausländerrat/Migrationsrat \n",
      "(Intercultural Parents' Association, Foreigners' Council/Migrants' Council ) \n",
      "-  Mrs Susanne Meyer, päd. -aktiv  \n",
      "-  Mrs Barbara Mün ch, Fachberaterin für Grundschulen (expert advisor for elementary schools ) \n",
      "-  Mrs Dr. Maria Susana Oder -Peña, Auslä nderrat/Migrationsrat (Foreigners' Council/Migrants' \n",
      "Council ) \n",
      "-  Mr Sotirios Papadopoulos -Herzhauser, Ausländerrat/Migrationsrat (Foreigners'  Council/Migrants' \n",
      "Council ) \n",
      "-  Mrs Ute Salize, päd. -aktiv  \n",
      "-  Mrs Dubravka Santak, Zentrum für Inte gration durch Bildung  \n",
      "-  Mrs Nora Schönberger, Pan -Afrikanische -Organisation (Pan-African Organisation ) \n",
      "-  Mrs Silvia Selke, Pädagogische Hoc hschule, Interk ulturelles Kompetenzzentrum (University of \n",
      "Education, Intercultural Centre of Excellence ) \n",
      "-  Mrs Nathalie Sommer, Heidelberg Intern ational School  \n",
      "-  Mrs Margarete Zwink -Eisele, Internation ale Gesamtschule  \n",
      "  \n",
      "Subgroup : Youth and Adults  \n",
      "Spokeswoman : Mrs Dr. Luitgard Nipp -Stolzenburg, Volkshochschule  City of Heidelberg Municipal Integration Plan - Summary  Page 12 \n",
      " -  Mrs Magdalena Adamczyk, Alpha -Aktiv Sprachschule  \n",
      "-  Mr Nicolas Apfel -Totaro, Jugendgemei nderat (City Youth Council ) \n",
      "-  Mr Karl-Heinz Bareuther, Internationaler Bund  \n",
      "-  Mr Malte Burmester, Jugendgemeinderat  (City Youth Council ) \n",
      "-  Mrs Claudia Emmendörfer -Brößler, Volk shochschule  \n",
      "-  Mrs Renate Kneise, Bezirksbeirätin (Member of the District Advisory Council ) \n",
      "-  Mrs Nadine Marschik, Diakonisches Werk  \n",
      "-  Mrs Regine Mitternacht, Stadtbücherei (Public  Library )  \n",
      "-  Mr Michael Weigel, Heidelberger Päd agogium  \n",
      "-  Mrs Antje von Wolff, Kinderbeauftragte Stadtteil Wieblingen (Ombudswoman for Children ) \n",
      "-  Mrs Yaldir Züleyha, Alpha -Aktiv Sprac hschule  \n",
      "  \n",
      "2. Training, job market and higher education  \n",
      "Spokesman : Mr Özkan E rgen, Jugendagentur eG  \n",
      "-  Mr Malte Burmester, Jugendgemeinderat (City Youth Council ) \n",
      "-  Mrs Dörthe Domzig, Amt für Chance ngleichheit (Equal Oppurtunities Office ) \n",
      "-  Mrs Viktoria Engelhart, Internationaler Bund  \n",
      "-  Mr Dr. Maximilian Eberius, Deutsch -Polnis che Gesellschaft (German -Polish Association ) \n",
      "-  Mr. Dr. Nihat Genc, Verein zur Förderung des Gedankenguts Atatürks (Kemalist Thought \n",
      "Association ) \n",
      "-  Mr Joachim Hahn, Amt für Stadtentwic klung und Statistik (Office for Urban Development and \n",
      "Statistics ) \n",
      "-  Mr Alexander Hornschuch, Agentur für Arbeit (Employment Agency ) \n",
      "-  Mr Jens Katzenberger, Verein zur berufl ichen Integration und Qualifizierung (VBI) (Association \n",
      "for professional Integration and Qualification ) \n",
      "-  Mrs Anna Kloppenburg, Akademisches Auslandsam t der Universität (International Office of the \n",
      "University ) \n",
      "-  Mr Siegfried Köhler, SRH Berufliche R ehabilitation Berufsförderungswerk Heidelberg  \n",
      "-  Mr Karl-Heinz Löhr, Job Center  \n",
      "-  Mrs Sonja Mechler, Heidelberger Dienste  \n",
      "-  Mr Jörg Schmidt -Rohr, VBI  \n",
      "-  Mr Heinz Schorr, Industrie -und Handelskammer Rhein -Neckar, Geschäftsstelle He idelberg \n",
      "(Rhine -Neckar Chamber of Industry and Commerce, Heidelberg Field Office ) \n",
      "-  Mr Leopold Übelhör, Kreishandwerke rschaft (Local Council of Skilled Crafts ) \n",
      " \n",
      "3. Family, healt h and welfare systems  \n",
      "Spokeswoman : Mrs Christine Köhl, Gesun dheitsamt Heidelberg (Heidelberg Public Health Office ) \n",
      "-  Mr Wolfgang Blam, Gesundheitsamt (Public Health Office ) \n",
      "-  Mr Sylla Bachir  \n",
      "-  Mrs Aysel Celep -Monz  \n",
      "-  Mr Dr. Ulrich Deutschmann, Kultur kreis Emmertsgrund -Boxberg  \n",
      "-  Mrs Marion Duscha, Heidelberger Selbs thilfebüro  \n",
      "-  Mrs Anja Dühring, BiBeZ -Bildung int egriert, Beratung eröffnet Zukunft  \n",
      "-  Mrs Wiebke Hartmann, Asylarbeit skreis  (Asylum Working Group ) (MediNetz Rhein -Neckar)  \n",
      "-  Mr Dirk Ho fmann, Amt für Sport und Gesundheitsförderung (Office of Sports and Health \n",
      "Promotion ) \n",
      "-  Mrs Birgit Kurz, Diakonisches Werk  \n",
      "-  Mrs Annemarie Lerch, Kinderschutzbund (Association for the Protection of Children ) \n",
      "-  Mr Prof. Dr. Bernard -M. Mechler, Auslä nderrat/Migrationsrat (Foreigners' Council/Migration \n",
      "Council )  \n",
      "-  Mrs Iris Mühlhausen, Kinderschutzbund ((Association for the Protection of Children ) \n",
      "-  Mrs Güler Olgun, Türkisch -Islamischer Kulturverein (Turkish  Islamic Cultur e Association ) \n",
      "-  Mr Choukri Rasc ho, Initiative zur Integr ation kurdischer Migranten (Initiative for the Integration of \n",
      "Kurdish Migrants ) \n",
      "-  Mrs Dr. Semra Serdaroglu -Baloch, Inte rnationales Frauen -und Familienzentrum (International \n",
      "Women's and Children's Centre ) City of Heidelberg Municipal Integration Plan - Summary  Page 13 \n",
      " -  Mrs Sadje Sürer, Türkis ch-Islamischer Ku lturverein (Turkish  Islamic Cultur e Association ) \n",
      "-  Mrs Susanne Völker, BiBeZ  \n",
      "  \n",
      "4. Neighbourhoods and voluntary work  \n",
      "Spokesman : Mr Prof. Dr. Martin Albert, SRH Hochschule Heidelberg  \n",
      "-  Mrs Annette Diefenbacher, Amt für Sozi ales und Seni oren (Office of Social Matters and Senior \n",
      "Citizens ) \n",
      "-  Mr Dr. Maximilian Eberius, Deutsch -Polnische Gesellschaft (German -Polish Association ) \n",
      "-  Mr Karl Emer, Caritasverband  \n",
      "-  Mrs Bärbel Fabig, Amt für Soziales und Senioren  (Office of Social Matters and S enior Citizens ) \n",
      "-  Mrs Heidi Farrenkopf, Diakonisches Werk  \n",
      "-  Mrs Heidi Flassak, Kinderbeauftragte Südstadt/Weststadt (Ombudswoman for Children ) \n",
      "-  Mrs Ulrike Jessberger, Kulturkreis Emmertsgrund -Boxberg, Bezirksbeirätin Boxberg  (Member of \n",
      "the District Ad visory Council ) \n",
      "-  Mr Gerald Kraus, Gesellschaft für Grund -und Hausbesitz (GGH)  \n",
      "-  Mrs Annette Kritzer, Asylarbeitskreis (Asylum Working Group ) \n",
      "-  Mrs Desiree Künsberg, SRH Hochschule  \n",
      "-  Mrs Ulli Leßmann, FreiwilligenBörse  \n",
      "-  Mrs Cecilia Lima -Wüst  \n",
      "-  Mrs  Dr.  Heidrun Mollenkopf, Kulturkreis Emmertsgrund -Boxberg, Bezirksbeirätin Emmertsgrund \n",
      "(Member of the District Advisory Council ) \n",
      "-  Mr Christoph Nestor, Mieterverein  \n",
      "-  Mrs Gabriele Riedke -Dschangaei, Seni orenzentrum Rohrbach  \n",
      "-  Mr Ernst Schwemmer, Arbeitsgemei nschaft Stadtteilvereine (Working Group of District Residents' \n",
      "Associations ) \n",
      "-  Mrs Gudrun Sidrassi -Harth, Asylarbeit skreis (Asylu m Working Group ) \n",
      "-  Mrs Dr. Karin Weinmann -Abel, Kulturkreis Emmertsgrund -Boxberg, Em -Box-Info  \n",
      "-  Mr Dr. Edgar Wu nder, Geographisches Institut (Geographical Institute ) \n",
      "  \n",
      "5. Sport and culture  \n",
      "Spokesman : Mr Dr. Hamdi Galal El -Din, Car itasverband / Sportkreis  \n",
      "-  Mrs Alexandra Eberhard, Kulturamt (Office for Cultural Affairs ) \n",
      "-  Mrs Cornelia Gans, TSG Ziegelhausen  \n",
      "-  Mr Reiner Greulich, Polizeidirektion (Police Headquarters ) \n",
      "-  Mrs Michaela Günter, Caritasverband  \n",
      "-  Mrs Saadet Kirici, Türkisch -Islamischer Kulturverein (Turkish  Islamic Cultur e Association ) \n",
      "-  Mr Rainer Römer, TSG 78 Heidelberg  \n",
      "-  Mr Karlheinz Schrumpf , Turnerbund Rohrbach  \n",
      "-  Mr Ulrich Sudhölter, Budo -Club Emmertsgrund -Boxberg  \n",
      "-  Mrs Ingrid Wolschin, Kulturhaus Karlsto rbahnhof  \n",
      " \n",
      "Furthermore the  Department of the Lord Mayor  (01), the Personnel and Organis ation Office (11), \n",
      "the Office for Urban Developm ent and Statistics  (12), the Public Relations Office  (13), the Local \n",
      "Administration Office (15), the Equal Opportunities Office  (16), the Office for Schools and \n",
      "Education  (40), the Office of  Cultural Affairs (41), the Theatre and Orchestra  (44), the Public  \n",
      "Library  (45), the Municipal School of Music and Singing  (46), the Office of Social Welfare and \n",
      "Senior Citizens (50), the Children and Youth Office (51), the Office of Sports Facilities and Health \n",
      "Promotion  (52), the City Planning Office  (61), the Landscap e Architects and Forestry Office  (67), \n",
      "the Office of Economic Development and Employment  (80), the Gesellschaft für Grund - und \n",
      "Hausbesitz, the Karlstorbahnhof Cultur al Centre  and Heidelberg Technolog y Park have made \n",
      "major contributions to the process of in tegrat ion as an across -the-board task . \n",
      " \n",
      "The City of Heidelberg will update the Municipal Integration Plan. The aim will be to monitor the \n",
      "implementation of the measures and modify the aims according to changes in the overall \n",
      "conditions. In the course of im plementation, the City of Heidelberg will define performance \n",
      "indicators in order to gauge the success of the measures. As part of this process, the City of City of Heidelberg Municipal Integration Plan - Summary  Page 14 \n",
      " Heidelberg will continue to involve the City’s internal and external experts in order to take accoun t \n",
      "of different perspectives, take on board the experience gained and live out intercultural diversity in \n",
      "its own practice as well . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Impressum  \n",
      " \n",
      "Herausgeberin  \n",
      "Stadt Heidelberg  \n",
      " \n",
      "Bürgeramt und  \n",
      "Amt für Chancengleichheit  \n",
      "Telefon: 06221  58 15530  \n",
      "E-Mail: chancengleichheit@heidelberg.de  \n",
      "Postfach 10 55 20  \n",
      "69045 Heidelberg  \n",
      "http://www.heidelberg.de/integration  \n",
      " \n",
      "Stand 10. Februar 2011  \n",
      " \n",
      "Übersetzung  \n",
      "Büro Käbe  \n",
      "Obere Seegasse 29  \n",
      "69124 Heidelberg  \n",
      "www.heidelberg.de/chancengleichheit\n",
      "German Courses\n",
      "in Heidelberg \n",
      "for Immigrants\n",
      "Table of Contents\n",
      "1. Learning German in Heidelberg . . . . . . . . . . . . . . 4\n",
      "2. German Course Offer  . . . . . . . . . . . . . . . . . . . . . . . 6\n",
      "2.1 Affordable or Free of Charge Language Learning \n",
      "Opportunities by the City of Heidelberg  � � � � � � � � � � � 6\n",
      "2.2 Free of Charge German Courses and  \n",
      "Exam Preparation by Associations and  \n",
      "Charitable Institutions  � � � � � � � � � � � � � � � � � � � � � � � � � � 8\n",
      "2.3 Courses Certified by the Federal Office for  \n",
      "Migration and Refugees � � � � � � � � � � � � � � � � � � � � � � � � 12\n",
      "2.3.1 Integration Course  � � � � � � � � � � � � � � � � � � � � � 12\n",
      "2.3.2 Integration Course for Parents � � � � � � � � � � � 16\n",
      "2.3.3 German for Professional Purposes  � � � � � � � 20\n",
      "2.3.4 Integration Course Providers in Heidelberg � �24\n",
      "3. Legal Notice  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27“Every new language is like \n",
      "an open window that shows \n",
      "a new view of the world and \n",
      "expands your attitude to-\n",
      "wards life.”\n",
      "Frank Harris (1856 – 1931)\n",
      "Irish-American author, journalist, publisher and editorLearning German in Heidelberg | 5 4 | Learning German in Heidelberg\n",
      "1. Learning  \n",
      "German in Heidelberg\n",
      " \n",
      "In this brochure, you will find an overview of affordable \n",
      "language learning opportunities offered by the City of \n",
      "Heidelberg (Stadt Heidelberg), free of charge German \n",
      "courses offered by charitable institutions and fee-based \n",
      "German courses that are certified by the German Feder -\n",
      "al Office for Migration and Refugees (Bundesamt für \n",
      "Migration und Flüchtlinge, BAMF). Fee-based German courses that are not certified by the \n",
      "Federal Office for Migration and Refugees are not listed \n",
      "in this brochure. Some of these courses are listed in an \n",
      "overview compiled by Migration Hub Heidelberg: \n",
      "https://migrationhub-heidelberg.org/our-work/arriving-  \n",
      "in-heidelberg/\n",
      "The present overview is not exhaustive. It is updated \n",
      "regularly. If you have any suggestions for changes, you \n",
      "may contact us via the following mail address: \n",
      "sprachfoerderung@heidelberg.de.\n",
      "This brochure is intended to provide immigrants and \n",
      "professionals working in migration with an overview.Affordable or Free of Charge Language Learning  | 7\n",
      "Opportunities by the City of Heidelberg6 | German Course Offer\n",
      "2. German Course Offer\n",
      "2.1 Affordable or Free of Charge \n",
      "Language Learning Opportuni-\n",
      "ties by the City of Heidelberg\n",
      "What does the City of Heidelberg Offer in Terms of \n",
      "Language Learning?\n",
      "Anyone coming to Heidelberg to apply for asylum \n",
      "qualifies for financial support from the City to partici-\n",
      "pate in a language course at the Adult Education \n",
      "Center (Volkshochschule, VHS). The course takes a mini-\n",
      "mum of four weeks and comprises 100 course hours. \n",
      "Participants will learn the basics for day-to-day commu-\n",
      "nication. The City of Heidelberg pays for the course. \n",
      "Participants may also progress to an advanced course. \n",
      "The advanced course also comprises 100 course hours.\n",
      "Who is eligible to participate in a language course \n",
      "paid for by the City of Heidelberg?\n",
      "You are eligible if you\n",
      "–are in preliminary accommodations by \n",
      "the City of Heidelberg,\n",
      "–are not of school age,\n",
      "–have been living in Germany for a \n",
      "maximum of 15 months,\n",
      "–qualify for benefits according to Germany’s Asylum-\n",
      "Seekers’ Benefits Act (Asylbewerberleistungsgesetz).Applications for these benefits need to be submitted \n",
      "to the Office of Social Welfare and Senior Citizens (Amt \n",
      "für Soziales und Senioren) of the City of Heidelberg. \n",
      "These courses are intended for everyone who is not \n",
      "eligible to participate in a BAMF integration course.\n",
      "How much are the language courses?\n",
      "The first language course is free of charge for partici-\n",
      "pants. The City’s Office of Social Welfare and Senior \n",
      "Citizens pays a part of it. The rest can be payed using \n",
      "the Heidelberg Pass. Everyone eligible to receive \n",
      "benefits according to the German Asylum-Seekers’ \n",
      "Benefits Act qualifies to receive the Heidelberg Pass.\n",
      "The advanced course costs EUR 32. The rest is payed \n",
      "for using an educational voucher by the City of Heidelberg \n",
      "which is funded by donations.\n",
      "More information\n",
      "Stadt Heidelberg\n",
      "Amt für Soziales und Senioren\n",
      "Bergheimer Straße 155, 69115 Heidelberg\n",
      "Ms. Kobs, Phone +49 6221 58-37392 or\n",
      "Mr. Fröhlich, Phone +49 6221 58-37260\n",
      "sozialamt@heidelberg.deFree of Charge German Courses by Associations and Charitable Institutions | 9\n",
      "Café Talk\n",
      "Offer in cooperation with Evangelische Kirche Heidelberg, \n",
      "the initiative Weststadt sagt JA!, Asyl arbeitskreis \n",
      " Heidelberg e. V. and Diakonisches  Werk – Evangelische \n",
      "Kirche Heidelberg.\n",
      "Contact\n",
      "Café Talk\n",
      "Phone +49 176 52085027 or +49 159 04867912 \n",
      "info@café-talk.com\n",
      "Vangerowstraße 5, 69115 Heidelberg\n",
      "Caritasverband Heidelberg e. V.\n",
      "Language courses by volunteers for women from a \n",
      "refugee background. \n",
      "Contact and Course Location\n",
      "Caritasverband Heidelberg e. V.\n",
      "Phone +49 6221 727 81 91\n",
      "fluechtlingssozialdienst@caritas-heidelberg.de\n",
      "Flüchtlingsunterkunft Hardtstraße 10/1\n",
      "69124 Heidelberg\n",
      "Caritasverband Heidelberg e. V. & Diakonisches \n",
      "Werk der Evangelischen Kirche Heidelberg\n",
      "Volunteers offer language courses for the inhabitants of \n",
      "the refugee arrival centre (Ankunftszentrum) of the state \n",
      "of Baden-Württemberg in Patrick-Henry-Village (PHV).\n",
      "Contact and Course Location\n",
      "Mr. Theisen and Ms. Straub\n",
      "Phone +49 6221 739 58 69\n",
      "phvehrenamt@caritasdiakoniehd.de\n",
      "Independent social counseling and advice on \n",
      "administrative procedures\n",
      "Ankunftszentrum Heidelberg\n",
      "North Gettysburg Avenue 4517, 69124 Heidelberg8 | Free of Charge German Courses by Associations and Charitable Institutions\n",
      " 2.2 Free of Charge German \n",
      "Courses and Exam Preparation \n",
      "by Associations and Charitable \n",
      "Institutions\n",
      "For more information you may reach \n",
      "out to the contact persons listed below. \n",
      "The following language learning opportunities are\n",
      "not certified. \n",
      "Asylarbeitskreis Heidelberg e. V.\n",
      "Volunteers support immigrants in learning German; \n",
      "they offer language courses, individual (1:1) language \n",
      "tutoring and help in preparing for exams.\n",
      "Contact\n",
      "Ms. Sommer\n",
      "Phone +49 6221 182797\n",
      "asylarbeitskreis-heidelberg@t-online.de\n",
      "Plöck 101, 69117 Heidelberg\n",
      "Course Location\n",
      "Flüchtlingsunterkunft\n",
      "Henkel-Teroson-Straße 14 – 16\n",
      "69123 Heidelberg\n",
      "Former Hotel Metropol\n",
      "Alte Eppelheimer Straße 80\n",
      "69115 Heidelberg\n",
      "WeltHaus – kleiner Meetingraum\n",
      "Willy-Brandt-Platz 5\n",
      "69115 HeidelbergFree of Charge German Courses by Associations and Charitable Institutions | 11 10 | Free of Charge German Course by Associations and Charitable Institutions\n",
      "Evangelische Gemeinde Bonhoeffer\n",
      "Two language courses in small groups for women; \n",
      "childcare is available during class hours of one of the \n",
      "courses.\n",
      "Contact\n",
      "Dr. Bindseil\n",
      "Phone +49 6221 712248\n",
      "christiane.bindseil@kbz.ekiba.de\n",
      "Evangelische Bonhoeffer-Gemeinde \n",
      "Oppelner Straße 2, 69 124 Heidelberg\n",
      "Diakonisches Werk der Evangelischen  \n",
      "Kirche Heidelberg\n",
      "Voluntary integration guides support immigrants in learn-\n",
      "ing German by offering individual language tutoring (1:1).  \n",
      "The SprachmittlerNetzwerk Heidelberg (Network of In-\n",
      "terpreters Heidelberg) offers language support and \n",
      "interprets during official appointments or counseling \n",
      "sessions.\n",
      "Contact\n",
      "Ms. Arnold\n",
      "Phone +49 6221 53750\n",
      "integrationsbegleiter@dwhd.de\n",
      "Karl-Ludwig-Straße 6, 69117 Heidelberg \n",
      "Frauen-Forum Emmertsgrund\n",
      "Free of charge German courses for women and girls.\n",
      "Contact and Course Location\n",
      "Ms. Oedel\n",
      "frauen.initiative@gmx.de\n",
      "Information between 11.30 am and 12.00 pm\n",
      "(except during school holidays)\n",
      "Emmertsgrundpassage 31, (im Jugendcafé)  \n",
      "69126 HeidelbergInternationales Frauen- und Familienzentrum \n",
      "Heidelberg e. V.\n",
      "Contact and Course Location\n",
      "Ms. Lindner\n",
      "Phone +49 6221 182334\n",
      "deutschkurs@ifz-heidelberg.de\n",
      "Theaterstraße 16, 69117 Heidelberg\n",
      "Interkulturelles Frauencafé, TES e. V. Stadtteilma -\n",
      "nagement Emmertsgrund\n",
      "Free of charge German courses for women\n",
      "Contact and Course Location\n",
      "Ms. Bertolo  \n",
      "Phone +49 6221 1394016\n",
      "stadtteilmanagement@emmertsgrund.de\n",
      "Emmertsgrundpassage 11b, 69126 Heidelberg\n",
      "Rotary Club Heidelberg – Alte Brücke and Volks- \n",
      "hochschule Heidelberg (VHS), project \n",
      "“Deutsch-Paten” (German buddy project) \n",
      "Voluntary mentors offer additional support for VHS stu-\n",
      "dents of German.\n",
      "Contact\n",
      "Ms. Reichenbach\n",
      "Volkshochschule Heidelberg\n",
      "Phone +49 6221 911960\n",
      "reichenbach@vhs-hd.deIntegration Course | 13 12 |  Courses Certified by the Federal Office for Migration and Refugees\n",
      "2.3. Courses Certified by the \n",
      "Federal Office for Migration   \n",
      "and Refugees\n",
      "2.3.1 Integration Course\n",
      "What is the integration course of the Federal Office \n",
      "for Migration and Refugees (BAMF)?\n",
      "The general BAMF integration course comprises 700 \n",
      "course hours. It consists of a language course with 600 \n",
      "course hours on important day-to-day topics, and an \n",
      "orientation course with 100 course hours, explaining the \n",
      "legal system, history and culture of Germany. In addition, \n",
      "there are special types of integration courses for specific \n",
      "groups. These are literacy courses, integration courses \n",
      "for young adults, intensive integration courses for quick \n",
      "learners, integration courses for women, integration \n",
      "courses for parents, integration courses for students \n",
      "learning an additional alphabet as well as integration \n",
      "courses for people with disabilities.\n",
      "What is the goal of a BAMF integration course?\n",
      "The integration course is set to enable participants to \n",
      "speak on a B1 level after completion and to pass the test \n",
      "Life in Germany. If you pass these tests, you will receive \n",
      "the Zertifikat Integrationskurs (Certificate Integration \n",
      "Course). Among others, this certificate is helpful, when \n",
      "people with a migratory background want to  apply for a \n",
      "residence title or apply for naturalization and it helps to \n",
      "integrate into the job market.\n",
      "Who can participate in a BAMF integration course?\n",
      " –Ethnic German resettlers,\n",
      " –newly immigrated people with a residence title,\n",
      " –people from other countries who have been living in \n",
      "Germany for some time,\n",
      " –EU citizens,\n",
      " –asylum applicants with good prospects to remain,\n",
      " –people whose deportation has been temporarily \n",
      "suspended and who have a residence permit.You can be “allowed to participate” in an integra-\n",
      "tion course (voluntary participation) or you can be \n",
      "“obligated to participate” in an integration course \n",
      "(obligatory participation) by the Jobcenter, the Em-\n",
      "ployment Agency (Agentur für Arbeit) or by official \n",
      "executive bodies of the German Asylum-Seekers’ \n",
      "Benefits Act.\n",
      "How much is the integration course?\n",
      "Every course hour costs EUR 2.29. Final examinations \n",
      "are free of charge. If you receive support like the citizens’ \n",
      "benefits (Bürgergeld), social security (Sozialhilfe), hous-\n",
      "ing benefits (Wohngeld) or benefits according to the Asy-\n",
      "lum-Seekers’ Benefits Act, you may apply for reim-\n",
      "bursement. You may also be eligible for a partial refund \n",
      "of traveling expenses. If you pass the test and had to \n",
      "pay for it yourself, you are eligible for a 50 % refund, pro-\n",
      "vided you have successfully completed the course \n",
      "within two years. You will need to submit an application \n",
      "to that end. Ethnic German resettlers do not need to \n",
      "pay a course fee.\n",
      "Do you offer childcare during class hours?\n",
      "Your language school will advise you on the options \n",
      "for childcare and will, if applicable, inform you about in-\n",
      "tegration courses for parents that include childcare \n",
      "offers. See also page 16.\n",
      "More information\n",
      "BAMF-Regionalkoordinatorin – \n",
      "Integration\n",
      "Ms. Saka\n",
      "Referat 52 B, AS Karlsruhe \n",
      "Bundesamt für Migration und Flüchtlinge\n",
      "Pfizerstraße 1, 76139 Karlsruhe \n",
      "Phone +49 911 943 80 269\n",
      "nuray.saka@bamf.bund.de\n",
      "or from any integration course provider in Hei-\n",
      "delberg and online: https://www.bamf.de/EN/\n",
      "Themen/Integration/ZugewanderteTeilneh-\n",
      "mende/Integrationskurse/integra-\n",
      "tionskurse-node.html;jsessionid=4CC1C39E89CB-\n",
      "F3B61A278CF8378830BB.internet271\n",
      "* Please submit your application to Bundesamt für Migration \n",
      "und Flüchtlinge, Regionalstelle Karlsruhe, Ref. 610, Pfizerstraße \n",
      "1, Gebäude F, 76139 Karlsruhe. Your local integration course \n",
      "providers will help you with your application.Integration Course | 15 14 | Integration Course\n",
      "How do I participate in a BAMF integration course?\n",
      "Application to participate \n",
      "in an integration course \n",
      "to be submitted to the \n",
      "Federal Oﬃ   ce for Migra-\n",
      "tion and Refugees (vol-\n",
      "untary participation)*\n",
      "With the Permission (Berechtigungsschein) or Obligation Cer-\n",
      "tiﬁ  cate (Verpﬂ  ichtungsschein) you will receive information on \n",
      "the assessment / language test. You need to take this test to be \n",
      "allowed to participate in the integration course.\n",
      "Participants will then be told where they can attend the course. \n",
      "This course will match their language skills.\n",
      "Participants will start the course offered by the provider that \n",
      "was allocated to them or that they have chosen.\n",
      "You will ﬁ  nd an overview of all integration course providers in \n",
      "Heidelberg in this brochure.\n",
      "Participants will attend the general integration course or a special \n",
      "type of integration course and need to pass the ﬁ  nal examinations.\n",
      "Zertifikat Integrationskurs (Certificate Integration Course)\n",
      "The certiﬁ  cate comes with legal advantages. It is for example \n",
      "required to receive a settlement permit and is helpful if you \n",
      "would like to get naturalized.Obligation Certiﬁ  cate \n",
      "from immigration author-\n",
      "ities, Jobcenter, Employ-\n",
      "ment Agency etc. (oblig-\n",
      "atory participation)or\n",
      "Flipchart with exercises in German in a German course.Integration Course for Parents | 17 16 | Integration Course for Parents\n",
      "2.3.2 Integration Course for Parents\n",
      "What is an integration course for parents?\n",
      "An integration course for parents is set to allow parents \n",
      "to attend an integration course, while their children \n",
      "aged between 0 and 5 will be looked after by professional \n",
      "childcare providers on the premises. The course con-\n",
      "sists of a language course with 900 and an orientation \n",
      "course with 100 units that parents can attend part \n",
      "time over a longer period. The language courses revolve \n",
      "around day-to-day family situations and the German \n",
      "educational system. This allows parents to learn how \n",
      "they can best support their child.The orientation course centers around laws, values, \n",
      "politics and the history of Germany. This course is offered \n",
      "by VHS Heidelberg in cooperation with and with the \n",
      " financial support of the Office of Equal Opportunities \n",
      "(Amt für Chancengleichheit) of the City of Heidelberg.\n",
      "What is the goal of an integration course  for \n",
      "parents?\n",
      "To complete the integration course for parents you will \n",
      "take the German Test for Immigrants (Deutschtest für \n",
      "Zuwanderer , DTZ) while the test Life in Germany (Leben \n",
      "in Deutschland, LiD) will conclude the orientation course. \n",
      "Participants receive the Zertifikat Integrationskurs (Cer-\n",
      "tificate Integration Course), provided they have reached \n",
      "the B1 level and have successfully passed the orientation \n",
      "test.Integration Course for Parents | 19 18 | Integration Course for Parents\n",
      "Who is eligible to attend the integration course \n",
      "for parents?\n",
      "–Ethnic German resettlers with children,\n",
      "–newly immigrated people with a residence title and \n",
      "with children,\n",
      "–people with children from other  countries   who have \n",
      "been living in Germany for some time,\n",
      "–EU citizens with children,\n",
      "–asylum applicants with good prospects to remain \n",
      "and with children.\n",
      "How much is the integration course for parents\n",
      "with childcare?\n",
      "Every course hour costs EUR 2.29. Final examinations \n",
      "are free of charge. If you receive support like citizens’ \n",
      "benefits (Bürgergeld), social security (Sozialhilfe), hous-\n",
      "ing benefits (Wohngeld) or benefits according to the \n",
      "Asylum-Seekers’ Benefits Act, you may apply for reim-bursement. You may also be eligible for a partial refund \n",
      "of traveling expenses. If you pass the test and had to \n",
      "pay for it yourself, you are eligible for a 50 % refund \n",
      "provided you have successfully completed the course \n",
      "within two years. You will need to submit an applica-\n",
      "tion to that end. Ethnic German resettlers do not need \n",
      "to pay a course fee. Childcare is free of charge for all \n",
      "participants.\n",
      "More information\n",
      "Volkshochschule Heidelberg e. V.\n",
      "Bergheimer Straße 76\n",
      "69115 Heidelberg\n",
      "Ms. Reichenbach\n",
      "Phone +49 6221 911960\n",
      "reichenbach@vhs-hd.deGerman for Professional Purposes | 21 20 | German for Professional Purposes\n",
      "2.3.3 German for Professional \n",
      " Purposes\n",
      "What is a German for professional purposes BAMF \n",
      "course and what is its goal?\n",
      "German for professional purposes builds on the integra-\n",
      "tion courses. Participants learn German used in a work \n",
      "context. There are different modules: Basic module (400 \n",
      "and 500 units) for the general professional language, \n",
      "modules for subject specific contents (300 units) and \n",
      "modules specifically for people who are undergoing \n",
      "the procedure to have their foreign professional qualifi-\n",
      "cation recognized (600 units). You may also combine \n",
      "the language course with qualification measures of the \n",
      "Federal Employment Agency (Bundesagentur für Arbeit).\n",
      "Who can participate in a German for professional \n",
      "purposes BAMF course?\n",
      "First Condition\n",
      "Participants belong to one of the following groups:\n",
      " –Immigrants, including refugees that are undergoing \n",
      "the procedure to have their foreign professional \n",
      "qualification recognized and have good prospects to \n",
      "remain,\n",
      " –people whose deportation has been temporarily sus-\n",
      "pended according to the German Residence Act \n",
      "(Aufenthaltsgesetz) Section 60a Subsection 2 Clause 3,\n",
      " –EU citizens,\n",
      " –Germans with a migratory background.Second Condition\n",
      "Participants also belong to one of the following groups:\n",
      "–People who are registered as jobseekers, as unem-\n",
      "ployed or as looking for an apprenticeship and/or \n",
      "people receiving citizens’ benefits or unemployment \n",
      "benefits (benefits according to the German Social \n",
      "Code, Sozialgesetzbuch II or III).\n",
      "–people in employment,\n",
      "–people who are undergoing the procedure to have \n",
      "their foreign professional qualification recognized \n",
      "and apprentices/trainees.\n",
      "Third Condition\n",
      "Participants have already completed an integration \n",
      "course and / or they speak German at B1 level but need \n",
      "more language skills for their job. The Emplyoment \n",
      "Agency, the Jobcenter and in some cases the BAMF will \n",
      "decide on who will participate in the course German \n",
      "for professional purposes.How much is the course?\n",
      "In general, German for professional purposes is free of \n",
      "charge. If have a job and you do not receive any other \n",
      "social benefits, you need to pay 50% of the fees  (currently \n",
      "EUR 2.42 per course hour).\n",
      "More information\n",
      "BAMF, Ms. Lang\n",
      "Regionalkoordinatorin Berufsbezogene \n",
      "Sprachförderung \n",
      "Wolframstraße 62, 70191 Stuttgart\n",
      "Phone +49 911 943-73942 \n",
      "BSK.Stuttgart@bamf.bund.de\n",
      "Agentur für Arbeit Heidelberg\n",
      "Kaiserstraße 69 – 71, 69115 Heidelberg\n",
      "Phone +49 800 4555500\n",
      "Jobcenter Heidelberg\n",
      "Speyerer Straße 6, 69115 Heidelberg\n",
      "jobcenter-heidelberg@jobcenter-ge.de22 | German for Professional Purposes German for Professional Purposes | 23Integration Course Providers in Heidelberg | 25 24 | Integration Course Providers in Heidelberg\n",
      "2.3.4 Integration Course Providers \n",
      "in Heidelberg\n",
      "German for  \n",
      "Professional  \n",
      "PurposesIntegration \n",
      "CourseIntegration \n",
      "Course for  \n",
      "Parents\n",
      "Berlitz Deutschland GmbH     \n",
      "Mr . Lieder\n",
      "Phone +49 6221 164004\n",
      "christian.lieder@berlitz.de\n",
      "Sofienstraße 7a, 69115 Heidelberg\n",
      "BBQ Bildung und      \n",
      "Berufliche Qualifizierung gGmbH  \n",
      "Mr . Knanbi\n",
      "Phone +49 6221 8907725\n",
      "knanbi.marouane@biwe.de \n",
      "Eppelheimer Straße 13, 69115 Heidelberg \n",
      "F+U Academy of Languages gGmbH    \n",
      "Ms. Wagner\n",
      "Phone +49 6221 912034\n",
      "tatjana.wagner@fuu.de\n",
      "Hauptstraße 1, 69117 Heidelberg\n",
      "Heidelberger Dienste gGmbH     \n",
      "Ms. Engelsberg\n",
      "Phone +49 157 71461202\n",
      "engelsberg@hddienste.de \n",
      "Hospitalstraße 5, 69115 Heidelberg\n",
      "Heidelberger Pädagogium     \n",
      "Mr . Weigel\n",
      "Phone +49 6221 45680\n",
      "info@heidelberger-paedagogium.de\n",
      "Schröderstraße 22a, 69120 HeidelbergInternationaler Bund e. V.      \n",
      "Ms. Egelhof\n",
      "Phone +49 6221 3169531\n",
      "Sprachkurse.Heidelberg@ib.de\n",
      "Belfortstraße 2, 69115 Heidelberg\n",
      "(Offer includes an integration course for young people \n",
      "between 16 and 26)\n",
      "USS GmbH      \n",
      "Ms. Weißbach\n",
      "+49 6221 9987840\n",
      "deutsch@uss.de\n",
      "Englerstraße 6, 69126 Heidelberg\n",
      "Volkshochschule Heidelberg e. V.        \n",
      "Ms. Türschmann-Qataoui\n",
      "Phone +49 6221 911988\n",
      "daf@vhs-hd.de\n",
      "Bergheimer Straße 76, 69115 Heidelberg\n",
      "GebärdenVerstehen e. Kfr.     \n",
      "(Only language courses specifically for the deaf)\n",
      "Ms. Füll\n",
      "Phone +49 6221 7287478 \n",
      "heidelberg@gebaerdenverstehen.deLegal Notice\n",
      "Stadt Heidelberg\n",
      "Amt für Chancengleichheit\n",
      "Fachbereich Integration – Sprachförderung\n",
      "Bergheimer Straße 69\n",
      "69115 Heidelberg\n",
      "sprachfoerderung@heidelberg.de\n",
      "www.heidelberg.de/chancengleichheit\n",
      "Layout \n",
      "Stadt Heidelberg, Markenkommunikation\n",
      "Translation  \n",
      "Susanne J. Schneider\n",
      "Photos \n",
      "Cover, pages 5, 12, 26: Annette Schiffmann,  \n",
      "Asylarbeitskreis Heidelberg e. V.\n",
      "Page 4: Heidelberg Marketing GmbH\n",
      "Pages 6–7, 13: Philipp Rothe\n",
      "Pages 16–17, 18–19, 20–21, 22–23:  \n",
      "rido/shutterstock.com\n",
      "Copyright © 2023 Stadt Heidelberg. \n",
      "All rights reserved. \n",
      "Reproduction (in part or in whole) only allowed with \n",
      "the express authorization of the City of Heidelberg \n",
      "(Stadt Heidelberg).\n",
      "Edition\n",
      "Oktober 2023, 3rd edition\n",
      "    \n",
      "Stadt Heidelberg\n",
      "Bergheimer Straße 69\n",
      "69115 Heidelberg\n",
      "Phone +49 6221 58-15500\n",
      "Fax +49 6221 58-49160\n",
      "chancengleichheit\n",
      "@heidelberg.de\n",
      "www.heidelberg.deAmt für \n",
      "Chancengleichheit\n",
      "Bahnstadt\n",
      "The place to be in the science \n",
      "city of Heidelberg\n",
      "www. bahnstadt.heidelberg.deDear reader,\n",
      "On a site that was once a marshaling yard for freight trains, we are today\n",
      "developing one of the world’s largest passive house settlement – Bahnstadt. \n",
      "Located in central Heidelberg, the district builds on the tradition of European \n",
      "cities of science, offering an attractive mix of residential life and research, \n",
      "leisure and culture, all in the same district. 6,000 jobs are being created here, \n",
      "predominantly in research and science-based companies. On an area \n",
      "covering more than 100 hectares, this zero-emission district is one of the \n",
      "largest urban development projects in Germany.\n",
      "As a science hub, Bahnstadt projects a unique sense of dynamism, attracting \n",
      "both high-tech companies with their research and development depart-\n",
      "ments and private universities. The offi  ce and laboratory buildings “SkyLabs” \n",
      "and “SkyAngle” created by the foundation Max-Jarecki-Stiftung are \n",
      "hothouses for innovation – as is the InnovationLab of the leading-edge \n",
      "cluster Organic Electronics. The Heidelberg Technology Park offers outstand-\n",
      "ing development opportunities for start-ups at two locations in Bahnstadt. \n",
      "Furthermore, the new conference center will provide a home for interna-\n",
      "tional congresses and trade fairs in the science city of Heidelberg.\n",
      "Nearby, the Heidelberg Innovation Park (HIP) is a breeding ground for \n",
      "innovation in the fi  elds of IT, digital media and bioinformatics.\n",
      "Spacious green areas, excellent transport links and numerous child daycare \n",
      "facilities, leisure activities and shops make Bahnstadt a highly attractive \n",
      "working environment. Other research locations in Heidelberg can be reached \n",
      "in just a few minutes.\n",
      "This brochure is intended to provide an overview of the Bahnstadt science \n",
      "hub. I hope you enjoy learning more about Heidelberg’s district of the future.\n",
      "Yours sincerely\n",
      "Prof. Dr. Eckart Würzner\n",
      "Mayor\n",
      "Jürgen Odszuck\n",
      "First Deputy Mayor4 | Living – researching – developing. Living – researching – developing. | 5\n",
      "Living – researching – developing.\n",
      "6,800  people\n",
      "will be living in the \n",
      "district\n",
      "An estimated\n",
      "2 billion \n",
      "euros are being invested\n",
      "50 % of Bahnstadt \n",
      "residents are younger \n",
      "than 30 years old \n",
      "Science and commerce, residential living and culture have \n",
      "been closely interwoven in Heidelberg life for centuries. \n",
      "This tradition is being continued in Bahnstadt. The concept \n",
      "is a successful one.\n",
      "An urban environment with daycare centers, a school \n",
      "and shops, excellent transport links, energy-effi  -\n",
      "cient buildings, green areas – Heidelberg’s Bahnstadt \n",
      "reconciles the needs of employers and employees \n",
      "in a single location. It provides suitable commercial \n",
      "premises with appropriate infrastructure for knowl-\n",
      "edge-intensive and research-based companies, as \n",
      "well as trades and retail enterprises. Bahnstadt is one \n",
      "of the biggest urban development projects in Germany \n",
      "and one of the world´s largest passive house settle-\n",
      "ment. The new district is situated next to Heidelberg’s \n",
      "main railway station. It is just a few minutes by public \n",
      "transport to other science centers such as Neuen-\n",
      "heimer Feld and the Old Town. In future there will be \n",
      "6,800 people living in the district, and 6,000 people \n",
      "working there.\n",
      "Bahnstadt is growing rapidly, with 4,300 people al-\n",
      "ready living here by the middle of 2019. Parents have \n",
      "eight daycare centers to choose from for their chil-\n",
      "dren. Another daycare facility is under construction. \n",
      "Many shops are already open. The B³ community \n",
      "center at Gadamerplatz is a central meeting point. \n",
      "Popular in Heidelberg and beyond, the “Halle02” events venue stages concerts, exhibitions and other \n",
      "unique cultural events in what used to be a freight \n",
      "depot.\n",
      "All buildings in the Bahnstadt district are constructed \n",
      "in compliance with the passive house standard. This \n",
      "dramatically reduces the buildings’ energy require-\n",
      "ments. Remaining energy needs are met in an environ-\n",
      "mentally responsible manner. A fi  rst step in this re-\n",
      "gard was the construction by Heidelberg’s public utility \n",
      "company of a wood-fi  red CHP plant, which has been \n",
      "in operation since 2013. The CHP plant produces \n",
      "enough electricity and heat to make Bahnstadt a \n",
      "zero-emission district.\n",
      "Bahnstadt stands for quality in all respects – in par-\n",
      "ticular for individual quality of life. This is refl  ected \n",
      "in the high demand for living space. 2,521 residences \n",
      "with a living area totaling 163,844 square meters \n",
      "have already been built. Virtually all residential \n",
      "prop erties in Bahnstadt have been sold or let.1  “Bauhaus” DIY store\n",
      "  41,000 square meters, of which 20,000 square meters are sales area\n",
      "2  Residential quarters in third phase of construction with\n",
      " “urban element” and “urban view” (LBBW Immobilien)\n",
      "3  “Tankturm“ (water tower at Bauhaus)\n",
      "  Cultural and events center and architectural offi  ce\n",
      "4  Residential property in second phase of construction\n",
      "5  Promenade in second phase of construction\n",
      " with farmyard-themed playground\n",
      "6  “Heidelberg Village”\n",
      "  cross-generation residential quarter\n",
      "7  First section of tramway along the Green Mile\n",
      "8  MEILEN.STEIN  \n",
      "  185 apartments, daycare facility, commercial premises,\n",
      "  offi  ces, restaurant and hotel (GGH)\n",
      "9  B3 at Gadamerplatz\n",
      "  School, community center and daycare facility (IBA project)\n",
      "10  “Luxor Filmpalast” cinema complex (FTB Englert GmbH)\n",
      "11  Residential quarters in fi  rst phase of construction\n",
      "12  Promenade in fi  rst phase of construction\n",
      "  Recreation area with the two themed playgrounds \n",
      "  (fi  re department and ICE)13  Student ﬂ  ats, campus accommodation and micro apartments\n",
      "14  Zollhofgarten daycare facility in the freight halls\n",
      "15  “Campus Gardens” quarter \n",
      "370 apartments (i Live Heidelberg GmbH)\n",
      "16  Junges Wohnen (Young living)\n",
      "106 rental apartments (SOKA building)\n",
      "17  “halle02” cultural center in the former freight halls\n",
      "18  “SkyLabs” offi  ce and laboratory building \n",
      "19,000 square meters for research-oriented companies, including\n",
      " Heidelberg Engineering, Reckitt Benckiser and the international\n",
      " Schiller University\n",
      "19 Second tramway section through the Green Mile and Czernyring\n",
      "20  New fi  re station\n",
      "21  Schwetzinger Terrasse\n",
      "Recreation area with daycare facility of the same name\n",
      "22  “Colours” quarter\n",
      "11,600 square meters for offi  ce, commercial and residential use\n",
      "23  “SkyAngle” offi  ce and laboratory building\n",
      "16,000 square meters for research-oriented and \n",
      " science-based companies\n",
      "24 Zollhofgarten recreation area25  “Stadttor” offi  ce building\n",
      "  Approx. 11,000 square meters of rental premises\n",
      "26  Main station, Bahnstadt exit\n",
      "  Direct access to Bahnstadt from the station concourse\n",
      "27  Heidelberg Technology Park with two of its fi  ve locations\n",
      "  including the InnovationLab GmbH research platform of\n",
      "  leading-edge cluster Organic Electronics\n",
      "28  “Stadttor Ost” offi  ce building\n",
      "  cbs Corporate Business Solutions, “Proaesthetic” clinic, outpatient\n",
      "  surgery center, medical and physiotherapy practices, offi  ce areas\n",
      "  (11,000 square meters)\n",
      "29  XXXL and Mömax furniture stores\n",
      "30  Recreation area at Promenade\n",
      "31  “Spitzes Eck” recreation area\n",
      "32  Apartments und offi  ce building\n",
      "  (Grüne Meile Projektentwicklung GmbH)\n",
      "33  “Pfaffengrunder Terrasse” recreation area\n",
      "34  Health Center Bahnstadt\n",
      "  7,500 square meters for use as medical and specialist practices \n",
      "35  “Westarkaden” shopping center\n",
      " Supermarkets, drugstore, specialist retailers, restaurants, cafés,\n",
      " daycare facility and apartments (Unmüssig Bauträgergesellschaft)36  Gneisenaustraße cycle path and footbridge\n",
      "37  Pfi  tzenmeier Premium Plus Resort \n",
      "  AquaDome and restaurants\n",
      "38  Kopernikusquartier\n",
      "  Apartments, stores and offi  ces\n",
      "39  Redesigned Czernyplatz and Czernyring\n",
      "40  Residential building\n",
      " Approx. 160 apartments, mainly three to fi  ve rooms\n",
      "41  New conference center\n",
      "42  Europaplatz \n",
      "lively quarter with offi  ces, residential properties, restaurants,   \n",
      " commercial premises and a conference hotel (Gustav Zech Stiftung)\n",
      "43  District cooling\n",
      "delivered to the conference center, buildings at Europaplatz and   \n",
      " near offi  ces \n",
      " Projects realized    Projects in planning / under constructionHeidelberg’s Bahnstadt\n",
      "A district with a successful mix of residential living, \n",
      "research and commerce\n",
      "29\n",
      "1\n",
      "2\n",
      "233234\n",
      "31\n",
      "3314\n",
      "1519\n",
      "4039\n",
      "59\n",
      "8\n",
      "1310\n",
      "11\n",
      "121735\n",
      "1636\n",
      "24\n",
      "1841\n",
      "21\n",
      "2046\n",
      "30\n",
      "27\n",
      "2273\n",
      "2542\n",
      "282637\n",
      "38\n",
      "436 | Heidelberg’s Bahnstadt Heidelberg’s Bahnstadt | 78 | Heidelberg: International hotspot for science Heidelberg: International hotspot for science | 9\n",
      "Heidelberg: \n",
      "International hotspot for science\n",
      "22-hectare\n",
      "Bahnstadt-Campus\n",
      "35,000 m² \n",
      "“SkyLabs” and\n",
      "“SkyAngle” Heidelberg enjoys an outstanding reputation worldwide as a pres-\n",
      "  tigious location for scientifi  c research. The reasons lie partly in the \n",
      "city’s long history, but also in the fact that science is part of the \n",
      "urban environment. It has been closely interwoven with Heidelberg’s \n",
      "economy and daily life for six hundred years.\n",
      "This tradition continues with the creation of the Bahn-\n",
      "stadt campus. On a site covering around 22 hectares, \n",
      "the campus offers state-of-the-art spaces and facilities \n",
      "for pioneering knowledge-based companies and \n",
      "research institutions – for example in the fi  elds of life \n",
      "sciences, biotechnology, ICT or energy and environ-\n",
      "mental sciences. As with the city as a whole, the Bahn -\n",
      "stadt campus stands for openness, dialog, and creativity in a vibrant urban environment. Today seven out of \n",
      "ten people in Heidelberg are employed in scientifi  c \n",
      "research or by high-tech companies. The unemploy-\n",
      "ment rate is consistently below fi  ve percent. In par-\n",
      "ticular, researchers and scientists value the urban en-\n",
      "vironment Heidelberg provides, and the opportunities \n",
      "it offers for a lunchtime walk from the laboratory \n",
      "to a nearby café.\n",
      "The Bahnstadt campus is no different. In addition, \n",
      "several science and biotech companies have made \n",
      "Bahnstadt their base. They are part of the Heidelberg \n",
      "Technology Park, which has created ideal conditions \n",
      "and services for biotech and science companies in \n",
      "Heidelberg.\n",
      "The heart of the campus project was set up by the \n",
      "not-for-profi  t foundation Max-Jarecki-Heidelberg-\n",
      "Stiftung. The foundation provided capital with a view \n",
      "to working with the city of Heidelberg to promote \n",
      "interdisciplinary cooperation and knowledge transfer. \n",
      "The organization’s founder Dr. Henry Jarecki, who \n",
      "studied medicine at Heidelberg University in the 1950s, \n",
      "was immediately enthused by the idea of developing \n",
      "a campus for new research-oriented companies on \n",
      "the Bahnstadt site.\n",
      "It all started with the “SkyLabs” building at Zollhof-\n",
      "garten, which has become the most visible landmark \n",
      "of Bahnstadt on account of its height, striking facade, \n",
      "and unique architectural design featuring two pro-\n",
      "jecting upper fl  oors. But “SkyLabs” also points the \n",
      "way in terms of Bahnstadt’s research ambitions. The \n",
      "striking building offers state-of-the-art labs and re-\n",
      "search facilities and perfectly embodies the successful \n",
      "interplay between science and industry in this for-\n",
      "ward-looking district.A second project fi  nanced by the Max-Jarecki founda-\n",
      "tion, the “SkyAngle” laboratory and offi  ce building, \n",
      "is in close proximity to “SkyLabs”. A new conference \n",
      "center in Bahnstadt will in future provide the perfect \n",
      "venue for meetings and conferences. Ideally located, \n",
      "the center is directly adjacent to the main station \n",
      "with tram connections to the historic old town. With \n",
      "opening scheduled for 2023, the conference center \n",
      "is a fl  agship project for the city of Heidelberg as a hub \n",
      "for science and business.\n",
      "Close to the Bahnstadt site on land formerly occupied \n",
      "by the Patton Barracks, the city is also developing \n",
      "the Heidelberg Innovation Park (hip), which will be \n",
      "a hotspot for innovative companies in the fi  elds of \n",
      "IT, digital media and bioinformatics. As a multifunc-\n",
      "tional offi  ce and laboratory building, the Business \n",
      "Development Center Organic Electronics offers ideal \n",
      "conditions for young high-tech companies.\n",
      " 1,800Conference center \n",
      "seats in the \n",
      "main hall10 | Attractive mix for companies Attractive mix for companies | 11\n",
      "Attractive mix \n",
      "for companies\n",
      "Approx.250 rooms\n",
      "at the conference hotel at \n",
      "EuropaplatzThe new district is an equally attractive proposition for \n",
      "high-tech and research companies, service providers and \n",
      "B2B companies, retailers and trades.The vibrant mix makes Bahnstadt attractive to cor-\n",
      "porations and small traders alike. In contrast to \n",
      "most monostructured offi  ce centers, companies fi  nd \n",
      "themselves here in a vibrant and inspiring environ-\n",
      "ment. Employees have the chance to live close to their \n",
      "place of work. There are eight daycare centers of-\n",
      "fering an excellent range of childcare services. Shopping \n",
      "is quick and easy with a range of stores for everyday \n",
      "needs.\n",
      "In addition, Bahnstadt boasts all the traditional hard \n",
      "location factors. Transport connections are ideal. \n",
      "Heidelberg and the Rhine-Neckar metropolitan region \n",
      "offer huge potential in terms of skilled employees. \n",
      "And Bahnstadt is specifi  cally designed to the passive \n",
      "house standard, which also applies to all offi  ce, lab-\n",
      "oratory and commercial property. Consequently, \n",
      "energy costs are lower than in conventionally con-\n",
      "structed buildings.\n",
      "Innovative and research-oriented companies value \n",
      "the very favorable location and positive environ-\n",
      "ment close to other research-based companies and \n",
      "facilities. In the direct vicinity are Bahnstadt Campus \n",
      "and Czernyring, two of the fi  ve locations of the \n",
      "Heidelberg Technology Park. In addition, top inter-\n",
      "national companies and a private university have \n",
      "located in close proximity.\n",
      "Bahnstadt is also the home to the leading-edge cluster \n",
      "Organic Electronics with its research platform Inno-\n",
      "vationLab GmbH. The core of the 5,000 square meter \n",
      "offi  ce and laboratory space is a 650-square-meter \n",
      "cleanroom laboratory. Over 190 researchers from \n",
      "various disciplines work here on innovations in the \n",
      "fi  eld of printed electronics, including digital dental \n",
      "imprints, intelligent glass and intelligent fl  oor mats.\n",
      "The “Stadttor” building also demonstrates that the \n",
      "right underlying conditions are in place: Many com-\n",
      "panies have already been here for some time. The \n",
      "Heidelberg company io-consultants moved its head-\n",
      "quarters to the “Stadttor” offi  ce building in Bahnstadt \n",
      "in early 2013. This is the ideal location for io-con-\n",
      "sultants – it provides an inspiring environment for staff \n",
      "and an attractive and contemporary ambience for \n",
      "customers.\n",
      "The “Stadttor Ost” complex is located in the imme-\n",
      "diate vicinity.  The two buildings on the corner of \n",
      "Speyerer Straße and Rudolf-Diesel-Straße opened \n",
      "for occupancy in the fall of 2018. The offering to \n",
      "tenants includes modern offi  ces and conference rooms \n",
      "as well as co-working spaces.\n",
      "World’s fi  rst passive \n",
      "house cinema\n",
      "15screens & \n",
      "open-air area12 | Attractive mix for companies Attractive mix for companies | 13\n",
      "6,000\n",
      "jobs are being \n",
      "created in \n",
      "BahnstadtOther companies to have relocated to Bahnstadt \n",
      "also benefi  t from the excellent underlying condi-\n",
      "tions – including the Luxor Filmpalast cinema \n",
      "complex with 15 screens and outdoor area, sev-\n",
      "eral hotels and the DIY store Bauhaus. The retail \n",
      "park is also home to the “XXXL” and “Mömax” \n",
      "furniture stores. \n",
      "The new “Westarkaden Heidelberg” shopping \n",
      "center provides an offering of retail and food \n",
      "outlets covering a total area of 11,700 square \n",
      "meters. The offering includes a grocery super-\n",
      "market, a discount grocery store, a drugstore, \n",
      "a pharmacy, a hair salon and restaurants. The \n",
      "First Steps daycare center is also opening another \n",
      "location there for up to 40 toddlers. In addi-\n",
      "tion, the center offers offi  ce space, 284 apart-\n",
      "ments, and a two-storied parking garage. The \n",
      "retail outlets are scheduled to open from the \n",
      "beginning of 2020. The entire project is to be \n",
      "completed in the spring of 2020.\n",
      "A lively quarter is emerging on the south side \n",
      "of the main station, featuring offi  ces, residential \n",
      "properties, shops, restaurants and a high-class \n",
      "conference hotel. The fi  rst buildings are sched-\n",
      "uled for completion in 2021. The new Europa-\n",
      "platz will form the center of this quarter. The new Convention Center opposite Europa-\n",
      "platz will in future provide a venue for national \n",
      "and international conferences in Heidelberg, \n",
      "the city of science. There are plans for up to \n",
      "1,800 seats in the main hall and 800 seats in \n",
      "the small hall. The opening is scheduled for the \n",
      "beginning of 2023.\n",
      "Bahnstadt also builds on Heidelberg’s tradition \n",
      "as a high-quality location for medicine and \n",
      "medical technology, in particular with a planned \n",
      "medical center, accommodation for patients’ \n",
      "families, and research into appliances for ophthal-\n",
      "mological diagnostics by Heidelberg Engineer-\n",
      "ing. Heidelberg is one of the world’s leading \n",
      "medical locations, with 13 hospitals and a \n",
      "reha bilitation facility. The University Hospital \n",
      "Heidelberg with its medical faculty is the \n",
      "city’s largest employer, with around 13,700 staff \n",
      "– including 1,700 doctors. It is one of the \n",
      "three largest hospitals in Germany. The medical \n",
      "faculty is ranked number one in Germany.14 | A place for all lifestyles A place for all lifestyles | 15\n",
      "A place for all lifestyles\n",
      "4,300\n",
      "people are already \n",
      "living in Bahnstadt\n",
      "One in eight households receives \n",
      "housing assistance \n",
      "from the city\n",
      "Every 4 days\n",
      "a new Bahnstadt \n",
      "resident  is bornSingle people have different needs to families with children; the elderly \n",
      "have different lifestyles to students. A young offi  ce worker for a \n",
      "creative IT company pursues different leisure activities compared \n",
      "to an experienced scientist engaged in lab research. But they are \n",
      "all Bahnstadt residents and help give the new quarter its unique \n",
      "character.16 | A place for all lifestyles A place for all lifestyles | 17\n",
      "Bahnstadt reflects the diversity of life plans that comes with such a mixed \n",
      "resident structure. Lifestyles are reflected in the architecture, the public \n",
      "outdoor spaces, the squares, and the housing. From traditional and spacious \n",
      "apartments to airy townhouses and villas – Bahnstadt has an accommo -\n",
      "dation type to meet every requirement.\n",
      "Yet as varied as it is, the properties for the future 6,800 residents all have \n",
      "one thing in common: Each one is compact in size and offers privacy – in \n",
      "the form of a garden, a spacious green inner courtyard, or a fifth-floor roof \n",
      "terrace. Generously proportioned and quality-designed outdoor areas \n",
      "also provide excellent opportunities for time spent in the open air. Bahn -\n",
      "stadt meets all the requirements for improving everyone’s quality of life. \n",
      "Living in Bahnstadt means having a sense of wellbeing inside one’s own \n",
      "four walls and in an attractive residential district.\n",
      "18 | First-class education and daycare facilities\n",
      "First-class education, daycare and \n",
      "community facilities\n",
      " 8 daycare \n",
      "centers \n",
      "are already operational\n",
      "The B³ community \n",
      "hall has seating for \n",
      "200 people Heidelberg’s long tradition in education is being practiced \n",
      "and developed in Bahnstadt. \n",
      "The city of Heidelberg is ranked top in Germany for \n",
      "early learning initiatives for children: The rate of \n",
      "daycare provision for young children (under three \n",
      "years old) is 54,9 percent. Every child between three \n",
      "and six years old has a kindergarten place and \n",
      "around 87 percent of all Heidelberg primary school \n",
      "children also receive afternoon daycare following \n",
      "lessons at school. According to the German Learning \n",
      "Atlas of the Bertelsmann Foundation, Heidelberg is \n",
      "one of the top locations for schooling in Germany. \n",
      "In addition, many educational institutions offer a range \n",
      "of activities to promote lifelong learning. These \n",
      "high social standards clearly apply to Bahnstadt in \n",
      "particular, where residents seek specifi  cally to reconcile \n",
      "career and family.\n",
      "Centrally located on Gadamerplatz, the B³ community \n",
      "center forms the heart of Bahnstadt. One of eight \n",
      "child daycare center in the district opened here. An \n",
      "inclusive all-day primary school with gymnasium \n",
      "and canteen offers a fresh outlook in the fi  eld of \n",
      "education. The model school provides teaching for \n",
      "children with and without disability. The B³ building \n",
      "also benefi  ts from a community center, which is \n",
      "open to adults and the elderly as a meeting place for \n",
      "different generations. Its location at the heart of \n",
      "Bahnstadt illustrates the importance the future district \n",
      "attaches to education and social needs. Creating \n",
      "links between daycare facilities and primary school \n",
      "ensures a smooth transition from pre-school to primary \n",
      "school education.The importance attached to education is refl  ected \n",
      "also in the architectural design of the B³ building. \n",
      "Instead of relying on off-the-peg concepts, the de-\n",
      "velopers applied modern fi  ndings based in architec-\n",
      "tural psychology. A key role is played here by room \n",
      "and building dimensions – these establish the learning \n",
      "environment that infl  uences learners. And, of course, \n",
      "the climate-protecting passive house standard is \n",
      "also an integral aspect of building design. The B³ \n",
      "project was implemented in a public-private partner-\n",
      "ship by the municipal construction and service com-\n",
      "pany and is a project of the Heidelberg International \n",
      "Architecture Exhibition (IBA).\n",
      "The B³ community center is a central point of contact \n",
      "for residents of the district. A large hall with a stage \n",
      "provides space for events. Group and seminar rooms \n",
      "offer space for additional activities, and a public \n",
      "café encourages visitors to stay and chat.\n",
      "150\n",
      "the elementary schoolpupils\n",
      "attendFirst-class education and daycare facilities | 1920 | Short distances in the science city Short distances in the science city | 21\n",
      "Short distances in the science city\n",
      "2.2 km\n",
      "of tram lines \n",
      "through Bahnstadt\n",
      "3.5 km\n",
      "of cycle paths\n",
      "3tram stops in \n",
      "BahnstadtThe new district is situated next to Heidelberg’s main railway station. \n",
      "The station concourse has been specially extended so that platforms \n",
      "are also easily and quickly accessible from the Bahnstadt district via \n",
      "the station’s future south entrance. A more mobile district than \n",
      "Bahnstadt? – Unlikely.A new 2.2-kilometer stretch of tramway links Bahnstadt \n",
      "with the other districts of Heidelberg and various \n",
      "research locations such as the Neuenheimer Feld cam-\n",
      "pus and the Old Town. Two lines will in future connect \n",
      "the district to the city’s tram network.\n",
      "For car drivers, too, Bahnstadt is ideally situated. The \n",
      "A5 highway is just a few minutes away via the highway \n",
      "feeder road; the Walldorf highway intersection and \n",
      "SAP are around 15 minutes’ drive; and it takes around \n",
      "30 minutes to drive to Karlsruhe and 50 minutes to \n",
      "Frankfurt International Airport. Frankfurt and Stuttgart \n",
      "are also easily accessible as workplaces by rail – even \n",
      "as a daily commute.\n",
      "But in a city of short distances like Heidelberg, the bicy-\n",
      "cle is king. As a result, Bahnstadt will see the construc-tion of 3.5 kilometers of cycle paths. In addition, the \n",
      "fi  rst cycle and pedestrian bridge linking the southern \n",
      "areas of the city is already complete. An additional cycle \n",
      "bridge over the railway lines and accessing northern \n",
      "parts of the city is planned. Both bridges are compo-\n",
      "nents of the main cycle route between Neuen-\n",
      "heimer Feld and southern parts of the city.\n",
      "Long-stay and resident parking is in underground car \n",
      "parks and on individual plots – and even more is being \n",
      "done to make mobility in Bahnstadt environmentally \n",
      "compatible: Charging points for electric cars have \n",
      "been installed in underground car parks and at Gad-\n",
      "amerplatz.22 | District with world-class reference model status District with world-class reference model status | 23\n",
      "District with world-class reference \n",
      "model status\n",
      "Up to  100%  sustainable:\n",
      "All energy and heat supplied \n",
      "entirely from renewable energies.Bahnstadt is one of the world’s largest passive house settlement. \n",
      "It far exceeds all legal specifi  cations in Germany – in particular \n",
      "concerning energy-saving regulations. Bahnstadt received the \n",
      "“Passive House Award 2014” from the Passive House Institute of \n",
      "Darmstadt in recognition of its international reference model \n",
      "status. Thanks to its Bahnstadt district, Heidelberg also received \n",
      "the “Global Green City Award” in 2015 from the United Nations \n",
      "in New York.\n",
      "CO2-emission in passive house buildings is less than \n",
      "half that of conventional buildings. Constructors and \n",
      "developers are advised on energy issues in order to \n",
      "ensure compliance with the stringent requirements \n",
      "of the passive house energy standard. This includes \n",
      "advising and discussion of funding possibilities – \n",
      "the city of Heidelberg supports developers by award-\n",
      "ing grants, for example.\n",
      "Furthermore, Bahnstadt is the largest area in\n",
      "Heidelberg to be fully equipped with the smart \n",
      "meter concept. Intelligent electricity meters offer \n",
      "each household the chance to gain a better over-\n",
      "view of their energy consumption and costs. And \n",
      "it goes a step further: Bahnstadt has adopted a concept that supports natural soil functions. This \n",
      "means an increased proportion of rainwater is able \n",
      "to evaporate, improving the urban climate. In addi-\n",
      "tion, some of the rainwater can seep away into con-\n",
      "struction fi  elds. This in turn promotes local ground-\n",
      "water formation.\n",
      "The non-use of environmentally valuable outdoor areas \n",
      "also demonstrates that Bahnstadt is not just about \n",
      "building houses for the future. Furthermore, biodi-\n",
      "versity has an important role to play in Bahnstadt. \n",
      "Around 3,600 sand and wall lizards were moved to \n",
      "specially created new habitats when construction \n",
      "work started.5\n",
      "656\n",
      "5Haupt-\n",
      "bahnhofSchloss\n",
      "BahnstadtNeckar\n",
      "Karlsruhe/Basel StuttgartMannheim\n",
      "Mannheim/\n",
      "Frankfurt \n",
      "am MainFrankfurt am Main/\n",
      "Frankfurt International Airport\n",
      "Heidelberg\n",
      "© KAR TOGRAPHIE P eh & Sc hefcik0 500 1000 mIm Neuenheimer Feld\n",
      " University campus\n",
      "(2.5 km)\n",
      "Campus\n",
      "UniversityOld Town\n",
      "(3 km)24 | Facts & figures / Contact\n",
      "Facts & figures\n",
      "Living – researching – developing  \n",
      "Bahnstadt is a district that boasts residential property, research facilities, commercial and leisure activities and jobs \n",
      "in keeping with Heidelberg’s tradition. At the same time, it is one of the world’s largest passive house settlement\n",
      "and one of the biggest urban development projects in Germany.\n",
      "Location  \n",
      "Former freight and marshaling depot to the southwest of Heidelberg city center.\n",
      "The area also includes former military sites that have become available.\n",
      "Total area  102,5 hectars\n",
      "building area  61 % \n",
      "green area and open spaces   18 % \n",
      "traffic network   21 % \n",
      "First residents arrived in June 2012  \n",
      "There are currently more than 4.300 people living in Bahnstadt (as at year-middle 2019);\n",
      "Target: 6,800 residents and 3,700 residential units\n",
      "Jobs: 6,000\n",
      "Project term\n",
      "2008 to 2022\n",
      "Development trust  \n",
      "Deutsche Stadt- und Grundstücksentwicklungsgesellschaft mbH & Co. KG (DSK)\n",
      "City partners  \n",
      "Entwicklungsgesellschaft Heidelberg GmbH & Co. KG (EGH)\n",
      "Private and public investment  \n",
      "EURO 2 billion (estimate), of which around EUR 300 million on infrastructureBahnstadt research hub\n",
      "The district is centrally situated close to the city’s main \n",
      "railway station and with access to the Heidelberg tram \n",
      "network. The highway network is accessible in just a few \n",
      "minutes.Research hub | 25\n",
      "Contact\n",
      "Stadtverwaltung Heidelberg\n",
      "Geschäftsstelle Bahnstadt  \n",
      "Telefon 06221 58-20250\n",
      "bahnstadt@heidelberg.de\n",
      "Amt für Wirtschaftsförderung und Wissenschaft\n",
      "Telefon 06221 58-30000,\n",
      "wifoe@heidelberg.de\n",
      "Amt für Öffentlichkeitsarbeit  \n",
      "Telefon 06221 58-12000\n",
      "oeffentlichkeitsarbeit@heidelberg.de\n",
      "Partner\n",
      "Entwicklungsgesellschaft Heidelberg GmbH & Co. KG (EGH)\n",
      "Telefon 06221 718660\n",
      "info@egh-bahnstadt.deVertrieb Wohnen & Gewerbe\n",
      "S-Immobilien Heidelberg GmbH\n",
      "info@s-immo-hd.de  \n",
      "Telefon 06221 511-5500\n",
      "Entwicklungstreuhänder\n",
      "DSK Deutsche Stadt- und  \n",
      "Grundstücksentwicklungsgesellschaft mbH & Co. KG\n",
      "Telefon 06221 99849-20\n",
      "Research & Innovation\n",
      "Technologiepark Heidelberg GmbH\n",
      "technologiepark@heidelberg.de  \n",
      "Telefon 06221 502-5715\n",
      "www.heidelberg.de\n",
      "www.bahnstadt.heidelberg.deHeidelberg \n",
      "Innovation Park (hip)\n",
      "(1 km)26 | Publishing information\n",
      "Publishing information\n",
      "City of Heidelberg\n",
      "City Hall, Marktplatz 10\n",
      "D-69117 Heidelberg\n",
      "Text \n",
      "Public Relations Office\n",
      "Layout \n",
      "Mayor’s Office\n",
      "Photo credits  \n",
      "Christian Buck: Page 4, 5, 13, 14, 15, 16, 17, 19\n",
      "Philipp Rothe: Page 3, 18\n",
      "Klaus Venus: Page 6, 7\n",
      "DEGELO Architekten: Page 8\n",
      "Thilo Ross: Page 9\n",
      "Winking Froh Architekten BDA: Page 10, 11\n",
      "Unmüssig Bauträgergesellschaft: Page 12, 13\n",
      "City of Heidelberg: Page 16\n",
      "Steffen Diemer: Cover, Page 16, 20, 21, 22, 23\n",
      "Issue\n",
      "3rd Edition, September 2019Public Relations Office\n",
      "City of Heidelberg\n",
      "City of Heidelberg\n",
      "Marktplatz 10\n",
      "69117 Heidelberg\n",
      "Phone +49 6221 58-12000  \n",
      "Fax +49 6221 58-12900  \n",
      "oeffentlichkeitsarbeit@heidelberg.de\n",
      "www.heidelberg.de\n",
      "HeidelbergCARD\n",
      " Heidelberg in your pocket!\n",
      "Gültig ab / Valid from\n",
      "4\n",
      "Gültig ab / Valid from\n",
      "2Family\n",
      "Gültig ab / Valid from\n",
      "2\n",
      "Gültig ab / Valid from\n",
      "1Information\n",
      "and service hotline\n",
      "Please refer to the opening hours of the\n",
      "Tourist Information on our website . \n",
      "The  HDCARD  is available at:\n",
      " –Tourist Information at the main station, at the Neckarmünzplatz \n",
      "and in the town hall at the Marktplatz (Market Square)\n",
      " –Käthe Wohlfahrt store at the Universitätsplatz (University Square, \n",
      "Hauptstraße 124)\n",
      " –Heidelberg Youth Hostel (Tiergartenstraße 5) \n",
      " –numerous Heidelberg hotels+49 6221\n",
      "58-44444\n",
      "INFORMATION& TICKETS\n",
      "info@heidelberg-marketing.dewww.heidelberg-marketing.comHeidelbergCARD\n",
      "Benefit from numerous advantages in the city area with the \n",
      " HDCARD . \n",
      "The following services are included in the  HDCARD :\n",
      " –castle ticket including funicular railway\n",
      " –free travel on public transport in the city area\n",
      " –Combo ticket (one-time admission to the University  \n",
      "Museum, the Student Prison and the special exhibition)\n",
      " –many other discounts for guided tours, museums, \n",
      "leisure activities, gastronomy and shopping.\n",
      "*  HDCARD  for pupils, students and trainees up to the \n",
      "age of 28 (with ID), including discounted castle ticket.\n",
      "Partners / sponsors:1 day – € 24  | € 20 discount price *\n",
      "Valid from midnight to midnight\n",
      "on the day of validity.\n",
      "2 days  – € 26  | € 22 discount price *\n",
      "Valid all day on the first day until\n",
      "midnight the following day.\n",
      "4 days  – € 28  | € 24 discount price *\n",
      "Valid all day on the first day until \n",
      "midnight of the fourth day.  \n",
      "2 days family  – € 57\n",
      "Valid all day on the first day until \n",
      "midnight the following day for a family  \n",
      "(2 adults and up to 3 children or 1 adult \n",
      "and up to 4 children under the age of 16).\n",
      "Gültig ab / Valid from\n",
      "2Family\n",
      "Gültig ab / Valid from\n",
      "4\n",
      "Gültig ab / Valid from\n",
      "1\n",
      "Gültig ab / Valid from\n",
      "2The  HDCARD includes:\n",
      "Castle ticket + funicular railway\n",
      "The castle ticket includes admission to the castle court -\n",
      "yard, barrel cellar as well as to the German Pharmacy \n",
      "Museum, and the ride on the funicular railway to the cas -\n",
      "tle with continuation to the Molkenkur station (round \n",
      "trip including one stop).\n",
      "Opening hours Heidelberg Castle\n",
      "9:00 am – 6:00 pm (castle courtyard, barrel cellar),\n",
      "last entrance: 5:30 pm\n",
      "Opening hours\n",
      "German Pharmacy Museum\n",
      "April – October: 10:00 am – 6:00 pm, last entrance: 5:40 pm\n",
      "November – March: 10:00 am – 5:30 pm,\n",
      "last entrance: 5:10 pm\n",
      "Timetable funicular railway\n",
      "Annual funicular railway maintenance \n",
      "March 6 – March 19, 2023 \n",
      "Summer timetable\n",
      "April 1 – November 1, 2023\n",
      "Kornmarkt – Castle: 9:00 am (every 10 minutes), \n",
      "last trip: 8:00 pm\n",
      "Castle – Kornmarkt: 9:03 am (every 10 minutes), \n",
      "last trip: 8:03 pm\n",
      "Winter timetable \n",
      "November 2, 2023 – March 31, 2024\n",
      "Kornmarkt – Castle: 9:00 am (every 10 minutes), \n",
      "last trip: 5:10 pm\n",
      "Castle – Kornmarkt: 9:03 am (every 10 minutes), \n",
      "last trip: 5:43 pm\n",
      "(subject to change)Combo ticket of the university\n",
      "One-time admission to the University \n",
      "Museum, the Student Prison and the \n",
      "special exhibition.\n",
      "For more information about  \n",
      "opening hours, please click  here .\n",
      "Free rides on public transport\n",
      "The  HDCARD  entitles holders to travel on all the Rhine \n",
      "Neckar public transport system’s (VRN) \n",
      "buses, trams and authorized trains (in \n",
      "the case of DB: RE, RB and S-Bahn trains, \n",
      "all 2nd class) in the greater Heidelberg\n",
      "area (Heidelberg “Wabe 125” fare zone) \n",
      "for the duration of validity.\n",
      "Further benefits \n",
      "Dicounts for guided tours, bus tours, boat trips as \n",
      "well as museums, stores and restaurants are listed \n",
      "on the following pages. If no district is indicated, \n",
      "the service partner is located in the Old Town. City Tours and Excursions\n",
      "City and Castle Sightseeing Tour * \n",
      "A discovery tour by bus to all the major sights and points of in -\n",
      "terest in the city. The central attraction is the visit to Heidelberg \n",
      "Castle. Enjoy an exciting guided tour of this world-famous \n",
      "ruin. In addition to an exterior tour, you will explore the in -\n",
      "ner courtyard and the Great Barrel (approx. 1 hour bus tour, \n",
      "1 hour castle tour, funicular ride back to the Old Town).\n",
      "Prices  € 30, € 27 discount price **, with  HDCARD € 28 \n",
      "including castle ticket (funicular + castle courtyard admission)\n",
      "Duration  2 hours\n",
      "April – October\n",
      "Friday, Saturday and Whitsunday 1:30 pm (German)  \n",
      "There is no tour during the Old Town Festival Heidelberger\n",
      "Herbst (Heidelberg Autumn) on September 30, 2023.\n",
      "November – March  \n",
      "Saturday 1:30 pm (German)\n",
      "Meeting point  Neckarmünzplatz, information board  \n",
      "(bus stop)\n",
      "* Reservation required: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de* Reservation required: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deWalking Tour of the Old Town * \n",
      "This tour features major attractions as well as historic narrow \n",
      "alleys and squares: the many faces of Heidelberg’s Old Town, \n",
      "a whole range of magical settings, all of which have their sto -\n",
      "ries to tell.\n",
      "Prices  € 12, € 10 discount price **, with  HDCARD € 10\n",
      "Duration  1.5 hours\n",
      "April – October\n",
      "daily 10:30 am, additionally Friday 6:00 pm and Saturday \n",
      "2:30 pm (German), Thursday – Saturday 10:30 am (English)  \n",
      "There is no tour during the Old Town Festival Heidelberger \n",
      "Herbst (Heidelberg Autumn) on September 30, 2023 at 2:30 pm.\n",
      "November – March  \n",
      "Friday 2:30 pm and Saturday 10:30 am (German)\n",
      "Meeting point  Neckarmünzplatz, in front of the Tourist \n",
      "InformationChristmas Market Walking Tour * \n",
      "Discover the most beautiful corners of the Old Town during \n",
      "this entertaining guided tour and learn interesting facts \n",
      "about the Christmas market and the region’s Christmas \n",
      "traditions. The tour ends convivially at a mulled wine booth.\n",
      "Prices € 14, € 12 discount price **, with   HDCARD € 12\n",
      "Duration  1.5 hours\n",
      "Advent Saturdays   \n",
      "December 2, 9, and 16, 2023, 4:30 pm (German)\n",
      "Meeting point Neckarmünzplatz, in front of the Tourist \n",
      "Information\n",
      "* Reservation required: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deSegway Tour: Highly Philosophical  *\n",
      "Discover Heidelberg in an unusual way – driving pleasure at \n",
      "its best. Get to know the city from an entirely new perspec -\n",
      "tive.\n",
      "Prices € 59, with  HDCARD € 54 including helmet fee \n",
      "and segway licence\n",
      "Duration  1 3/4 hours\n",
      "February and November  \n",
      "daily 1:00 pm (German / English, depending on weather \n",
      "conditions)\n",
      "March – October  \n",
      "daily 9:30 am, 1:00 pm and 4:00 pm (German / English)\n",
      "Meeting point Neckarmünzplatz\n",
      "* Reservation required: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deCabriobus Sightseeing Tour\n",
      "The open roof of the convertible bus offers completely \n",
      "new perspectives! Enjoy an unrestricted view of your beau -\n",
      "tiful surroundings, even if it rains. An audio guide in mul -\n",
      "tiple languages keeps you informed.\n",
      "Prices  € 12, € 7 / € 11 discount price  *** , \n",
      "with  HDCARD € 11 \n",
      "Duration  40 minutes\n",
      "Languages German, English, Chinese, Dutch, French, Italian, \n",
      "Japanese, Korean, Russian, Spanish\n",
      "March, November and December  \n",
      "daily 10:00 am – 4:00 pm, departure every full hour\n",
      "April – October\n",
      "daily 10:00 am – 5:00 pm, departure every half and full hour \n",
      "There is no tour during the Old Town Festival Heidelberger \n",
      "Herbst (Heidelberg Autumn) on September 30, 2023.\n",
      "Meeting point  Karlsplatz (Carl’s Square), information board \n",
      "(bus stop)Naturally Heidelberg\n",
      "Neuenheimer Schweiz, Mausbachstollen, Felsenmeer, Klima-\n",
      "oase Kohlhof – not only the city itself, but also Heidelberg‘s \n",
      "natural environment has special highlights to offer. The \n",
      "nature guides of the environmental education and experience \n",
      "platform “Natürlich Heidelberg” (Naturally Heidelberg) lead \n",
      "you on the traces of the Geo-Naturepark to romantic places \n",
      "beyond the touristic hotspots. Information on topics, dates \n",
      "and meeting points can be obtained via the online booking \n",
      "portal at www.natuerlich.heidelberg.de. \n",
      "Prices  with  HDCARD € 2 reduced \n",
      "** Eligibility criteria for the reduced price: school and university students (up to \n",
      "28 years old), people with disabilities and a disabled person’s pass. One \n",
      "accompanying person of a severely disabled person with the characteristic \n",
      "“B” in the severely disabled person’s ID as well as of children and young people\n",
      "up to 18 years with a disability is free of charge. Tickets are available at our\n",
      "Tourist Information.\n",
      "*** Children younger than 6 years are free of charge, children aged 6 to 12 \n",
      "pay € 7. People with disabilities and a disabled person’s pass as well as owners \n",
      "of the HD CARD pay € 11.Leisure Time\n",
      "With the   HDCARD  you receive a 10 % discount.\n",
      "City & Quest \n",
      "City rally with a kick for your\n",
      "team, Dr. Nadja Pentzlin\n",
      "Phone +49 176 38003103\n",
      "www.cityquest-tour.de/en\n",
      "Märchenparadies \n",
      "Heidelberg\n",
      "Königstuhl 5\n",
      "www.maerchenparadies.de  \n",
      "€ 1 discount for one park entrance. \n",
      "Valid for ticket holders and their \n",
      "own children\n",
      "Radhof Bergheim  \n",
      "Bergheimer Straße 101 \n",
      "www.vbi-heidelberg.de  \n",
      "10 % discount on bike rental/repair\n",
      "Paddle Tours Heidelberg  \n",
      "Kayak tours in and around \n",
      "Heidelberg \n",
      "Phone +49 151 61466148  \n",
      "benighaus@paddle-tours.de\n",
      "www.paddle-tours.de/en\n",
      "Discount on all kayak tours\n",
      " Solar boat “Neckarsonne” \n",
      "Landing pier Karl-Theodor-\n",
      "Brücke (Alte Brücke, Old \n",
      "Bridge) \n",
      "www.hdsolarschiff.com\n",
      "Discount on the 50 minute\n",
      "Heidelberg round trip\n",
      "Weisse Flotte Heidelberg\n",
      "Landing pier  \n",
      "Stadthalle Heidelberg\n",
      "www.weisseflottehd.de\n",
      "Discount on the castle to ur from\n",
      "Heidelberg to Neckarsteinach and \n",
      "return\n",
      "Zoo Heidelberg\n",
      "“Experience life live“\n",
      "Tiergartenstraße 3 \n",
      "(Neuenheimer Feld)\n",
      "www.zoo-heidelberg.de \n",
      "10 % disc ount on the regular zoo\n",
      "entrance fee at the ticket office\n",
      "Museums and Exhibitions\n",
      "With the  HDCARD  you receive a  20 % discount on\n",
      "the admission price  at the following museums.\n",
      "Documentation and Cultural Center\n",
      "of German Sinti and Roma\n",
      "Bremeneckgasse 2\n",
      "www.sintiundroma.de \n",
      "20 % discount on events like concerts\n",
      "Körperwelten Museum (Body Worlds Museum)\n",
      "Altes Hallenbad Heidelberg\n",
      "Poststraße 36 / 5 and Bergheimer Straße 41 (Bergheim)\n",
      "www.bodyworlds.com/heidelberg\n",
      "Discount on the admission price for the single ticket for adults.\n",
      "Kurpfälzisches Museum Heidelberg \n",
      "(Palatinate Museum), Hauptstraße 97 \n",
      "www.museum.heidelberg.de \n",
      "Museum Haus Cajeth   \n",
      "Primitive Malerei – Art of Outsiders \n",
      "Haspelgasse 12\n",
      "www.cajeth.de  \n",
      "Museum Sammlung Prinzhorn (Prinzhorn Collection)\n",
      "University Psychiatric Hospital, Voßstraße 2 (Bergheim)\n",
      "www.sammlung-prinzhorn.de   \n",
      "Völkerkundemuseum VPST \n",
      "Hauptstraße 235 (Palais Weimar)\n",
      "www.voelkerkundemuseum-vpst.de\n",
      "Culture\n",
      "With the  HDCARD  you receive a 20 % discount on\n",
      "tickets.\n",
      "Augustinum Heidelberg\n",
      "Cinema, concerts, shows \n",
      "and more\n",
      "Jaspersstraße 2  \n",
      "(Emmertsgrund)\n",
      "www.augustinum.de/heidelberg\n",
      "Enjoy Jazz\n",
      "Festival for Jazz and More,\n",
      "numerous event locations\n",
      "in and around Heidelberg\n",
      "www.enjoyjazz.de/en\n",
      "Discount at the box  office for\n",
      "events that are not sold out\n",
      "International Music\n",
      "Festival\n",
      "“Heidelberger Frühling”  \n",
      "March 17 – April 15, 2023\n",
      "www.heidelberger-fruehling.de/en/\n",
      "Tickets +49 6221 5840-044 \n",
      "10 % discount on tickets onlyKulturhaus  \n",
      "Karlstorbahnhof\n",
      "Concerts, club culture,\n",
      "readings, cabaret & theater\n",
      "Marlene-Dietrich-Platz 3 \n",
      "(Südstadt)\n",
      "www.karlstorbahnhof.de\n",
      "10 % discount on event tick ets \n",
      "(excluding rentals and special events). \n",
      "The offer is redeemable at the \n",
      "evening box office Karlstorbahnhof.\n",
      "Theater and Orchestra  \n",
      "Heidelberg  \n",
      "Musical theater, concert, \n",
      "drama, dance as well as \n",
      "theater for children and \n",
      "young people and various \n",
      "festivals  \n",
      "www.theaterheidelberg.de/en /\n",
      "Tickets +49 6221 58-20000 \n",
      "The discount applies to  perfor -\n",
      "mances by the theater incl. the\n",
      "Heidelberger Schlossfestspiele\n",
      "(Heidelberg Castle Festival), not \n",
      "valid for special events\n",
      "Gastronomy\n",
      "Show your  HDCARD  and receive a discount  at the \n",
      "following providers.\n",
      "Bubble‘s Heidelberg  \n",
      "Hauptstr. 177\n",
      "www.bubbles-heidelberg.de\n",
      "2 Bubble Waffle Wra ps for the  \n",
      "price of 1\n",
      "Cocktailcafé Regie  \n",
      "Theaterstraße 2\n",
      "Phone +49 6221 652226\n",
      "www.regie-heidelberg.de\n",
      "20 % discount on cocktails\n",
      "Gasthaus Backmulde \n",
      "Schiffgasse 11\n",
      "Phone +49 6221 53660\n",
      "www.gasthaus-backmulde.de \n",
      "A glass of prosecco free of charge\n",
      "in combination with a dinner\n",
      "Hans Hirsch‘s  \n",
      "Kurpfalzbräu \n",
      "Hauptstraße 190\n",
      "Phone +49 6221 5991142\n",
      "One beer 0,3 l with each main course\n",
      "Hotel Bar at the Leonardo \n",
      "Heidelberg City Center \n",
      "Bergheimer Straße 63 \n",
      "(Bergheim)\n",
      "Phone +49 6221 5080 \n",
      "www.leonardo-hotels.com\n",
      "10 % discount at the hotel barLeo Bar at the Leonardo  \n",
      "Heidelberg\n",
      "Pleikartsförster Straße 101\n",
      "(Kirchheim) \n",
      "Phone +49 6221 7880\n",
      "www.leonardo-hotels.com\n",
      "10 % discount at the hotel bar  \n",
      "MyCurrywurst\n",
      "Rohrbacher Str. 2\n",
      "(Bergheim)\n",
      "Phone +49 6221 7250175\n",
      "www.mycurrywurst-heidelberg.de\n",
      "A large menu for the price of the \n",
      "small menu\n",
      "MyCurrywurst\n",
      "Hauptstraße 166\n",
      "Phone +49 6221 5991401\n",
      "www.mycurrywurst-heidelberg.de\n",
      "A large menu for the price of the \n",
      "small menu\n",
      "RADA Coffee & Rösterei\n",
      "Untere Straße 21\n",
      "Phone +49 6221 1805585\n",
      "www.rada-roesterei.com\n",
      "10 % on hot drinksRestaurant  \n",
      "Zum Roten Ochsen \n",
      "Hauptstraße 217\n",
      "Phone +49 6221 20977 \n",
      "www.roterochsen.de/en\n",
      "A glass of apple win e rosé as\n",
      "aperitif free of charge\n",
      "Schlemmermeyer \n",
      "Hauptstraße 31\n",
      "Phone +49 6221 20959 \n",
      "www.schlemmermeyer.de \n",
      "10 % discount on the entire rangeSchlosshotel Molkenkur\n",
      "Klingenteichstraße 31\n",
      "Phone +49 6221 654080\n",
      "www.molkenkur.de \n",
      "10 % discount at the hote l \n",
      "restaurant\n",
      "Yolicious Frozen Yogurt\n",
      "Ziegelgasse 26\n",
      "Phone +49 6221 3544670 \n",
      "www.yolicious.de\n",
      "Pay 1 small cup in cluding  \n",
      "1 topping – receive 1 basic cup \n",
      "including 2 toppings\n",
      "Shopping\n",
      "With the   HDCARD  you receive a 10 % discount on \n",
      "your purchase  from the following providers.\n",
      "Bofinger \n",
      "Hauptstr. 33\n",
      "www.niebelmode.de\n",
      "Discount on regular ite ms. Not valid \n",
      "on already discounted products.\n",
      "Bofinger Femme\n",
      "Sofienstraße 17\n",
      "(Bergheim)\n",
      "www.niebelmode.de\n",
      "Discount on regular ite ms. Not valid \n",
      "on already discounted products.\n",
      "Bofinger Men\n",
      "Sofienstraße 15\n",
      "(Bergheim)\n",
      "www.niebelmode.de\n",
      "Discount on regular ite ms. Not valid \n",
      "on already discounted products.\n",
      "Brauerei zum Klosterhof \n",
      "Stiftweg 4 (Ziegelhausen) \n",
      "www.brauerei-zum-klosterhof.de \n",
      "Applies only to organic beer\n",
      "Büro- und Schreibwaren  \n",
      "Knoblauch GmbH \n",
      "Plöck 2 \n",
      "www.ploeck2.de\n",
      "Chocolaterie St. Anna No. 1 \n",
      "St.-Anna-Gasse 1\n",
      "www.chocolaterie-st-anna.de \n",
      "Applies only to purchase of goodsEau de Wald \n",
      "Kettengasse 2\n",
      "www.eaudewald.de\n",
      "The gin can be tasted for free.\n",
      "Heidelberg Bonbon \n",
      "Manufactory \n",
      "Steingasse 4 \n",
      "www.heidelbonbon.de\n",
      "Discount on bonbons.\n",
      "Heimat – Beautiful local \n",
      "products\n",
      "Kettengasse 3 \n",
      "www.heimatheidelberg.de\n",
      "i-AM design manufactory \n",
      "& graphic studio\n",
      "Hauptstraße 152 \n",
      "www.i-am-for-you.com\n",
      "Käthe Wohlfahrt  \n",
      "Hauptstraße 124 \n",
      "www.kaethe-wohlfahrt.com \n",
      "3 % discount\n",
      "Lindt Boutique \n",
      "Marktplatz 3 \n",
      "www.lindt.de \n",
      "Excluding discounted items\n",
      "MAJA – Mieder-, Wäsche-, \n",
      "Bademoden \n",
      "Neugasse 13 \n",
      "www.maja-feine-waesche.deNapapijri Heidelberg \n",
      "Hauptstraße 100 \n",
      "www.napapijri.de\n",
      "Optik Dieterich \n",
      "Friedrich-Ebert-Platz 1 \n",
      "www.optik-dieterich.de \n",
      "Perfumery Frosch \n",
      "Hauptstraße 140\n",
      "Excluding services,  discounted \n",
      "items and selected brandsTrendy Bags Heidelberg\n",
      "Hauptstraße 179\n",
      "Discount on travel luggage\n",
      "VIU Heidelberg\n",
      "Hauptstr. 95\n",
      "www.shopviu.com\n",
      "Discount on all glasses\n",
      "Yabis Cashmere\n",
      "Untere Straße 9\n",
      "www.yabis-cashmere.de\n",
      "Tourist Information\n",
      "With the  HDCARD  you get a  10 % discount on all\n",
      "souvenirs and 30 % discount on clothes.\n",
      "Tourist Information at the main station\n",
      "Willy-Brandt-Platz 1 (Bergheim) \n",
      "Phone +49 6221 58-44444\n",
      "Tourist Information at the Neckarmünzplatz\n",
      "Neckarmünzplatz \n",
      "Phone +49 6221 58-44444\n",
      "Tourist Information in the Rathaus (town hall)\n",
      "Marktplatz\n",
      "Phone +49 6221 58-44444\n",
      "Imprint\n",
      "Heidelberg Marketing GmbH  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg\n",
      "Phone +49 6221 5840-200  \n",
      "Fax +49 6221 5840-209  \n",
      "info@heidelberg-marketing.de\n",
      "www.heidelberg-marketing.com\n",
      "The Heidelberg Marketing GmbH is\n",
      "a subsidiary of the City of Heidelberg\n",
      "Content\n",
      "Heidelberg Marketing GmbH \n",
      "Photos \n",
      "cover page, pages 6, 7, 8, 10, 11, \n",
      "12, 13, 17, 20 – Tobias Schwerdt\n",
      "page 9 – Stadtsafari\n",
      "page 14 – Augustinum Hei -\n",
      "delberg / Christian Topp \n",
      "page 15 – Theater and Orchestra \n",
      "Heidelberg / Sebastian Bühler\n",
      "page 19 – Christoph Düpper\n",
      "© Copyright 2020 – 2023  \n",
      "All contents, in particular texts, pho -\n",
      "tographs and graphics, are protected \n",
      "by copyright. Unless expressly stated \n",
      "otherwise, Heidelberg Marketing \n",
      "GmbH owns the copyright.KornmarktCabriobusCity tourNeckarmünzplatz\n",
      "Molkenkur\n",
      "KönigstuhlBergbah n\n",
      "(funicular \n",
      "railway)Karlstor/Altstadt\n",
      "Bussemergasse\n",
      "Kl. Mantelgasse\n",
      "Große MantelgasseHaspelgasseWehrsteg\n",
      "Floring.KrämergasseMittelbadgasse\n",
      "ApothekergasseFischerg .Semmelsg.\n",
      "SteingasseLeyerg.\n",
      "Obere NeckarstraßeMönchg asse\n",
      "Dreikönigsstr.Kettengasse\n",
      "Schulgasse\n",
      "Grabengasse\n",
      "Sandgasse\n",
      "Theaterstra ße\n",
      "Friedrichstr aße\n",
      "Märzgasse\n",
      "Akademiestra ße\n",
      "Neugasse\n",
      "Rohrbacher Straß eBismarckstraßeNadlerstr aße\n",
      "St. An na GasseFahrtgasse\n",
      "Thibautstra ßePfaffengasse Am BrückentorZoo\n",
      "Heiliggeiststr.\n",
      "Marstallstr aße\n",
      "Schiffgasse\n",
      "Bauamtsgasse\n",
      "Ziegelgasse\n",
      "BrunnengasseBienenstraße\n",
      "Karpfengasse\n",
      "Unter e Stra ße\n",
      "Fischmarkt\n",
      "Marsiliusplat z\n",
      "Richard -\n",
      "Hauser-\n",
      "PlatzStadthalle\n",
      "Friedrich-\n",
      "Ebert-PlatzBismar ck-\n",
      "PlatzRathaus\n",
      "KornmarktSchlangenweg\n",
      "Unter e Neckarstra ße\n",
      "Landfriedstr aßeNeuenheimer Landstr aße Ziegelhäuser Landstr aße\n",
      "NeckarstadenFriedrich-Ebert-AnlageGaisbergtunnel\n",
      "Schloss\n",
      "berg-tunnelFriedrich-Ebert-Anlag\n",
      "eKurfürsten-AnlageNeckarstaden Schurmanstr aßeUferstr aße\n",
      "Neuenheimer LandstraßePlöck\n",
      "PlöckHauptstr aße\n",
      "Bergheimer St raßeBahnhofstra\n",
      "ßeHauptstr aße\n",
      "Merianstr aße\n",
      "Ingrimstr aßeZwingerstr\n",
      "aßeUn\n",
      "t. Fauler PelzOber\n",
      "er Fauler Pelz\n",
      "Neue Schlo\n",
      "ssstr\n",
      "aße\n",
      "Theodor-Heuss-BrückeKarl-Theodor-\n",
      "Brücke\n",
      "BrückenstraßeNeue Schlossstraße\n",
      "Märc henpar adies  \n",
      "(Fairy T ale Par adise)Schlierbach\n",
      "RohrbachNeuenheim\n",
      "NeckarwieseHirschgasse\n",
      "Alte Brücke \n",
      "(Old Bridge)Fußgängerübergan g\n",
      "(pedestrian cr ossing)\n",
      "KarlsplatzNeckarmünz-\n",
      "platz\n",
      "Marktplatz\n",
      "Universitäts-\n",
      "platz\n",
      "SchlossSolar-po wered boat Neckar ferry Liselotte\n",
      "Heidelberge r\n",
      "Schloss (Castle)Neuenheimer Feld\n",
      "Kliniken (Hospital)\n",
      "Hauptbahnhof (main station)Kloster (Benedictine\n",
      "abbe y) Stift Neubur gPhilosophenweg (Philosophers‘ Walk)\n",
      "Kirchheim / WeststadtZiegelhausen\n",
      "Weisse FlotteFootpath\n",
      "Pier Bus toursPublic \n",
      "toiletsTourist  \n",
      "InformationHDCard saleonly entry  \n",
      "and exitParking\n",
      "garageMeeting pointFunicular  \n",
      "railwayBus\n",
      "parking\n",
      "Rohan Chopra, Aniruddha M. Godbole, Nipun Sadvilkar,  \n",
      "Muzaffar Bashir Shah, Sohom Ghosh, and Dwight GunningConfidently design and build your own NLP projects \n",
      "with this easy-to-understand practical guide The \n",
      "Natural  \n",
      "Language \n",
      "Processing  \n",
      "WorkshopThe Natural Language Processing Workshop\n",
      "Copyright © 2020 Packt Publishing\n",
      "All rights reserved. No part of this course may be reproduced, stored in a retrieval \n",
      "system, or transmitted in any form or by any means, without the prior written \n",
      "permission of the publisher, except in the case of brief quotations embedded in \n",
      "critical articles or reviews.\n",
      "Every effort has been made in the preparation of this course to ensure the accuracy \n",
      "of the information presented. However, the information contained in this course \n",
      "is sold without warranty, either express or implied. Neither the authors, nor Packt \n",
      "Publishing, and its dealers and distributors will be held liable for any damages caused \n",
      "or alleged to be caused directly or indirectly by this course.\n",
      "Packt Publishing has endeavored to provide trademark information about all of the \n",
      "companies and products mentioned in this course by the appropriate use of capitals. \n",
      "However, Packt Publishing cannot guarantee the accuracy of this information.\n",
      "Authors: Rohan Chopra, Aniruddha M. Godbole, Nipun Sadvilkar,  \n",
      "Muzaffar Bashir Shah, Sohom Ghosh, and Dwight Gunning\n",
      "Reviewers: Ankit Bhatia, Nagendra Nagaraj, Nimish Narang, Sumit Kumar Raj,  \n",
      "Tom Taulli, and Ankit Verma\n",
      "Managing Editor: Saumya Jha\n",
      "Acquisitions Editors: Royluis Rodrigues, Kunal Sawant, Sneha Shinde, Archie Vankar, \n",
      "and Karan Wadekar\n",
      "Production Editor: Roshan Kawale\n",
      "Editorial Board: Megan Carlisle, Samuel Christa, Mahesh Dhyani, Heather Gopsill, \n",
      "Manasa Kumar, Alex Mazonowicz, Monesh Mirpuri, Bridget Neale, Dominic Pereira, \n",
      "Shiny Poojary, Abhishek Rane, Brendan Rodrigues, Erol Staveley, Ankita Thakur, \n",
      "Nitesh Thakur, and Jonathan Wray\n",
      "First published: August 2020\n",
      "Production reference: 1130820\n",
      "ISBN: 978-1-80020-842-1\n",
      "Published by Packt Publishing Ltd.\n",
      "Livery Place, 35 Livery Street\n",
      "Birmingham B3 2PB, UKTable of Contents\n",
      "Preface   i\n",
      "Chapter 1: Introduction to Natural  \n",
      "Language Processing   1\n",
      "Introduction  ..............................................................................................  2\n",
      "History of NLP  ..........................................................................................  3\n",
      "Text Analytics and NLP  ............................................................................  3\n",
      "Exercise 1.01: Basic Text Analytics  .............................................................  5\n",
      "Various Steps in NLP  ................................................................................  8\n",
      "Tokenization  .................................................................................................  8\n",
      "Exercise 1.02: Tokenization of a Simple Sentence  ...................................  9\n",
      "PoS Tagging  .................................................................................................  10\n",
      "Exercise 1.03: PoS Tagging  ........................................................................  11\n",
      "Stop Word Removal  ...................................................................................  12\n",
      "Exercise 1.04: Stop Word Removal  ...........................................................  13\n",
      "Text Normalization  ....................................................................................  15\n",
      "Exercise 1.05: Text Normalization ............................................................  16\n",
      "Spelling Correction  ....................................................................................  17\n",
      "Exercise 1.06: Spelling Correction of a Word and a Sentence  ..............  18\n",
      "Stemming  ....................................................................................................  20\n",
      "Exercise 1.07: Using Stemming  .................................................................  20\n",
      "Lemmatization  ...........................................................................................  22\n",
      "Exercise 1.08: Extracting the Base Word Using Lemmatization  ...........  23\n",
      "Named Entity Recognition (NER)  ..............................................................  24Exercise 1.09: Treating Named Entities  ...................................................  25\n",
      "Word Sense Disambiguation  ...............................................................  26\n",
      "Exercise 1.10: Word Sense Disambiguation  ............................................  27\n",
      "Sentence Boundary Detection  ............................................................  28\n",
      "Exercise 1.11: Sentence Boundary Detection  .........................................  29\n",
      "Activity 1.01: Preprocessing of Raw Text  ................................................  30\n",
      "Kick Starting an NLP Project  ................................................................  31\n",
      "Data Collection  ...........................................................................................  32\n",
      "Data Preprocessing  ....................................................................................  32\n",
      "Feature Extraction  .....................................................................................  32\n",
      "Model Development  ..................................................................................  33\n",
      "Model Assessment  .....................................................................................  33\n",
      "Model Deployment  ....................................................................................  33\n",
      "Summary  ................................................................................................  33\n",
      "Chapter 2: Feature Extraction Methods   35\n",
      "Introduction  ...........................................................................................  36\n",
      "Types of Data  .........................................................................................  36\n",
      "Categorizing Data Based on Structure  ....................................................  37\n",
      "Categorizing Data Based on Content  ......................................................  39\n",
      "Cleaning Text Data  ................................................................................  40\n",
      "Tokenization  ...............................................................................................  41\n",
      "Exercise 2.01: Text Cleaning and Tokenization  ......................................  42\n",
      "Exercise 2.02: Extracting n-grams  ............................................................  44\n",
      "Exercise 2.03: Tokenizing Text with Keras and TextBlob  ......................  47\n",
      "Types of Tokenizers   ..................................................................................  50\n",
      "Exercise 2.04: Tokenizing Text Using Various Tokenizers  .....................  51Stemming  ....................................................................................................  58\n",
      "RegexpStemmer  .........................................................................................  58\n",
      "Exercise 2.05: Converting Words in the Present Continuous  \n",
      "Tense into Base Words with RegexpStemmer  .......................................  59\n",
      "The Porter Stemmer  ..................................................................................  60\n",
      "Exercise 2.06: Using the Porter Stemmer  ...............................................  60\n",
      "Lemmatization  ...........................................................................................  61\n",
      "Exercise 2.07: Performing Lemmatization  ..............................................  62\n",
      "Exercise 2.08: Singularizing and Pluralizing Words  ................................  63\n",
      "Language Translation  ................................................................................  64\n",
      "Exercise 2.09: Language Translation  .......................................................  65\n",
      "Stop-Word Removal  ...................................................................................  66\n",
      "Exercise 2.10: Removing Stop Words from Text  .....................................  66\n",
      "Activity 2.01: Extracting Top Keywords from the News Article  ............  67\n",
      "Feature Extraction from Texts  ............................................................  68\n",
      "Extracting General Features from Raw Text  ..........................................  68\n",
      "Exercise 2.11: Extracting General Features from Raw Text  ..................  69\n",
      "Exercise 2.12: Extracting General Features from Text  ..........................  72\n",
      "Bag of Words (BoW)  ...................................................................................  80\n",
      "Exercise 2.13: Creating a Bag of Words  ...................................................  80\n",
      "Zipf's Law   ...................................................................................................  83\n",
      "Exercise 2.14: Zipf's Law  ............................................................................  84\n",
      "Term Frequency–Inverse Document Frequency (TFIDF)  .......................  89\n",
      "Exercise 2.15: TFIDF Representation  .......................................................  89\n",
      "Finding Text Similarity – Application of Feature Extraction  ............  91\n",
      "Exercise 2.16: Calculating Text Similarity Using Jaccard  \n",
      "and Cosine Similarity  .................................................................................  92Word Sense Disambiguation Using the Lesk Algorithm  .......................  95\n",
      "Exercise 2.17: Implementing the Lesk Algorithm Using  \n",
      "String Similarity and Text Vectorization  .................................................  96\n",
      "Word Clouds  ...............................................................................................  98\n",
      "Exercise 2.18: Generating Word Clouds  ..................................................  99\n",
      "Other Visualizations  ................................................................................  102\n",
      "Exercise 2.19: Other Visualizations Dependency Parse  \n",
      "Trees and Named Entities  .......................................................................  102\n",
      "Activity 2.02: Text Visualization  ..............................................................  104\n",
      "Summary  ..............................................................................................  105\n",
      "Chapter 3: Developing a Text Classifier   107\n",
      "Introduction  .........................................................................................  108\n",
      "Machine Learning  ...............................................................................  108\n",
      "Unsupervised Learning  ...........................................................................  108\n",
      "Hierarchical Clustering  ............................................................................  110\n",
      "Exercise 3.01: Performing Hierarchical Clustering  ..............................  111\n",
      "k-means Clustering  ..................................................................................  118\n",
      "Exercise 3.02: Implementing k-means Clustering  ................................  119\n",
      "Supervised Learning  ...........................................................................  124\n",
      "Classification  ............................................................................................  124\n",
      "Logistic Regression  ..................................................................................  125\n",
      "Exercise 3.03: Text Classification – Logistic Regression  ......................  126\n",
      "Naive Bayes Classifiers  ............................................................................  130\n",
      "Exercise 3.04: Text Classification – Naive Bayes  ..................................  131\n",
      "k-nearest Neighbors  ................................................................................  136\n",
      "Exercise 3.05: Text Classification Using the k-nearest  \n",
      "Neighbors Method  ...................................................................................  137Regression  ................................................................................................  141\n",
      "Linear Regression  ....................................................................................  141\n",
      "Exercise 3.06: Regression Analysis Using Textual Data  .......................  142\n",
      "Tree Methods  ...........................................................................................  148\n",
      "Exercise 3.07: Tree-Based Methods – Decision Tree  ............................  149\n",
      "Random Forest  .........................................................................................  154\n",
      "Gradient Boosting Machine and Extreme Gradient Boost  .................  155\n",
      "Exercise 3.08: Tree-Based Methods – Random Forest  .........................  157\n",
      "Exercise 3.09: Tree-Based Methods – XGBoost  ....................................  162\n",
      "Sampling  ...................................................................................................  167\n",
      "Exercise 3.10: Sampling (Simple Random, Stratified,  \n",
      "and Multi-Stage)  .......................................................................................  168\n",
      "Developing a Text Classifier  ..............................................................  173\n",
      "Feature Extraction  ...................................................................................  173\n",
      "Feature Engineering  ................................................................................  173\n",
      "Removing Correlated Features  ..............................................................  173\n",
      "Exercise 3.11: Removing Highly Correlated Features (Tokens)  ..........  174\n",
      "Dimensionality Reduction .......................................................................  179\n",
      "Exercise 3.12: Performing Dimensionality Reduction  \n",
      "Using Principal Component Analysis  .....................................................  180\n",
      "Deciding on a Model Type  .......................................................................  185\n",
      "Evaluating the Performance of a Model  ...............................................  186\n",
      "Exercise 3.13: Calculating the RMSE and MAPE of a Dataset  ..............  189\n",
      "Activity 3.01: Developing End-to-End Text Classifiers  .........................  191\n",
      "Building Pipelines for NLP Projects  ..................................................  192\n",
      "Exercise 3.14: Building the Pipeline for an NLP Project  ......................  192\n",
      "Saving and Loading Models  ...............................................................  194Exercise 3.15: Saving and Loading Models  ............................................  194\n",
      "Summary  ..............................................................................................  198\n",
      "Chapter 4: Collecting Text Data with  \n",
      "Web Scraping and APIs   201\n",
      "Introduction  .........................................................................................  202\n",
      "Collecting Data by Scraping Web Pages  ...........................................  202\n",
      "Exercise 4.01: Extraction of Tag-Based Information from  \n",
      "HTML Files  .................................................................................................  204\n",
      "Requesting Content from Web Pages  ...................................................  208\n",
      "Exercise 4.02: Collecting Online Text Data  ............................................  208\n",
      "Exercise 4.03: Analyzing the Content of Jupyter Notebooks  \n",
      "(in HTML Format)  .....................................................................................  211\n",
      "Activity 4.01: Extracting Information from an Online HTML Page  ..... 214\n",
      "Activity 4.02: Extracting and Analyzing Data Using  \n",
      "Regular Expressions  ................................................................................  215\n",
      "Dealing with Semi-Structured Data  ..................................................  216\n",
      "JSON  ...........................................................................................................  216\n",
      "Exercise 4.04: Working with JSON Files  .................................................  218\n",
      "XML  ............................................................................................................  220\n",
      "Exercise 4.05: Working with an XML File  ...............................................  222\n",
      "Using APIs to Retrieve Real-Time Data  ..................................................  224\n",
      "Exercise 4.06: Collecting Data Using APIs  .............................................  224\n",
      "Extracting data from Twitter Using the OAuth API ..............................  226\n",
      "Activity 4.03: Extracting Data from Twitter  ..........................................  228\n",
      "Summary  ..............................................................................................  229Chapter 5: Topic Modeling   231\n",
      "Introduction  .........................................................................................  232\n",
      "Topic Discovery  ...................................................................................  232\n",
      "Exploratory Data Analysis  .......................................................................  233\n",
      "Transforming Unstructured Data to Structured Data .........................  233\n",
      "Bag of Words  ............................................................................................  234\n",
      "Topic-Modeling Algorithms  ................................................................  235\n",
      "Latent Semantic Analysis (LSA)  ..............................................................  235\n",
      "LSA – How It Works  ..................................................................................  236\n",
      "Key Input Parameters for LSA Topic Modeling ................................  237\n",
      "Exercise 5.01: Analyzing Wikipedia World Cup Articles  \n",
      "with Latent Semantic Analysis  ...............................................................  238\n",
      "Dirichlet Process and Dirichlet Distribution   ........................................  244\n",
      "Latent Dirichlet Allocation (LDA)  ............................................................  245\n",
      "LDA – How It Works  .................................................................................  245\n",
      "Measuring the Predictive Power of a Generative Topic Model  ..........  246\n",
      "Exercise 5.02: Finding Topics in Canadian Open Data  \n",
      "Inventory Using the LDA Model  .............................................................  247\n",
      "Activity 5.01: Topic-Modeling Jeopardy Questions  ..............................  252\n",
      "Hierarchical Dirichlet Process (HDP)  ................................................  253\n",
      "Exercise 5.03: Topics in Around the World in Eighty Days  ..................  254\n",
      "Exercise 5.04: Topics in The Life and Adventures  \n",
      "of Robinson Crusoe by Daniel Defoe  .....................................................  259\n",
      "Practical Challenges  .................................................................................  266\n",
      "State-of-the-Art Topic Modeling  .............................................................  266\n",
      "Activity 5.02: Comparing Different Topic Models  ................................  267\n",
      "Summary  ..............................................................................................  268Chapter 6: Vector Representation   271\n",
      "Introduction  .........................................................................................  272\n",
      "What Is a Vector?  ................................................................................  272\n",
      "Frequency-Based Embeddings  ...............................................................  273\n",
      "Exercise 6.01: Word-Level One-Hot Encoding  .......................................  277\n",
      "Character-Level One-Hot Encoding  .......................................................  283\n",
      "Exercise 6.02: Character One-Hot Encoding – Manual  ........................  284\n",
      "Exercise 6.03: Character-Level One-Hot Encoding with Keras  ...........  286\n",
      "Learned Word Embeddings  ....................................................................  293\n",
      "Word2Vec  ..................................................................................................  293\n",
      "Exercise 6.04: Training Word Vectors  ....................................................  294\n",
      "Using Pre-Trained Word Vectors  ............................................................  301\n",
      "Exercise 6.05: Using Pre-Trained Word Vectors  ...................................  302\n",
      "Document Vectors  ...................................................................................  309\n",
      "Uses of Document Vectors  .....................................................................  310\n",
      "Exercise 6.06: Converting News Headlines to Document Vectors  ..... 310\n",
      "Activity 6.01: Finding Similar News Article Using  \n",
      "Document Vectors  ...................................................................................  316\n",
      "Summary  ..............................................................................................  316\n",
      "Chapter 7: Text Generation and Summarization   319\n",
      "Introduction  .........................................................................................  320\n",
      "Generating Text with Markov Chains  ...............................................  320\n",
      "Markov Chains  ..........................................................................................  320\n",
      "Exercise 7.01: Text Generation Using a Random Walk  \n",
      "over a Markov Chain ................................................................................  322Text Summarization  ...........................................................................  327\n",
      "TextRank  ...................................................................................................  327\n",
      "Key Input Parameters for TextRank  .................................................  329\n",
      "Exercise 7.02: Performing Summarization Using TextRank  ...............  329\n",
      "Exercise 7.03: Summarizing a Children's Fairy Tale  \n",
      "Using TextRank  ........................................................................................  333\n",
      "Activity 7.01: Summarizing Complaints in the Consumer  \n",
      "Financial Protection Bureau Dataset  ....................................................  337\n",
      "Recent Developments in Text Generation and Summarization  ... 338\n",
      "Practical Challenges in Extractive Summarization  .........................  340\n",
      "Summary  ..............................................................................................  340\n",
      "Chapter 8: Sentiment Analysis   343\n",
      "Introduction  .........................................................................................  344\n",
      "Why Is Sentiment Analysis Required?  ...................................................  344\n",
      "The Growth of Sentiment Analysis  ........................................................  345\n",
      "The Monetization of Emotion  .................................................................  345\n",
      "Types of Sentiments  ................................................................................  345\n",
      "Emotion  .............................................................................................................. 345\n",
      "Key Ideas and Terms  ...............................................................................  347\n",
      "Applications of Sentiment Analysis  .......................................................  348\n",
      "Tools Used for Sentiment Analysis  ...................................................  349\n",
      "NLP Services from Major Cloud Providers  ............................................  349\n",
      "Online Marketplaces  ...............................................................................  350\n",
      "Python NLP Libraries  ...............................................................................  350\n",
      "Deep Learning Frameworks  ...................................................................  351The textblob library  ............................................................................  352\n",
      "Exercise 8.01: Basic Sentiment Analysis Using  \n",
      "the textblob Library  .................................................................................  352\n",
      "Activity 8.01: Tweet Sentiment Analysis Using  \n",
      "the textblob library  ..................................................................................  354\n",
      "Understanding Data for Sentiment Analysis  ...................................  356\n",
      "Exercise 8.02: Loading Data for Sentiment Analysis ............................  356\n",
      "Training Sentiment Models  ...............................................................  360\n",
      "Activity 8.02: Training a Sentiment Model Using TFIDF  \n",
      "and Logistic Regression  ...........................................................................  361\n",
      "Summary  ..............................................................................................  362\n",
      "Appendix   365\n",
      "Index   425Prefaceii | Preface\n",
      "About the Book\n",
      "Do you want to learn how to communicate with computer systems using Natural \n",
      "Language Processing (NLP) techniques, or make a machine understand human \n",
      "sentiments? Do you want to build applications like Siri, Alexa, or chatbots, even if \n",
      "you've never done it before?  \n",
      "With The Natural Language Processing Workshop , you can expect to make consistent \n",
      "progress as a beginner, and get up to speed in an interactive way, with the help of \n",
      "hands-on activities and fun exercises. \n",
      "The book starts with an introduction to NLP. You'll study different approaches to \n",
      "NLP tasks, and perform exercises in Python to understand the process of preparing \n",
      "datasets for NLP models. Next, you'll use advanced NLP algorithms and visualization \n",
      "techniques to collect datasets from open websites, and to summarize and generate \n",
      "random text from a document. In the final chapters, you'll use NLP to create a chatbot \n",
      "that detects positive or negative sentiment in text documents such as movie reviews.  \n",
      "By the end of this book, you'll be equipped with the essential NLP tools \n",
      "and techniques you need to solve common business problems that involve \n",
      "processing text.\n",
      "Audience\n",
      "This book is for beginner to mid-level data scientists, machine learning developers, \n",
      "and NLP enthusiasts. A basic understanding of machine learning and NLP is required \n",
      "to help you grasp the topics in this workshop more quickly.\n",
      "About the Chapters\n",
      "Chapter 1 , Introduction to Natural Language Processing , starts by defining natural \n",
      "language processing and the different types of natural language processing tasks, \n",
      "using practical examples for each type. This chapter also covers the process of \n",
      "structuring and implementing a natural language processing project.\n",
      "Chapter 2 , Feature Extraction Methods , covers basic feature extraction methods \n",
      "from unstructured text. These include tokenization, stemming, lemmatization, and \n",
      "stopword removal. We also discuss observations we might see from these extraction \n",
      "methods and introduce Zipf's Law. Finally, we discuss the Bag of Words model and \n",
      "Term Frequency-Inverse Document Frequency (TF-IDF).About the Book | iii\n",
      "Chapter 3 , Developing a Text Classifier , teaches you how to create a simple text \n",
      "classifier with feature extraction methods covered in the previous chapters.\n",
      "Chapter 4 , Collecting Text Data with Web Scraping and APIs , introduces you to web \n",
      "scraping and discusses various methods of collecting and processing text data from \n",
      "online sources, such as HTML and XML files and APIs.\n",
      "Chapter 5 , Topic Modeling , introduces topic modeling, an unsupervised natural \n",
      "language processing technique that groups documents according to topic. You will \n",
      "see how this is done using Latent Dirichlet Allocation (LDA), Latent Semantic Analysis \n",
      "(LSA), and Hierarchical Dirichlet Processes (HDP). \n",
      "Chapter 6 , Vector Representation , discusses the importance of representing text as \n",
      "vectors, and various vector representations, such as Word2Vec and Doc2Vec.\n",
      "Chapter 7 , Text Generation and Summarization,  teaches you two simple natural \n",
      "language processing tasks: creating text summaries and generating random text with \n",
      "statistical assumptions and algorithms.\n",
      "Chapter 8 , Sentiment Analysis , teaches you how to detect sentiment in text, using \n",
      "simple techniques. Sentiment analysis is the use of computer algorithms to detect \n",
      "whether the sentiment of text is positive or negative.\n",
      "Conventions\n",
      "Code words in text, database table names, folder names, filenames, file extensions, \n",
      "pathnames, dummy URLs, user input, and Twitter handles are shown as follows: \n",
      "\"We find that the summary for the Wikipedia article is much more coherent than the \n",
      "short story. We can also see that the summary with a ratio  of 0.20  is a subset of a \n",
      "summary with a ratio  of 0.25 .\"\n",
      "Words that you see on the screen, for example, in menus or dialog boxes, also appear \n",
      "in the text like this: \"On this page, click on Keys  option to access the secret keys.\"\n",
      "A block of code is set as follows:\n",
      "text_after_twenty=text_after_twenty.replace('\\n',' ')\n",
      "text_after_twenty=re.sub(r\"\\s+\",\" \",text_after_twenty)\n",
      "New terms and important words are shown like this: \"A Markov chain  consists of a \n",
      "state space and a specific type of successor function.\"iv | Preface\n",
      "Long code snippets are truncated and the corresponding names of the code files on \n",
      "GitHub are placed at the top of the truncated code. The permalinks to the entire code \n",
      "are placed below the code snippet. It should look as follows:\n",
      "Exercise 7.01.ipynb\n",
      "1  HANDLE = '@\\w+\\n'\n",
      "2  LINK = 'https?://t\\.co/\\w+'\n",
      "3  SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "4  PARA='\\n+'\n",
      "5  def clean(text):\n",
      "6      #text = re.sub(HANDLE, ' ', text)\n",
      "7      text = re.sub(LINK, ' ', text)\n",
      "8      text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "9      text = re.sub(PARA, '\\n', text)\n",
      "The full code can be found at https://packt.live/2D7RPPZ .\n",
      "Code Presentation\n",
      "Lines of code that span multiple lines are split using a backslash ( \\ ). When the code \n",
      "is executed, Python will ignore the backslash, and treat the code on the next line as a \n",
      "direct continuation of the current line.\n",
      "For example:\n",
      "history = model.fit(X, y, epochs=100, batch_size=5, verbose=1, \\\n",
      "                    validation_split=0.2, shuffle=False)\n",
      "Comments are added into code to help explain specific bits of logic. Single-line \n",
      "comments are denoted using the # symbol, as follows:\n",
      "# Print the sizes of the dataset\n",
      "print(\"Number of Examples in the Dataset = \", X.shape[0])\n",
      "print(\"Number of Features for each example = \", X.shape[1])\n",
      "Multi-line comments are enclosed by triple quotes, as shown below:\n",
      "\"\"\"\n",
      "Define a seed for the random number generator to ensure the \n",
      "result will be reproducible\n",
      "\"\"\"\n",
      "seed = 1\n",
      "np.random.seed(seed)\n",
      "random.set_seed(seed)About the Book | v\n",
      "Setting up Your Environment\n",
      "Before we explore the book in detail, we need to set up specific software and tools. In \n",
      "the following section, we shall see how to do that.\n",
      "Installation and Setup\n",
      "Jupyter notebooks are available once you install Anaconda on your system. Anaconda \n",
      "can be installed for Windows systems using the steps available at https://docs.\n",
      "anaconda.com/anaconda/install/windows/ .\n",
      "For other systems, navigate to the respective installation guide from https://docs.\n",
      "anaconda.com/anaconda/install/ .\n",
      "These installations will be executed in the C drive of your system. You can choose to \n",
      "change the destination.\n",
      "Installing the Required Libraries\n",
      "Open Anaconda Prompt and follow the steps given here to get your system ready. \n",
      "We will create a new environment on Anaconda where we will install all the required \n",
      "libraries and run our code:\n",
      "1. To create a new environment, run the following command:\n",
      "conda create --name nlp\n",
      "2. To activate the environment, type the following:\n",
      "conda activate nlp\n",
      "For this course, whenever you are asked to open a terminal, you need to open \n",
      "Anaconda Prompt, activate the environment, and then proceed.\n",
      "3. To install all the libraries, download the environment file from  \n",
      "https://packt.live/30qfL9V  and run the following command:\n",
      "pip install -f requirements.txt\n",
      "4. Jupyter notebooks allow us to run code and experiment with code blocks. To \n",
      "start Jupyter Notebook, run the following inside the nlp  environment:\n",
      "jupyter notebook\n",
      "A new browser window will open up with the Jupyter interface. You can now \n",
      "navigate to the project location and run Jupyter Notebook.vi | Preface\n",
      "Installing Libraries\n",
      "pip  comes pre-installed with Anaconda. Once Anaconda is installed on your \n",
      "machine, all the required libraries can be installed using pip , for example, pip \n",
      "install numpy . Alternatively, you can install all the required libraries using pip \n",
      "install –r requirements.txt . You can find the requirements.txt  file at \n",
      "https://packt.live/39RZuOh .\n",
      "The exercises and activities will be executed in Jupyter Notebooks. Jupyter is a \n",
      "Python library and can be installed in the same way as the other Python libraries – \n",
      "that is, with pip install jupyter , but fortunately, it comes pre-installed with \n",
      "Anaconda. To open a notebook, simply run the command jupyter notebook  in \n",
      "the Terminal or Command Prompt.\n",
      "Accessing the Code Files\n",
      "You can find the complete code files of this book at https://packt.live/3fJ4qap . You can \n",
      "also run many activities and exercises directly in your web browser by using the \n",
      "interactive lab environment at https://packt.live/3gwk4WQ .\n",
      "We've tried to support interactive versions of all activities and exercises, but we \n",
      "recommend a local installation as well for instances where this support isn't available.\n",
      "If you have any issues or questions about installation, please email us at \n",
      "workshops@packt.com .Overview\n",
      "In this chapter, you will learn the difference between Natural Language \n",
      "Processing  (NLP) and basic text analytics. You will implement various \n",
      "preprocessing tasks such as tokenization, lemmatization, stemming, stop \n",
      "word removal, and more. By the end of this chapter, you will have a deep \n",
      "understanding of the various phases of an NLP project, from data collection \n",
      "to model deployment.Introduction to Natural \n",
      "Language Processing12 | Introduction to Natural Language Processing\n",
      "Introduction\n",
      "Before we can get into NLP in any depth, we first need to understand what natural \n",
      "language is. To put it in simple terms, it is a means for us to express our thoughts \n",
      "and ideas. To define it more specifically, language is a mutually agreed upon set of \n",
      "protocols involving words/sounds that we use to communicate with each other.\n",
      "In this era of digitization and computation, we are constantly interacting with \n",
      "machines around us through various means, such as voice commands and \n",
      "typing instructions in the form of words. Thus, it has become essential to develop \n",
      "mechanisms by which human language can be comprehended accurately by \n",
      "computers. NLP helps us do this. So, NLP can be defined as a field of computer \n",
      "science that is concerned with enabling computer algorithms to understand, analyze, \n",
      "and generate natural languages.\n",
      "Let's look at an example. You have probably interacted with Siri or Alexa at some \n",
      "point. Ask Alexa for a cricket score, and it will reply with the current score. The \n",
      "technology behind this is NLP. Siri and Alexa use techniques such as Speech to Text \n",
      "with the help of a search engine to do this magic. As the name suggests, Speech to \n",
      "Text is an application of NLP in which computers are trained to understand verbally \n",
      "spoken words.\n",
      "NLP works at different levels, which means that machines process and understand \n",
      "natural language at different levels. These levels are as follows:\n",
      "• Morphological level : This level deals with understanding word structure and \n",
      "word information.\n",
      "• Lexical level : This level deals with understanding the part of speech of the word.\n",
      "• Syntactic level : This level deals with understanding the syntactic analysis of a \n",
      "sentence, or parsing a sentence.\n",
      "• Semantic level : This level deals with understanding the actual meaning  \n",
      "of a sentence.\n",
      "• Discourse level : This level deals with understanding the meaning of a sentence \n",
      "beyond just the sentence level, that is, considering the context.\n",
      "• Pragmatic level : This level deals with using real-world knowledge to understand \n",
      "the sentence.History of NLP | 3\n",
      "History of NLP\n",
      "NLP is a field that has emerged from various other fields such as artificial intelligence, \n",
      "linguistics, and data science. With the advancement of computing technologies and \n",
      "the increased availability of data, NLP has undergone a huge change. Previously, \n",
      "a traditional rule-based system was used for computations, in which you had to \n",
      "explicitly write hardcoded rules. Today, computations on natural language are being \n",
      "done using machine learning and deep learning techniques.\n",
      "Consider an example. Let's say we have to extract the names of some politicians from \n",
      "a set of political news articles. So, if we want to apply rule-based grammar, we must \n",
      "manually craft certain rules based on human understanding of language. Some of the \n",
      "rules for extracting a person's name can be that the word should be a proper noun, \n",
      "every word should start with a capital letter, and so on. As we can see, using a rule-\n",
      "based system like this would not yield very accurate results.\n",
      "Rule-based systems do work well in some cases, but the disadvantages far outweigh \n",
      "the advantages. One major disadvantage is that the same rule cannot be applicable \n",
      "in all cases, given the complex and nuanced nature of most language. These \n",
      "disadvantages can be overcome by using machine learning, where we write an \n",
      "algorithm that tries to learn a language using the text corpus (training data) rather \n",
      "than us explicitly programming it to do so.\n",
      "Text Analytics and NLP\n",
      "Text analytics  is the method of extracting meaningful insights and answering \n",
      "questions from text data, such as those to do with the length of sentences, length of \n",
      "words, word count, and finding words from the text. Let's understand this with  \n",
      "an example.\n",
      "Suppose we are doing a survey using news articles. Let's say we have to find the top \n",
      "five countries that contributed the most in the field of space technology in the past 5 \n",
      "years. So, we will collect all the space technology-related news from the past 5 years \n",
      "using the Google News API. Now, we must extract the names of countries in these \n",
      "news articles. We can perform this task using a file containing a list of all the countries \n",
      "in the world.4 | Introduction to Natural Language Processing\n",
      "Next, we will create a dictionary in which keys will be the country names and their \n",
      "values will be the number of times the country name is found in the news articles. To \n",
      "search for a country in the news articles, we can use a simple word regex. After we \n",
      "have completed searching all the news articles, we can sort the country names by the \n",
      "values associated with them. In this way, we will come up with the top five countries \n",
      "that contributed the most to space technology in the last 5 years.\n",
      "This is a typical example of text analytics, in which we are generating insights from \n",
      "text without getting into the semantics of the language.\n",
      "It is important here to note the difference between text analytics and NLP. The art of \n",
      "extracting useful insights from any given text data can be referred to as text analytics. \n",
      "NLP, on the other hand, helps us in understanding the semantics and the underlying \n",
      "meaning of text, such as the sentiment of a sentence, top keywords in text, and parts \n",
      "of speech for different words. It is not just restricted to text data; voice (speech) \n",
      "recognition and analysis also come under the domain of NLP. It can be broadly \n",
      "categorized into two types: Natural Language Understanding  (NLU ) and Natural \n",
      "Language Generation  (NLG ). A proper explanation of these terms is provided here:\n",
      "• NLU : NLU refers to a process by which an inanimate object with computing \n",
      "power is able to comprehend spoken language. As mentioned earlier, Siri and \n",
      "Alexa use techniques such as Speech to Text to answer different questions, \n",
      "including inquiries about the weather, the latest news updates, live match \n",
      "scores, and more. \n",
      "• NLG : NLG refers to a process by which an inanimate object with computing \n",
      "power is able to communicate with humans in a language that they can \n",
      "understand or is able to generate human-understandable text from a dataset. \n",
      "Continuing with the example of Siri or Alexa, ask one of them about the chances \n",
      "of rainfall in your city. It will reply with something along the lines of, \"Currently, \n",
      "there is no chance of rainfall in your city.\" It gets the answer to your query from \n",
      "different sources using a search engine and then summarizes the results. Then, \n",
      "it uses Text to Speech to relay the results in verbally spoken words.\n",
      "So, when a human speaks to a machine, the machine interprets the language with \n",
      "the help of the NLU process. By using the NLG process, the machine generates an \n",
      "appropriate response and shares it with the human, thus making it easier for humans \n",
      "to understand the machine. These tasks, which are part of NLP, are not part of text \n",
      "analytics. Let's walk through the basics of text analytics and see how we can execute \n",
      "it in Python.Text Analytics and NLP | 5\n",
      "Before going to the exercises, let's define some prerequisites for running the \n",
      "exercises. Whether you are using Windows, Mac or Linux, you need to run your \n",
      "Jupyter Notebook in a virtual environment. You will also need to ensure that  \n",
      "you have installed the requirements as stated in the requirements.txt  file on  \n",
      "https://packt.live/3fJ4qap .\n",
      "Exercise 1.01: Basic Text Analytics\n",
      "In this exercise, we will perform some basic text analytics on some given text data, \n",
      "including searching for a particular word, finding the index of a word, and finding \n",
      "a word at a given position. Follow these steps to implement this exercise using the \n",
      "following sentence:\n",
      "\"The quick brown fox jumps over the lazy dog.\"\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Assign a sentence  variable the value 'The quick brown fox jumps \n",
      "over the lazy dog' . Insert a new cell and add the following code to \n",
      "implement this:\n",
      "sentence = 'The quick brown fox jumps over the lazy dog'\n",
      "sentence\n",
      "3. Check whether the word 'quick'  belongs to that text using the following code:\n",
      "def find_word(word, sentence):\n",
      "    return word in sentence\n",
      "find_word('quick', sentence)\n",
      "The preceding code will return the output 'True' .\n",
      "4. Find out the index  value of the word 'fox'  using the following code:\n",
      "def get_index(word, text):\n",
      "    return text.index(word)\n",
      "get_index('fox', sentence)\n",
      "The code will return the output 16.6 | Introduction to Natural Language Processing\n",
      "5. To find out the rank of the word 'lazy' , use the following code:\n",
      "get_index('lazy', sentence.split())\n",
      "This code generates the output 7.\n",
      "6. To print the third word of the given text, use the following code:\n",
      "def get_word(text,rank):\n",
      "    return text.split()[rank]\n",
      "get_word(sentence,2)\n",
      "This will return the output brown .\n",
      "7. To print the third word of the given sentence in reverse order, use the  \n",
      "following code:\n",
      "get_word(sentence,2)[::-1]\n",
      "This will return the output nworb .\n",
      "8. To concatenate the first and last words of the given sentence, use the  \n",
      "following code:\n",
      "def concat_words(text):\n",
      "    \"\"\"\n",
      "    This method will concat first and last \n",
      "    words of given text\n",
      "    \"\"\"\n",
      "    words = text.split()\n",
      "    first_word = words[0]\n",
      "    last_word = words[len(words)-1]\n",
      "    return first_word + last_word\n",
      "concat_words(sentence)\n",
      "Note\n",
      "The triple-quotes ( \"\"\" ) shown in the code snippet above are used to \n",
      "denote the start and end points of a multi-line code comment. Comments \n",
      "are added into code to help explain specific bits of logic. \n",
      "The code will generate the output Thedog .Text Analytics and NLP | 7\n",
      "9. To print words at even positions, use the following code:\n",
      "def get_even_position_words(text):\n",
      "    words = text.split()\n",
      "    return [words[i] for i in range(len(words)) if i%2 == 0]\n",
      "get_even_position_words(sentence)\n",
      "This code generates the following output:\n",
      "['The', 'brown', 'jumps', 'the', 'dog']\n",
      "10. To print the last three letters of the text, use the following code:\n",
      "def get_last_n_letters(text, n):\n",
      "    return text[-n:]\n",
      "get_last_n_letters(sentence,3)\n",
      "This will generate the output dog .\n",
      "11. To print the text in reverse order, use the following code:\n",
      "def get_reverse(text):\n",
      "    return text[::-1]\n",
      "get_reverse(sentence)\n",
      "This code generates the following output:\n",
      "'god yzal eht revo spmuj xof nworb kciuq ehT'\n",
      "12. To print each word of the given text in reverse order, maintaining their \n",
      "sequence, use the following code:\n",
      "def get_word_reverse(text):\n",
      "    words = text.split()\n",
      "    return ' '.join([word[::-1] for word in words])\n",
      "get_word_reverse(sentence)\n",
      "This code generates the following output:\n",
      "ehT kciuq nworb xof spmuj revo eht yzal god8 | Introduction to Natural Language Processing\n",
      "We are now well acquainted with basic text analytics techniques.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/38Yrf77 .\n",
      "You can also run this example online at https://packt.live/2ZsCvpf .\n",
      "In the next section, let's dive deeper into the various steps and subtasks in NLP.\n",
      "Various Steps in NLP\n",
      "We've talked about the types of computations that are done with natural language. \n",
      "Apart from these basic tasks, you can also design your own tasks as per your \n",
      "requirements. In the coming sections, we will discuss the various preprocessing tasks \n",
      "in detail and demonstrate each of them with an exercise. \n",
      "To perform these tasks, we will be using a Python library called NLTK  (Natural \n",
      "Language Toolkit ). NLTK is a powerful open source tool that provides a set of \n",
      "methods and algorithms to perform a wide range of NLP tasks, including tokenizing, \n",
      "parts-of-speech tagging, stemming, lemmatization, and more.\n",
      "Tokenization\n",
      "Tokenization  refers to the procedure of splitting a sentence into its constituent \n",
      "parts—the words and punctuation that it is made up of. It is different from simply \n",
      "splitting the sentence on whitespaces, and instead actually divides the sentence \n",
      "into constituent words, numbers (if any), and punctuation, which may not always \n",
      "be separated by whitespaces. For example, consider this sentence: \"I am reading a \n",
      "book.\" Here, our task is to extract words/tokens from this sentence. After passing this \n",
      "sentence to a tokenization program, the extracted words/tokens would be \"I,\" \"am,\" \n",
      "\"reading,\" \"a,\" \"book,\" and \".\" – this example extracts one token at a time. Such tokens \n",
      "are called unigrams . \n",
      "NLTK provides a method called word_tokenize() , which tokenizes given text into \n",
      "words. It actually separates the text into different words based on punctuation and \n",
      "spaces between words.\n",
      "To get a better understanding of tokenization, let's solve an exercise based on it in \n",
      "the next section.Various Steps in NLP | 9\n",
      "Exercise 1.02: Tokenization of a Simple Sentence\n",
      "In this exercise, we will tokenize the words in a given sentence with the help of the \n",
      "NLTK  library. Follow these steps to implement this exercise using the sentence, \"I am \n",
      "reading NLP Fundamentals.\"\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries \n",
      "and download the different types of NLTK data that we are going to use for \n",
      "different tasks in the following exercises:\n",
      "from nltk import word_tokenize, download\n",
      "download(['punkt','averaged_perceptron_tagger','stopwords'])\n",
      "In the preceding code, we are using NLTK's download()  method, which \n",
      "downloads the given data from NLTK. NLTK data contains different corpora \n",
      "and trained models. In the preceding example, we will be downloading the stop \n",
      "word list, 'punkt' , and a perceptron tagger, which is used to implement parts \n",
      "of speech tagging using a structured algorithm. The data will be downloaded at \n",
      "nltk_data/corpora/  in the home directory of your computer. Then, it will \n",
      "be loaded from the same path in further steps.\n",
      "3. The word_tokenize()  method is used to split the sentence into words/\n",
      "tokens. We need to add a sentence as input to the word_tokenize()  method \n",
      "so that it performs its job. The result obtained will be a list, which we will store in \n",
      "a word  variable. To implement this, insert a new cell and add the following code:\n",
      "def get_tokens(sentence):\n",
      "    words = word_tokenize(sentence)\n",
      "    return words\n",
      "4. In order to view the list of tokens generated, we need to view it using the \n",
      "print()  function. Insert a new cell and add the following code to  \n",
      "implement this:\n",
      "print(get_tokens(\"I am reading NLP Fundamentals.\"))\n",
      "This code generates the following output:\n",
      "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']10 | Introduction to Natural Language Processing\n",
      "We can see the list of tokens generated with the help of the  \n",
      "word_tokenize()  method.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/30bGG85 .\n",
      "You can also run this example online at https://packt.live/30dK1mZ .\n",
      "In the next section, we will see another pre-processing step:  \n",
      "Parts-of-Speech (PoS) tagging .\n",
      "PoS Tagging\n",
      "In NLP, the term PoS refers to parts of speech. PoS tagging refers to the process \n",
      "of tagging words within sentences with their respective PoS. We extract the PoS of \n",
      "tokens constituting a sentence so that we can filter out the PoS that are of interest \n",
      "and analyze them. For example, if we look at the sentence, \"The sky is blue,\" we get \n",
      "four tokens, namely \"The,\" \"sky,\" \"is,\" and \"blue\", with the help of tokenization. Now, \n",
      "using a PoS tagger , we tag the PoS for each word/token. This will look as follows:\n",
      "[('The', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('blue', 'JJ')]\n",
      "The preceding format is an output of the NLTK pos_tag() method. It is a list of \n",
      "tuples in which every tuple consists of the word followed by the PoS tag:\n",
      "DT = Determiner\n",
      "NN = Noun, common, singular or mass\n",
      "VBZ  = Verb, present tense, third-person singular\n",
      "JJ = Adjective\n",
      "For the complete list of PoS tags in NLTK, you can refer  \n",
      "to https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/ .\n",
      "PoS tagging is performed using different techniques, one of which is a rule-based \n",
      "approach that builds a list to assign a possible tag for each word.Various Steps in NLP | 11\n",
      "PoS tagging finds application in many NLP tasks, including word sense \n",
      "disambiguation, classification, Named Entity Recognition  (NER ), and coreference \n",
      "resolution. For example, consider the usage of the word \"planted\" in these two \n",
      "sentences: \"He planted the evidence for the case \" and \" He planted five trees in \n",
      "the garden. \" We can see that the PoS tag of \"planted\" would clearly help us in \n",
      "differentiating between the different meanings of the sentences.\n",
      "Let's perform a simple exercise to understand how PoS tagging is done in Python.\n",
      "Exercise 1.03: PoS Tagging\n",
      "In this exercise, we will find out the PoS for each word in the sentence, I am \n",
      "reading NLP Fundamentals . We first make use of tokenization in order to get \n",
      "the tokens. Later, we will use the pos_tag()  method, which will help us find the PoS \n",
      "for each word/token. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import word_tokenize, pos_tag\n",
      "3. To find the tokens in the sentence, we make use of the word_tokenize()  \n",
      "method. Insert a new cell and add the following code to implement this:\n",
      "def get_tokens(sentence):\n",
      "    words = word_tokenize(sentence)\n",
      "    return words\n",
      "4. Print the tokens with the help of the print()  function. To implement this,  \n",
      "add a new cell and write the following code:\n",
      "words  = get_tokens(\"I am reading NLP Fundamentals\")\n",
      "print(words)\n",
      "This code generates the following output:\n",
      "['I', 'am', 'reading', 'NLP', 'Fundamentals']12 | Introduction to Natural Language Processing\n",
      "5. We'll now use the pos_tag()  method. Insert a new cell and add the  \n",
      "following code:\n",
      "def get_pos(words):\n",
      "    return pos_tag(words)\n",
      "get_pos(words)\n",
      "This code generates the following output:\n",
      "[('I', 'PRP'),\n",
      " ('am', 'VBP'),\n",
      " ('reading', 'VBG'),\n",
      " ('NLP', 'NNP'),\n",
      " ('Fundamentals', 'NNS')]\n",
      "In the preceding output, we can see that for each token, a PoS has been allotted. \n",
      "Here, PRP stands for personal pronoun , VBP stands for verb present , VGB  stands \n",
      "for verb gerund , NNP  stands for proper noun singular , and NNS  stands for  \n",
      "noun plural .\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/306WY24 .\n",
      "You can also run this example online at https://packt.live/38VLDpF .\n",
      "We have learned about assigning appropriate PoS labels to tokens in a sentence.  \n",
      "In the next section, we will learn about stop words  in sentences and ways to deal \n",
      "with them.\n",
      "Stop Word Removal\n",
      "Stop words are the most frequently occurring words in any language and they are just \n",
      "used to support the construction of sentences and do not contribute anything to the \n",
      "semantics of a sentence. So, we can remove stop words from any text before an NLP \n",
      "process, as they occur very frequently and their presence doesn't have much impact \n",
      "on the sense of a sentence. Removing them will help us clean our data, making its \n",
      "analysis much more efficient. Examples of stop words include \"a,\" \"am,\" \"and,\" \"the,\" \n",
      "\"in,\" \"of,\" and more.\n",
      "In the next exercise, we will look at the practical implementation of removing stop \n",
      "words from a given sentence.Various Steps in NLP | 13\n",
      "Exercise 1.04: Stop Word Removal\n",
      "In this exercise, we will check the list of stop words provided by the nltk  library. \n",
      "Based on this list, we will filter out the stop words included in our text:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import download\n",
      "download('stopwords')\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "3. In order to check the list of stop words provided for English , we pass it as a \n",
      "parameter to the words()  function. Insert a new cell and add the following \n",
      "code to implement this:\n",
      "stop_words = stopwords.words('english')\n",
      "4. In the code, the list of stop words provided by English  is stored in the stop_\n",
      "words  variable. In order to view the list, we make use of the print()  function. \n",
      "Insert a new cell and add the following code to view the list:\n",
      "print(stop_words)\n",
      "This code generates the following output:\n",
      "Figure 1.1: List of stop words provided by English\n",
      "14 | Introduction to Natural Language Processing\n",
      "5. To remove the stop words from a sentence, we first assign a string to the \n",
      "sentence  variable and tokenize it into words using the word_tokenize()  \n",
      "method. Insert a new cell and add the following code to implement this: \n",
      "sentence = \"I am learning Python. It is one of the \"\\\n",
      "           \"most popular programming languages\"\n",
      "sentence_words = word_tokenize(sentence)\n",
      "Note \n",
      "The code snippet shown here uses a backslash ( \\ ) to split the logic  \n",
      "across multiple lines. When the code is executed, Python will ignore the \n",
      "backslash, and treat the code on the next line as a direct continuation of the \n",
      "current line.\n",
      "6. To print the list of tokens, insert a new cell and add the following code:\n",
      "print(sentence_words)\n",
      "This code generates the following output:\n",
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', \n",
      "'the', 'most', 'popular', 'programming', 'languages']\n",
      "7. To remove the stop words, we need to loop through each word in the sentence, \n",
      "check whether there are any stop words, and then finally combine them to  \n",
      "form a complete sentence. To implement this, insert a new cell and add the \n",
      "following code:\n",
      "def remove_stop_words(sentence_words, stop_words):\n",
      "    return ' '.join([word for word in sentence_words if \\\n",
      "                     word not in stop_words])\n",
      "8. To check whether the stop words are filtered out from our sentence, print the \n",
      "sentence_no_stops  variable. Insert a new cell and add the following code  \n",
      "to print:\n",
      "print(remove_stop_words(sentence_words,stop_words))\n",
      "This code generates the following output:\n",
      "I learning Python. It one popular programming languagesVarious Steps in NLP | 15\n",
      "As you can see in the preceding code snippet, stop words such as \"am,\" \"is,\" \"of,\" \n",
      "\"the,\" and \"most\" are being filtered out and text without stop words is produced \n",
      "as output.\n",
      "9. Add your own stop words to the stop word list:\n",
      "stop_words.extend(['I','It', 'one'])\n",
      "print(remove_stop_words(sentence_words,stop_words))\n",
      "This code generates the following output:\n",
      "learning Python . popular programming languages\n",
      "As we can see from the output, now words such as \"I,\" \"It,\" and* \"One\" are removed \n",
      "as we have added them to our custom stop word list. We have learned how to \n",
      "remove stop words from given text.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3j4KBw7 .\n",
      "You can also run this example online at https://packt.live/3fyYSir .\n",
      "In the next section, we will focus on normalizing text.\n",
      "Text Normalization\n",
      "There are some words that are spelled, pronounced, and represented differently—for \n",
      "example, words such as Mumbai and Bombay, and US and United States. Although \n",
      "they are different, they refer to the same thing. There are also different forms of \n",
      "words that need to be converted into base forms. For example, words such as \"does\" \n",
      "and \"doing,\" when converted to their base form, become \"do.\" Along these lines, text \n",
      "normalization  is a process wherein different variations of text get converted into a \n",
      "standard form. We need to perform text normalization as there are some words that \n",
      "can mean the same thing as each other. There are various ways of normalizing text, \n",
      "such as spelling correction, stemming, and lemmatization, which will be covered later.\n",
      "For a better understanding of this topic, we will look into a practical implementation \n",
      "of text normalization in the next section.16 | Introduction to Natural Language Processing\n",
      "Exercise 1.05: Text Normalization\n",
      "In this exercise, we will normalize some given text. Basically, we will be trying to \n",
      "replace select words with new words, using the replace()  function, and finally \n",
      "produce the normalized text. replace()  is a built-in Python function that works \n",
      "on strings and takes two arguments. It will return a copy of a string in which the \n",
      "occurrence of the first argument will be replaced by the second argument.\n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to assign a string to the  \n",
      "sentence  variable:\n",
      "sentence = \"I visited the US from the UK on 22-10-18\"\n",
      "3. We want to replace \"US\"  with \"United States\" , \"UK\"  with \"United \n",
      "Kingdom\" , and \"18\"  with \"2018\" . To do so, use the replace()  function \n",
      "and store the updated output in the \"normalized_sentence\"  variable. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "def normalize(text):\n",
      "    return text.replace(\"US\", \"United States\")\\\n",
      "               .replace(\"UK\", \"United Kingdom\")\\\n",
      "               .replace(\"-18\", \"-2018\")\n",
      "4. To check whether the text has been normalized, insert a new cell and add the \n",
      "following code to print it:\n",
      "normalized_sentence = normalize(sentence)\n",
      "print(normalized_sentence)\n",
      "The code generates the following output:\n",
      "I visited the United States from the United Kingdom on 22-10-2018\n",
      "5. Add the following code:\n",
      "normalized_sentence = normalize('US and UK are two superpowers')\n",
      "print(normalized_sentence)\n",
      "The code generates following output:\n",
      "United States and United Kingdom are two superpowersVarious Steps in NLP | 17\n",
      "In the preceding code, we can see that our text has been normalized.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2Wm49T8 .\n",
      "You can also run this example online at https://packt.live/2Wm4d5k .\n",
      "Over the next sections, we will explore various other ways in which text can  \n",
      "be normalized.\n",
      "Spelling Correction\n",
      "Spelling correction is one of the most important tasks in any NLP project.  \n",
      "It can be time-consuming, but without it, there are high chances of losing  \n",
      "out on important information.\n",
      "Spelling correction is executed in two steps:\n",
      "1. Identify the misspelled word, which can be done by a simple dictionary lookup.  \n",
      "If there is no match found in the language dictionary, it is considered to  \n",
      "be misspelled.\n",
      "2. Replace it or suggest the correctly spelled word. There are a lot of algorithms for \n",
      "this task. One of them is the minimum edit distance algorithm, which chooses \n",
      "the nearest correctly spelled word for a misspelled word. The nearness is \n",
      "defined by the number of edits that need to be made to the misspelled word \n",
      "to reach the correctly spelled word. For example, let's say there is a misspelled \n",
      "word, \"autocorect.\" Now, to make it \"autocorrect,\" we need to add one \"r,\" and to \n",
      "make it \"auto,\" we need to delete 6 characters, which means that \"autocorrect\" is \n",
      "the correct spelling because it requires the fewest edits.\n",
      "We make use of the autocorrect  Python library to correct spellings. \n",
      "autocorrect  is a Python library used to correct the spelling of misspelled words \n",
      "for different languages. It provides a method called spell() , which takes a word as \n",
      "input and returns the correct spelling of the word.\n",
      "Let's look at the following exercise to get a better understanding of this.18 | Introduction to Natural Language Processing\n",
      "Exercise 1.06: Spelling Correction of a Word and a Sentence\n",
      "In this exercise, we will perform spelling correction on a word and a sentence, with \n",
      "the help of Python's autocorrect  library. Follow these steps in order to complete \n",
      "this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import word_tokenize\n",
      "from autocorrect import Speller\n",
      "3. In order to correct the spelling of a word, pass a wrongly spelled word as a \n",
      "parameter to the spell()  function. Before that, you have to create a spell  \n",
      "object of the Speller  class using lang='en'  to signify the English language. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "spell = Speller(lang='en')\n",
      "spell('Natureal')\n",
      "This code generates the following output:\n",
      "'Natural'\n",
      "4. To correct the spelling of a sentence, first tokenize it into tokens. After that, loop \n",
      "through each token in sentence , autocorrect the words, and finally combine \n",
      "the words. Insert a new cell and add the following code to implement this:\n",
      "sentence = word_tokenize(\"Ntural Luanguage Processin deals with \"\\\n",
      "                         \"the art of extracting insightes from \"\\\n",
      "                         \"Natural Languaes\")\n",
      "5. Use the print()  function to print all tokens. Insert a new cell and add the \n",
      "following code to print the tokens:\n",
      "print(sentence)\n",
      "This code generates the following output:\n",
      "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', \n",
      "'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']Various Steps in NLP | 19\n",
      "6. Now that we have got the tokens, loop through each token in sentence , \n",
      "correct the tokens, and assign them to a new variable. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "def correct_spelling(tokens):\n",
      "    sentence_corrected = ' '.join([spell(word) \\\n",
      "                                   for word in tokens])\n",
      "    return sentence_corrected\n",
      "7. To print the correct sentence, insert a new cell and add the following code:\n",
      "print(correct_spelling(sentence))\n",
      "This code generates the following output:\n",
      "['Natural', 'Language', 'Procession', 'deals', 'with', 'the', 'art', \n",
      " 'of', 'extracting', 'insights', 'from', 'Natural', 'Languages']\n",
      "In the preceding code snippet, we can see that most of the wrongly spelled words \n",
      "have been corrected. But the word \" Processin \" was wrongly converted into \n",
      "\"Procession .\" It should have been \" Processing .\" This happened because to \n",
      "change \"Processin \" to \"Procession \" or \"Processing ,\" an equal number of \n",
      "edits is required. To rectify this, we need to use other kinds of spelling correctors that \n",
      "are aware of context.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/38YVCKJ .\n",
      "You can also run this example online at https://packt.live/3gVpbj4 .\n",
      "In the next section, we will look at stemming, which is another form of  \n",
      "text normalization.20 | Introduction to Natural Language Processing\n",
      "Stemming\n",
      "In most languages, words get transformed into various forms when being used in a \n",
      "sentence. For example, the word \"product\" might get transformed into \"production\" \n",
      "when referring to the process of making something or transformed into \"products\" in \n",
      "plural form. It is necessary to convert these words into their base forms, as they carry \n",
      "the same meaning in any case. Stemming is the process that helps us to do so. If we \n",
      "look at the following figure, we get a perfect idea of how words get transformed into \n",
      "their base forms:\n",
      "Figure 1.2: Stemming of the word \"product\"\n",
      "To get a better understanding of stemming, let's perform a simple exercise.\n",
      "In this exercise, we will be using two algorithms, called the porter stemmer and the \n",
      "snowball stemmer, provided by the NLTK library. The porter stemmer is a rule-based \n",
      "algorithm that transforms words to their base form by removing suffixes from words. \n",
      "The snowball stemmer is an improvement over the porter stemmer and is a little bit \n",
      "faster and uses less memory. In NLTK, this is done by the stem()  method provided \n",
      "by the PorterStemmer  class.\n",
      "Exercise 1.07: Using Stemming\n",
      "In this exercise, we will pass a few words through the stemming process so that they \n",
      "get converted into their base forms. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import stem\n",
      "Various Steps in NLP | 21\n",
      "3. Now pass the following words as parameters to the stem()  method. To \n",
      "implement this, insert a new cell and add the following code:\n",
      "def get_stems(word,stemmer):\n",
      "    return stemmer.stem(word)\n",
      "porterStem = stem.PorterStemmer()\n",
      "get_stems(\"production\",porterStem)\n",
      "4. When the input is \"production\" , the following output is generated:\n",
      "'product'\n",
      "5. Similarly, the following code would be used for the input \"coming\" .\n",
      "get_stems(\"coming\",porterStem)\n",
      "We get the following output:\n",
      "'come'\n",
      "6. Similarly, the following code would be used for the input \"firing\" .\n",
      "  get_stems(\"firing\",porterStem)\n",
      "When the input is \"firing\" , the following output is generated:\n",
      "'fire'\n",
      "7. The following code would be used for the input \"battling\" .\n",
      "  get_stems(\"battling\",porterStem)\n",
      "If we give the input \"battling\" , the following output is generated:\n",
      "'battl'\n",
      "8. The following code will also generate the same output as above, for the  \n",
      "input \"battling\".\n",
      "stemmer = stem.SnowballStemmer(\"english\")\n",
      "get_stems(\"battling\",stemmer)\n",
      "The output will be as follows:\n",
      "'battl'22 | Introduction to Natural Language Processing\n",
      "As you have seen while using the snowball stemmer, we have to provide the language \n",
      "as \"english\" . We can also use the stemmer for different languages such as \n",
      "Spanish, French, and many more. From the preceding code snippets, we can see that \n",
      "the entered words are converted into their base forms.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2DLzisD .\n",
      "You can also run this example online at https://packt.live/30h147K .\n",
      "In the next section, we will focus on lemmatization , which is another form of  \n",
      "text normalization.\n",
      "Lemmatization\n",
      "Sometimes, the stemming process leads to incorrect results. For example, in the \n",
      "last exercise, the word battling  was transformed to \"battl\" , which is not a \n",
      "word. To overcome such problems with stemming, we make use of lemmatization. \n",
      "Lemmatization is the process of converting words to their base grammatical form, \n",
      "as in \"battling\" to \"battle,\" rather than just randomly axing words. In this process, an \n",
      "additional check is made by looking through a dictionary to extract the base form \n",
      "of a word. Getting more accurate results requires some additional information; for \n",
      "example, PoS tags along with words will help in getting better results.\n",
      "In the following exercise, we will be using WordNetLemmatizer , which is an \n",
      "NLTK interface of WordNet. WordNet is a freely available lexical English database \n",
      "that can be used to generate semantic relationships between words. NLTK's \n",
      "WordNetLemmatizer  provides a method called lemmatize() , which returns  \n",
      "the lemma (grammatical base form) of a given word using WordNet.\n",
      "To put lemmatization into practice, let's perform an exercise where we'll use the \n",
      "lemmatize()  function.Various Steps in NLP | 23\n",
      "Exercise 1.08: Extracting the Base Word Using Lemmatization\n",
      "In this exercise, we will use the lemmatization process to produce the proper form of \n",
      "a given word. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import download\n",
      "download('wordnet')\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "3. Create an object of the WordNetLemmatizer  class. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "4. Bring the word to its proper form by using the lemmatize()  method of the \n",
      "WordNetLemmatizer  class. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "def get_lemma(word):\n",
      "    return lemmatizer.lemmatize(word)\n",
      "get_lemma('products')\n",
      "With the input products , the following output is generated:\n",
      "'product'\n",
      "5. Similarly, use the input as production  now:\n",
      "get_lemma('production')\n",
      "With the input production , the following output is generated:\n",
      "'production'\n",
      "6. Similarly, use the input as coming  now:\n",
      "get_lemma('coming')\n",
      "With the input coming , the following output is generated:\n",
      "'coming'24 | Introduction to Natural Language Processing\n",
      "Hence, we have learned how to use the lemmatization process to transform a \n",
      "given word into its base form.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3903ETS .\n",
      "You can also run this example online at https://packt.live/2Wlqu33 .\n",
      "In the next section, we will look at another preprocessing step in NLP: named entity \n",
      "recognition  (NER ).\n",
      "Named Entity Recognition (NER)\n",
      "NER is the process of extracting important entities, such as person names, place \n",
      "names, and organization names, from some given text. These are usually not present \n",
      "in dictionaries. So, we need to treat them differently. The main objective of this \n",
      "process is to identify the named entities (such as proper nouns) and map them to \n",
      "categories, which are already defined. For example, categories might include names \n",
      "of people, places, and so on.\n",
      "NER has found use in many NLP tasks, including assigning tags to news articles, \n",
      "search algorithms, and more. NER can analyze a news article and extract the major \n",
      "people, organizations, and places discussed in it and assign them as tags for  \n",
      "new articles. \n",
      "In the case of search algorithms, let's suppose we have to create a search engine, \n",
      "meant specifically for books. If we were to submit a given query for all the words, \n",
      "the search would take a lot of time. Instead, if we extract the top entities from all the \n",
      "books using NER and run a search query on the entities rather than all the content, \n",
      "the speed of the system would increase dramatically.\n",
      "To get a better understanding of this process, we'll perform an exercise. Before \n",
      "moving on to the exercise, let me introduce you to chunking, which we are going to \n",
      "use in the following exercise. Chunking is the process of grouping words together into \n",
      "chunks, which can be further used to find noun groups and verb groups, or can also \n",
      "be used for sentence partitioning.Various Steps in NLP | 25\n",
      "Exercise 1.09: Treating Named Entities\n",
      "In this exercise, we will find the named entities in a given sentence. Follow these steps \n",
      "to implement this exercise using the following sentence:\n",
      "\"We are reading a book published by Packt which is based out of Birmingham.\"\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import download\n",
      "from nltk import pos_tag\n",
      "from nltk import ne_chunk\n",
      "from nltk import word_tokenize\n",
      "download('maxent_ne_chunker')\n",
      "download('words')\n",
      "3. Declare the sentence  variable and assign it a string. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "sentence = \"We are reading a book published by Packt \"\\\n",
      "           \"which is based out of Birmingham.\"\n",
      "4. To find the named entities from the preceding text, insert a new cell and add the \n",
      "following code:\n",
      "def get_ner(text):\n",
      "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
      "    return [a for a in i if len(a)==1]\n",
      "get_ner(sentence)\n",
      "This code generates the following output:\n",
      "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]26 | Introduction to Natural Language Processing\n",
      "In the preceding code, we can see that the code identifies the named  \n",
      "entities \"Packt \" and \"Birmingham \" and maps them to an already-defined \n",
      "category, \"NNP .\" \n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3ezeukC .\n",
      "You can also run this example online at https://packt.live/32rsOJs .\n",
      "In the next section, we will focus on word sense disambiguation, which helps us to \n",
      "identify the right sense of any word.\n",
      "Word Sense Disambiguation\n",
      "There's a popular saying: \"A man is known by the company he keeps.'' Similarly, a \n",
      "word's meaning depends on its association with other words in a sentence. This \n",
      "means two or more words with the same spelling may have different meanings \n",
      "in different contexts. This often leads to ambiguity. Word sense disambiguation \n",
      "is the process of mapping a word to the sense that it should carry. We need to \n",
      "disambiguate words based on the sense they carry so that they can be treated \n",
      "as different entities when being analyzed. The following figure displays a perfect \n",
      "example of how ambiguity is caused due to the usage of the same word in  \n",
      "different sentences:\n",
      "Figure 1.3: Word sense disambiguation\n",
      "One of the algorithms to solve word sense disambiguation is the Lesk algorithm. \n",
      "It has a huge corpus in the background (generally WordNet  is used) that contains \n",
      "definitions of all the possible synonyms of all the possible words in a language. Then \n",
      "it takes a word and the context as input and finds a match between the context and \n",
      "all the definitions of the word. The meaning with the highest number of matches with \n",
      "the context of the word will be returned. \n",
      "Word Sense Disambiguation | 27\n",
      "For example, suppose we have a sentence such as \"We play only soccer\" in a given \n",
      "text. Now, we need to find the meaning of the word \"play\" in this sentence. In the \n",
      "Lesk algorithm, each word with ambiguous meaning is saved in background synsets. \n",
      "In this case, the word \"play\" will be saved with all possible definitions. Let's say we \n",
      "have two definitions of the word \"play\":\n",
      "1. Play: Participating in a sport or game\n",
      "2. Play: Using a musical instrument\n",
      "Then, we will find the similarity between the context of the word \"play\" in the text \n",
      "and both of the preceding definitions using text similarity techniques. The definition \n",
      "best suited to the context of \"play\" in the sentence will be considered the meaning \n",
      "or definition of the word. In this case, we will find that our first definition fits best in \n",
      "context, as the words \"sport\" and \"game\" are present in the preceding sentences.\n",
      "In the next exercise, we will be using the Lesk module from NLTK. It takes a sentence \n",
      "and the word as input, and returns the meaning or definition of the word. The output \n",
      "of the Lesk method is synset , which contains the ID of the matched definition. \n",
      "These IDs can be matched with their definitions using the definition()  method \n",
      "of wsd.synset('word') .\n",
      "To get a better understanding of this process, let's look at an exercise.\n",
      "Exercise 1.10: Word Sense Disambiguation\n",
      "In this exercise, we will find the sense of the word \"bank\" in two different sentences. \n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import nltk\n",
      "nltk.download('wordnet')\n",
      "from nltk.wsd import lesk\n",
      "from nltk import word_tokenize\n",
      "3. Declare two variables, sentence1  and sentence2 , and assign them with \n",
      "appropriate strings. Insert a new cell and the following code to implement this:\n",
      "sentence1 = \"Keep your savings in the bank\"\n",
      "sentence2 = \"It's so risky to drive over the banks of the road\"28 | Introduction to Natural Language Processing\n",
      "4. To find the sense of the word \"bank\" in the preceding two sentences, use the \n",
      "Lesk algorithm provided by the nltk.wsd  library. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "def get_synset(sentence, word):\n",
      "    return lesk(word_tokenize(sentence), word)\n",
      "get_synset(sentence1,'bank')\n",
      "This code generates the following output:\n",
      "Synset('savings_bank.n.02')\n",
      "5. Here, savings_bank.n.02  refers to a container for keeping money safely at \n",
      "home. To check the other sense of the word \"bank,\" write the following code:\n",
      "get_synset(sentence2,'bank')\n",
      "This code generates the following output:\n",
      "Synset('bank.v.07')\n",
      "Here, bank.v.07  refers to a slope in the turn of a road.\n",
      "Thus, with the help of the Lesk algorithm, we were able to identify the sense of a \n",
      "word in whatever context. \n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/399JCq5 .\n",
      "You can also run this example online at https://packt.live/30haCQ6 .\n",
      "In the next section, we will focus on sentence boundary detection , which helps \n",
      "detect the start and end points of sentences.\n",
      "Sentence Boundary Detection\n",
      "Sentence boundary detection is the method of detecting where one sentence ends \n",
      "and another begins. If you are thinking that this sounds pretty easy, as a period (.) \n",
      "or a question mark (?) denotes the end of a sentence and the beginning of another \n",
      "sentence, then you are wrong. There can also be instances where the letters of \n",
      "acronyms are separated by full stops, for instance. Various analyses need to be \n",
      "performed at a sentence level; detecting the boundaries of sentences is essential.Sentence Boundary Detection | 29\n",
      "An exercise will provide us with a better understanding of this process.\n",
      "Exercise 1.11: Sentence Boundary Detection\n",
      "In this exercise, we will extract sentences from a paragraph. To do so, we'll be using \n",
      "the sent_tokenize()  method, which is used to detect sentence boundaries. The \n",
      "following steps need to be performed:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import nltk\n",
      "from nltk.tokenize import sent_tokenize\n",
      "3. Use the sent_tokenize()  method to detect sentences in some given text. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "def get_sentences(text):\n",
      "    return sent_tokenize(text)\n",
      "get_sentences(\"We are reading a book. Do you know who is \"\\\n",
      "              \"the publisher? It is Packt. Packt is based \"\\\n",
      "              \"out of Birmingham.\")\n",
      "This code generates the following output:\n",
      "['We are reading a book.'\n",
      " 'Do you know who is the publisher?'\n",
      " 'It is Packt.',\n",
      " 'Packt is based out of Birmingham.']\n",
      "4. Use the sent_tokenize()  method for text that contains periods (.) other \n",
      "than those found at the ends of sentences:\n",
      "get_sentences(\"Mr. Donald John Trump is the current \"\\\n",
      "              \"president of the USA. Before joining \"\\\n",
      "              \"politics, he was a businessman.\")\n",
      "The code will generate the following output:\n",
      "['Mr. Donald John Trump is the current president of the USA.',\n",
      " 'Before joining politics, he was a businessman.']30 | Introduction to Natural Language Processing\n",
      "As you can see in the code, the sent_tokenize  method is able to differentiate \n",
      "between the period (.) after \"Mr\" and the one used to end the sentence. We have \n",
      "covered all the preprocessing steps that are involved in NLP. \n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2ZseU86 .\n",
      "You can also run this example online at https://packt.live/2CC8Ukp .\n",
      "Now, using the knowledge we've gained, let's perform an activity.\n",
      "Activity 1.01: Preprocessing of Raw Text\n",
      "We have a text corpus that is in an improper format. In this activity, we will perform \n",
      "all the preprocessing steps that were discussed earlier to get some meaning out of \n",
      "the text.\n",
      "Note\n",
      "The text corpus, file.txt , can be found at this location:  \n",
      "https://packt.live/30cu54z\n",
      "After downloading the file, place it in the same directory as the notebook.\n",
      "Follow these steps to implement this activity:\n",
      "1. Import the necessary libraries.\n",
      "2. Load the text corpus to a variable.\n",
      "3. Apply the tokenization process to the text corpus and print the first 20 tokens.\n",
      "4. Apply spelling correction on each token and print the initial 20 corrected tokens \n",
      "as well as the corrected text corpus.\n",
      "5. Apply PoS tags to each of the corrected tokens and print them.\n",
      "6. Remove stop words from the corrected token list and print the initial 20 tokens.\n",
      "7. Apply stemming and lemmatization to the corrected token list and then print the \n",
      "initial 20 tokens.Kick Starting an NLP Project | 31\n",
      "8. Detect the sentence boundaries in the given text corpus and print the total \n",
      "number of sentences. \n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "We have learned about and achieved the preprocessing of given data. By now, you \n",
      "should be familiar with what NLP is and what basic preprocessing steps are needed \n",
      "to carry out any NLP project. In the next section, we will focus on the different phases \n",
      "of an NLP project.\n",
      "Kick Starting an NLP Project\n",
      "We can divide an NLP project into several sub-projects or phases. These phases are \n",
      "completed in a particular sequence. This tends to increase the overall efficiency of the \n",
      "process, as memory usage changes from one phase to the next. An NLP project has \n",
      "to go through six major phases, which are outlined in the following figure:\n",
      "Figure 1.4: Phases of an NLP project\n",
      "32 | Introduction to Natural Language Processing\n",
      "Suppose you are working on a project in which you need to classify emails as \n",
      "important and unimportant. We will explain how this is carried out by discussing each \n",
      "phase in detail.\n",
      "Data Collection\n",
      "This is the initial phase of any NLP project. Our sole purpose is to collect data as per \n",
      "our requirements. For this, we may either use existing data, collect data from various \n",
      "online repositories, or create our own dataset by crawling the web. In our case, we \n",
      "will collect different email data. We can even get this data from our personal emails \n",
      "as well, to start with.\n",
      "Data Preprocessing\n",
      "Once the data is collected, we need to clean it. For the process of cleaning, we will \n",
      "make use of the different preprocessing steps that we have learned about in this \n",
      "chapter. It is necessary to clean the collected data to ensure effectiveness and \n",
      "accuracy. In our case, we will follow these preprocessing steps:\n",
      "1. Converting all the text data to lowercase\n",
      "2. Stop word removal\n",
      "3. Text normalization, which will include replacing all numbers with some common \n",
      "term and replacing punctuation with empty strings\n",
      "4. Stemming and lemmatization\n",
      "Feature Extraction\n",
      "Computers understand only binary digits: 0 and 1. As such, every instruction we \n",
      "feed into a computer gets transformed into binary digits. Similarly, machine learning \n",
      "models tend to understand only numeric data. Therefore, it becomes necessary to \n",
      "convert text data into its equivalent numerical form. \n",
      "To convert every email into its equivalent numerical form, we will create a dictionary \n",
      "of all the unique words in our data and assign a unique index to each word. Then, we \n",
      "will represent every email with a list having a length equal to the number of unique \n",
      "words in the data. The list will have 1 at the indices of words that are present in the \n",
      "email and 0 at the other indices. This is called one-hot encoding. We will learn more \n",
      "about this in coming chapters.Summary | 33\n",
      "Model Development\n",
      "Once the feature set is ready, we need to develop a suitable model that can be \n",
      "trained to gain knowledge from the data. These models are generally statistical, \n",
      "machine learning-based, deep learning-based, or reinforcement learning-based. In \n",
      "our case, we will build a model that is capable of differentiating between important \n",
      "and unimportant emails.\n",
      "Model Assessment\n",
      "After developing a model, it is essential to benchmark it. This process of \n",
      "benchmarking is known as model assessment. In this step, we will evaluate the \n",
      "performance of our model by comparing it to others. This can be done by using \n",
      "different parameters or metrics. These parameters include precision, recall, and \n",
      "accuracy. In our case, we will evaluate the newly created model by seeing how well it \n",
      "performs at classifying emails as important and unimportant.\n",
      "Model Deployment\n",
      "This is the final stage for most industrial NLP projects. In this stage, the models \n",
      "are put into production. They are either integrated into an existing system or new \n",
      "products are created by keeping this model as a base. In our case, we will deploy our \n",
      "model to production, so that it can classify emails as important and unimportant in \n",
      "real time.\n",
      "Summary\n",
      "In this chapter, we learned about the basics of NLP and how it differs from text \n",
      "analytics. We covered the various preprocessing steps that are included in NLP, such \n",
      "as tokenization, PoS tagging, stemming, lemmatization, and more. We also looked \n",
      "at the different phases an NLP project has to pass through, from data collection to \n",
      "model deployment. \n",
      "In the next chapter, you will learn about the different methods of extracting features \n",
      "from unstructured text, such as TF-IDF and bag of words. You will also learn about \n",
      "NLP tasks such as tokenization, lemmatization, and stemming in more detail. \n",
      "Furthermore, text visualization techniques such as word clouds will be introduced.Overview\n",
      "In this chapter, you will be able to categorize data based on its content \n",
      "and structure. You will be able to describe preprocessing steps in detail \n",
      "and implement them to clean up text data. You will learn about feature \n",
      "engineering and calculate the similarity between texts. Once you \n",
      "understand these concepts, you will be able to use word clouds and  \n",
      "some other techniques to visualize text.Feature Extraction Methods236 | Feature Extraction Methods\n",
      "Introduction\n",
      "In the previous chapter, we learned about the concepts of Natural Language \n",
      "Processing  (NLP ) and text analytics. We also took a quick look at various \n",
      "preprocessing steps. In this chapter, we will learn how to make text understandable \n",
      "to machine learning algorithms.\n",
      "As we know, to use a machine learning algorithm on textual data, we need a \n",
      "numerical or vector representation of text data since most of these algorithms are \n",
      "unable to work directly with plain text or strings. But before converting the text data \n",
      "into numerical form, we will need to pass it through some preprocessing steps such \n",
      "as tokenization, stemming, lemmatization, and stop-word removal.\n",
      "So, in this chapter, we will learn a little bit more about these preprocessing steps and \n",
      "how to extract features from the preprocessed text and convert them into vectors. \n",
      "We will also explore two popular methods for feature extraction (Bag of Words and \n",
      "Term Frequency-Inverse Document Frequency), as well as various methods for finding \n",
      "similarity between different texts. By the end of this chapter, you will have gained an \n",
      "in-depth understanding of how text data can be visualized.\n",
      "Types of Data\n",
      "To deal with data effectively, we need to understand the various forms in which it \n",
      "exists. First, let's explore the types of data that exist. There are two main ways to \n",
      "categorize data (by structure and by content), as explained in the upcoming sections.Types of Data | 37\n",
      "Categorizing Data Based on Structure\n",
      "Data can be divided on the basis of structure into three categories, namely, \n",
      "structured, semi-structured, and unstructured data, as shown in the  \n",
      "following diagram:\n",
      "Figure 2.1: Categorization based on content\n",
      "These three categories are as follows:\n",
      "• Structured data : This is the most organized form of data. It is represented in \n",
      "tabular formats such as Excel files and Comma-Separated Value  (CSV) files. The \n",
      "following image shows what structured data usually looks like:\n",
      "Figure 2.2: Structured data\n",
      "The preceding table contains information about five people, with each row \n",
      "representing a person and each column representing one of their attributes.\n",
      "38 | Feature Extraction Methods\n",
      "• Semi-structured data : This type of data is not presented in a tabular structure, \n",
      "but it can be transformed into a table. Here, information is usually stored \n",
      "between tags following a definite pattern. XML and HTML files can be referred to \n",
      "as semi-structured data. The following screenshot shows how semi-structured \n",
      "data can appear:\n",
      "Figure 2.3: Semi-structured data\n",
      "Types of Data | 39\n",
      "The format shown in the preceding screenshot is called markup language \n",
      "format. Here, the data is stored between tags, hierarchically. It is a universally \n",
      "accepted format, and there are a lot of parsers available that can convert this \n",
      "data into structured data.\n",
      "• Unstructured data : This type of data is the most difficult to deal with. Machine \n",
      "learning algorithms would find it difficult to comprehend unstructured data \n",
      "without any loss of information. Text corpora and images are examples of \n",
      "unstructured data. The following image shows what unstructured data looks like:\n",
      "Figure 2.4: Unstructured data\n",
      "This is called unstructured data because if we want to get employee details from \n",
      "the preceding text snippet with our program, we will not be able to do so by simple \n",
      "parsing. We have to make our algorithm understand the semantics of the language to \n",
      "make it able to extract information from this.\n",
      "Categorizing Data Based on Content\n",
      "Data can be divided into four categories based on content, as shown in the  \n",
      "following diagram:\n",
      "Figure 2.5: Categorizing data based on structure\n",
      "40 | Feature Extraction Methods\n",
      "Let's look at each category here:\n",
      "• Text data : This refers to text corpora consisting of written sentences. This type \n",
      "of data can only be read. An example would be the text corpus of a book.\n",
      "• Image data : This refers to pictures that are used to communicate messages. \n",
      "This type of data can only be seen.\n",
      "• Audio data : This refers to voice recordings, music, and so on. This type of data \n",
      "can only be heard.\n",
      "• Video data : A continuous series of images coupled with audio forms a video. \n",
      "This type of data can be seen as well as heard.\n",
      "With that, we have learned about the different types of data and their categorization \n",
      "on the basis of structure and content. When dealing with unstructured data, it \n",
      "is necessary to clean it first. In the next section, we will look into some of the \n",
      "preprocessing steps for cleaning data.\n",
      "Cleaning Text Data\n",
      "The text data that we are going to discuss here is unstructured text data, which \n",
      "consists of written sentences. Most of the time, this text data cannot be used as it is \n",
      "for analysis because it contains some noisy elements, that is, elements that do not \n",
      "really contribute much to the meaning of the sentence at all. These noisy elements \n",
      "need to be removed because they do not contribute to the meaning and semantics \n",
      "of the text. If they're not removed, they can not only waste system memory and \n",
      "processing time, but also negatively impact the accuracy of the results. Data cleaning \n",
      "is the art of extracting meaningful portions from data by eliminating unnecessary \n",
      "details. Consider the sentence, \"He tweeted, 'Live coverage of General Elections \n",
      "available at this.tv/show/ge2019. _/\\_ Please tune in :) ' . \"\n",
      "In this example, to perform NLP tasks on the sentence, we will need to remove the \n",
      "emojis, punctuation, and stop words, and then change the words into their base \n",
      "grammatical form.Cleaning Text Data | 41\n",
      "To achieve this, methods such as stopword removal, tokenization, and stemming are \n",
      "used. We will explore them in detail in the upcoming sections. Before we do so, let's \n",
      "get acquainted with some basic NLP libraries that we will be using here:\n",
      "• Re: This is a standard Python library that's used for string searching and string \n",
      "manipulation. It contains methods such as match() , search() , findall() , \n",
      "split() , and sub() , which are used for basic string matching, searching, \n",
      "replacing, and more, using regular expressions. A regular expression is nothing \n",
      "but a set of characters in a specific order that represents a pattern. This pattern \n",
      "is searched for in the texts.\n",
      "• textblob : This is an open source Python library that provides different \n",
      "methods for performing various NLP tasks such as tokenization and PoS tagging. \n",
      "It is similar to nltk , which was introduced in Chapter 1, Introduction to Natural \n",
      "Language Processing . It is built on the top of nltk  and is much simpler as it has \n",
      "an easier to use interface and excellent documentation. In projects that don't \n",
      "involve a lot of complexity, it should be preferable to nltk .\n",
      "• keras : This is an open source, high-level neural network library that's was \n",
      "developed on top of another neural network library called TensorFlow . In \n",
      "addition to neural network functionality, it also provides methods for basic text \n",
      "processing and NLP tasks.\n",
      "Tokenization\n",
      "Tokenization and word tokenizers were briefly described in Chapter 1 , Introduction to \n",
      "Natural Language Processing . Tokenization is the process of splitting sentences into \n",
      "their constituents; that is, words and punctuation. Let's perform a simple exercise to \n",
      "see how this can be done using various packages.42 | Feature Extraction Methods\n",
      "Exercise 2.01: Text Cleaning and Tokenization\n",
      "In this exercise, we will clean some text and extract the tokens from it. Follow these \n",
      "steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the re package:\n",
      "import re\n",
      "3. Create a method called clean_text()  that will delete all characters other \n",
      "than digits, alphabetical characters, and whitespaces from the text and split \n",
      "the text into tokens. For this, we will use the text which matches with all \n",
      "non-alphanumeric characters, and we will replace all of them with an  \n",
      "empty string:\n",
      "def clean_text(sentence):\n",
      "    return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
      "4. Store the sentence to be cleaned in a variable named sentence  and pass it \n",
      "through the preceding function. Add the following code to this: implement\n",
      "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day \"\\\n",
      "            \"of India from Rajpath, New Delhi. \"\\\n",
      "            \"Mesmerizing performance by Indian Army! \"\\\n",
      "            \"Awesome airshow! @india_official \"\\\n",
      "            \"@indian_army #India #70thRepublic_Day. \"\\\n",
      "            \"For more photos ping me sunil@photoking.com :)\"'\n",
      "clean_text(sentence)Cleaning Text Data | 43\n",
      "The preceding command fragments the string wherever any blank space is \n",
      "present. The output should be as follows:\n",
      "Figure 2.6: Fragmented string\n",
      "44 | Feature Extraction Methods\n",
      "With that, we have learned how to extract tokens from text. Often, extracting each \n",
      "token separately does not help. For instance, consider the sentence, \"I don't hate \n",
      "you, but your behavior.\" Here, if we process each of the tokens, such as \"hate\" \n",
      "and \"behavior,\" separately, then the true meaning of the sentence would not be \n",
      "comprehended. In this case, the context in which these tokens are present becomes \n",
      "essential. Thus, we consider n consecutive tokens at a time. n-grams  refers to the \n",
      "grouping of n consecutive tokens together.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2CQikt7 .\n",
      "You can also run this example online at https://packt.live/33cn0nF .\n",
      "Next, we will look at an exercise where n-grams can be extracted from a given text.\n",
      "Exercise 2.02: Extracting n-grams\n",
      "In this exercise, we will extract n-grams using three different methods. First, we will \n",
      "use custom-defined functions, and then the nltk  and textblob  libraries. Follow \n",
      "these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the re package and create a custom-defined function, which we can use \n",
      "to extract n-grams. Add the following code to do this:\n",
      "import re\n",
      "def n_gram_extractor(sentence, n):\n",
      "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
      "    for i in range(len(tokens)-n+1):\n",
      "        print(tokens[i:i+n])\n",
      "In the preceding function, we are splitting the sentence into tokens using regex, \n",
      "then looping over the tokens, taking n consecutive tokens at a time.Cleaning Text Data | 45\n",
      "3. If n is 2, two consecutive tokens will be taken, resulting in bigrams. To check the \n",
      "bigrams, we pass the function the text and with n=2. Add the following code to \n",
      "do this:\n",
      "n_gram_extractor('The cute little boy is playing with the kitten.', \\\n",
      "                 2)\n",
      "The preceding code generates the following output:\n",
      "['The', 'cute']\n",
      "['cute', 'little']\n",
      "['little', 'boy']\n",
      "['boy', 'is']\n",
      "['is', 'playing']\n",
      "['playing', 'with']\n",
      "['with', 'the']\n",
      "['the', 'kitten']\n",
      "4. To check the trigrams, we pass the function with the text and with n=3. Add the \n",
      "following code to do this:\n",
      "n_gram_extractor('The cute little boy is playing with the kitten.', \\\n",
      "                 3)\n",
      "The preceding code generates the following output:\n",
      "['The', 'cute', 'little']\n",
      "['cute', 'little', 'boy']\n",
      "['little', 'boy', 'is']\n",
      "['boy', 'is', 'playing']\n",
      "['is', 'playing', 'with']\n",
      "['playing', 'with', 'the']\n",
      "['with', 'the', 'kitten']\n",
      "5. To check the bigrams using the nltk  library, add the following code:\n",
      "from nltk import ngrams\n",
      "list(ngrams('The cute little boy is playing with the kitten.'\\\n",
      "            .split(), 2))46 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "[('The', 'cute'),\n",
      " ('cute', 'little'),\n",
      " ('little', 'boy'),\n",
      " ('boy', 'is'),\n",
      " ('is', 'playing'),\n",
      " ('playing', 'with'),\n",
      " ('with', 'the'),\n",
      " ('the', 'kitten')]\n",
      "6. To check the trigrams using the nltk  library, add the following code:\n",
      "list(ngrams('The cute little boy is playing with the \n",
      "kitten.'.split(), 3))\n",
      "The preceding code generates the following output:\n",
      "[('The', 'cute', 'little'),\n",
      " ('cute', 'little', 'boy'),\n",
      " ('little', 'boy', 'is'),\n",
      " ('boy', 'is', 'playing'),\n",
      " ('playing', 'with', 'the'),\n",
      " ('with', 'the', 'kitten.')]\n",
      "7. To check the bigrams using the textblob  library, add the following code:\n",
      "!pip install -U textblob\n",
      "from textblob import TextBlob\n",
      "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
      "blob.ngrams(n=2)\n",
      "The preceding code generates the following output:\n",
      "[WordList(['The', 'cute']),\n",
      " WordList(['cute', 'little']),\n",
      " WordList(['little', 'boy']),\n",
      " WordList(['boy', 'is']),\n",
      " WordList(['is', 'playing']),\n",
      " WordList(['playing', 'with']),\n",
      " WordList(['with', 'the']),\n",
      " WordList(['the', 'kitten'])]Cleaning Text Data | 47\n",
      "8. To check the trigrams using the textblob  library, add the following code:\n",
      "blob.ngrams(n=3)\n",
      "The preceding code generates the following output:\n",
      "[WordList(['The', 'cute', 'little']),\n",
      " WordList(['cute', 'little', 'boy']),\n",
      " WordList(['little', 'boy', 'is']),\n",
      " WordList(['boy', 'is' 'playing']),\n",
      " WordList(['is', 'playing' 'with']),\n",
      " WordList(['playing', 'with' 'the']),\n",
      " WordList(['with', 'the' 'kitten'])]\n",
      "In this exercise, we learned how to generate n-grams using various methods.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2PabHUK .\n",
      "You can also run this example online at https://packt.live/2XbjFRX .\n",
      "Exercise 2.03: Tokenizing Text with Keras and TextBlob\n",
      "In this exercise, we will use keras  and textblob  to tokenize texts. Follow these \n",
      "steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook and insert a new cell.\n",
      "2. Import the keras  and textblob  libraries and declare a variable named \n",
      "sentence , as follows.\n",
      "from keras.preprocessing.text import text_to_word_sequence\n",
      "from textblob import TextBlob\n",
      "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day \"\\\n",
      "            \"of India from Rajpath, New Delhi. \"\\\n",
      "            \"Mesmerizing performance by Indian Army! \"\\\n",
      "            \"Awesome airshow! @india_official \"\\\n",
      "            \"@indian_army #India #70thRepublic_Day. \"\\\n",
      "            \"For more photos ping me sunil@photoking.com :)\"'48 | Feature Extraction Methods\n",
      "3. To tokenize using the keras  library, add the following code:\n",
      "def get_keras_tokens(text):\n",
      "    return text_to_word_sequence(text)\n",
      "get_keras_tokens(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.7: Tokenization using Keras\n",
      "Cleaning Text Data | 49\n",
      "4. To tokenize using the textblob  library, add the following code:\n",
      "def get_textblob_tokens(text):\n",
      "    blob = TextBlob(text)\n",
      "    return blob.words\n",
      "get_textblob_tokens(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.8: Tokenization using textblob\n",
      "With that, we have learned how to tokenize texts using the keras  and  \n",
      "textblob  libraries.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3393hFi .\n",
      "You can also run this example online at https://packt.live/39Dtu09 .\n",
      "In the next section, we will discuss the different types of tokenizers.\n",
      "50 | Feature Extraction Methods\n",
      "Types of Tokenizers \n",
      "There are different types of tokenizers that come in handy for specific tasks. Let's look \n",
      "at the ones provided by nltk  one by one:\n",
      "• Whitespace tokenizer : This is the simplest type of tokenizer. It splits a string \n",
      "wherever a space, tab, or newline character is present. \n",
      "• Tweet tokenizer : This is specifically designed for tokenizing tweets. It takes care \n",
      "of all the special characters and emojis used in tweets and returns clean tokens.\n",
      "• MWE tokenizer : MWE stands for Multi-Word Expression. Here, certain groups \n",
      "of multiple words are treated as one entity during tokenization, such as \"United \n",
      "States of America,\" \"People's Republic of China,\" \"not only,\" and \"but also.\" These \n",
      "predefined groups are added at the beginning with mwe()  methods.\n",
      "• Regular expression tokenizer : These tokenizers are developed using regular \n",
      "expressions. Sentences are split based on the occurrence of a specific pattern (a \n",
      "regular expression).\n",
      "• WordPunctTokenizer : This splits a piece of text into a list of alphabetical and \n",
      "non-alphabetical characters. It actually splits text into tokens using a fixed \n",
      "regex , that is, '\\w+|[^\\w\\s]+' .\n",
      "Now that we have learned about the different types of tokenizers, in the next section, \n",
      "we will carry out an exercise to get a better understanding of them.Cleaning Text Data | 51\n",
      "Exercise 2.04: Tokenizing Text Using Various Tokenizers\n",
      "In this exercise, we will use different tokenizers to tokenize text. Perform the following \n",
      "steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and the following code to import all the tokenizers and declare \n",
      "a variable sentence:\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "from nltk.tokenize import MWETokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "from nltk.tokenize import WordPunctTokenizer\n",
      "sentence = 'Sunil tweeted, \"Witnessing 70th Republic Day \"\\\n",
      "            \"of India from Rajpath, New Delhi. \"\\\n",
      "            \"Mesmerizing performance by Indian Army! \"\\\n",
      "            \"Awesome airshow! @india_official \"\\\n",
      "            \"@indian_army #India #70thRepublic_Day. \"\\\n",
      "            \"For more photos ping me sunil@photoking.com :)\"'\n",
      "3. To tokenize the text using TweetTokenizer , add the following code:\n",
      "def tokenize_with_tweet_tokenizer(text):\n",
      "    # Here will create an object of tweetTokenizer\n",
      "    tweet_tokenizer = TweetTokenizer() \n",
      "    \"\"\"\n",
      "    Then we will call the tokenize method of \n",
      "    tweetTokenizer which will return token list of sentences.\n",
      "    \"\"\"\n",
      "    return tweet_tokenizer.tokenize(text) \n",
      "tokenize_with_tweet_tokenizer(sentence)\n",
      "Note\n",
      "The # symbol in the code snippet above denotes a code comment. \n",
      "Comments are added into code to help explain specific bits of logic. 52 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "Figure 2.9: Tokenization using TweetTokenizer\n",
      "As you can see, the hashtags, emojis, websites, and Twitter IDs are extracted as \n",
      "single tokens. If we had used the white space tokenizer, we would have got hash, \n",
      "dots, and the @ symbol as separate tokens.\n",
      "Cleaning Text Data | 53\n",
      "4. To tokenize the text using MWETokenizer , add the following code:\n",
      "def tokenize_with_mwe(text):\n",
      "    mwe_tokenizer = MWETokenizer([('Republic', 'Day')])\n",
      "    mwe_tokenizer.add_mwe(('Indian', 'Army'))\n",
      "    return mwe_tokenizer.tokenize(text.split())\n",
      "tokenize_with_mwe(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.10: Tokenization using the MWE tokenizer\n",
      "54 | Feature Extraction Methods\n",
      "In the preceding screenshot, the words \"Indian\" and \"Army!\", which should \n",
      "have been treated as a single identity, were treated separately. This is because \n",
      "\"Army!\" (not \"Army\") is treated as a token. Let's see how this can be fixed in the \n",
      "next step.\n",
      "5. Add the following code to fix the issues in the previous step:\n",
      "tokenize_with_mwe(sentence.replace('!',''))\n",
      "The preceding code generates the following output:\n",
      "Figure 2.11: Tokenization using the MWE tokenizer after removing the \"!\" sign\n",
      "Here, we can see that instead of being treated as separate tokens, \"Indian\" and \n",
      "\"Army\" are treated as a single entity.\n",
      "Cleaning Text Data | 55\n",
      "6. To tokenize the text using the regular expression tokenizer, add the  \n",
      "following code:\n",
      "def tokenize_with_regex_tokenizer(text):\n",
      "    reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
      "    return reg_tokenizer.tokenize(text)\n",
      "tokenize_with_regex_tokenizer(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.12: Tokenization using the regular expression tokenizer\n",
      "56 | Feature Extraction Methods\n",
      "7. To tokenize the text using the whitespace tokenizer, add the following code:\n",
      "def tokenize_with_wst(text):\n",
      "    wh_tokenizer = WhitespaceTokenizer()\n",
      "    return wh_tokenizer.tokenize(text)\n",
      "tokenize_with_wst(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.13: Tokenization using the whitespace tokenizer\n",
      "Cleaning Text Data | 57\n",
      "8. To tokenize the text using the Word Punct tokenizer, add the following code:\n",
      "def tokenize_with_wordpunct_tokenizer(text):\n",
      "    wp_tokenizer = WordPunctTokenizer()\n",
      "    return wp_tokenizer.tokenize(text)\n",
      "tokenize_with_wordpunct_tokenizer(sentence)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.14: Tokenization using the Word Punct tokenizer\n",
      "58 | Feature Extraction Methods\n",
      "In this section, we have learned about different tokenization techniques and their \n",
      "nltk  implementation.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hSbDWi .\n",
      "You can also run this example online at https://packt.live/3hOi7oR .\n",
      "Now, we're ready to use them in our programs. \n",
      "Stemming\n",
      "In many languages, the base forms of words change when they're used in sentences. \n",
      "For example, the word \"produce\" can be written as \"production\" or \"produced\" or \n",
      "even \"producing,\" depending on the context. The process of converting a word back \n",
      "into its base form is known as stemming. It is essential to do this, because without \n",
      "it, algorithms would treat two or more different forms of the same word as different \n",
      "entities, despite them having the same semantic meaning. So, the words \"producing\" \n",
      "and \"produced\" would be treated as different entities, which can lead to erroneous \n",
      "inferences. In Python, RegexpStemmer  and PorterStemmer  are the most widely \n",
      "used stemmers. Let's explore them one at a time.\n",
      "RegexpStemmer\n",
      "RegexpStemmer  uses regular expressions to check whether morphological or \n",
      "structural prefixes or suffixes are present. For instance, in many cases, verbs in the \n",
      "present continuous tense (the present tense form ending with \"ing\") can be restored \n",
      "to their base form simply by removing \"ing\" from the end; for example, \"playing\" \n",
      "becomes \"play\".\n",
      "Let's complete the following exercise to get some hands-on experience with \n",
      "RegexpStemmer .Cleaning Text Data | 59\n",
      "Exercise 2.05: Converting Words in the Present Continuous Tense into Base \n",
      "Words with RegexpStemmer\n",
      "In this exercise, we will use RegexpStemmer  on text to convert words into their \n",
      "basic form by removing some generic suffixes such as \"ing\" and \"ed\". To use nltk 's \n",
      "regex_stemmer , we have to create an object of RegexpStemmer  by passing the \n",
      "regex of the suffix or prefix and an integer, min , which indicates the minimum length \n",
      "of the stemmed string. Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and import RegexpStemmer :\n",
      "from nltk.stem import RegexpStemmer\n",
      "3. Use regex_stemmer  to stem each word of the sentence  variable. Add the \n",
      "following code to do this:\n",
      "def get_stems(text):\n",
      "    \"\"\"\n",
      "    Creating an object of RegexpStemmer, any string ending \n",
      "    with the given regex 'ing$' will be removed.\n",
      "    \"\"\"\n",
      "    regex_stemmer = RegexpStemmer('ing$', min=4) \n",
      "    \"\"\"\n",
      "    The below code line will convert every word into its \n",
      "    stem using regex stemmer and then join them with space.\n",
      "    \"\"\"\n",
      "    return ' '.join([regex_stemmer.stem(wd) for \\\n",
      "                     wd in text.split()])\n",
      "sentence = \"I love playing football\"\n",
      "get_stems(sentence)\n",
      "The preceding code generates the following output:\n",
      "'I love play football'60 | Feature Extraction Methods\n",
      "As we can see, the word playing  has been changed into its base form, play . In this \n",
      "exercise, we learned how we can perform stemming using nltk 's RegexpStemmer .\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hRYUm6 .\n",
      "You can also run this example online at https://packt.live/2D0Ztvk .\n",
      "The Porter Stemmer\n",
      "The Porter stemmer is the most common stemmer for dealing with English words. It \n",
      "removes various morphological and inflectional endings (such as suffixes, prefixes, \n",
      "and the plural \"s\") from English words. In doing so, it helps us extract the base form \n",
      "of a word from its variations. To get a better understanding of this, let's carry out a \n",
      "simple exercise.\n",
      "Exercise 2.06: Using the Porter Stemmer\n",
      "In this exercise, we will apply the Porter stemmer to some text. Follow these steps to \n",
      "complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import nltk  and any related packages and declare a sentence  variable. Add \n",
      "the following code to do this:\n",
      "from nltk.stem.porter import *\n",
      "sentence = \"Before eating, it would be nice to \"\\\n",
      "           \"sanitize your hands with a sanitizer\"Cleaning Text Data | 61\n",
      "3. Now, we'll make use of the Porter stemmer to stem each word of the  \n",
      "sentence  variables:\n",
      "def get_stems(text):\n",
      "    ps_stemmer = PorterStemmer()\n",
      "    return ' '.join([ps_stemmer.stem(wd) for \\\n",
      "                     wd in text.split()])\n",
      "get_stems(sentence)\n",
      "The preceding code generates the following output:\n",
      "'befor eating, it would be nice to sanit your hand wash with a sanit'\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2CUqelc .\n",
      "You can also run this example online at https://packt.live/2X8WUhD .\n",
      "PorterStemmer  is a generic rule-based stemmer that tries to convert a word into \n",
      "its basic form by removing common suffixes and prefixes of the English language.\n",
      "Though stemming is a useful technique in NLP, it has a severe drawback. As we can \n",
      "see from this exercise, we find that, while eating  has been converted into eat  \n",
      "(which is its proper grammatical base form), the word sanitize  has been converted \n",
      "into sanit  (which isn't the proper grammatical base form). This may lead to some \n",
      "problems if we use it. To overcome this issue, there is another technique we can use \n",
      "called lemmatization.\n",
      "Lemmatization\n",
      "As we saw in the previous section, there is a problem with stemming. It often \n",
      "generates meaningless words. Lemmatization deals with such cases by using \n",
      "vocabulary and analyzing the words' morphologies. It returns the base forms of \n",
      "words that can be found in dictionaries. Let's walk through a simple exercise to \n",
      "understand this better.62 | Feature Extraction Methods\n",
      "Exercise 2.07: Performing Lemmatization\n",
      "In this exercise, we will perform lemmatization on some text. Follow these steps to \n",
      "complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import nltk  and its related packages, and then declare a sentence  variable. \n",
      "Add the following code to implement this:\n",
      "import nltk\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk import word_tokenize\n",
      "nltk.download('wordnet')\n",
      "nltk.download('punkt')\n",
      "sentence = \"The products produced by the process today are \"\\\n",
      "           \"far better than what it produces generally.\"\n",
      "3. To lemmatize the tokens, we extracted from the sentence, add the  \n",
      "following code:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "def get_lemmas(text):\n",
      "    lemmatizer = WordNetLemmatizer()\n",
      "    return ' '.join([lemmatizer.lemmatize(word) for \\\n",
      "                     word in word_tokenize(text)])\n",
      "get_lemmas(sentence)\n",
      "The preceding code generates the following output:\n",
      "'The product produced by the process today are far better than what \n",
      "it produce generally.'Cleaning Text Data | 63\n",
      "With that, we learned how to generate the lemma of a word. The lemma is the correct \n",
      "grammatical base form. They use the vocabulary to match the word to its correct \n",
      "nearest grammatical form.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2X5JEKA .\n",
      "You can also run this example online at https://packt.live/30Zqt6v .\n",
      "In the next section, we will deal with other kinds of word variations by looking at \n",
      "singularizing and pluralizing words using textblob .\n",
      "Exercise 2.08: Singularizing and Pluralizing Words\n",
      "In this exercise, we will make use of the textblob  library to singularize and pluralize \n",
      "words in the given text. Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import TextBlob  and declare a sentence  variable. Add the following code to \n",
      "implement this:\n",
      "from textblob import TextBlob\n",
      "sentence = TextBlob('She sells seashells on the seashore')\n",
      "To check the list of words in the sentence, type the following code:\n",
      "sentence.words\n",
      "The preceding code generates the following output:\n",
      "WordList(['She', 'sells', 'seashells', 'on', 'the', 'seashore'])64 | Feature Extraction Methods\n",
      "3. To singularize the third word in the sentence, type the following code:\n",
      "def singularize(word):\n",
      "    return word.singularize()\n",
      "singularize(sentence.words[2])\n",
      "The preceding code generates the following output:\n",
      "'seashell'\n",
      "4. To pluralize the fifth word in the given sentence, type the following code:\n",
      "def pluralize(word):\n",
      "    return word.pluralize()\n",
      "pluralize(sentence.words[5])\n",
      "The preceding code generates the following output:\n",
      "'seashores'\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gooUoQ .\n",
      "You can also run this example online at https://packt.live/309Gqrm .\n",
      "Now, in the next section, we will learn about another preprocessing task:  \n",
      "language translation.\n",
      "Language Translation\n",
      "You might have used Google Translate before, which gives the exact translation of \n",
      "a word in another language; this is an example of language translation or machine \n",
      "translation. In Python, we can use TextBlob  to translate text from one language \n",
      "into another. TextBlob  provides a method called translate() , in which you have \n",
      "to pass text in the source language. The method will return the translated word in the \n",
      "destination language. Let's look at how this is done.Cleaning Text Data | 65\n",
      "Exercise 2.09: Language Translation\n",
      "In this exercise, we will make use of the TextBlob  library to translate a sentence \n",
      "from Spanish into English. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import TextBlob , as follows:\n",
      "from textblob import TextBlob\n",
      "3. Make use of the translate()  function of TextBlob  to translate the input \n",
      "text from Spanish to English. Add the following code to do this:\n",
      "def translate(text,from_l,to_l):\n",
      "    en_blob = TextBlob(text)\n",
      "    return en_blob.translate(from_lang=from_l, to=to_l)\n",
      "translate(text='muy bien',from_l='es',to_l='en')\n",
      "The preceding code generates the following output:\n",
      "TextBlob(\"very well\")\n",
      "With that, we have seen how we can use TextBlob  to translate from one language \n",
      "to another.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2XquGiH .\n",
      "You can also run this example online at https://packt.live/3hQiVK8 .\n",
      "In the next section, we will look at another preprocessing task: stop-word removal.66 | Feature Extraction Methods\n",
      "Stop-Word Removal\n",
      "Stop words, such as \"am,\" \"the,\" and \"are,\" occur frequently in text data. Although \n",
      "they help us construct sentences properly, we can find the meaning even if we \n",
      "remove them. This means that the meaning of text can be inferred even without \n",
      "them. So, removing stop words from text is one of the preprocessing steps in NLP \n",
      "tasks. In Python, nltk , and textblob , text can be used to remove stop words from \n",
      "text. To get a better understanding of this, let's look at an exercise.\n",
      "Exercise 2.10: Removing Stop Words from Text\n",
      "In this exercise, we will remove the stop words from a given text. Follow these steps \n",
      "to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import nltk  and declare a sentence  variable with the text in question:\n",
      "from nltk import word_tokenize\n",
      "sentence = \"She sells seashells on the seashore\"\n",
      "3. Define a remove_stop_words  method and remove the custom list of stop \n",
      "words from the sentence by using the following lines of code:\n",
      "def remove_stop_words(text,stop_word_list):\n",
      "    return ' '.join([word for word in word_tokenize(text) \\\n",
      "                     if word.lower() not in stop_word_list])\n",
      "custom_stop_word_list = ['she', 'on', 'the', 'am', 'is', 'not']\n",
      "remove_stop_words(sentence,custom_stop_word_list)\n",
      "The preceding code generates the following output:\n",
      "'sells seashells seashore'Cleaning Text Data | 67\n",
      "Thus, we've seen how stop words can be removed from a sentence.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/337aMwH .\n",
      "You can also run this example online at https://packt.live/30buvJF .\n",
      "In the next activity, we'll put our knowledge of preprocessing steps into practice.\n",
      "Activity 2.01: Extracting Top Keywords from the News Article\n",
      "In this activity, you will extract the most frequently occurring keywords from a sample \n",
      "news article. \n",
      "Note\n",
      "The new article that's being used for this activity can be found  \n",
      "at https://packt.live/314mg1r .\n",
      "The following steps will help you implement this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import nltk  and any other necessary libraries.\n",
      "3. Define some functions to help you load the text file, convert the string into \n",
      "lowercase, tokenize the text, remove the stop words, and perform stemming on \n",
      "all the remaining tokens. Finally, define a function to calculate the frequency of \n",
      "all these words.\n",
      "4. Load news_article.txt  using a Python file reader into a single string.\n",
      "5. Convert the text string into lowercase.\n",
      "6. Split the string into tokens using a white space tokenizer.68 | Feature Extraction Methods\n",
      "7. Remove any stop words.\n",
      "8. Perform stemming on all the tokens.\n",
      "9. Calculate the frequency of all the words after stemming.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "With that, we have learned about the various ways we can clean unstructured data. \n",
      "Now, let's examine the concept of extracting features from texts.\n",
      "Feature Extraction from Texts\n",
      "As we already know, machine learning algorithms do not understand textual data \n",
      "directly. We need to represent the text data in numerical form or vectors. To convert \n",
      "each textual sentence into a vector, we need to represent it as a set of features. This \n",
      "set of features should uniquely represent the text, though, individually, some of the \n",
      "features may be common across many textual sentences. Features can be classified \n",
      "into two different categories:\n",
      "• General features : These features are statistical calculations and do not depend \n",
      "on the content of the text. Some examples of general features could be the \n",
      "number of tokens in the text, the number of characters in the text, and so on.\n",
      "• Specific features : These features are dependent on the inherent meaning of \n",
      "the text and represent the semantics of the text. For example, the frequency of \n",
      "unique words in the text is a specific feature.\n",
      "Let's explore these in detail.\n",
      "Extracting General Features from Raw Text\n",
      "As we've already learned, general features refer to those that are not directly \n",
      "dependent on the individual tokens constituting a text corpus. Let's consider these \n",
      "two sentences: \"The sky is blue\" and \"The pillar is yellow\". Here, the sentences have \n",
      "the same number of words (a general feature)—that is, four. But the individual \n",
      "constituent tokens are different. Let's complete an exercise to understand this better.Feature Extraction from Texts | 69\n",
      "Exercise 2.11: Extracting General Features from Raw Text\n",
      "In this exercise, we will extract general features from input text. These general \n",
      "features include detecting the number of words, the presence of \"wh\" words (words \n",
      "beginning with \"wh\", such as \"what\" and \"why\") and the language in which the text is \n",
      "written. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the pandas  library and create a DataFrame with four sentences. Add the \n",
      "following code to implement this:\n",
      "import pandas as pd\n",
      "from textblob import TextBlob\n",
      "df = pd.DataFrame([['The interim budget for 2019 will '\\\n",
      "                    'be announced on 1st February.'], \\\n",
      "                   ['Do you know how much expectation '\\\n",
      "                    'the middle-class working population '\\\n",
      "                    'is having from this budget?'], \\\n",
      "                   ['February is the shortest month '\\\n",
      "                    'in a year.'], \\\n",
      "                   ['This financial year will end on '\\\n",
      "                    '31st March.']])\n",
      "df.columns = ['text']\n",
      "df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 2.15: DataFrame consisting of four sentences\n",
      "70 | Feature Extraction Methods\n",
      "3. Use the apply()  function to iterate through each row of the column text, \n",
      "convert them into TextBlob  objects, and extract words from them. Add the \n",
      "following code to implement this:\n",
      "def add_num_words(df):\n",
      "    df['number_of_words'] = df['text'].apply(lambda x : \\\n",
      "                            len(TextBlob(str(x)).words))\n",
      "    return df\n",
      "add_num_words(df)['number_of_words']\n",
      "The preceding code generates the following output:\n",
      "0     11\n",
      "1     15\n",
      "2      8\n",
      "3      8\n",
      "Name:  number_of_words, dtype: int64\n",
      "The preceding code line will print the number_of_words  column of the \n",
      "DataFrame to represent the number of words in each row.\n",
      "4. Use the apply()  function to iterate through each row of the column text, \n",
      "convert the text into TextBlob  objects, and extract the words from them \n",
      "to check whether any of them belong to the list of \"wh\" words that has been \n",
      "declared. Add the following code to do so:\n",
      "def is_present(wh_words, df):\n",
      "    \"\"\"\n",
      "    The below line of code will find the intersection \n",
      "    between set of tokens of every sentence and the \n",
      "    wh_words and will return true if the length of \n",
      "    intersection set is non-zero.\n",
      "    \"\"\"\n",
      "    df['is_wh_words_present'] = df['text'].apply(lambda x : \\\n",
      "                                True if \\\n",
      "                                len(set(TextBlob(str(x)).\\\n",
      "                                words).intersection(wh_words))\\\n",
      "                                >0 else False)\n",
      "    return dfFeature Extraction from Texts | 71\n",
      "wh_words = set(['why', 'who', 'which', 'what', \\\n",
      "                'where', 'when', 'how'])\n",
      "is_present(wh_words, df)['is_wh_words_present']\n",
      "The preceding code generates the following output:\n",
      "0     False\n",
      "1     True\n",
      "2     False\n",
      "3     False\n",
      "Name:  is_wh_words_present, dtype: bool\n",
      "The preceding code line will print the is_wh_words_present  column that \n",
      "was added by the is_present  method to df, which means for every row, we \n",
      "will see whether wh_word  is present.\n",
      "5. Use the apply()  function to iterate through each row of the column text, \n",
      "convert them into TextBlob  objects, and detect their languages:\n",
      "def get_language(df):\n",
      "    df['language'] = df['text'].apply(lambda x : \\\n",
      "                     TextBlob(str(x)).detect_language())\n",
      "    return df\n",
      "get_language(df)['language']\n",
      "The preceding code generates the following output:\n",
      "0     en\n",
      "1     en\n",
      "2     en\n",
      "3     en\n",
      "Name:  language, dtype: object\n",
      "With that, we have learned how to extract general features from text data.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2X9jLcS .\n",
      "You can also run this example online at https://packt.live/3fgrYSK .\n",
      "Let's perform another exercise to get a better understanding of this.72 | Feature Extraction Methods\n",
      "Exercise 2.12: Extracting General Features from Text\n",
      "In this exercise, we will extract various general features from documents. The dataset \n",
      "that we will be using here consists of random statements. Our objective is to find the \n",
      "frequency of various general features such as punctuation, uppercase and lowercase \n",
      "words, letters, digits, words, and whitespaces.\n",
      "Note\n",
      "The dataset that is being used in this exercise can be found at this link: \n",
      "https://packt.live/3k0qCPR .\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "from string import punctuation\n",
      "import nltk\n",
      "nltk.download('tagsets')\n",
      "from nltk.data import load\n",
      "nltk.download('averaged_perceptron_tagger')\n",
      "from nltk import pos_tag\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "3. To see what different kinds of parts of speech nltk  provides, add the  \n",
      "following code:\n",
      "def get_tagsets():\n",
      "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
      "    return list(tagdict.keys())\n",
      "tag_list = get_tagsets()\n",
      "print(tag_list)Feature Extraction from Texts | 73\n",
      "The preceding code generates the following output:\n",
      "Figure 2.16: List of PoS\n",
      "4. Calculate the number of occurrences of each PoS by iterating through each \n",
      "document and annotating each word with the corresponding pos  tag. Add the \n",
      "following code to implement this:\n",
      "\"\"\"\n",
      "This method will count the occurrence of pos \n",
      "tags in each sentence.\n",
      "\"\"\"\n",
      "def get_pos_occurrence_freq(data, tag_list):\n",
      "    # Get list of sentences in text_list\n",
      "    text_list = data.text\n",
      "    \n",
      "    # create empty dataframe\n",
      "    feature_df = pd.DataFrame(columns=tag_list)\n",
      "    for text_line in text_list:\n",
      "        \n",
      "        # get pos tags of each word.\n",
      "        pos_tags = [j for i, j in \\\n",
      "                    pos_tag(word_tokenize(text_line))]\n",
      "        \n",
      "        \"\"\"\n",
      "        create a dict of pos tags and their frequency \n",
      "        in given sentence.\n",
      "        \"\"\"\n",
      "        row = dict(Counter(pos_tags))\n",
      "        feature_df = feature_df.append(row, ignore_index=True)\n",
      "    feature_df.fillna(0, inplace=True)\n",
      "    return feature_df\n",
      "tag_list = get_tagsets()\n",
      "data = pd.read_csv('../data/data.csv', header=0)\n",
      "feature_df = get_pos_occurrence_freq(data, tag_list)\n",
      "feature_df.head()\n",
      "74 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "Figure 2.17: Number of occurrences of each PoS in the sentence\n",
      "5. To calculate the number of punctuation marks, add the following code:\n",
      "def add_punctuation_count(feature_df, data):\n",
      "    \n",
      "    feature_df['num_of_unique_punctuations'] = data['text'].\\\n",
      "        apply(lambda x: len(set(x).intersection\\\n",
      "        (set(punctuation))))\n",
      "    return feature_df\n",
      "feature_df = add_punctuation_count(feature_df, data)\n",
      "feature_df['num_of_unique_punctuations'].head()\n",
      "The add_punctuation_count()  method will find the intersection of the \n",
      "set of punctuation marks in the text and punctuation sets that were imported \n",
      "from the string  module. Then, it will find the length of the intersection set in \n",
      "each row and add it to the num_of_unique_punctuations  column of the \n",
      "DataFrame. The preceding code generates the following output:\n",
      "0      0\n",
      "1      0\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "Name:  num_of_unique_punctuations, dtype: int64\n",
      "Feature Extraction from Texts | 75\n",
      "6. To calculate the number of capitalized words, add the following code:\n",
      "def get_capitalized_word_count(feature_df, data):\n",
      "    \"\"\"\n",
      "    The below code line will tokenize text in every row and \n",
      "    create a set of only capital words, ten find the length of \n",
      "    this set and add it to the column 'number_of_capital_words' \n",
      "    of dataframe.\n",
      "    \"\"\"\n",
      "    feature_df['number_of_capital_words'] = data['text'].\\\n",
      "        apply(lambda x: len([word for word in \\\n",
      "        word_tokenize(str(x)) if word[0].isupper()]))\n",
      "    return feature_df\n",
      "feature_df = get_capitalized_word_count(feature_df, data)\n",
      "feature_df['number_of_capital_words'].head()\n",
      "The preceding code will tokenize the text in every row and create a set of words \n",
      "consisting of only capital words. It will then find the length of this set and add it \n",
      "to the number_of_capital_words  column of the DataFrame. The preceding \n",
      "code generates the following output:\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "Name:  number_of_capital_words, dtype: int64\n",
      "The last line of the preceding code will print the number_of_capital_words  \n",
      "column, which represents the count of the number of capital letter words in  \n",
      "each row. 76 | Feature Extraction Methods\n",
      "7. To calculate the number of lowercase words, add the following code:\n",
      "def get_small_word_count(feature_df, data):\n",
      "    \"\"\"\n",
      "    The below code line will tokenize text in every row and \n",
      "    create a set of only small words, then find the length of \n",
      "    this set and add it to the column 'number_of_small_words' \n",
      "    of dataframe.\n",
      "    \"\"\"\n",
      "    feature_df['number_of_small_words'] = data['text'].\\\n",
      "        apply(lambda x: len([word for word in \\\n",
      "        word_tokenize(str(x)) if word[0].islower()]))\n",
      "    return feature_df\n",
      "feature_df = get_small_word_count(feature_df, data)\n",
      "feature_df['number_of_small_words'].head()\n",
      "The preceding code will tokenize the text in every row and create a set of only \n",
      "small words, then find the length of this set and add it to the number_of_\n",
      "small_words  column of the DataFrame. The preceding code generates the \n",
      "following output:\n",
      "0      4\n",
      "1      3\n",
      "2      7\n",
      "3      3\n",
      "4      2\n",
      "Name:  number_of_small_words, dtype: int64\n",
      "The last line of the preceding code will print the number_of_small_words  \n",
      "column, which represents the number of small letter words in each row.\n",
      "8. To calculate the number of letters in the DataFrame, use the following code:\n",
      "def get_number_of_alphabets(feature_df, data):\n",
      "    feature_df['number_of_alphabets'] = data['text']. \\\n",
      "        apply(lambda x: len([ch for ch in str(x) \\\n",
      "        if ch.isalpha()]))\n",
      "    return feature_dfFeature Extraction from Texts | 77\n",
      "feature_df = get_number_of_alphabets(feature_df, data)\n",
      "feature_df['number_of_alphabets'].head()\n",
      "The preceding code will break the text line into a list of characters in each row \n",
      "and add the count of that list to the number_of_alphabets  columns. This will \n",
      "produce the following output:\n",
      "0     19\n",
      "1     18\n",
      "2     28\n",
      "3     14\n",
      "4     13\n",
      "Name:  number_of_alphabets, dtype: int64\n",
      "The last line of the preceding code will print the number_of_columns  column, \n",
      "which represents the count of the number of alphabets in each row.\n",
      "9. To calculate the number of digits in the DataFrame, add the following code:\n",
      "def get_number_of_digit_count(feature_df, data):\n",
      "    \"\"\"\n",
      "    The below code line will break the text line in a list of \n",
      "    digits in each row and add the count of that list into \n",
      "    the columns 'number_of_digits'\n",
      "    \"\"\"\n",
      "    feature_df['number_of_digits'] = data['text']. \\\n",
      "        apply(lambda x: len([ch for ch in str(x) \\\n",
      "        if ch.isdigit()]))\n",
      "    return feature_df\n",
      "feature_df = get_number_of_digit_count(feature_df, data)\n",
      "feature_df['number_of_digits'].head()\n",
      "The preceding code will get the digit count from each row and add the count of \n",
      "that list to the number_of_digits  columns. The preceding code generates \n",
      "the following output:\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "Name:  number_of_digits, dtype: int6478 | Feature Extraction Methods\n",
      "10. To calculate the number of words in the DataFrame, add the following code:\n",
      "def get_number_of_words(feature_df, data):\n",
      "    \"\"\"\n",
      "    The below code line will break the text line in a list of \n",
      "    words in each row and add the count of that list into \n",
      "    the columns 'number_of_digits'\n",
      "    \"\"\"\n",
      "    feature_df['number_of_words'] = data['text'].\\\n",
      "        apply(lambda x : len(word_tokenize(str(x))))\n",
      "    return feature_df\n",
      "feature_df = get_number_of_words(feature_df, data)\n",
      "feature_df['number_of_words'].head()\n",
      "The preceding code will split the text line into a list of words in each row and \n",
      "add the count of that list to the number_of_digits  columns. We will get the \n",
      "following output:\n",
      "0      5\n",
      "1      4\n",
      "2      9\n",
      "3      5\n",
      "4      3\n",
      "Name:  number_of_words, dtype: int64\n",
      "11. To calculate the number of whitespaces in the DataFrame, add the  \n",
      "following code:\n",
      "def get_number_of_whitespaces(feature_df, data):\n",
      "    \"\"\"\n",
      "    The below code line will generate list of white spaces \n",
      "    in each row and add the length of that list into \n",
      "    the columns 'number_of_white_spaces\n",
      "    \"\"\"\n",
      "    feature_df['number_of_white_spaces'] = data['text']. \\\n",
      "        apply(lambda x: len([ch for ch in str(x) \\\n",
      "        if ch.isspace()]))\n",
      "    return feature_dfFeature Extraction from Texts | 79\n",
      "feature_df = get_number_of_whitespaces(feature_df, data)\n",
      "feature_df['number_of_white_spaces'].head()\n",
      "The preceding code will generate a list of whitespaces in each row and add the \n",
      "length of that list to the number_of_white_spaces  columns. The preceding \n",
      "code generates the following output:\n",
      "0      4\n",
      "1      3\n",
      "2      7\n",
      "3      3\n",
      "4      2\n",
      "Name:  number_of_white_spaces, dtype: int64\n",
      "12. To view the full feature set we have just created, add the following code:\n",
      "feature_df.head()\n",
      "We will be printing the head of the final DataFrame, which means we will print \n",
      "five rows of all the columns. We will get the following output:\n",
      "Figure 2.18: DataFrame consisting of the features we have created\n",
      "With that, we have learned how to extract general features from the given text.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3jSsLNh .\n",
      "You can also run this example online at https://packt.live/3hPFmPA .\n",
      "Now, let's explore how we can extract unique features.\n",
      "80 | Feature Extraction Methods\n",
      "Bag of Words (BoW)\n",
      "The Bag of Words  (BoW ) model is one of the most popular methods for extracting \n",
      "features from raw texts.\n",
      "In this technique, we convert each sentence into a vector. The length of this vector is \n",
      "equal to the number of unique words in all the documents. This is done in two steps:\n",
      "1. The vocabulary or dictionary of all the words is generated.\n",
      "2. The document is represented in terms of the presence or absence of all words.\n",
      "A vocabulary or dictionary is created from all the unique possible words available in \n",
      "the corpus (all documents) and every single word is assigned a unique index number. \n",
      "In the second step, every document is represented by a list whose length is equal to \n",
      "the number of words in the vocabulary. The following exercise illustrates how BoW \n",
      "can be implemented using Python.\n",
      "Exercise 2.13: Creating a Bag of Words\n",
      "In this exercise, we will create a BoW representation for all the terms in a document \n",
      "and ascertain the 10 most frequent terms. In this exercise, we will use the \n",
      "CountVectorizer  module from sklearn , which performs the following tasks:\n",
      "• Tokenizes the collection of documents, also called a corpus\n",
      "• Builds the vocabulary of unique words\n",
      "• Converts a document into vectors using the previously built vocabulary\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the necessary libraries and declare a list corpus. Add the following code \n",
      "to implement this:\n",
      "import pandas as pd\n",
      "from sklearn.feature_extraction.text import CountVectorizerFeature Extraction from Texts | 81\n",
      "3. Use the CountVectorizer  function to create the BoW model. Add the \n",
      "following code to do this:\n",
      "def vectorize_text(corpus):\n",
      "    \"\"\"\n",
      "    Will return a dataframe in which every row will ,be\n",
      "    vector representation of a document in corpus\n",
      "    :param corpus: input text corpus\n",
      "    :return: dataframe of vectors\n",
      "    \"\"\"\n",
      "    bag_of_words_model = CountVectorizer()\n",
      "    \"\"\"\n",
      "    performs the above described three tasks on \n",
      "    the given data corpus.\n",
      "    \"\"\"\n",
      "    dense_vec_matrix = bag_of_words_model.\\\n",
      "                       fit_transform(corpus).todense()\n",
      "    bag_of_word_df = pd.DataFrame(dense_vec_matrix)\n",
      "    bag_of_word_df.columns = sorted(bag_of_words_model.\\\n",
      "                                    vocabulary_)\n",
      "    return bag_of_word_df\n",
      "corpus = ['Data Science is an overlap between Arts and Science',\\\n",
      "          'Generally, Arts graduates are right-brained and '\\\n",
      "          'Science graduates are left-brained',\\\n",
      "          'Excelling in both Arts and Science at a time '\\\n",
      "          'becomes difficult',\\\n",
      "          'Natural Language Processing is a part of Data Science']\n",
      "df = vectorize_text(corpus)\n",
      "df.head()\n",
      "The vectorize_text  method will take a document corpus as an argument \n",
      "and return a DataFrame in which every row will be a vector representation of a \n",
      "document in the corpus.82 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "Figure 2.19: DataFrame of the output of the BoW model\n",
      "4. Create a BoW model for the 10 most frequent terms. Add the following code to \n",
      "implement this:\n",
      "def bow_top_n(corpus, n):\n",
      "    \"\"\"\n",
      "    Will return a dataframe in which every row \n",
      "    will be represented by presence or absence of top 10 most \n",
      "    frequently occurring words in data corpus\n",
      "    :param corpus: input text corpus\n",
      "    :return: dataframe of vectors\n",
      "    \"\"\"\n",
      "    bag_of_words_model_small = CountVectorizer(max_features=n)\n",
      "    bag_of_word_df_small = pd.DataFrame\\\n",
      "    (bag_of_words_model_small.fit_transform\\\n",
      "    (corpus).todense())\n",
      "    bag_of_word_df_small.columns = \\\n",
      "    sorted(bag_of_words_model_small.vocabulary_)\n",
      "    return bag_of_word_df_small\n",
      "df_2 = bow_top_n(corpus, 10)\n",
      "df_2.head()\n",
      "In the preceding code, we are checking the occurrence of the top 10 most \n",
      "frequent words in each sentence and creating a DataFrame out of it.\n",
      "Feature Extraction from Texts | 83\n",
      "The preceding code generates the following output:\n",
      "Figure 2.20: DataFrame of the output of the BoW model for the 10 most frequent terms\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gdhViJ .\n",
      "You can also run this example online at https://packt.live/3hPUTi8 .\n",
      "In this section, we learned what BoW is and how to can use it to convert a sentence or \n",
      "document into a vector. BoW is the easiest way to convert text into a vector; however, \n",
      "it has a severe disadvantage. This method only considers the presence and absence \n",
      "of words in a sentence or document—not the frequency of the words/tokens in a \n",
      "document. If we are going to use the semantics of any sentence, the frequency of \n",
      "the words plays an important role. To overcome this issue, there is another feature \n",
      "extraction model called TFIDF, which we will discuss later in this chapter.\n",
      "Zipf's Law \n",
      "According to Zipf's law, the number of times a word occurs in a corpus is inversely \n",
      "proportional to its rank in the frequency table. In simple terms, if the words in a \n",
      "corpus are arranged in descending order of their frequency of occurrence, then the \n",
      "frequency of the word at the ith rank will be proportional to 1/i:\n",
      "Figure 2.21: Zipf's law\n",
      "84 | Feature Extraction Methods\n",
      "This also means that the frequency of the most frequent word will be twice the \n",
      "frequency of the second most frequent word. For example, if we look at the Brown \n",
      "University Standard Corpus of Present-Day American English, the word \"the\" is the \n",
      "most frequent word (its frequency is 69,971), while the word \"of\" is the second most \n",
      "frequent (with a frequency of 36,411). As we can see, its frequency is almost half \n",
      "of the most frequently occurring word. To get a better understanding of this, let's \n",
      "perform a simple exercise.\n",
      "Exercise 2.14: Zipf's Law\n",
      "In this exercise, we will plot both the expected and actual ranks and frequencies \n",
      "of tokens with the help of Zipf's law. We will be using the 20newsgroups  dataset \n",
      "provided by the sklearn  library, which is a collection of newsgroup documents. \n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the necessary libraries:\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import string\n",
      "from collections import Counter\n",
      "Add two methods for loading stop words and the data from the  \n",
      "newsgroups_data_sample  variable:\n",
      "def get_stop_words():\n",
      "    stop_words = stopwords.words('english')\n",
      "    stop_words = stop_words + list(string.printable)\n",
      "    return stop_wordsFeature Extraction from Texts | 85\n",
      "def get_and_prepare_data(stop_words):\n",
      "    \"\"\"\n",
      "    This method will load 20newsgroups data and \n",
      "    and remove stop words from it using given stop word list.\n",
      "    :param stop_words: \n",
      "    :return: \n",
      "    \"\"\"\n",
      "    newsgroups_data_sample = \\\n",
      "    fetch_20newsgroups(subset='train')\n",
      "    tokenized_corpus = [word.lower() for sentence in \\\n",
      "                        newsgroups_data_sample['data'] \\\n",
      "                        for word in word_tokenize\\\n",
      "                        (re.sub(r'([^\\s\\w]|_)+', ' ', sentence)) \\\n",
      "                        if word.lower() not in stop_words]\n",
      "    return tokenized_corpus\n",
      "In the preceding code, there are two methods; get_stop_words()  will load \n",
      "stop word list from nltk  data, while get_and_prepare_data()  will load the \n",
      "20newsgroups  data and remove stop words from it using the given stop  \n",
      "word list.\n",
      "3. Add the following method to calculate the frequency of each token:\n",
      "def get_frequency(corpus, n):\n",
      "    token_count_di = Counter(corpus)\n",
      "    return token_count_di.most_common(n)\n",
      "The preceding method uses the Counter  class to count the frequency of tokens \n",
      "in the corpus and then return the most common n tokens.\n",
      "4. Now, call all the preceding methods to calculate the frequency of the top 50 \n",
      "most frequent tokens:\n",
      "stop_word_list = get_stop_words()\n",
      "corpus = get_and_prepare_data(stop_word_list)\n",
      "get_frequency(corpus, 50)86 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "Figure 2.22: The 50 most frequent words of the corpus\n",
      "Feature Extraction from Texts | 87\n",
      "5. Plot the actual ranks of words that we got from frequency dictionary and the \n",
      "ranks expected as per Zipf's law. Calculate the frequencies of the top 10,000 \n",
      "words using the preceding get_frequency()  method and the expected \n",
      "frequencies of the same list using Zipf's law. For this, create two lists—an \n",
      "actual_frequencies  and an expected_frequencies  list. Use the log \n",
      "of actual frequencies to downscale the numbers. After getting the actual and \n",
      "expected frequencies, plot them using matplotlib:\n",
      "def get_actual_and_expected_frequencies(corpus):\n",
      "    freq_dict = get_frequency(corpus, 1000)\n",
      "    actual_frequencies = []\n",
      "    expected_frequencies = []\n",
      "    for rank, tup in enumerate(freq_dict):\n",
      "        actual_frequencies.append(log(tup[1]))\n",
      "        rank = 1 if rank == 0 else rank\n",
      "        # expected frequency 1/rank as per zipf's law\n",
      "        expected_frequencies.append(1 / rank)\n",
      "    return actual_frequencies, expected_frequencies\n",
      "def plot(actual_frequencies, expected_frequencies):\n",
      "    plt.plot(actual_frequencies, 'g*', \\\n",
      "             expected_frequencies, 'ro')\n",
      "    plt.show()\n",
      "# We will plot the actual and expected frequencies\n",
      "actual_frequencies, expected_frequencies = \\\n",
      "get_actual_and_expected_frequencies(corpus)\n",
      "plot(actual_frequencies, expected_frequencies)88 | Feature Extraction Methods\n",
      "The preceding code generates the following output:\n",
      "Figure 2.23: Illustration of Zipf's law\n",
      "So, as we can see from the preceding output, both lines have almost the same slope. \n",
      "In other words, we can say that the lines (or graphs) depict the proportionality of  \n",
      "two lists.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/30ZnKtD .\n",
      "You can also run this example online at https://packt.live/3f9ZFoT .\n",
      "Feature Extraction from Texts | 89\n",
      "Term Frequency–Inverse Document Frequency (TFIDF)\n",
      "Term Frequency-Inverse Document Frequency  (TFIDF ) is another method of \n",
      "representing text data in a vector format. Here, once again, we'll represent each \n",
      "document as a list whose length is equal to the number of unique words/tokens in \n",
      "all documents (corpus), but the vector here not only represents the presence and \n",
      "absence of a word, but also the frequency of the word—both in the current document \n",
      "and the whole corpus. \n",
      "This technique is based on the idea that the rarely occurring words are better \n",
      "representatives of the document than frequently occurring words. Hence, this \n",
      "representation gives more weightage to the rarer or less frequent words than \n",
      "frequently occurring words. It does so with the following formula:\n",
      "Figure 2.24: TFIDF formula\n",
      "Here, term frequency is the frequency of a word in the given document. Inverse \n",
      "document frequency can be defined as log(D/df), where df is document frequency \n",
      "and D is the total number of documents in the background corpus.\n",
      "Now, let's complete an exercise and learn how TFIDF can be implemented in Python.\n",
      "Exercise 2.15: TFIDF Representation\n",
      "In this exercise, we will represent the input texts with their TFIDF vectors. We will use \n",
      "a sklearn  module named TfidfVectorizer , which converts text into TFIDF \n",
      "vectors. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import all the necessary libraries and create a method to calculate the TFIDF of \n",
      "the corpus. Add the following code to implement this:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "def get_tf_idf_vectors(corpus):\n",
      "    tfidf_model = TfidfVectorizer()\n",
      "    vector_list = tfidf_model.fit_transform(corpus).todense()\n",
      "    return vector_list\n",
      "90 | Feature Extraction Methods\n",
      "3. To create a TFIDF model, write the following code:\n",
      "corpus = ['Data Science is an overlap between Arts and Science',\\\n",
      "          'Generally, Arts graduates are right-brained and '\\\n",
      "          'Science graduates are left-brained',\\\n",
      "          'Excelling in both Arts and Science at a '\\\n",
      "          'time becomes difficult',\\\n",
      "          'Natural Language Processing is a part of Data Science']\n",
      "vector_list = get_tf_idf_vectors(corpus)\n",
      "print(vector_list)\n",
      "In the preceding code, the get_tf_idf_vectors()  method will generate \n",
      "TFIDF vectors from the corpus. You will then call this method on a given corpus. \n",
      "The preceding code generates the following output:\n",
      "Figure 2.25: TFIDF representation of the 10 most frequent terms\n",
      "Finding Text Similarity – Application of Feature Extraction | 91\n",
      "The preceding output represents the TFIDF vectors for each row. As you can see \n",
      "from the results, each document is represented by a list whose length is equal to the \n",
      "unique words in the corpus and in each list (vector). The vector contains the TFIDF \n",
      "values of the words at their corresponding index.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gdzsHA .\n",
      "You can also run this example online at https://packt.live/3fdP5gS .\n",
      "In the next section, we will solve an activity to extract specific features from texts.\n",
      "Finding Text Similarity – Application of Feature Extraction\n",
      "So far in this chapter, we have learned how to generate vectors from text. These \n",
      "vectors are then fed to machine learning algorithms to perform various tasks. Other \n",
      "than using them in machine learning applications, we can also perform simple \n",
      "NLP tasks using these vectors. Finding the string similarity is one of them. This is a \n",
      "technique in which we find the similarity between two strings by converting them into \n",
      "vectors. The technique is mainly used in full-text searching.\n",
      "There are different techniques for finding the similarity between two strings or texts. \n",
      "They are explained one by one here:\n",
      "• Cosine similarity : The cosine similarity is a technique to find the similarity \n",
      "between the two vectors by calculating the cosine of the angle between them. \n",
      "As we know, the cosine of a zero-degree angle is 1 (meaning the cosine similarity \n",
      "of two identical vectors is 1), while the cosine of 180 degrees is -1 (meaning \n",
      "the cosine of two opposite vectors is -1). Thus, we can use this cosine angle to \n",
      "find the similarity between the vectors from 1 to -1. To use this technique in \n",
      "finding text similarity, we convert text into vectors using one of the previously \n",
      "discussed techniques and find the similarity between the vectors of the text. This \n",
      "is calculated as follows:\n",
      "Figure 2.26: Cosine similarity\n",
      "92 | Feature Extraction Methods\n",
      "Here, A and B are two vectors, A.B is the dot product of two vectors, and |A| and \n",
      "|B| are the magnitude of two vectors.\n",
      "• Jaccard similarity : This is another technique that's used to calculate the \n",
      "similarity between the two texts, but it only works on BoW vectors. The Jaccard \n",
      "similarity is calculated as the ratio of the number of terms that are common \n",
      "between two text documents to the total number of unique terms present in \n",
      "those texts.\n",
      "Consider the following example. Suppose there are two texts:\n",
      "Text 1: I like detective Byomkesh Bakshi.\n",
      "Text 2: Byomkesh Bakshi is not a detective; he is a truth seeker.\n",
      "The common terms are \"Byomkesh,\" \"Bakshi,\" and \"detective.\"\n",
      "The number of common terms in the texts is three.\n",
      "The unique terms present in the texts are \"I,\" \"like,\" \"is,\" \"not,\" \"a,\" \"he,\" \"is,\" \n",
      "\"truth,\" and \"seeker.\" So, the number of unique terms is nine.\n",
      "Therefore, the Jaccard similarity is 3/9 = 0.3.\n",
      "To get a better understanding of text similarity, we will complete an exercise.\n",
      "Exercise 2.16: Calculating Text Similarity Using Jaccard and Cosine Similarity\n",
      "In this exercise, we will calculate the Jaccard and cosine similarity for a given pair of \n",
      "texts. Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "lemmatizer = WordNetLemmatizer()Finding Text Similarity – Application of Feature Extraction | 93\n",
      "3. Create a function to extract the Jaccard similarity between a pair of sentences by \n",
      "adding the following code:\n",
      "def extract_text_similarity_jaccard(text1, text2):\n",
      "    \"\"\"\n",
      "    This method will return Jaccard similarity between two texts\n",
      "    after lemmatizing them.\n",
      "    :param text1: text1\n",
      "    :param text2: text2\n",
      "    :return: similarity measure\n",
      "    \"\"\"\n",
      "    lemmatizer = WordNetLemmatizer()\n",
      "    words_text1 = [lemmatizer.lemmatize(word.lower()) \\\n",
      "                   for word in word_tokenize(text1)]\n",
      "    words_text2 = [lemmatizer.lemmatize(word.lower()) \\\n",
      "                   for word in word_tokenize(text2)]\n",
      "    nr = len(set(words_text1).intersection(set(words_text2)))\n",
      "    dr = len(set(words_text1).union(set(words_text2)))\n",
      "    jaccard_sim = nr / dr\n",
      "    return jaccard_sim\n",
      "4. Declare three variables named pair1 , pair2 , and pair3 , as follows.\n",
      "pair1 = [\"What you do defines you\", \"Your deeds define you\"]\n",
      "pair2 = [\"Once upon a time there lived a king.\", \\\n",
      "         \"Who is your queen?\"]\n",
      "pair3 = [\"He is desperate\", \"Is he not desperate?\"]\n",
      "5. To check the Jaccard similarity between the statements in pair1 , write the \n",
      "following code: \n",
      "extract_text_similarity_jaccard(pair1[0],pair1[1])\n",
      "The preceding code generates the following output:\n",
      "0.1428571428571428594 | Feature Extraction Methods\n",
      "6. To check the Jaccard similarity between the statements in pair2 , write the \n",
      "following code:\n",
      "extract_text_similarity_jaccard(pair2[0],pair2[1])\n",
      "The preceding code generates the following output:\n",
      "0.0\n",
      "7. To check the Jaccard similarity between the statements in pair3 , write the \n",
      "following code:\n",
      "extract_text_similarity_jaccard(pair3[0],pair3[1])\n",
      "The preceding code generates the following output:\n",
      "0.6\n",
      "8. To check the cosine similarity, use the TfidfVectorizer()  method to get the \n",
      "vectors of each text: \n",
      "def get_tf_idf_vectors(corpus):\n",
      "    tfidf_vectorizer = TfidfVectorizer()\n",
      "    tfidf_results = tfidf_vectorizer.fit_transform(corpus).\\\n",
      "                    todense()\n",
      "    return tfidf_results\n",
      "9. Create a corpus as a list of texts and get the TFIDF vectors of each text \n",
      "document. Add the following code to do this:\n",
      "corpus = [pair1[0], pair1[1], pair2[0], \\\n",
      "          pair2[1], pair3[0], pair3[1]]\n",
      "tf_idf_vectors = get_tf_idf_vectors(corpus)\n",
      "10. To check the cosine similarity between the initial two texts, write the  \n",
      "following code:\n",
      "cosine_similarity(tf_idf_vectors[0],tf_idf_vectors[1])\n",
      "The preceding code generates the following output:\n",
      "array([[0.3082764]])Finding Text Similarity – Application of Feature Extraction | 95\n",
      "11. To check the cosine similarity between the third and fourth texts, write the \n",
      "following code:\n",
      "cosine_similarity(tf_idf_vectors[2],tf_idf_vectors[3])\n",
      "The preceding code generates the following output:\n",
      "array([[0.]])\n",
      "12. To check the cosine similarity between the fifth and sixth texts, write the \n",
      "following code:\n",
      "cosine_similarity(tf_idf_vectors[4],tf_idf_vectors[5])\n",
      "The preceding code generates the following output:\n",
      "array([[0.80368547]])\n",
      "So, in this exercise, we learned how to check the similarity between texts. As you can \n",
      "see, the texts \"He is desperate\"  and \"Is he not desperate?\"  returned \n",
      "similarity results of 0.80 (meaning they are highly similar), whereas sentences such \n",
      "as \"Once upon a time there lived a king.\"  and \"Who is your \n",
      "queen?\"  returned zero as their similarity measure.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2Eyw0JC .\n",
      "You can also run this example online at https://packt.live/2XbGRQ3 .\n",
      "Word Sense Disambiguation Using the Lesk Algorithm\n",
      "The Lesk algorithm is used for resolving word sense disambiguation. Suppose \n",
      "we have a sentence such as \"On the bank of river Ganga, there lies the scent of \n",
      "spirituality\" and another sentence, \"I'm going to withdraw some cash from the bank\". \n",
      "Here, the same word—that is, \"bank\"—is used in two different contexts. For text \n",
      "processing results to be accurate, the context of the words needs to be considered. \n",
      "In the Lesk algorithm, words with ambiguous meanings are stored in the background \n",
      "in synsets . The definition that is closer to the meaning of a word being used in the \n",
      "context of the sentence will be taken as the right definition. Let's perform a simple \n",
      "exercise to get a better idea of how we can implement this.96 | Feature Extraction Methods\n",
      "Exercise 2.17: Implementing the Lesk Algorithm Using String Similarity and \n",
      "Text Vectorization\n",
      "In this exercise, we are going to implement the Lesk algorithm step by step using the \n",
      "techniques we have learned so far. We will find the meaning of the word \"bank\" in \n",
      "the sentence, \"On the banks of river Ganga, there lies the scent of spirituality.\" We will \n",
      "use cosine similarity as well as Jaccard similarity here. Follow these steps to complete \n",
      "this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from nltk import word_tokenize\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "import numpy as np\n",
      "3. Define a method for getting the TFIDF vectors of a corpus:\n",
      "def get_tf_idf_vectors(corpus):\n",
      "    tfidf_vectorizer = TfidfVectorizer()\n",
      "    tfidf_results = tfidf_vectorizer.fit_transform\\\n",
      "                    (corpus).todense()\n",
      "    return tfidf_results\n",
      "4. Define a method to convert the corpus into lowercase:\n",
      "def to_lower_case(corpus):\n",
      "    lowercase_corpus = [x.lower() for x in corpus]\n",
      "    return lowercase_corpus\n",
      "5. Define a method to find the similarity between the sentence and the possible \n",
      "definitions and return the definition with the highest similarity score:\n",
      "def find_sentence_definition(sent_vector,defnition_vectors):\n",
      "    \"\"\"\n",
      "    This method will find cosine similarity of sentence with\n",
      "    the possible definitions and return the one with \n",
      "    highest similarity score along with the similarity score.\n",
      "    \"\"\"\n",
      "    result_dict = {}Finding Text Similarity – Application of Feature Extraction | 97\n",
      "    for definition_id,def_vector in definition_vectors.items():\n",
      "        sim = cosine_similarity(sent_vector,def_vector)\n",
      "        result_dict[definition_id] = sim[0][0]\n",
      "    definition  = sorted(result_dict.items(), \\\n",
      "                         key=lambda x: x[1], \\\n",
      "                         reverse=True)[0]\n",
      "    return definition[0],definition[1]\n",
      "6. Define a corpus with random sentences with the sentence and the two \n",
      "definitions as the top three sentences:\n",
      "corpus = [\"On the banks of river Ganga, there lies the scent \"\\\n",
      "          \"of spirituality\",\\\n",
      "          \"An institute where people can store extra \"\\\n",
      "          \"cash or money.\",\\\n",
      "          \"The land alongside or sloping down to a river or lake\"\n",
      "          \"What you do defines you\",\\\n",
      "          \"Your deeds define you\",\\\n",
      "          \"Once upon a time there lived a king.\",\\\n",
      "          \"Who is your queen?\",\\\n",
      "          \"He is desperate\",\\\n",
      "          \"Is he not desperate?\"]\n",
      "7. Use the previously defined methods to find the definition of the word bank:\n",
      "lower_case_corpus  = to_lower_case(corpus)\n",
      "corpus_tf_idf  = get_tf_idf_vectors(lower_case_corpus)\n",
      "sent_vector = corpus_tf_idf[0]\n",
      "definition_vectors = {'def1':corpus_tf_idf[1],\\\n",
      "                      'def2':corpus_tf_idf[2]}\n",
      "definition_id, score = \\\n",
      "find_sentence_definition(sent_vector,definition_vectors)\n",
      "print(\"The definition of word {} is {} with similarity of {}\".\\\n",
      "      format('bank',definition_id,score))\n",
      "You will get the following output:\n",
      "The definition of word bank is def2 with similarity of \n",
      "0.1441913068627889798 | Feature Extraction Methods\n",
      "As we already know, def2  represents a riverbank. So, we have found the correct \n",
      "definition of the word here. In this exercise, we have learned how to use text \n",
      "vectorization and text similarity to find the right definition of ambiguous words. \n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/39GzJAs .\n",
      "You can also run this example online at https://packt.live/3fbxQwK .\n",
      "Word Clouds\n",
      "Unlike numeric data, there are very few ways in which text data can be represented \n",
      "visually. The most popular way of visualizing text data is by using word clouds. A \n",
      "word cloud is a visualization of a text corpus in which the sizes of the tokens (words) \n",
      "represent the number of times they have occurred, as shown in the following image:\n",
      "Figure 2.27: Example of a word cloud\n",
      "Finding Text Similarity – Application of Feature Extraction | 99\n",
      "In the following exercise, we will be using a Python library called wordcloud  to build \n",
      "a word cloud from the 20newsgroups  dataset.\n",
      "Let's go through an exercise to understand this better.\n",
      "Exercise 2.18: Generating Word Clouds\n",
      "In this exercise, we will visualize the most frequently occurring words in the first 1,000 \n",
      "articles from sklearn 's fetch_20newsgroups  text dataset using a word cloud. \n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the necessary libraries and dataset. Add the following code to do this:\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 200\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from nltk.corpus import stopwords\n",
      "from wordcloud import WordCloud\n",
      "import matplotlib as mpl\n",
      "mpl.rcParams['figure.dpi'] = 200\n",
      "3. Write the get_data()  method to fetch the data:\n",
      "def get_data(n):\n",
      "    newsgroups_data_sample = fetch_20newsgroups(subset='train')\n",
      "    text = str(newsgroups_data_sample['data'][:n])\n",
      "    return text100 | Feature Extraction Methods\n",
      "4. Add a method to remove stop words:\n",
      "def load_stop_words():\n",
      "    other_stopwords_to_remove = ['\\\\n', 'n', '\\\\', '>', \\\n",
      "                                 'nLines', 'nI',\"n'\"]\n",
      "    stop_words = stopwords.words('english')\n",
      "    stop_words.extend(other_stopwords_to_remove)\n",
      "    stop_words = set(stop_words)\n",
      "    return stop_words\n",
      "5. Add the generate_word_cloud()  method to generate a word cloud object:\n",
      "def generate_word_cloud(text, stopwords):\n",
      "    \"\"\"\n",
      "    This method generates word cloud object\n",
      "    with given corpus, stop words and dimensions\n",
      "    \"\"\"\n",
      "    wordcloud = WordCloud(width = 800, height = 800, \\\n",
      "                          background_color ='white', \\\n",
      "                          max_words=200, \\\n",
      "                          stopwords = stopwords, \\\n",
      "                          min_font_size = 10).generate(text)\n",
      "    return wordcloud\n",
      "6. Get 1,000 documents from the 20newsgroup  data, get the stop word list, \n",
      "generate a word cloud object, and finally plot the word cloud with matplotlib:\n",
      "text = get_data(1000)\n",
      "stop_words = load_stop_words()\n",
      "wordcloud = generate_word_cloud(text, stop_words)\n",
      "plt.imshow(wordcloud, interpolation='bilinear')\n",
      "plt.axis(\"off\")\n",
      "plt.show()Finding Text Similarity – Application of Feature Extraction | 101\n",
      "The preceding code generates the following output:\n",
      "Figure 2.28: Word cloud representation of the first 10 articles\n",
      "So, in this exercise, we learned what word clouds are and how to generate word \n",
      "clouds with Python's wordcloud  library and visualize this with matplotlib.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/30eaSRn .\n",
      "You can also run this example online at https://packt.live/2EzqLJJ .\n",
      "In the next section, we will explore other visualizations, such as dependency parse \n",
      "trees and named entities.\n",
      "102 | Feature Extraction Methods\n",
      "Other Visualizations\n",
      "Apart from word clouds, there are various other ways of visualizing texts. Some of the \n",
      "most popular ways are listed here:\n",
      "• Visualizing sentences using a dependency parse tree : Generally, the phrases \n",
      "constituting a sentence depend on each other. We depict these dependencies by \n",
      "using a tree structure known as a dependency parse tree. For instance, the word \n",
      "\"helps \" in the sentence \"God helps those who help themselves\" depends on two \n",
      "other words. These words are \" God\" (the one who helps) and \" those \" (the ones \n",
      "who are helped).\n",
      "• Visualizing named entities in a text corpus : In this case, we extract the named \n",
      "entities from texts and highlight them by using different colors.\n",
      "Let's go through the following exercise to understand this better.\n",
      "Exercise 2.19: Other Visualizations Dependency Parse Trees and Named Entities\n",
      "In this exercise, we will look at two of the most popular visualization methods, after \n",
      "word clouds, which are dependency parse trees and using named entities. Follow \n",
      "these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "!python -m spacy download\n",
      "en_core_web_sm\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "3. Depict the sentence \"God helps those who help themselves\" using a dependency \n",
      "parse tree with the following code:\n",
      "doc = nlp('God helps those who help themselves')\n",
      "displacy.render(doc, style='dep', jupyter=True)Finding Text Similarity – Application of Feature Extraction | 103\n",
      "The preceding code generates the following output:\n",
      "Figure 2.29: Dependency parse tree\n",
      "4. Visualize the named entities of the text corpus by adding the following code: \n",
      "text = 'Once upon a time there lived a saint named '\\\n",
      "       'Ramakrishna Paramahansa. His chief disciple '\\\n",
      "       'Narendranath Dutta also known as Swami Vivekananda '\\\n",
      "       'is the founder of Ramakrishna Mission and '\\\n",
      "       'Ramakrishna Math.'\n",
      "doc2 = nlp(text)\n",
      "displacy.render(doc2, style='ent', jupyter=True)\n",
      "The preceding code generates the following output:\n",
      "Figure 2.30: Named entities\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/313m4iD .\n",
      "You can also run this example online at https://packt.live/3103fgr .\n",
      "Now that you have learned about visualizations, we will solve an activity based on \n",
      "them to gain an even better understanding.\n",
      "104 | Feature Extraction Methods\n",
      "Activity 2.02: Text Visualization\n",
      "In this activity, you will create a word cloud for the 50 most frequent words in a \n",
      "dataset. The dataset we will use consists of random sentences that are not clean. \n",
      "First, we need to clean them and create a unique set of frequently occurring words. \n",
      "Note\n",
      "The text_corpus.txt file that's being used in this activity can be found at \n",
      "https://packt.live/2DiVIBj .\n",
      "Follow these steps to implement this activity:\n",
      "1. Import the necessary libraries.\n",
      "2. Fetch the dataset.\n",
      "3. Perform the preprocessing steps, such as text cleaning, tokenization, and \n",
      "lemmatization, on the fetched data.\n",
      "4. Create a set of unique words along with their frequencies for the 50 most \n",
      "frequently occurring words.\n",
      "5. Create a word cloud for these top 50 words. \n",
      "6. Justify the word cloud by comparing it with the word frequency that  \n",
      "you calculated.\n",
      "Note\n",
      "The solution for this activity can be found via this link .Summary | 105\n",
      "Summary\n",
      "In this chapter, you have learned about various types of data and ways to deal with \n",
      "unstructured text data. Text data is usually extremely noisy and needs to be cleaned \n",
      "and preprocessed, which mainly consists of tokenization, stemming, lemmatization, \n",
      "and stop-word removal. After preprocessing, features are extracted from texts using \n",
      "various methods, such as BoW and TFIDF. These methods convert unstructured text \n",
      "data into structured numeric data. New features are created from existing features \n",
      "using a technique called feature engineering. In the last part of this chapter, we \n",
      "explored various ways of visualizing text data, such as word clouds.\n",
      "In the next chapter, you will learn how to develop machine learning models to classify \n",
      "texts using the feature extraction methods you have learned about in this chapter. \n",
      "Moreover, different sampling techniques and model evaluation parameters will  \n",
      "be introduced.Overview\n",
      "This chapter starts with an introduction to the various types of machine \n",
      "learning methods, that is, the supervised and unsupervised methods. \n",
      "You will learn about hierarchical clustering and k-means clustering and \n",
      "implement them using various datasets. Next, you will explore tree-based \n",
      "methods such as random forest and XGBoost. Finally, you will implement \n",
      "an end-to-end text classifier in order to categorize text on the basis of  \n",
      "its content.Developing a Text Classifier3108 | Developing a Text Classifier\n",
      "Introduction\n",
      "In the previous chapters, you learned about various extraction methods, such as \n",
      "tokenization, stemming, lemmatization, and stop-word removal, which are used to \n",
      "extract features from unstructured text. We also discussed Bag of Words and Term \n",
      "Frequency-Inverse Document Frequency  (TFIDF ).\n",
      "In this chapter, you will learn how to use these extracted features to develop machine \n",
      "learning models. These models are capable of solving real-world problems, such as \n",
      "detecting whether sentiments carried by texts are positive or negative, predicting \n",
      "whether emails are spam or not, and so on. We will also cover concepts such as \n",
      "supervised and unsupervised learning, classifications and regressions, sampling \n",
      "and splitting data, along with evaluating the performance of a model in depth. This \n",
      "chapter also discusses how to load and save these models for future use.\n",
      "Machine Learning\n",
      "Machine learning is the scientific study of algorithms and statistical models that \n",
      "computer systems use to perform a specific task without using explicit instructions, \n",
      "relying on patterns and inference instead.\n",
      "Machine learning algorithms are fed with large amounts of data that they can work \n",
      "on to build a model. This model is later used by businesses to generate solutions that \n",
      "help them analyze data and build strategies for the future. For example, a beverage \n",
      "production company can make use of multiple datasets to better understand the \n",
      "trends of their product's consumption over the course of a year. This would help \n",
      "them reduce wastage and better predict the requirements of their consumers. \n",
      "Machine learning is further categorized into unsupervised  and supervised  \n",
      "learning. Let's explore these two terms in detail.\n",
      "Unsupervised Learning\n",
      "Unsupervised learning is the method by which algorithms learn patterns within \n",
      "data that is not labeled. Since labels (supervisors) are absent, it is referred to as \n",
      "unsupervised learning. In unsupervised learning, you provide the algorithm with the \n",
      "feature data and it learns patterns from the data on its own.Machine Learning | 109\n",
      "Unsupervised learning is further classified into clustering and association:\n",
      "• Clustering : Clustering is the process of combining objects into groups called \n",
      "clusters. For example, if there are 50 students who need to be categorized based \n",
      "on their attributes, we do not use any specific attribute(s) to create segments. \n",
      "Rather, we try to learn the hidden patterns that exist in their attributes and \n",
      "categorize them accordingly. This process is known as cluster analysis or \n",
      "clustering (one of the most popular types of unsupervised learning). When \n",
      "handed a set of text documents, we can divide them into groups that are similar \n",
      "with the help of clustering. A common example of clustering could be when you \n",
      "search for a term on Google and similar pages or links are recommended. These \n",
      "recommendations are powered by document clustering.\n",
      "• Association : Another type of unsupervised learning is association rule mining. \n",
      "We use association rule mining to obtain groups of items that occur together \n",
      "frequently. The most common use case of association rule mining is to identify \n",
      "customers' buying patterns. For example, in a supermarket, customers who tend \n",
      "to buy milk and bread generally tend to buy cheese. This information can be \n",
      "used to design supermarket layouts. An application of association rule mining \n",
      "in Natural Language Processing  (NLP ) is to find similar words; for example, \n",
      "outstanding , excellent , and superb  are all synonyms of good . Association rule \n",
      "mining can easily find patterns like this in any NLP dataset. However, the  \n",
      "detailed theoretical explanations of these algorithms are beyond the scope  \n",
      "of this chapter.\n",
      "Let's explore the different types of clustering. In particular, we will be talking about \n",
      "hierarchical and k-means clustering, and the different scenarios in which they \n",
      "should be used. However, before we dive into those, it's important to understand \n",
      "the concept of distance metrics, which is what we use to create clusters and identify \n",
      "similar data points. The most common distance metric is Euclidean, which is \n",
      "calculated as follows:\n",
      "Figure 3.1: Formula for Euclidean distance\n",
      "In the case of machine learning, p and q are different data points in the dataset and \n",
      "pi, qi  are the different features of those data points.\n",
      "110 | Developing a Text Classifier\n",
      "Hierarchical Clustering\n",
      "Hierarchical clustering algorithms group similar objects together to create a cluster \n",
      "with the help of a dendrogram . In this algorithm, we can vary the number of clusters \n",
      "as per our requirements. First, we construct a matrix consisting of distances between \n",
      "each pair of instances (data points). After that, we construct a dendrogram  (a \n",
      "representation of clusters in the form of a tree) based on the distances between \n",
      "them. We truncate the tree at a location corresponding to the number of clusters \n",
      "we need.\n",
      "For example, imagine that you have 10 documents and want to group them into a \n",
      "number of categories based on their attributes (the number of words they contain, \n",
      "the number of paragraphs, punctuation, and so on) and don't have any fixed number \n",
      "of categories in mind. This is a use case of hierarchical clustering. Let's assume that \n",
      "we have a dataset containing the features of the 10 documents. Firstly, the distances \n",
      "between each pair of documents from the set of 10 documents are calculated. After \n",
      "that, we construct a dendrogram  and truncate it at a suitable position to get a \n",
      "suitable number of clusters:\n",
      "Figure 3.2: Output dendrogram after performing hierarchical clustering\n",
      "Machine Learning | 111\n",
      "In the preceding graph, we can perform a truncation at distance 3.5 to get two \n",
      "clusters or at 2.5 to get three clusters, depending on the requirements. To create a \n",
      "dendrogram using scikit-learn, we can use the following code:\n",
      "import scipy.cluster.hierarchy as sch\n",
      "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
      "plt.title('Dendrogram')\n",
      "plt.show()\n",
      "Here, X is the dataset that we want to perform hierarchical clustering with. Let's \n",
      "perform an exercise to understand how we can implement this.\n",
      "Exercise 3.01: Performing Hierarchical Clustering\n",
      "In this exercise, we will analyze the text documents in sklearn's \n",
      "fetch_20newsgroups  dataset. The 20 newsgroups dataset contains news \n",
      "articles on 20 different topics. We will make use of hierarchical clustering to classify \n",
      "the documents into different groups. Once the clusters have been created, we will \n",
      "compare them with their actual categories. Follow these steps to implement  \n",
      "this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from scipy.cluster.hierarchy import ward, dendrogram\n",
      "import matplotlib as mpl\n",
      "from scipy.cluster.hierarchy import fcluster\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter112 | Developing a Text Classifier\n",
      "from pylab import *\n",
      "import nltk\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Download a list of stop words and the Wordnet  corpus from nltk . Insert a new \n",
      "cell and add the following code to implement this:\n",
      "nltk.download('stopwords')\n",
      "stop_words=stopwords.words('english')\n",
      "stop_words=stop_words+list(string.printable)\n",
      "nltk.download('wordnet')\n",
      "lemmatizer=WordNetLemmatizer()\n",
      "4. Specify the categories of news articles we want to fetch to perform our clustering \n",
      "task. We will use three categories: \"For sale\", \"Electronics\", and \"Religion\". Add \n",
      "the following code to do this:\n",
      "categories= ['misc.forsale', 'sci.electronics', \\\n",
      "             'talk.religion.misc']\n",
      "5. To fetch the dataset, add the following lines of code:\n",
      "news_data = fetch_20newsgroups(subset='train', \\\n",
      "                               categories=categories, \\\n",
      "                               shuffle=True, random_state=42, \\\n",
      "                               download_if_missing=True)\n",
      "6. To view the data of the fetched content, add the following code:\n",
      "news_data['data'][:5]Machine Learning | 113\n",
      "The preceding code generates the following output:\n",
      "Figure 3.3: The first five news articles\n",
      "7. To check the categories of news articles, insert a new cell and add the  \n",
      "following code:\n",
      "print(news_data.target)\n",
      "The target is the variable that we predict by making use of the rest of the \n",
      "variables in a dataset. The preceding code generates the following output:\n",
      "[0 0 1 … 0 1 0]\n",
      "Here, 0 refers to misc.forsale , 1 refers to sci.electronics , and 2 refers \n",
      "to talk.religion.misc .\n",
      "8. To store news_data  and the corresponding categories in a pandas \n",
      "DataFrame  and view it, write the following code:\n",
      "news_data_df = pd.DataFrame({'text' : news_data['data'], \\\n",
      "                             'category': news_data.target})\n",
      "news_data_df.head()\n",
      "114 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.4: Text corpus of news data corresponding to the categories in a DataFrame\n",
      "9. To count the number of occurrences of each category appearing in this dataset, \n",
      "write the following code:\n",
      "news_data_df['category'].value_counts()\n",
      "The preceding code generates the following output:\n",
      "1        591\n",
      "0        585\n",
      "2        377\n",
      "Name:  category, dtype: int64\n",
      "10. Use a lambda function to extract tokens from each \"text\" of the news_data_\n",
      "df DataFrame. Check whether any of these tokens is a stop word, lemmatize \n",
      "the ones that are not stop words, and then concatenate them to recreate the \n",
      "sentence. Make use of the join  function to concatenate a list of words into a \n",
      "single sentence. To replace anything other than letters, digits, and whitespaces \n",
      "with blank space, use a regular expression ( re). Add the following code to  \n",
      "do this:\n",
      "news_data_df['cleaned_text'] = news_data_df['text']\\\n",
      "                               .apply(lambda x : ' '.join\\\n",
      "                               ([lemmatizer.lemmatize\\\n",
      "                                 (word.lower())\\\n",
      "                               for word in word_tokenize\\\n",
      "                               (re.sub(r'([^\\s\\w]|_)+', ' ',\\\n",
      "                                str(x))) if word.lower() \\\n",
      "                                not in stop_words]))\n",
      "Machine Learning | 115\n",
      "11. Create a TFIDF matrix and transform it into a DataFrame. Add the following code \n",
      "to do this:\n",
      "tfidf_model = TfidfVectorizer(max_features=200)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (news_data_df['cleaned_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.5: TFIDF representation as a DataFrame\n",
      "12. Calculate the distance using the sklearn library:\n",
      "from sklearn.metrics.pairwise import \\\n",
      "euclidean_distances as euclidean\n",
      "dist = 1 - euclidean(tfidf_df)\n",
      "13. Now, create a dendrogram for the TFIDF representation of documents:\n",
      "import scipy.cluster.hierarchy as sch\n",
      "dendrogram = sch.dendrogram(sch.linkage(dist, method='ward'))\n",
      "plt.xlabel('Data Points')\n",
      "plt.ylabel('Euclidean Distance')\n",
      "plt.title('Dendrogram')\n",
      "plt.show()\n",
      "116 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.6: Truncated dendrogram\n",
      "Here, you can see that a cluster count of four seems optimal.\n",
      "14. Use the fcluster()  function to obtain the cluster labels of the clusters that \n",
      "were obtained by hierarchical clustering:\n",
      "k=4\n",
      "clusters = fcluster(sch.linkage(dist, method='ward'), k, \\\n",
      "                    criterion='maxclust')\n",
      "clusters\n",
      "The preceding code generates the following output:\n",
      "array([3, 3, 3, …, 4, 4, 1], dtype=int32)\n",
      "Machine Learning | 117\n",
      "15. Make use of the crosstab  function of pandas to compare the clusters we have \n",
      "obtained with the actual categories of news articles. Add the following code to \n",
      "implement this:\n",
      "news_data_df['obtained_clusters'] = clusters\n",
      "pd.crosstab(news_data_df['category']\\\n",
      "            .replace({0:'misc.forsale', \\\n",
      "                      1:'sci.electronics', \\\n",
      "                      2:'talk.religion.misc'}),\\\n",
      "            news_data_df['obtained_clusters']\\\n",
      "            .replace({1 : 'cluster_1', 2 : 'cluster_2', \\\n",
      "                      3 : 'cluster_3', 4: 'cluster_4'}))\n",
      "The preceding code generates the following output:\n",
      "Figure 3.7: Crosstab between actual categories and obtained clusters \n",
      "Using the preceding image, we can analyze the high-level patterns that the clustering \n",
      "algorithm found to group the articles into one of the four clusters. As you can see, \n",
      "cluster 2 has mostly religion-related articles, while cluster 3 consists of primarily sales-\n",
      "related articles. The other two clusters do not have a proper distinction. The reason \n",
      "for this could be that the model figured out that words related to \"religion\" and \"for \n",
      "sale\" appeared frequently in the articles that were classified into those respective \n",
      "clusters, while the articles on \"electronics\" consist of mostly generic words.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/39A4wyL .\n",
      "You can also run this example online at https://packt.live/3ge4ezQ .\n",
      "118 | Developing a Text Classifier\n",
      "One major disadvantage of hierarchical clustering is scalability. Using hierarchical \n",
      "clustering for large datasets is very difficult; for such cases, we can use k-means \n",
      "clustering. Let us explore how this works.\n",
      "k-means Clustering\n",
      "In this algorithm, we segregate the given instances (data points) into \"k\" number of \n",
      "groups (here, k is a natural number). First, we choose k centroids. We assign each \n",
      "instance to its nearest centroid, thereby creating k groups. This is the assignment \n",
      "phase, which is followed by the update phase.\n",
      "In the update phase, new centroids for each of these k groups are calculated. \n",
      "The data points are reassigned to their nearest newly calculated centroids. The \n",
      "assignment phase and the update phase are carried on repeatedly until the \n",
      "assignment of data points no longer changes.\n",
      "For example, suppose you have 10 documents. You want to group them into three \n",
      "categories based on their attributes, such as the number of words they contain, the \n",
      "number of paragraphs, punctuation, and the tone of the document. In this case, we \n",
      "will assume that k is 3; that is, we want to create these three groups. Firstly, three \n",
      "centroids need to be chosen. In the initialization phase, each of these 10 documents \n",
      "is assigned to one of these three categories, thereby forming three groups. In the \n",
      "update phase, the centroids of the three newly formed groups are calculated. To \n",
      "decide the optimal number of clusters (that is, k), we execute k-means clustering \n",
      "for various values of k and note down their performances (sum of squared errors). \n",
      "We try to select a small value for k that has the lowest sum of squared errors. This \n",
      "method is called the elbow method .\n",
      "The scikit-learn library can be used to perform k-means in Python using the  \n",
      "following code:\n",
      "from sklearn.cluster import KMeans\n",
      "kmeans = KMeans(n_clusters=4)\n",
      "kmeans.fit(X)\n",
      "clusters = kmeans.predict(X)\n",
      "Here, we create the base model using the kmeans  class of scikit-learn. Then, we train \n",
      "the model using the fit  function. The trained model can then be used to get clusters \n",
      "using the predict function, where X represents a DataFrame of independent variables. \n",
      "Let's perform an exercise to get a better understanding of k-means clustering.Machine Learning | 119\n",
      "Exercise 3.02: Implementing k-means Clustering\n",
      "In this exercise, we will create four clusters from text documents in sklearn's \n",
      "fetch_20newsgroups  text dataset using k-means clustering. We will compare \n",
      "these clusters with the actual categories and use the elbow method to obtain the \n",
      "optimal number of clusters. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "import seaborn as sns \n",
      "sns.set()\n",
      "import numpy as np\n",
      "from scipy.spatial.distance import cdist\n",
      "from sklearn.cluster import KMeans\n",
      "3. To use stop words for the English language and the WordNet  corpus for \n",
      "lemmatization, add the following code:\n",
      "stop_words = stopwords.words('english')\n",
      "stop_words = stop_words + list(string.printable)\n",
      "lemmatizer = WordNetLemmatizer()120 | Developing a Text Classifier\n",
      "4. To specify the categories of news articles, add the following code:\n",
      "categories= ['misc.forsale', 'sci.electronics', \\\n",
      "             'talk.religion.misc']\n",
      "5. Use the following lines of code to fetch the dataset and store it in a  \n",
      "pandas DataFrame:\n",
      "news_data = fetch_20newsgroups(subset='train', \\\n",
      "                               categories=categories, \\\n",
      "                               shuffle=True, \\\n",
      "                               random_state=42, \\\n",
      "                               download_if_missing=True)\n",
      "news_data_df = pd.DataFrame({'text' : news_data['data'], \\\n",
      "                             'category': news_data.target})\n",
      "6. Use the lambda function to extract tokens from each \" text \" of the news_\n",
      "data_df  DataFrame. Discard the tokens if they're stop words, lemmatize them \n",
      "if they're not, and then concatenate them to recreate the sentence. Use the join \n",
      "function to concatenate a list of words into a single sentence and use the regular \n",
      "expression method ( re) to replace anything other than alphabets, digits, and \n",
      "whitespaces with a blank space. Add the following code to do this:\n",
      "news_data_df['cleaned_text'] = news_data_df['text']\\\n",
      "                               .apply(lambda x : ' '.join\\\n",
      "                               ([lemmatizer.lemmatize(word.lower()) \\\n",
      "                               for word in word_tokenize\\\n",
      "                               (re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                       str(x))) \\\n",
      "                                if word.lower() not in stop_words]))\n",
      "7. Use the following lines of code to create a TFIDF matrix and transform it into  \n",
      "a DataFrame:\n",
      "tfidf_model = TfidfVectorizer(max_features=200)\n",
      "tfidf_df = pd.DataFrame(tfidf_model\\\n",
      "                        .fit_transform\\\n",
      "                        (news_data_df['cleaned_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()Machine Learning | 121\n",
      "The preceding code generates the following output:\n",
      "Figure 3.8: TFIDF representation as a DataFrame\n",
      "8. Use the KMeans  function of sklearn to create four clusters from a TFIDF \n",
      "representation of news articles. Add the following code to do this:\n",
      "kmeans = KMeans(n_clusters=4)\n",
      "kmeans.fit(tfidf_df)\n",
      "y_kmeans = kmeans.predict(tfidf_df)\n",
      "news_data_df['obtained_clusters'] = y_kmeans\n",
      "9. Use pandas' crosstab  function to compare the clusters we have obtained with \n",
      "the actual categories of the news articles. Add the following code to do this:\n",
      "pd.crosstab(news_data_df['category']\\\n",
      "            .replace({0:'misc.forsale', \\\n",
      "                      1:'sci.electronics', \\\n",
      "                      2:'talk.religion.misc'}),\\\n",
      "            news_data_df['obtained_clusters']\\\n",
      "            .replace({0 : 'cluster_1',\\\n",
      "                      1 : 'cluster_2', 2 : 'cluster_3', \\\n",
      "                      3: 'cluster_4'}))\n",
      "122 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.9: Crosstab between the actual categories and obtained clusters xxx\n",
      "From the figure above, you can see, cluster 2 has majorly religion related articles \n",
      "and cluster 4 has mostly for sale related articles. The other two clusters do now \n",
      "have a proper distinction but cluster 3 has majority of the electronic articles.\n",
      "10. Finally, to obtain the optimal value of k (that is, the number of clusters), execute \n",
      "the k-means algorithm for values of k ranging from 1 to 6. For each value of k, \n",
      "store the distortion—that is, the mean of the distances of the documents from \n",
      "their nearest cluster center. Look for the value of k where the slope of the plot \n",
      "changes rapidly. Add the following code to implement this:\n",
      "distortions = []\n",
      "K = range(1,6)\n",
      "for k in K:\n",
      "    kmeanModel = KMeans(n_clusters=k)\n",
      "    kmeanModel.fit(tfidf_df)\n",
      "    distortions.append(sum(np.min(cdist\\\n",
      "    (tfidf_df, kmeanModel.cluster_centers_, \\\n",
      "     'euclidean'), axis=1)) / tfidf_df.shape[0])\n",
      "plt.plot(K, distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal number '\\\n",
      "          'of clusters')\n",
      "plt.show()\n",
      "Machine Learning | 123\n",
      "The preceding code generates the following output:\n",
      "Figure 3.10: Optimal clusters represented in a graph using the elbow method\n",
      "From the preceding graph, we can conclude that the optimal number of clusters is 2.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2EuZckB .\n",
      "You can also run this example online at https://packt.live/333x6Hw .\n",
      "We have seen how unsupervised learning can be implemented in Python. Now, let us \n",
      "talk about supervised learning.\n",
      "124 | Developing a Text Classifier\n",
      "Supervised Learning\n",
      "Unlike unsupervised learning, supervised learning algorithms need labeled data. They \n",
      "learn how to automatically generate labels or predict values by analyzing various \n",
      "features of the data provided. For example, say you have already starred important \n",
      "text messages on your phone, and you want to automate the task of going through \n",
      "all your messages daily (considering they are important and marked already). This is \n",
      "a use case for supervised learning. Here, messages that have been starred previously \n",
      "can be used as labeled data. Using this data, you can create two types of models that \n",
      "are capable of the following:\n",
      "• Classifying whether new messages are important\n",
      "• Predicting the probability of new messages being important\n",
      "The first type is called classification, while the second type is called regression. Let's \n",
      "learn about classification first.\n",
      "Classification\n",
      "Say you have two types of food, of which type 1 tastes sweet and type 2 tastes salty, \n",
      "and you need to determine how an unknown food will taste using various attributes \n",
      "of the food (such as color, fragrance, shape, and ingredients). This is an instance \n",
      "of classification.\n",
      "Here, the two classes are class 1, which tastes sweet, and class 2, which tastes salty. \n",
      "The features that are used in this classification are color, fragrance, the ingredients \n",
      "used to prepare the dish, and so on. These features are called independent variables. \n",
      "The class (sweet or salty) is called a dependent variable.\n",
      "Formally, classification algorithms are those that learn patterns from a given \n",
      "dataset to determine classes of unknown datasets. Some of the most widely used \n",
      "classification algorithms are logistic regression, Naive Bayes, k-nearest neighbor, and \n",
      "tree methods. Let's learn about each of them.Supervised Learning | 125\n",
      "Logistic Regression\n",
      "Despite having the term \"regression\" in it, logistic regression is used for probabilistic \n",
      "classification. In this case, the dependent variable (the outcome) is binary, which \n",
      "means that the values can be represented by 0 or 1. For example, consider that you \n",
      "need to decide whether an email is spam or not. Here, the value of the decision (the \n",
      "dependent variable, or the outcome) can be considered to be 1 if the email is spam; \n",
      "otherwise, it will be 0. No other outcome is possible. The independent variables (that \n",
      "is, the features) will consist of various attributes of the email, such as the number \n",
      "of occurrences of certain keywords and so on. We can then make use of the logistic \n",
      "regression algorithm to create a model that predicts if the email is spam (1) or not (0), \n",
      "as shown in the following graph:\n",
      "Figure 3.11: Example of logistic regression\n",
      "Here, the decision boundary is created by training a logistic regression model that \n",
      "helps us classify spam emails.\n",
      "126 | Developing a Text Classifier\n",
      "The scikit-learn library can be used to perform logistic regression in Python using the \n",
      "following code:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(X,y)\n",
      "predicted_labels = log_reg.predict(X)\n",
      "predicted_probability = log_reg.predict_proba(X)[:,1]\n",
      "Here, we create the base model using the LogisticRegression  class of  \n",
      "scikit-learn. Then, we train the model using the fit  function. The trained model \n",
      "can then be used to make predictions, and we can also get probability estimates for \n",
      "each class using the predict_proba  function. Here, X represents a DataFrame of \n",
      "independent variables, whereas y represents a DataFrame of dependent variables.\n",
      "Exercise 3.03: Text Classification – Logistic Regression\n",
      "In this exercise, we will classify reviews of musical instruments on Amazon with the \n",
      "help of the logistic regression classification algorithm.\n",
      "Note\n",
      "To download the dataset, visit https://packt.live/3hQ6UEe .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk \n",
      "nltk.download('punkt')Supervised Learning | 127\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Read the data file in JSON format using pandas. Add the following code to \n",
      "implement this:\n",
      "review_data = pd.read_json\\\n",
      "              ('data/reviews_Musical_Instruments_5.json', \\\n",
      "               lines=True)\n",
      "review_data[['reviewText', 'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.12: Data stored in a DataFrame\n",
      "4. Use a lambda  function to extract tokens from each 'reviewText'  of this \n",
      "DataFrame, lemmatize them, and concatenate them side by side. Use the  \n",
      "join  function to concatenate a list of words into a single sentence. Use the \n",
      "regular expression method ( re) to replace anything other than alphabetical \n",
      "characters, digits, and whitespaces with blank space. Add the following code to \n",
      "implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "review_data['cleaned_review_text'] = review_data['reviewText']\\\n",
      "                                     .apply(lambda x : ' '.join\\\n",
      "                                     ([lemmatizer.lemmatize\\\n",
      "                                      (word.lower()) \\\n",
      "                                      for word in word_tokenize\\\n",
      "                                      (re.sub(r'([^\\s\\w]|_)+', ' ',\\\n",
      "                                       str(x)))]))\n",
      "128 | Developing a Text Classifier\n",
      "5. Create a DataFrame from the TFIDF matrix representation of the cleaned version \n",
      "of reviewText . Add the following code to implement this:\n",
      "review_data[['cleaned_review_text', 'reviewText', \\\n",
      "             'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.13: Review texts before and after cleaning, along with their overall scores\n",
      "6. Create a TFIDF matrix and transform it into a DataFrame . Add the following \n",
      "code to do this:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (review_data['cleaned_review_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.14: A TFIDF representation as a DataFrame\n",
      "Supervised Learning | 129\n",
      "7. The following lines of code are used to create a new column target, which will \n",
      "have 0 if the overall  parameter is less than 4, and 1 otherwise. Add the \n",
      "following code to implement this:\n",
      "review_data['target'] = review_data['overall'].apply\\\n",
      "                        (lambda x : 0 if x<=4 else 1)\n",
      "review_data['target'].value_counts()\n",
      "The preceding code generates the following output:\n",
      "1     6938\n",
      "0     3323\n",
      "Name: target, dtype: int64\n",
      "8. Use sklearn's LogisticRegression()  function to fit a logistic regression \n",
      "model on the TFIDF representation of these reviews after cleaning them. Add the \n",
      "following code to implement this:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "logreg = LogisticRegression()\n",
      "logreg.fit(tfidf_df,review_data['target'])\n",
      "predicted_labels = logreg.predict(tfidf_df)\n",
      "logreg.predict_proba(tfidf_df)[:,1]\n",
      "The preceding code generates the following output:\n",
      "array([0.57146961, 0.68579907, 0.56068939, …, 0.65979968, \\\n",
      "       0.5495679 , 0.21186011])\n",
      "9. Use the crosstab  function of pandas to compare the results of our \n",
      "classification model with the actual classes ( 'target' , in this case) of the \n",
      "reviews. Add the following code to do this:\n",
      "review_data['predicted_labels'] = predicted_labels\n",
      "pd.crosstab(review_data['target'], \\\n",
      "            review_data['predicted_labels'])130 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.15: Crosstab between actual target values and predicted labels \n",
      "Here, we can see 1543  instances with the target label 0 that are correctly classified \n",
      "and 1780  such instances that are wrongly classified. Furthermore, 6312  instances \n",
      "with the target label 1 are correctly classified, whereas 626  such instances are \n",
      "wrongly classified.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hOaKxJ .\n",
      "You can also run this example online at https://packt.live/309yKWc .\n",
      "We've seen how to implement logistic regression; now, let's look at  \n",
      "Naïve Bayes classification.\n",
      "Naive Bayes Classifiers\n",
      "Just like logistic regression, a Naive Bayes classifier is another kind of probabilistic \n",
      "classifier. It is based on Bayes' theorem, which is shown here:\n",
      "Figure 3.16: Bayes' theorem\n",
      "In the preceding formula, A and B are events and P(B) is not equal to 0. P(A/B) is the \n",
      "probability of event A occurring, given that event B is true. Similarly, P(B/A) is the \n",
      "probability of event B occurring, given that event A is true. P(B) is the probability of \n",
      "the occurrence of event B.\n",
      "Supervised Learning | 131\n",
      "Say there is an online platform where hotel customers can provide a review for the \n",
      "service provided to them. The hotel now wants to figure out whether new reviews \n",
      "on the platform are appreciative in nature or not. Here, P(A) = the probability of \n",
      "the review being an appreciative one, while P(B) = the probability of the review \n",
      "being long. Now, we've come across a review that is long and want to figure out the \n",
      "probability of it being appreciative. To do that, we need to calculate P(A/B). P(B/A)  \n",
      "will be the probability of appreciative reviews being long. From the training dataset, \n",
      "we can easily calculate P(B/A), P(A), and P(B) and then use Bayes' theorem to  \n",
      "calculate P(A/B). \n",
      "Similar to logistic regression, the scikit-learn library can be used to perform naïve \n",
      "Bayes classification and can be implemented in Python using the following code:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "nb = GaussianNB()\n",
      "nb.fit(X,y)\n",
      "predicted_labels = nb.predict(X)\n",
      "predicted_probability = nb.predict_proba(X)[:,1]\n",
      "Here, we created the base model using the GaussianNB  class of scikit-learn. Then, \n",
      "we trained the model using the fit  function. The trained model can then be used \n",
      "to make predictions; we can also get probability estimates for each class using \n",
      "the predict_proba  function. Here, X represents a DataFrame of independent \n",
      "variables, whereas y represents a DataFrame of dependent variables. \n",
      "Exercise 3.04: Text Classification – Naive Bayes\n",
      "In this exercise, we will classify reviews of musical instruments on Amazon with the \n",
      "help of the Naïve Bayes classification algorithm. Follow these steps to implement  \n",
      "this exercise:\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3hQ6UEe .\n",
      "1. Open a Jupyter Notebook.132 | Developing a Text Classifier\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Read the data file in JSON format using pandas. Add the following code to \n",
      "implement this:\n",
      "review_data = pd.read_json\\\n",
      "              ('data/reviews_Musical_Instruments_5.json', \\\n",
      "               lines=True)\n",
      "review_data[['reviewText', 'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.17: Data stored in a DataFrame\n",
      "Supervised Learning | 133\n",
      "4. Use a lambda  function to extract tokens from each 'reviewText'  of this \n",
      "DataFrame, lemmatize them, and concatenate them side by side. Use the join  \n",
      "function to concatenate a list of words into a single sentence. Use the regular \n",
      "expression method ( re) to replace anything other than alphabets, digits, and \n",
      "whitespaces with blank space. Add the following code to implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "review_data['cleaned_review_text'] = review_data['reviewText']\\\n",
      "                                     .apply(lambda x : ' '.join\\\n",
      "                                     ([lemmatizer.lemmatize\\\n",
      "                                      (word.lower()) \\\n",
      "                                     for word in word_tokenize\\\n",
      "                                     (re.sub(r'([^\\s\\w]|_)+', ' ',\\\n",
      "                                      str(x)))]))\n",
      "5. Create a DataFrame from the TFIDF matrix representation of the cleaned version \n",
      "of reviewText . Add the following code to implement this:\n",
      "review_data[['cleaned_review_text', 'reviewText', \\\n",
      "             'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.18: Review texts before and after cleaning, along with their overall scores\n",
      "6. Create a TFIDF matrix and transform it into a DataFrame. Add the following code \n",
      "to do this:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (review_data['cleaned_review_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "134 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.19: A TFIDF representation as a DataFrame\n",
      "7. The following lines of code are used to create a new column target, which will \n",
      "have the value 0 if the 'overall'  parameter is less than 4, and 1 otherwise. \n",
      "Add the following code to implement this:\n",
      "review_data['target'] = review_data['overall']\\\n",
      "                        .apply(lambda x : 0 if x<=4 else 1)\n",
      "review_data['target'].value_counts()\n",
      "The preceding code generates the following output:\n",
      "1    6938\n",
      "0    3323\n",
      "Name: target, dtype: int64\n",
      "8. Use sklearn's GaussianNB()  function to fit a Gaussian Naive Bayes model on \n",
      "the TFIDF representation of these reviews after cleaning them. Add the following \n",
      "code to do this:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "nb = GaussianNB()\n",
      "nb.fit(tfidf_df,review_data['target'])\n",
      "predicted_labels = nb.predict(tfidf_df)\n",
      "nb.predict_proba(tfidf_df)[:,1]\n",
      "The preceding code generates the following output:\n",
      "array([9.97730158e-01, 3.63599675e-09, 9.45692105e-07, …,\n",
      "       2.46001047e-02, 3.43660991e-08, 1.72767906e-27])\n",
      "The preceding screenshot shows the predicted probabilities of the input  \n",
      "tfidf_df  dataset.\n",
      "Supervised Learning | 135\n",
      "9. Use the crosstab  function of pandas to compare the results of our \n",
      "classification model with the actual classes ( 'target' , in this case) of the \n",
      "reviews. Add the following code to do this:\n",
      "review_data['predicted_labels'] = predicted_labels\n",
      "pd.crosstab(review_data['target'], \\\n",
      "            review_data['predicted_labels'])\n",
      "The preceding code generates the following output:\n",
      "Figure 3.20: Crosstab between actual target values and predicted labels\n",
      "Here, we can see 2333  instances with the target label 0 that are correctly classified \n",
      "and 990  such instances that have been wrongly classified. Furthermore, 4558  \n",
      "instances with the target label 1 have been correctly classified, whereas 2380  such \n",
      "instances have been wrongly classified.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2DnoeBx .\n",
      "You can also run this example online at https://packt.live/3fcjT1t .\n",
      "We'll explore k-nearest neighbors in the next section.\n",
      "136 | Developing a Text Classifier\n",
      "k-nearest Neighbors\n",
      "k-nearest neighbors is an algorithm that can be used to solve both regression \n",
      "and classification. In this chapter, we will focus on the classification aspect of the \n",
      "algorithm as it is used for NLP applications. Consider, for instance, the saying  \n",
      "\"birds of a feather flock together.\" This means that people who have similar interests \n",
      "prefer to stay close to each other and form groups. This characteristic is called \n",
      "homophily . This characteristic is the main idea behind the k-nearest neighbors \n",
      "classification algorithm.\n",
      "To classify an unknown object, k number of other objects located nearest to it with \n",
      "class labels will be looked into. The class that occurs the most among them will be \n",
      "assigned to it—that is, the object with an unknown class. The value of k is chosen by \n",
      "running experiments on the training dataset to find the most optimal value. When \n",
      "dealing with text data for a given document, we interpret \"nearest neighbors\" as \n",
      "other documents that are the most similar to the unknown document.\n",
      "We can make use of the scikit-learn library to implement the k-nearest neighbors \n",
      "algorithm in Python using the following code:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(X,y)\n",
      "prediction = knn.predict(X)\n",
      "Here, we created the base model using the KNeighborsClassifier  class \n",
      "of scikit-learn and pass it the value of k, which in this case is 3. Then, we trained \n",
      "the model using the fit  function. The trained model can then be used to make \n",
      "predictions using the predict  function. X represents a DataFrame of independent \n",
      "variables, whereas y represents a DataFrame of dependent variables. \n",
      "Now that we have an understanding of different types of classification, let's see how \n",
      "we can implement them.Supervised Learning | 137\n",
      "Exercise 3.05: Text Classification Using the k-nearest Neighbors Method\n",
      "In this exercise, we will classify reviews of musical instruments on Amazon with \n",
      "the help of the k-nearest neighbors classification algorithm. Follow these steps to \n",
      "implement this exercise:\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3hQ6UEe .\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Read the data file in JSON format using pandas. Add the following code to \n",
      "implement this:\n",
      "review_data = pd.read_json\\\n",
      "              ('data/reviews_Musical_Instruments_5.json',\\\n",
      "               lines=True)\n",
      "review_data[['reviewText', 'overall']].head()138 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.21: Data stored in a DataFrame\n",
      "4. Use a lambda  function to extract tokens from each reviewText  of this \n",
      "DataFrame, lemmatize them, and concatenate them side by side. Use the join  \n",
      "function to concatenate a list of words into a single sentence. Use the regular \n",
      "expression method ( re) to replace anything other than alphabets, digits, and \n",
      "whitespaces with blank space. Add the following code to implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "review_data['cleaned_review_text'] = review_data['reviewText']\\\n",
      "                                     .apply(lambda x : ' '.join\\\n",
      "                                     ([lemmatizer.lemmatize\\\n",
      "                                       (word.lower()) \\\n",
      "                                     for word in word_tokenize\\\n",
      "                                     (re.sub(r'([^\\s\\w]|_)+', ' ',\\\n",
      "                                      str(x)))]))\n",
      "5. Create a DataFrame from the TFIDF matrix representation of the cleaned version \n",
      "of reviewText . Add the following code to implement this:\n",
      "review_data[['cleaned_review_text', 'reviewText', \\\n",
      "             'overall']].head()\n",
      "Supervised Learning | 139\n",
      "The preceding code generates the following output:\n",
      "Figure 3.22: Review texts before and after cleaning, along with their overall scores\n",
      "6. Create a TFIDF matrix and transform it into a DataFrame. Add the following code \n",
      "to do this:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (review_data['cleaned_review_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.23: A TFIDF representation as a DataFrame\n",
      "7. The following lines of code are used to create a new column target, which will \n",
      "have 0 if the overall  parameter is less than 4, and 1 otherwise. Add the \n",
      "following code to implement this:\n",
      "review_data['target'] = review_data['overall']\\\n",
      "                        .apply(lambda x : 0 if x<=4 else 1)\n",
      "review_data['target'].value_counts()\n",
      "140 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "1     6938\n",
      "0     3323\n",
      "Name:  target, dtype: int64\n",
      "8. Use sklearn's KNeighborsClassifier()  function to fit a three-nearest \n",
      "neighbor model on the TFIDF representation of these reviews after cleaning \n",
      "them. Use the crosstab  function of pandas to compare the results of our \n",
      "classification model with the actual classes ( 'target' , in this case) of  \n",
      "the reviews:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "knn.fit(tfidf_df,review_data['target'])\n",
      "review_data['predicted_labels_knn'] = knn.predict(tfidf_df)\n",
      "pd.crosstab(review_data['target'], \\\n",
      "            review_data['predicted_labels_knn'])\n",
      "The preceding code generates the following output:\n",
      "Figure 3.24: Crosstab between actual target values and predicted  \n",
      "labels by k-nearest neighbors \n",
      "Here, we can see 2594  instances with the target label as 0 correctly classified and \n",
      "729  such instances wrongly classified. Furthermore, 6563  instances with the target \n",
      "label as 1 are correctly classified, whereas 375  such instances are wrongly classified. \n",
      "You have just learned how to perform text classification with the help of various \n",
      "classification algorithms.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/338XQqb .\n",
      "You can also run this example online at https://packt.live/39E5zNW .\n",
      "Supervised Learning | 141\n",
      "In the next section, you will learn about regression, which is another type of \n",
      "supervised learning.\n",
      "Regression\n",
      "To better understand regression, consider a practical example. For example, say  \n",
      "you have photos of several people, along with a list of their respective ages, and you \n",
      "need to predict the ages of some other people from their photos. This is a use case \n",
      "for regression. \n",
      "In the case of regression, the dependent variable (age, in this example) is continuous. \n",
      "The independent variables—that is, features—consist of the attributes of the images, \n",
      "such as the color intensity of each pixel. Formally, regression analysis refers to the \n",
      "process of learning a mapping function, which relates features or predictors (inputs) \n",
      "to the dependent variable (output).\n",
      "There are various types of regression: univariate , multivariate , simple , multiple , \n",
      "linear , non-linear , polynomial  regression , stepwise  regression , ridge  regression , \n",
      "lasso  regression , and elastic  net regression . If there is just one dependent variable, \n",
      "then it is referred to as univariate regression. On the other hand, two or more \n",
      "dependent variables constitute multivariate regression. Simple regression has only \n",
      "one predictor or target variable, while multivariate regression has more than one \n",
      "predictor variable. \n",
      "Since linear regression in the base algorithm for all the different types of regression \n",
      "mentioned previously, in the next section, we will cover linear regression in detail.\n",
      "Linear Regression\n",
      "The term \"linear\" refers to the linearity of parameters. Parameters are the coefficients \n",
      "of predictor variables in the linear regression equation. The following formula \n",
      "represents the linear regression equation:\n",
      "Figure 3.25: Formula for linear regression\n",
      "Here, y is termed a dependent variable (output); it is continuous. X is an independent \n",
      "variable or feature (input). β0 and β1 are parameters. Є is the error component, \n",
      "which is the difference between the actual and predicted values of y. Since linear \n",
      "regression requires the variable to be linear, it is not used much in the real world. \n",
      "However, it is useful for high-level predictions, such as the sales revenue of a product \n",
      "given the price and advertising cost.\n",
      "142 | Developing a Text Classifier\n",
      "We can use the scikit-learn library to implement the linear regression algorithm in \n",
      "Python with the following code:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "linreg = LinearRegression()\n",
      "linreg.fit(X,y)\n",
      "coefficient = linreg.coef_\n",
      "intercept = linreg.intercept_\n",
      "linreg.predict(X)\n",
      "Here, we created the base model using the LinearRegression  class of scikit-learn. \n",
      "Then, we trained the model using the fit  function. Now that our linear regression \n",
      "model has been trained, we can use the coef_  and intercept_ parameters of \n",
      "the model to get the parameters and error components, as we discussed previously. \n",
      "Here, X represents a DataFrame of independent variables, whereas y represents a \n",
      "DataFrame of dependent variables. The trained model can then be used to make \n",
      "predictions using the predict  function. \n",
      "In the next section, we will solve an exercise to get a better understanding of \n",
      "regression analysis.\n",
      "Exercise 3.06: Regression Analysis Using Textual Data\n",
      "In this exercise, we will make use of linear regression to predict the overall ratings \n",
      "from the reviews of musical instruments on Amazon. Follow these steps to implement \n",
      "this exercise:\n",
      "Note\n",
      "The dataset for this exercise can be downloaded from  \n",
      "https://packt.live/3hQ6UEe .\n",
      "9. Open a Jupyter Notebook.Supervised Learning | 143\n",
      "10. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "11. Read the given data file in JSON  format using pandas . Add the following code \n",
      "to implement this:\n",
      "review_data = pd.read_json\\\n",
      "              ('data/reviews_Musical_Instruments_5.json', \\\n",
      "               lines=True)\n",
      "review_data[['reviewText', 'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.26: Reviews of musical instruments stored as a DataFrame\n",
      "144 | Developing a Text Classifier\n",
      "12. Use a lambda  function to extract tokens from each 'reviewText'  of this \n",
      "DataFrame, lemmatize them, and concatenate them side by side. Then, use the \n",
      "join  function to concatenate a list of words into a single sentence. In order to \n",
      "replace anything other than alphabets, digits, and whitespaces with blank  \n",
      "space, use the regular expression method ( re). Add the following code to \n",
      "implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "review_data['cleaned_review_text'] = review_data['reviewText']\\\n",
      "                                     .apply(lambda x : ' '.join\\\n",
      "                                     ([lemmatizer.lemmatize\\\n",
      "                                       (word.lower()) \\\n",
      "                                     for word in word_tokenize\\\n",
      "                                     (re.sub(r'([^\\s\\w]|_)+', ' ',\\\n",
      "                                      str(x)))]))\n",
      "review_data[['cleaned_review_text', 'reviewText', 'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.27: Review texts before and after cleaning, along with their overall scores\n",
      "13. Create a DataFrame  from the TFIDF matrix representation of cleaned \n",
      "reviewText . Add the following code to do this:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (review_data['cleaned_review_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "Supervised Learning | 145\n",
      "The preceding code generates the following output:\n",
      "Figure 3.28: TFIDF representation as a DataFrame \n",
      "14. Use sklearn's LinearRegression()  function to fit a linear regression model \n",
      "on this TFIDF DataFrame. Add the following code to do this:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "linreg = LinearRegression()\n",
      "linreg.fit(tfidf_df,review_data['overall'])\n",
      "linreg.coef_\n",
      "The preceding code generates the following output:\n",
      "Figure 3.29: Coefficients of the linear regression model\n",
      "146 | Developing a Text Classifier\n",
      "The preceding output shows the coefficients of the different features of the \n",
      "trained model.\n",
      "Please note that Figure 3.29  is truncated. \n",
      "15. To check the intercept or the error term of the linear regression model, type the \n",
      "following code:\n",
      "linreg.intercept_\n",
      "The preceding code generates the following output:\n",
      "4.218882428983381\n",
      "16. To check the prediction in a TFIDF DataFrame, write the following code:\n",
      "linreg.predict(tfidf_df)\n",
      "The preceding code generates the following output:\n",
      "array([4.19200071, 4.25771652, 4.23084868, …, 4.40384767, \n",
      "       4.49036403, 4.14791976])\n",
      "17. Finally, use this model to predict the 'overall'  score and store it in a column \n",
      "called 'predicted_score_from_linear_regression' . Add the \n",
      "following code to implement this:\n",
      "review_data['predicted_score_from_linear_regression'] = \\\n",
      "linreg.predict(tfidf_df)\n",
      "review_data[['overall', \\\n",
      "             'predicted_score_from_linear_regression']].head(10)Supervised Learning | 147\n",
      "The preceding code generates the following output:\n",
      "Figure 3.30: Actual scores and predictions of the linear regression model\n",
      "From the preceding table, we can see how the actual and predicted score varies for \n",
      "different instances. We will use this table later to evaluate the performance of  \n",
      "the model.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2P58eqy .\n",
      "You can also run this example online at https://packt.live/335pLqV .\n",
      "You have just learned how to perform regression analysis on given data. In the next \n",
      "section, you will learn about tree methods.\n",
      "148 | Developing a Text Classifier\n",
      "Tree Methods\n",
      "There are several algorithms that have both classification and regression forms. Tree-\n",
      "based methods are instances of such cases. In the context of machine learning, \"tree\" \n",
      "refers to a structure that aids decision-making—hence, the term decision tree . Tree-\n",
      "based methods have high accuracy and unlike linear methods, they model non-linear \n",
      "relationships as well. Additionally, decision trees handle categorical variables much \n",
      "better than linear regression.\n",
      "Let us use the example of a hotel trying to identify if the reviews provided by their \n",
      "patrons have a positive sentiment or a negative one. So, the reviews needed to be \n",
      "classified into two classes, namely, positive sentiments and negative sentiments. A \n",
      "data scientist working for the hotel can create a dataset of all online reviews of their \n",
      "hotel and create a decision tree, as shown in the following diagram:\n",
      "Figure 3.31: Decision tree\n",
      "In the preceding diagram, the first decision is made based on the length of the \n",
      "sentences. He finds that the short length reviews generally have a positive sentiment, \n",
      "whereas a medium length review has a negative sentiment. For reviews that were \n",
      "longer, he had to rely on keywords to determine the sentiment as longer length \n",
      "reviews are almost equally likely to be positive or negative. If the excellent  keyword is \n",
      "present, the review belongs to the positive  sentiment; otherwise, it belongs to the \n",
      "negative  sentiment.\n",
      "Supervised Learning | 149\n",
      "We can make use of the scikit-learn library to implement the decision tree algorithm \n",
      "in Python using the following code:\n",
      "from sklearn import tree\n",
      "dtc = tree.DecisionTreeClassifier()\n",
      "dtc = dtc.fit(X, y)\n",
      "predicted_labels = dtc.predict(X)\n",
      "Here, we created the base model using the DecisionTreeClassifier  class \n",
      "of scikit-learn. Then, we trained the model using the fit  function. The trained \n",
      "model can then be used to make predictions using the predict  function. Here, \n",
      "X represents a DataFrame of independent variables, whereas y represents a \n",
      "DataFrame of dependent variables. \n",
      "Exercise 3.07: Tree-Based Methods – Decision Tree\n",
      "In this exercise, we will use the tree-based method known as decision trees to predict \n",
      "the overall scores and labels of reviews of patio, lawn, and garden products on \n",
      "Amazon. Follow these steps to implement this exercise:\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3gczb7P .\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *150 | Developing a Text Classifier\n",
      "import nltk\n",
      "nltk.download('wordnet')\n",
      "nltk.download('punkt')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Now, read the given data file in JSON format using pandas. Add the following \n",
      "code to implement this:\n",
      "data_patio_lawn_garden = pd.read_json\\\n",
      "                         ('data/'\\\n",
      "                          'reviews_Patio_Lawn_and_Garden_5.json',\\\n",
      "                          lines = True)\n",
      "data_patio_lawn_garden[['reviewText', 'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.32: Storing reviews as a DataFrame\n",
      "4. Use the lambda  function to extract tokens from each 'reviewText'  of this \n",
      "DataFrame, lemmatize them using WorrdNetLemmatizer , and concatenate \n",
      "them side by side. Use the join  function to concatenate a list of words into a \n",
      "single sentence. Use the regular expression method ( re) to replace anything \n",
      "other than letters, digits, and whitespaces with blank spaces. Add the following \n",
      "code to do this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "data_patio_lawn_garden['cleaned_review_text'] = \\\n",
      "data_patio_lawn_garden['reviewText']\\\n",
      ".apply(lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
      "       for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "       str(x)))]))\n",
      "Supervised Learning | 151\n",
      "data_patio_lawn_garden[['cleaned_review_text', 'reviewText',\\\n",
      "                        'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.33: Review text before and after cleaning, along with overall scores\n",
      "5. Create a DataFrame from the TFIDF matrix representation of the cleaned version \n",
      "of reviewText  with the following code:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (data_patio_lawn_garden['cleaned_review_text'])\\\n",
      "            .todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.34: TFIDF representation as a DataFrame\n",
      "6. The following lines of code are used to create a new column called target, which \n",
      "will have 0 if the 'overall'  parameter is less than 4; otherwise, it will have 1:\n",
      "data_patio_lawn_garden['target'] = data_patio_lawn_garden\\\n",
      "                                   ['overall'].apply\\\n",
      "                                   (lambda x : 0 if x<=4 else 1)\n",
      "data_patio_lawn_garden['target'].value_counts()\n",
      "152 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "1     7037\n",
      "0     6235\n",
      "Name:  target, dtype: int64\n",
      "7. Use sklearn's tree()  function to fit a decision tree classification model on the \n",
      "TFIDF DataFrame we created earlier. Add the following code to do this:\n",
      "from sklearn import tree\n",
      "dtc = tree.DecisionTreeClassifier()\n",
      "dtc = dtc.fit(tfidf_df, data_patio_lawn_garden['target'])\n",
      "data_patio_lawn_garden['predicted_labels_dtc'] = dtc.predict\\\n",
      "                                                 (tfidf_df)\n",
      "8. Use pandas' crosstab  function to compare the results of the classification \n",
      "model with the actual classes ( 'target' , in this case) of the reviews. Add the \n",
      "following code to do this:\n",
      "pd.crosstab(data_patio_lawn_garden['target'], \\\n",
      "            data_patio_lawn_garden['predicted_labels_dtc'])\n",
      "The preceding code generates the following output:\n",
      "Figure 3.35: Crosstab between actual target values and predicted labels\n",
      "Here, we can see 6627  instances with a target label of 0 correctly classified, and \n",
      "8 such instances wrongly classified. Furthermore, 7036  instances with a target \n",
      "label of 1 are correctly classified, whereas 1 such instance is wrongly classified.\n",
      "Supervised Learning | 153\n",
      "9. Use sklearn's tree()  function to fit a decision tree regression model on the \n",
      "TFIDF representation of these reviews after cleaning. To predict the overall \n",
      "scores using this model, add the following code:\n",
      "from sklearn import tree\n",
      "dtr = tree.DecisionTreeRegressor()\n",
      "dtr = dtr.fit(tfidf_df, data_patio_lawn_garden['overall'])\n",
      "data_patio_lawn_garden['predicted_values_dtr'] = dtr.predict\\\n",
      "                                                 (tfidf_df)\n",
      "data_patio_lawn_garden[['predicted_values_dtr', \\\n",
      "                        'overall']].head(10)\n",
      "The preceding code generates the following output:\n",
      "Figure 3.36: Overall scores with predicted values\n",
      "154 | Developing a Text Classifier\n",
      "From the preceding table, we can see how the actual and predicted scores vary for \n",
      "different instances. We will use this table later to evaluate the performance of  \n",
      "the model.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/39CHhEc .\n",
      "You can also run this example online at https://packt.live/39EwKZ6 .\n",
      "Next, we will look at another tree-based method, random forest.\n",
      "Random Forest\n",
      "Imagine that you must decide whether to join a university. In one scenario, you ask \n",
      "only one person about the quality of the education the university provides. In another \n",
      "scenario, you ask several career counselors and academicians about this. Which \n",
      "scenario do you think would help you make a better and the most stable decision? \n",
      "The second one, right? This is because, in the first case, the only person you are \n",
      "consulting may be biased. \"Wisdom of the crowd\" tends to remove biases, thereby \n",
      "aiding better decision-making.\n",
      "In general terms, a forest is a collection of different types of trees. The same \n",
      "definition holds true in the case of machine learning as well. Instead of using a single \n",
      "decision tree for prediction, we use several of them. \n",
      "In the scenario we described earlier, the first case is equivalent to using a single \n",
      "decision tree, whereas the second one is equivalent to using several—that is, using \n",
      "a forest. In a random forest, an individual tree's vote impacts the final decision. Just \n",
      "like decision trees, random forest is capable of carrying out both classification and \n",
      "regression tasks.\n",
      "An advantage of the random forest algorithm is that it uses a sampling technique \n",
      "called bagging, which prevents overfitting . Bagging is the process of training meta-\n",
      "algorithms on a different subsample of the data and then combining these to create a \n",
      "better model. Overfitting refers to cases where a model learns the training dataset so \n",
      "well that it is unable to generalize or perform well on another validation/test dataset.Supervised Learning | 155\n",
      "Random forests also aid in understanding the importance of predictor variables and \n",
      "features. However, building a random forest often takes a huge amount of time and \n",
      "memory. We can make use of the scikit-learn library to implement the random forest \n",
      "algorithm in Python using the following code:\n",
      "from sklearn.ensemble import RandomForestClassifier \n",
      "rfc = RandomForestClassifier()\n",
      "rfc = rfc.fit(X, y)\n",
      "predicted_labels = rfc.predict(X)\n",
      "Here, we created the base model using the RandomForestClassifier  class of \n",
      "scikit-learn. Then, we trained the model using the fit  function. The trained model \n",
      "can then be used to make predictions using the predict  function. X represents \n",
      "a DataFrame of independent variables, whereas y represents a DataFrame of \n",
      "dependent variables. \n",
      "Gradient Boosting Machine and Extreme Gradient Boost\n",
      "There are various other tree-based algorithms, such as gradient boosting machines  \n",
      "(GBM ) and extreme gradient boosting  (XGBoost ). Boosting works by combining \n",
      "rough, less complex, or \"weak\" models into a single prediction that is more accurate \n",
      "than any single model. Iteratively, a subset of the training dataset is ingested into a \n",
      "\"weak\" algorithm or simple algorithm (such as a decision tree) to generate a weak \n",
      "model. These weak models are then combined to form the final prediction.\n",
      "GBM makes use of classification trees as the weak algorithm. The results are \n",
      "generated by improving estimations from these weak models using a differentiable \n",
      "loss function, which gives us the performance of the model by calculating how far the \n",
      "prediction is from the actual value. The model fits consecutive trees by considering \n",
      "the net loss of the previous trees; therefore, each tree is partially present in the  \n",
      "final solution.156 | Developing a Text Classifier\n",
      "XGBoost is an enhanced version of GBM that is portable and distributed, which \n",
      "means that it can easily be used in different architectures and can use multiple cores \n",
      "(a single machine) or multiple machines (clusters). As a bonus, the XGBoost  library is \n",
      "written in C++, which makes it fast. It is also useful when working with a huge dataset \n",
      "as it allows you to store data on an external disk rather than load all the data into \n",
      "memory. The main reasons for the popularity of XGBoost are as follows:\n",
      "• Ability to automatically deal with missing values\n",
      "• High-speed execution \n",
      "• High accuracy, if properly trained\n",
      "• Support for distributed frameworks such as Hadoop and Spark\n",
      "XGBoost uses a weighted combination of weak learners during the training phase. \n",
      "We can make use of the xgboost  library to implement the XGBoost algorithm in \n",
      "Python using the following code:\n",
      "from xgboost import XGBClassifier\n",
      "xgb_clf=XGBClassifier()\n",
      "xgb_clf = xgb_clf.fit(X, y)\n",
      "predicted_labels = rfc.predict(X)\n",
      "Here, we created the base model using the XGBClassifier  class of xgboost . \n",
      "Then, we trained the model using the fit  function. The trained model can then be \n",
      "used to make predictions using the predict function. Here, X represents a DataFrame \n",
      "of independent variables, whereas y represents a DataFrame of dependent variables. \n",
      "To get the important features for the trained model, we can use the following code:\n",
      "pd.DataFrame({'word':X.columns,'importance':xgb_clf.feature_\n",
      "importances_})\n",
      "Let's perform some exercises to get a better understanding of tree-based methods.Supervised Learning | 157\n",
      "Exercise 3.08: Tree-Based Methods – Random Forest\n",
      "In this exercise, we will use the tree-based method random forest to predict the \n",
      "overall scores and labels of reviews of patio, lawn, and garden products on Amazon. \n",
      "Follow these steps to implement this exercise:\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3gczb7P .\n",
      "1. Open a Jupyter Notebook. Insert a new cell and add the following code to import \n",
      "the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "2. Now, read the given data file in JSON  format using pandas . Add the following \n",
      "code to implement this:\n",
      "data_patio_lawn_garden = pd.read_json\\\n",
      "                         ('data/'\\\n",
      "                          'reviews_Patio_Lawn_and_Garden_5.json',\\\n",
      "                          lines = True)\n",
      "data_patio_lawn_garden[['reviewText', 'overall']].head() 158 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.37: Storing reviews as a DataFrame\n",
      "3. Use a lambda  function to extract tokens from each reviewText  of this \n",
      "DataFrame, lemmatize them using WordNetLemmatizer , and concatenate \n",
      "them side by side. Use the join  function to concatenate a list of words into a \n",
      "single sentence. Use a regular expression ( re) to replace anything other than \n",
      "letters, digits, and whitespaces with blank spaces. Add the following code to  \n",
      "do this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "data_patio_lawn_garden['cleaned_review_text'] = \\\n",
      "data_patio_lawn_garden['reviewText']\\\n",
      ".apply(lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
      "       for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                 str(x)))]))\n",
      "data_patio_lawn_garden[['cleaned_review_text', 'reviewText', \\\n",
      "                        'overall']].head()\n",
      "Supervised Learning | 159\n",
      "The preceding code generates the following output:\n",
      "Figure 3.38: Review text before and after cleaning, along with overall scores\n",
      "4. Create a DataFrame  from the TFIDF matrix representation of the cleaned \n",
      "version of reviewText  with the following code:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (data_patio_lawn_garden['cleaned_review_text'])\\\n",
      "           .todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.39: TFIDF representation as a DataFrame\n",
      "5. Add the following lines of code to create a new column called target, which will \n",
      "have 0 if the overall  parameter is less than 4; otherwise, it will have 1:\n",
      "data_patio_lawn_garden['target'] = data_patio_lawn_garden\\\n",
      "                                   ['overall'].apply\\\n",
      "                                   (lambda x : 0 if x<=4 else 1)\n",
      "data_patio_lawn_garden['target'].value_counts()\n",
      "160 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "1     7037\n",
      "0     6235\n",
      "Name:  target, dtype: int64\n",
      "6. Now, define a generic function for all the classifier models. Add the following \n",
      "code to do this:\n",
      "def clf_model(model_type, X_train, y):\n",
      "    model = model_type.fit(X_train,y)\n",
      "    predicted_labels = model.predict(tfidf_df)\n",
      "    return predicted_labels\n",
      "7. Train three kinds of classifier models—namely, random forest, gradient boosting \n",
      "machines, and XGBoost. For random forest, we predict the class labels of the \n",
      "given set of review texts and compare them with their actual class—that is, the \n",
      "target, using crosstabs. Add the following code to do this:\n",
      "from sklearn.ensemble import RandomForestClassifier \n",
      "rfc = RandomForestClassifier(n_estimators=20,max_depth=4,\\\n",
      "                             max_features='sqrt',random_state=1)\n",
      "data_patio_lawn_garden['predicted_labels_rfc'] = \\\n",
      "clf_model(rfc, tfidf_df, data_patio_lawn_garden['target'])\n",
      "pd.crosstab(data_patio_lawn_garden['target'], \\\n",
      "            data_patio_lawn_garden['predicted_labels_rfc'])\n",
      "The preceding code generates the following output:\n",
      "Figure 3.40: Crosstab between actual target values and predicted labels\n",
      "Here, we can see 3302  instances with a target label of 0 correctly classified, \n",
      "and 2933  such instances wrongly classified. Furthermore, 5480  instances with \n",
      "a target label of 1 are correctly classified, whereas 1557  such instances are \n",
      "wrongly classified.\n",
      "Supervised Learning | 161\n",
      "8. Now, define a generic function for all regression models. Add the following code \n",
      "to do this:\n",
      "def reg_model(model_type, X_train, y):\n",
      "    model = model_type.fit(X_train,y)\n",
      "    predicted_values = model.predict(tfidf_df)\n",
      "    return predicted_values\n",
      "9. Train three kinds of regression models: random forest, gradient boosting \n",
      "machines, and XGBoost. For random forest, we predict the overall score of the \n",
      "given set of review texts. Add the following code to do this:\n",
      "from sklearn.ensemble import RandomForestRegressor \n",
      "rfg = RandomForestRegressor(n_estimators=20,max_depth=4,\\\n",
      "                            max_features='sqrt',random_state=1)\n",
      "data_patio_lawn_garden['predicted_values_rfg'] = \\\n",
      "reg_model(rfg, tfidf_df, data_patio_lawn_garden['overall'])\n",
      "data_patio_lawn_garden[['overall', \\\n",
      "                        'predicted_values_rfg']].head(10)\n",
      "The preceding code generates the following output:\n",
      "Figure 3.41: Actual overall score and predicted values using a random forest regressor\n",
      "162 | Developing a Text Classifier\n",
      "From the preceding table, we can see how the actual and predicted scores vary for \n",
      "different instances. We will use this table later to evaluate the performance of  \n",
      "the model.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/33aowa4 .\n",
      "You can also run this example online at https://packt.live/2P8a89V .\n",
      "Now, let's perform a similar task using the XGBoost method.\n",
      "Exercise 3.09: Tree-Based Methods – XGBoost\n",
      "In this exercise, we will use the tree-based method XGBoost to predict the overall \n",
      "scores and labels of reviews of patio, lawn, and garden products on Amazon.\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3gczb7P .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *Supervised Learning | 163\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Now, read the given data file in JSON  format using pandas . Add the following \n",
      "code to implement this:\n",
      "data_patio_lawn_garden = pd.read_json\\\n",
      "                         ('data/'\\\n",
      "                          'reviews_Patio_Lawn_and_Garden_5.json',\\\n",
      "                          lines = True)\n",
      "data_patio_lawn_garden[['reviewText', 'overall']].head() \n",
      "The preceding code generates the following output:\n",
      "Figure 3.42: Storing reviews as a DataFrame\n",
      "4. Use a lambda  function to extract tokens from each 'reviewText'  of this \n",
      "DataFrame, lemmatize them using WorrdNetLemmatizer , and concatenate \n",
      "them side by side. Use the join  function to concatenate a list of words into a \n",
      "single sentence. Use the regular expression method ( re) to replace anything \n",
      "other than letters, digits, and whitespaces with blank spaces. Add the following \n",
      "code to do this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "data_patio_lawn_garden['cleaned_review_text'] = \\\n",
      "data_patio_lawn_garden['reviewText'].apply(lambda x : ' '.join\\\n",
      "                                           ([lemmatizer.lemmatize\\\n",
      "                                             (word.lower()) \\\n",
      "                                           for word in word_tokenize\\\n",
      "                                           (re.sub\\\n",
      "164 | Developing a Text Classifier\n",
      "                                            (r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                             str(x)))]))\n",
      "data_patio_lawn_garden[['cleaned_review_text', 'reviewText', \\\n",
      "                        'overall']].head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.43: Review text before and after cleaning, along with overall scores\n",
      "5. Create a DataFrame from the TFIDF matrix representation of the cleaned version \n",
      "of reviewText  with the following code:\n",
      "tfidf_model = TfidfVectorizer(max_features=500)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (data_patio_lawn_garden['cleaned_review_text'])\\\n",
      "           .todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.44: TFIDF representation as a DataFrame\n",
      "6. The following lines of code are used to create a new column called target, which \n",
      "will have 0 if the 'overall'  parameter is less than 4; otherwise, it will have 1:\n",
      "data_patio_lawn_garden['target'] = data_patio_lawn_garden\\\n",
      "                                   ['overall'].apply\\\n",
      "                                   (lambda x : 0 if x<=4 else 1)\n",
      "data_patio_lawn_garden['target'].value_counts()\n",
      "Supervised Learning | 165\n",
      "The preceding code generates the following output:\n",
      "1     7037\n",
      "0     6235\n",
      "Name:  target, dtype: int64\n",
      "7. Now, define a generic function for all the classifier models. Add the following \n",
      "code to do this:\n",
      "def clf_model(model_type, X_train, y):\n",
      "    model = model_type.fit(X_train,y)\n",
      "    predicted_labels = model.predict(tfidf_df)\n",
      "    return predicted_labels\n",
      "8. Predict the class labels of the given set of reviewText  and compare it with \n",
      "their actual class, that is, the target, using the crosstab  function. Add the \n",
      "following code to do this:\n",
      "pip install xgboost\n",
      "from xgboost import XGBClassifier\n",
      "xgb_clf=XGBClassifier(n_estimators=20,learning_rate=0.03,\\\n",
      "                      max_depth=5,subsample=0.6,\\\n",
      "                      colsample_bytree= 0.6,reg_alpha= 10,\\\n",
      "                      seed=42)\n",
      "data_patio_lawn_garden['predicted_labels_xgbc'] = \\\n",
      "clf_model(xgb_clf, tfidf_df, data_patio_lawn_garden['target'])\n",
      "pd.crosstab(data_patio_lawn_garden['target'], \\\n",
      "            data_patio_lawn_garden['predicted_labels_xgbc'])\n",
      "The preceding code generates the following output:\n",
      "Figure 3.45: Crosstab between actual target values and predicted labels using XGBoost\n",
      "166 | Developing a Text Classifier\n",
      "Here, we can see 4300  instances with a target label of 0 correctly classified, \n",
      "and 1935  such instances wrongly classified. Furthermore, 2164  instances with \n",
      "a target label of 1 are correctly classified, whereas 4873  such instances are \n",
      "wrongly classified.\n",
      "9. Now, define a generic function for all the regression models. Add the following \n",
      "code to do this:\n",
      "def reg_model(model_type, X_train, y):\n",
      "    model = model_type.fit(X_train,y)\n",
      "    predicted_values = model.predict(tfidf_df)\n",
      "    return predicted_values\n",
      "10. Predict the overall score of the given set of reviewText . Add the following \n",
      "code to do this:\n",
      "from xgboost import XGBRegressor \n",
      "xgbr = XGBRegressor(n_estimators=20,learning_rate=0.03,\\\n",
      "                    max_depth=5,subsample=0.6,\\\n",
      "                    colsample_bytree= 0.6,reg_alpha= 10,seed=42)\n",
      "data_patio_lawn_garden['predicted_values_xgbr'] = \\\n",
      "reg_model(xgbr, tfidf_df, data_patio_lawn_garden['overall'])\n",
      "data_patio_lawn_garden[['overall', \\\n",
      "                        'predicted_values_xgbr']].head(2)\n",
      "The preceding code generates the following output:\n",
      "Figure 3.46: Actual overall score and predicted values using an XGBoost regressor\n",
      "Supervised Learning | 167\n",
      "From the preceding table, we can see how the actual and predicted scores vary for \n",
      "different instances. We will use this table later to evaluate the performance of the \n",
      "model. With that, you have learned how to use tree-based methods to predict scores \n",
      "in data.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2P5woBi .\n",
      "You can also run this example online at https://packt.live/2DfTa71 .\n",
      "In the next section, you will learn about sampling.\n",
      "Sampling\n",
      "Sampling is the process of creating a subset from a given set of instances. If you have \n",
      "1,000 sentences in an article, out of which you choose 100 sentences for analysis, the \n",
      "subset of 100 sentences will be called a sample of the original article. This process is \n",
      "referred to as sampling. \n",
      "Sampling is necessary when creating models for imbalanced datasets. For example, \n",
      "consider that the number of bad comments on a review board for a company is 10 \n",
      "and the number of good comments is 1,000. If we input this data as it is into the \n",
      "model, it will not give us accurate results; classifying all the comments as \"good\" will \n",
      "provide a near-perfect accuracy, which isn't really applicable to most real datasets. \n",
      "Thus, we need to reduce the number of good reviews to a smaller number before \n",
      "using it as input for training. There are different kinds of sampling methods, such as \n",
      "the following:\n",
      "• Simple random sampling\n",
      "In this process, each instance of the set has an equal probability of being \n",
      "selected. For example, you have 10 balls of 10 different colors in a box. You need \n",
      "to select 4 out of 10 balls without looking at their color. In this case, each ball is \n",
      "equally likely to be selected. This is an instance of simple random sampling.168 | Developing a Text Classifier\n",
      "• Stratified sampling\n",
      "In this type of sampling, the original set is divided into parts called \"strata\", \n",
      "based on given criteria. Random samples are chosen from each of these \"strata.\" \n",
      "For example, you have 100 sentences, out of which 80 are non-sarcastic and 20 \n",
      "are sarcastic. To extract a stratified sample of 10 sentences, you need to select \n",
      "8 from 80 non-sarcastic sentences and 2 from 20 sarcastic sentences. This \n",
      "will ensure that the ratio of non-sarcastic to sarcastic sentences, that is, 80:20, \n",
      "remains unaltered in the sample that's selected.\n",
      "• Multi-Stage Sampling\n",
      "If you are analyzing the social media posts of all the people in a country related \n",
      "to the current weather, the text data will be huge as it will consist of the weather \n",
      "conditions of different cities. Drawing a stratified sample would be difficult. \n",
      "In this case, it is recommended to first extract a stratified sample by region \n",
      "and then further sample it within regions, that is, by cities. This is basically \n",
      "performing stratified sampling at each and every stage.\n",
      "To better understand these, let's perform a simple exercise.\n",
      "Exercise 3.10: Sampling (Simple Random, Stratified, and Multi-Stage)\n",
      "In this exercise, we will extract samples from an online retail dataset that contains \n",
      "details about the transactions of an e-commerce website with the help of simple \n",
      "random sampling, stratified sampling, and multi-stage sampling.\n",
      "Note\n",
      "To download the dataset for this exercise, visit https://packt.live/3fdsZuL .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import pandas and read  \n",
      "the dataset:\n",
      "!pip install xlrd\n",
      "import pandas as pd\n",
      "data = pd.read_excel('data/Online Retail.xlsx')\n",
      "data.shapeSupervised Learning | 169\n",
      "The preceding code generates the following output:\n",
      "(54190, 8)\n",
      "3. We use pandas' sample  function to extract a sample from the DataFrame . \n",
      "Add the following code to do this:\n",
      "# selecting 10% of the data randomly\n",
      "data_sample_random = data.sample(frac=0.1,random_state=42)\n",
      "data_sample_random.shape\n",
      "The preceding code generates the following output:\n",
      "(54191, 8)\n",
      "4. Use sklearn's train_test_split  function to create stratified samples. Add \n",
      "the following code to do this:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_valid, y_train, y_valid = train_test_split\\\n",
      "                                     (data, data['Country'],\\\n",
      "                                      test_size=0.2, \\\n",
      "                                      random_state=42,\\\n",
      "                                      stratify = data['Country'])\n",
      "You can confirm the stratified split by checking the percentage of each category \n",
      "in the country column after the split. To get the train set percentages, use the \n",
      "following code: \n",
      "y_train.value_counts()/y_train.shape[0]170 | Developing a Text Classifier\n",
      "The following is part of the output of the preceding code:\n",
      "Figure 3.47: The percentage of countries for the training set\n",
      "5. Similarly, for the validation set, add the following code:\n",
      "y_valid.value_counts()/y_valid.shape[0]\n",
      "Supervised Learning | 171\n",
      "The following is part of the output of the preceding code:\n",
      "Figure 3.48: The percentage of countries for the validation set\n",
      "As we can see, the percentages of all countries are similar in both the train and \n",
      "validation sets.\n",
      "172 | Developing a Text Classifier\n",
      "6. We filter out the data in various stages and extract random samples from it. \n",
      "We will extract a random sample of 2% from those transactions by country \n",
      "that occurred in the United Kingdom, Germany, or France and where the \n",
      "corresponding quantity is greater than or equal to 2. Add the following code to \n",
      "implement this:\n",
      "data_ugf = data[data['Country'].isin(['United Kingdom', \\\n",
      "                                      'Germany', 'France'])]\n",
      "data_ugf_q2 = data_ugf[data_ugf['Quantity']>=2]\n",
      "data_ugf_q2_sample = data_ugf_q2.sample(frac = .02, \\\n",
      "                                        random_state=42)\n",
      "data_ugf_q2.shape\n",
      "The preceding code generates the following output:\n",
      "(356940, 8)\n",
      "Now, add the following line of code:\n",
      "data_ugf_q2_sample.shape\n",
      "This will generate the following output:\n",
      "(7139, 8)\n",
      "We can see the reduction in size of the data when the filtering criteria is applied and \n",
      "then the reduction in size when a sample of the filtered data is taken. In this exercise, \n",
      "you learned about the three major sampling techniques that will help you create a \n",
      "good training dataset for the text classifier that you will learn how to build in the  \n",
      "next section.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2P7M4nD .\n",
      "You can also run this example online at https://packt.live/3jT8XsZ .Developing a Text Classifier | 173\n",
      "Developing a Text Classifier\n",
      "A text classifier is a machine learning model that is capable of labeling texts based \n",
      "on their content. For instance, a text classifier will help you understand whether \n",
      "a random text statement is sarcastic or not. Presently, text classifiers are gaining \n",
      "importance as manually classifying huge amounts of text data is impossible. In \n",
      "the next few sections, we will learn about the different parts of text classifiers and \n",
      "implement them in Python.\n",
      "Feature Extraction\n",
      "When dealing with text data, features denote its different attributes. Generally, \n",
      "they are numeric representations of the text. As we discussed in Chapter 2 , Feature \n",
      "Extraction Methods , TFIDF representations of texts are one of the most popular ways \n",
      "of extracting features from them.\n",
      "Feature Engineering\n",
      "Feature engineering is the art of extracting new features from existing ones. \n",
      "Extracting novel features, which tend to capture variation in data better, requires \n",
      "sound domain expertise.\n",
      "Removing Correlated Features\n",
      "Correlation refers to the statistical relationship between two variables. Two highly \n",
      "correlated variables provide the same kind of information. For example, the \n",
      "remaining battery life of a laptop and its screen time are highly correlated. The \n",
      "battery life will decrease as the screen time increases. Regression models, including \n",
      "logistic regression, are unable to perform well when correlation between features \n",
      "exists. Thus, features with correlation beyond a certain threshold need to be \n",
      "removed. The most widely used correlation statistic is Pearson correlation, which can \n",
      "be calculated as follows:\n",
      "Figure 3.49: Pearson correlation\n",
      "Here, cov is the covariance, σ is the standard deviation, and X and Y are two variables/\n",
      "features of the training data that we are testing for correlation.\n",
      "174 | Developing a Text Classifier\n",
      "Exercise 3.11: Removing Highly Correlated Features (Tokens)\n",
      "In this exercise, we will remove highly correlated words from a TFIDF matrix \n",
      "representation of sklearn's fetch_20newgroups  text dataset. Follow these steps \n",
      "to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "import matplotlib as mpl\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. We will be using stop words from the English language only. WordNet is the \n",
      "lemmatizer  we will be using. Add the following code to implement this:\n",
      "stop_words = stopwords.words('english')\n",
      "stop_words = stop_words + list(string.printable)\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "4. To specify the categories of news articles you want to fetch, add the  \n",
      "following code:\n",
      "categories= ['misc.forsale', 'sci.electronics', \\\n",
      "             'talk.religion.misc']Developing a Text Classifier | 175\n",
      "5. To fetch sklearn's 20newsgroups  text dataset, corresponding to the categories \n",
      "mentioned earlier, use the following lines of code:\n",
      "news_data = fetch_20newsgroups(subset='train', \\\n",
      "                               categories=categories, \\\n",
      "                               shuffle=True, random_state=42, \\\n",
      "                               download_if_missing=True)\n",
      "news_data_df = pd.DataFrame({'text' : news_data['data'], \\\n",
      "                             'category': news_data.target})\n",
      "news_data_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.50: Texts of news data as a DataFrame\n",
      "6. Now, use the lambda  function to extract tokens from each \"text\" of the news_\n",
      "data_df  DataFrame. Check whether any of these tokens are stop words, \n",
      "lemmatize them, and concatenate them side by side. Use the join  function to \n",
      "concatenate a list of words into a single sentence. Use the regular expression \n",
      "method (re) to replace anything other than letters, digits, and whitespaces with \n",
      "blank spaces. Add the following code to implement this:\n",
      "news_data_df['cleaned_text'] = news_data_df['text']\\\n",
      "                               .apply(lambda x : ' '.join\\\n",
      "                               ([lemmatizer.lemmatize\\\n",
      "                                 (word.lower()) \\\n",
      "                               for word in word_tokenize\\\n",
      "                               (re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                str(x))) if word.lower() \\\n",
      "                                not in stop_words]))\n",
      "176 | Developing a Text Classifier\n",
      "7. Add the following lines of code used to create a TFIDF matrix and transform it \n",
      "into a DataFrame:\n",
      "tfidf_model = TfidfVectorizer(max_features=20)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (news_data_df['cleaned_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.51: TFIDF representation as a DataFrame\n",
      "8. Calculate the correlation matrix for this TFIDF representation. Add the following \n",
      "code to implement this:\n",
      "correlation_matrix = tfidf_df.corr()\n",
      "correlation_matrix.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.52: Correlation matrix\n",
      "9. Now, plot the correlation matrix using seaborn's heatmap function. Add the \n",
      "following code to implement this:\n",
      "import seaborn as sns\n",
      "fig, ax = plt.subplots(figsize=(20, 20))\n",
      "sns.heatmap(correlation_matrix,annot=True, fmt='.1g', \\\n",
      "            vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')\n",
      "Developing a Text Classifier | 177\n",
      "The preceding code generates the following output:\n",
      " \n",
      "Figure 3.53: Heatmap representation of a correlation matrix\n",
      "178 | Developing a Text Classifier\n",
      "10. To identify a pair of terms with high correlation, create an upper triangular \n",
      "matrix from the correlation matrix. Create a stacked array out of it and traverse \n",
      "it. Add the following code to do this:\n",
      "import numpy as np\n",
      "correlation_matrix_ut = correlation_matrix.where(np.triu\\\n",
      "                        (np.ones(correlation_matrix.shape))\\\n",
      "                        .astype(np.bool))\n",
      "correlation_matrix_melted = correlation_matrix_ut.stack()\\\n",
      "                            .reset_index()\n",
      "correlation_matrix_melted.columns = ['word1', 'word2', \\\n",
      "                                     'correlation']\n",
      "correlation_matrix_melted[(correlation_matrix_melted['word1']\\\n",
      "                           !=correlation_matrix_melted['word2']) \\\n",
      "                           & (correlation_matrix_melted\\\n",
      "                              ['correlation']>.7)]\n",
      "The preceding code generates the following output:\n",
      "Figure 3.54: Highly correlated tokens\n",
      "You can see that the most highly correlated features are host , nntp , posting , \n",
      "organization , and subject . Next, we will remove nntp , posting , and \n",
      "organization  since host  and subject  are highly correlated with them.\n",
      "11. Remove terms for which the coefficient of correlation is greater than 0.7 and \n",
      "create a separate DataFrame with the remaining terms. Add the following code \n",
      "to do this:\n",
      "tfidf_df_without_correlated_word = tfidf_df.drop(['nntp', \\\n",
      "                                                  'posting', \\\n",
      "                                                  'organization'],\\\n",
      "                                                 axis = 1)\n",
      "tfidf_df_without_correlated_word.head()\n",
      "Developing a Text Classifier | 179\n",
      "The preceding code generates the following output:\n",
      "Figure 3.55: The DataFrame after removing correlated tokens\n",
      "After removing the highly correlated words from the TFIDF DataFrame, it appears like \n",
      "this. We have cleaned the dataset to remove highly correlated features and are now \n",
      "one step closer to building our text classifier.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/39RdJTz .\n",
      "You can also run this example online at https://packt.live/2XbeCAX .\n",
      "In the next section, we will learn how to reduce the size of the dataset and \n",
      "understand why this is necessary.\n",
      "Dimensionality Reduction\n",
      "There are some optional steps that we can follow on a case-to-case basis. For \n",
      "example, sometimes, the TFIDF matrix or Bag-of-Words representation of a text \n",
      "corpus is so big that it doesn't fit in memory. In this case, it would be necessary to \n",
      "reduce its dimension—that is, the number of columns in the feature matrix. The most \n",
      "popular method for dimension reduction is Principal Component Analysis  (PCA ).\n",
      "PCA is used to perform dimensionality reduction. It converts a list of features (which \n",
      "may be correlated) into a list of variables that are linearly uncorrelated. These \n",
      "linearly uncorrelated variables are known as principal components. These principal \n",
      "components are arranged in descending order of the amount of variability they \n",
      "capture in the dataset. For example, let's consider a Twitter tweet dataset where \n",
      "people misspell words such as good and instead write \"gud\". PCA will combine these \n",
      "two highly correlated features into a single feature and reduce the dimensionality. In \n",
      "the next section, we'll look at an exercise to get a better understanding of this.\n",
      "180 | Developing a Text Classifier\n",
      "Exercise 3.12: Performing Dimensionality Reduction Using Principal Component \n",
      "Analysis\n",
      "In this exercise, we will reduce the dimensionality of a TFIDF matrix representation \n",
      "of sklearn's fetch_20newsgroups  text dataset to two. Then, we'll create a scatter \n",
      "plot of these documents. Each category should be colored differently. Follow these \n",
      "steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "import matplotlib as mpl\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import re\n",
      "import string\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from collections import Counter\n",
      "from pylab import *\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "3. Use stop words from the English language only. WordNet  states the \n",
      "lemmatizer we will be using. Add the following code to implement this:\n",
      "stop_words = stopwords.words('english')\n",
      "stop_words = stop_words + list(string.printable)\n",
      "lemmatizer = WordNetLemmatizer()Developing a Text Classifier | 181\n",
      "4. To specify the categories of news articles we want to fetch by, add the  \n",
      "following code:\n",
      "categories= ['misc.forsale', 'sci.electronics', \\\n",
      "             'talk.religion.misc']\n",
      "5. To fetch sklearn's dataset corresponding to the categories we mentioned earlier, \n",
      "use the following lines of code:\n",
      "news_data = fetch_20newsgroups(subset='train', \\\n",
      "                               categories=categories, \\\n",
      "                               shuffle=True, random_state=42, \\\n",
      "                               download_if_missing=True)\n",
      "news_data_df = pd.DataFrame({'text' : news_data['data'], \\\n",
      "                             'category': news_data.target})\n",
      "news_data_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.56: News texts and their categories\n",
      "182 | Developing a Text Classifier\n",
      "6. Use the lambda  function to extract tokens from each text  item of the news_\n",
      "data_df  DataFrame, check whether any of these tokens are stop words, \n",
      "lemmatize them, and concatenate them side by side. Use the join  function to \n",
      "concatenate a list of words into a single sentence. Use the regular expression \n",
      "method (re) to replace anything other than letters, digits, and whitespaces with \n",
      "a blank space. Add the following code to implement this:\n",
      "news_data_df['cleaned_text'] = news_data_df['text']\\\n",
      "                               .apply(lambda x : ' '.join\\\n",
      "                               ([lemmatizer.lemmatize\\\n",
      "                                 (word.lower()) \\\n",
      "                               for word in word_tokenize\\\n",
      "                               (re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                str(x))) if word.lower() \\\n",
      "                                not in stop_words]))\n",
      "7. The following lines of code are used to create a TFIDF matrix and transform it \n",
      "into a DataFrame:\n",
      "tfidf_model = TfidfVectorizer(max_features=20)\n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (news_data_df['cleaned_text']).todense())\n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
      "tfidf_df.head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.57: TFIDF representation as a DataFrame\n",
      "Developing a Text Classifier | 183\n",
      "8. In this step, we are using sklearn's PCA  function to extract two principal \n",
      "components from the initial data. Add the following code to do this:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(2)\n",
      "pca.fit(tfidf_df)\n",
      "reduced_tfidf = pca.transform(tfidf_df)\n",
      "reduced_tfidf\n",
      "The preceding code generates the following output:\n",
      "Figure 3.58: Principal components\n",
      "In the preceding screenshot, you can see the two principal components that the \n",
      "PCA algorithm calculated.\n",
      "9. Now, we'll create a scatter  plot using these principal components and \n",
      "represent each category with a separate color. Add the following code to \n",
      "implement this:\n",
      "scatter = plt.scatter(reduced_tfidf[:, 0], \\\n",
      "                      reduced_tfidf[:, 1], \\\n",
      "                      c=news_data_df['category'], cmap='gray')\n",
      "plt.xlabel('dimension_1')\n",
      "plt.ylabel('dimension_2')\n",
      "plt.legend(handles=scatter.legend_elements()[0], \\\n",
      "           labels=categories, loc='lower left')\n",
      "plt.title('Representation of NEWS documents in 2D')\n",
      "plt.show()\n",
      "184 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      " \n",
      "Figure 3.59: Two-dimensional representation of news documents\n",
      "Developing a Text Classifier | 185\n",
      "From the preceding plot, we can see that a scatter plot has been created in which \n",
      "each category of article is represented by a different color. This plot shows another \n",
      "important use case of dimensionality reduction: visualization. We were able to plot \n",
      "this two-dimensional image because we had two principal components. With the \n",
      "earlier TFIDF matrix, we had 20 features, which is impossible to visualize. In this \n",
      "section, you learned how to perform dimensionality reduction to save memory space \n",
      "and visualize datasets.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2Xa5eh4 .\n",
      "You can also run this example online at https://packt.live/3jU0dD7 .\n",
      "Next, we will learn how to evaluate the machine learning models that we train.\n",
      "Deciding on a Model Type\n",
      "Once the feature set is ready, it's necessary to decide on the type of model that will \n",
      "be used to deal with the problem. Usually, unsupervised models are chosen when \n",
      "data is not labeled. If we have a predefined number of clusters in mind, we go for \n",
      "clustering algorithms such as k-means; otherwise, we opt for hierarchical clustering. \n",
      "For labeled data, we generally follow supervised learning methods such as regression \n",
      "and classification.\n",
      "If the outcome is continuous and numeric, we use regression. If it is discrete or \n",
      "categorical, we use classification. The Naive Bayes algorithm comes in handy for \n",
      "the fast development of simple classification models. More complex tree-based \n",
      "methods (such as decision trees, random forests, and so on) are needed when we \n",
      "want to achieve higher accuracy. In such cases, we sometimes compromise on model \n",
      "explainability and the time that's required to develop it. When the outcome of a \n",
      "model has to be the probability of the occurrence of a certain class, we use  \n",
      "logistic regression.186 | Developing a Text Classifier\n",
      "Evaluating the Performance of a Model\n",
      "Once a model is ready, it is necessary to evaluate its performance. This is because, \n",
      "without benchmarking it, we cannot be confident of how well or how badly it is \n",
      "functioning. It is not advisable to put a model into production without evaluating \n",
      "its efficiency. There are various ways to evaluate a model's performance. Let's work \n",
      "through them one by one:\n",
      "• Confusion Matrix\n",
      "This is a two-dimensional matrix mainly used for evaluating the performance \n",
      "of classification models. Its columns consist of predicted values, and its rows \n",
      "consist of actual values. In other words, for a given confusion matrix, it is a \n",
      "crosstab between actual and predicted values. The cell entries denote how \n",
      "many of the predicted values match with the actual values, and how many don't. \n",
      "Consider the following image:\n",
      "Figure 3.60: Confusion matrix\n",
      "In the preceding confusion matrix, the top-left cell will have the count of all \n",
      "correctly classified 0 values by the classifier, whereas the top-right cell will have \n",
      "the count of incorrectly classified 0 values, and so on. To create a confusion \n",
      "matrix using Python, you can use the following code:\n",
      "from sklearn.metrics import confusion_matrix\n",
      "confusion_matrix(actual_values,predicted_values)\n",
      "Developing a Text Classifier | 187\n",
      "• Accuracy\n",
      "Accuracy is defined as the ratio of correctly classified instances to the total \n",
      "number of instances. Whenever accuracy is used for model evaluation, we \n",
      "need to ensure that the data is balanced in terms of classes, meaning it should \n",
      "have an almost equal number of instances of each class. Let's use an example \n",
      "of a dataset that has 90% positive labels and 10% negative labels. A model that \n",
      "predicts all the data points as positive will receive 90% accuracy, but that will not \n",
      "be a good indicator of the performance of the model.\n",
      "To get the accuracy of the predicted values using Python, you can use the \n",
      "following code:\n",
      "from sklearn.metrics import accuracy_score\n",
      "accuracy_score(actual_values,predicted_values)\n",
      "• Precision and Recall\n",
      "For a better understanding of precision and recall, let's consider a real-life \n",
      "example. If your mother tells you to explore the kitchen of your house, find \n",
      "items that need to be restocked, and bring them back from the market, you will \n",
      "bring P number of items from the market and show them to your mother. Out \n",
      "of P items, she finds Q items to be relevant. The Q/P ratio is called precision. \n",
      "However, in this scenario, she was expecting you to bring R items relevant to her. \n",
      "The ratio, Q/R, is referred to as recall:\n",
      "Precision = True Positive / (True Positive + False Positive)\n",
      "Recall = True Positive / (True Positive + False Negative)\n",
      "• F1 Score\n",
      "For a given classification model, the F1 score is the harmonic mean of precision \n",
      "and recall. Harmonic mean is a way to find the average while giving equal weight \n",
      "to all numbers:\n",
      "F1 score = 2 * ((Precision * Recall) / (Precision + Recall))\n",
      "To get the F1 score, precision, and recall values using Python, you can use the \n",
      "following code:\n",
      "from sklearn.metrics import classification_report\n",
      "classification_report(actual_values,predicted_values)188 | Developing a Text Classifier\n",
      "• Receiver Operating Characteristic (ROC) Curve\n",
      "To understand the ROC curve, we need to get acquainted with the True Positive \n",
      "Rate (TPR)  and the False Positive Rate (FPR):\n",
      "TPR = True Positive / (True Positive + False Negative)\n",
      "FPR = False Positive / (False Positive + True Negative)\n",
      "The output of a classification model can be probabilities. In that case, we need to \n",
      "set a threshold to obtain classes from those probabilities. The ROC curve is a plot \n",
      "between the TPR and FPR for various values of the threshold. The area under \n",
      "the ROC  curve ( AUROC ) represents the efficiency of the model. The higher the \n",
      "AUROC, the better the model is. The maximum value of AUROC is 1. To create \n",
      "the ROC curve using Python, use the following code:\n",
      "fpr,tpr,threshold=roc_curve(actual_values, \\\n",
      "                            predicted_probabilities)\n",
      "print ('\\nArea under ROC curve for validation set:', auc(fpr,tpr))\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.plot(fpr,tpr,label='Validation set AUC')\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "ax.legend(loc='best')\n",
      "plt.show()\n",
      "Here, actual_values  refers to the actual dependent variable values, whereas \n",
      "predicted_probabilities  is the predicted probability of getting 1 from \n",
      "the trained predictor model.\n",
      "• Root Mean Square Error (RMSE)\n",
      "This is mainly used for evaluating the accuracy of regression models. We define \n",
      "it as follows:\n",
      "Figure 3.61: Formula for root mean square error\n",
      "Developing a Text Classifier | 189\n",
      "Here, n is the number of samples, Pi is the predicted value of the ith observation, \n",
      "and Oi is the observed value of the ith observation. To find the RMSE using \n",
      "Python, use the following code:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rmse = sqrt(mean_squared_error(y_actual, y_predicted))\n",
      "• Maximum Absolute Percentage Error (MAPE)\n",
      "Just like RMSE, this is another way to evaluate a regression model's performance. \n",
      "It is described as follows:\n",
      "Figure 3.62: Formula for maximum absolute percentage error\n",
      "Here, n is the number of samples, Pi is the predicted value (that is, the forecast \n",
      "value) of the ith observation, and Oi is the observed value (that is, the actual \n",
      "value) of the ith observation. To find MAPE in Python, use the following code:\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "mape = mean_absolute_error(y_actual, y_predicted) * 100\n",
      "Exercise 3.13: Calculating the RMSE and MAPE of a Dataset\n",
      "In this exercise, we will calculate the RMSE and MAPE of hypothetical predicted and \n",
      "actual values. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Use sklearn's mean_squared_error  to calculate the MSE and then use the \n",
      "sqrt  function to calculate the RMSE. Add the following code to implement this:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from math import sqrt\n",
      "y_actual = [0,1,2,1,0]\n",
      "y_predicted = [0.03,1.2,1.6,.9,0.1]\n",
      "rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
      "print('Root Mean Squared Error (RMSE) is:', rms)\n",
      "190 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Root Mean Squared Error (RMSE) is: 0.21019038988498018\n",
      "The preceding output shows the RMSE of the y_actual  and y_predicted  \n",
      "values that we created previously.\n",
      "3. Next, use sklearn's mean_absolute_error  to calculate the MAPE of a \n",
      "hypothetical model prediction. Add the following code to implement this:\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "y_actual = [0,1,2,1,0]\n",
      "y_predicted = [0.03,1.2,1.6,.9,0.1]\n",
      "mape = mean_absolute_error(y_actual, y_predicted) * 100\n",
      "print('Mean Absolute Percentage Error (MAPE) is:', \\\n",
      "      round(mape,2), '%')\n",
      "The preceding code generates the following output:\n",
      "Mean Absolute Percentage Error (MAPE) is 16.6 %\n",
      "The preceding output shows the MAPE of the y_actual  and y_predicted  \n",
      "values that we created previously.\n",
      "You have now learned how to evaluate the machine learning models that we train \n",
      "and are equipped to create your very own text classifier.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3ggqRnp .\n",
      "You can also run this example online at https://packt.live/39E7i5S .\n",
      "In the next section, we will solve an activity based on classifying text.Developing a Text Classifier | 191\n",
      "Activity 3.01: Developing End-to-End Text Classifiers\n",
      "For this activity, you will build an end-to-end classifier that figures out whether a news \n",
      "article is political or not.\n",
      "Note\n",
      "The dataset for this activity can be found at https://packt.live/39DUNHL .\n",
      "Follow these steps to implement this activity:\n",
      "1. Import the necessary packages.\n",
      "2. Read the dataset and clean it.\n",
      "3. Create a TFIDF matrix out of it.\n",
      "4. Divide the data into training and validation sets.\n",
      "5. Develop classifier models for the dataset.\n",
      "6. Evaluate the models that were developed using parameters such as confusion \n",
      "matrix, accuracy, precision, recall, F1 plot curve, and ROC curve.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "We have seen how to build end-to-end classifiers. Developing an end-to-end classifier \n",
      "was done in phases. Firstly, the text corpus was cleaned and tokenized, features were \n",
      "extracted using TFIDF, and then the dataset was divided into training and validation \n",
      "sets. The XGBoost algorithm was used to develop a classification model. Finally, \n",
      "the performance was measured using parameters such as the confusion matrix, \n",
      "accuracy, precision, recall, F1 plot curve, and ROC curve. In the next section, you will \n",
      "learn how to build pipelines for NLP projects.192 | Developing a Text Classifier\n",
      "Building Pipelines for NLP Projects\n",
      "In general, a pipeline refers to a structure that allows a streamlined flow of air, water, \n",
      "or something similar. In this context, pipeline has a similar meaning. It helps to \n",
      "streamline various stages of an NLP project.\n",
      "An NLP project is done in various stages, such as tokenization, stemming, feature \n",
      "extraction (TFIDF matrix generation), and model building. Instead of carrying out each \n",
      "stage separately, we create an ordered list of all these stages. This list is known as \n",
      "a pipeline. The Pipeline  class of sklearn helps us combine these stages into one \n",
      "object that we can use to perform these stages one after the other in a sequence. \n",
      "We will solve a text classification problem using a pipeline in the next section to \n",
      "understand the working of a pipeline better.\n",
      "Exercise 3.14: Building the Pipeline for an NLP Project\n",
      "In this exercise, we will develop a pipeline that will allow us to create a TFIDF matrix \n",
      "representation from sklearn's fetch_20newsgroups  text dataset. Follow these \n",
      "steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn import tree\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import pandas as pd\n",
      "3. Specify the categories of news articles you want to fetch. Add the following code \n",
      "to do this:\n",
      "categories = ['misc.forsale', 'sci.electronics', \\\n",
      "              'talk.religion.misc']Building Pipelines for NLP Projects | 193\n",
      "4. To fetch sklearn's 20newsgroups  dataset, corresponding to the categories \n",
      "mentioned earlier, we use the following lines of code:\n",
      "news_data = fetch_20newsgroups(subset='train', \\\n",
      "                               categories=categories, \\\n",
      "                               shuffle=True, random_state=42, \\\n",
      "                               download_if_missing=True)\n",
      "5. Define a pipeline consisting of two stages: CountVectorizer  and \n",
      "TfidfTransformer . Fit it on the news_data  we mentioned earlier and use it \n",
      "to transform that data. Add the following code to implement this:\n",
      "text_classifier_pipeline = Pipeline([('vect', \\\n",
      "                                      CountVectorizer()), \\\n",
      "                                     ('tfidf', \\\n",
      "                                      TfidfTransformer())])\n",
      "text_classifier_pipeline.fit(news_data.data, news_data.target)\n",
      "pd.DataFrame(text_classifier_pipeline.fit_transform\\\n",
      "             (news_data.data, news_data.target).todense()).head()\n",
      "The preceding code generates the following output:\n",
      "Figure 3.63: TFIDF representation of the DataFrame created using a pipeline\n",
      "Here, we created a pipeline consisting of the count vectorizer and TFIDF transformer. \n",
      "The outcome of this pipeline is the TFIDF representation of the text data that has \n",
      "been passed to it as an argument.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gqpeUt .\n",
      "You can also run this example online at https://packt.live/3113qrJ .\n",
      "194 | Developing a Text Classifier\n",
      "Saving and Loading Models\n",
      "After a model has been built and its performance matches our expectations, we \n",
      "may want to save it for future use. This eliminates the time needed for rebuilding it. \n",
      "Models can be saved on the hard disk using the joblib  and pickle  libraries.\n",
      "The pickle  module makes use of binary protocols to save and load Python objects. \n",
      "joblib  makes use of the pickle  library protocols, but it improves on them to \n",
      "provide an efficient replacement to save large Python objects. Both libraries have two \n",
      "main functions that we will make use of to save and load our models:\n",
      "• dump : This function is used to save a Python object to a file on the disk.\n",
      "• loads : This function is used to load a saved Python object from a file on  \n",
      "the disk.\n",
      "To deploy saved models, we need to load them from the hard disk to the memory. \n",
      "In the next section, we will perform an exercise based on this to get a better \n",
      "understanding of this process.\n",
      "Exercise 3.15: Saving and Loading Models\n",
      "In this exercise, we will create a TFIDF representation of sentences. Then, we will save \n",
      "this model on disk and later load it from the disk. Follow these steps to implement \n",
      "this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and the following code to import the necessary packages:\n",
      "import pickle\n",
      "from joblib import dump, load\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "3. Define a corpus consisting of four sentences by adding the following code:\n",
      "corpus = ['Data Science is an overlap between Arts and Science',\\\n",
      "          'Generally, Arts graduates are right-brained and '\\\n",
      "          'Science graduates are left-brained', \\\n",
      "          'Excelling in both Arts and Science at a time '\\\n",
      "          'becomes difficult', \\\n",
      "          'Natural Language Processing is a part of Data Science']Saving and Loading Models | 195\n",
      "4. Fit a TFIDF model to it. Add the following code to do this:\n",
      "tfidf_model = TfidfVectorizer()\n",
      "tfidf_vectors = tfidf_model.fit_transform(corpus).todense()\n",
      "print(tfidf_vectors)\n",
      "The preceding code generates the following output:\n",
      "Figure 3.64: TFIDF representation as a matrix\n",
      "5. Save this TFIDF model on disk using joblib . Add the following code to do this:\n",
      "dump(tfidf_model, 'tfidf_model.joblib')\n",
      "6. Finally, load this model from disk to memory and use it. Add the following code \n",
      "to do this:\n",
      "tfidf_model_loaded = load('tfidf_model.joblib')\n",
      "loaded_tfidf_vectors = tfidf_model_loaded.transform(corpus).todense()\n",
      "print(loaded_tfidf_vectors)\n",
      "196 | Developing a Text Classifier\n",
      "The preceding code generates the following output:\n",
      "Figure 3.65: TFIDF representation as a matrix\n",
      "7. Save this TFIDF model on disk using pickle . Add the following code to do this:\n",
      "pickle.dump(tfidf_model, open(\"tfidf_model.pickle.dat\", \"wb\"))\n",
      "8. Load this model from disk to memory and use it. Add the following code to  \n",
      "do this:\n",
      "loaded_model = pickle.load(open(\"tfidf_model.pickle.dat\", \"rb\"))\n",
      "loaded_tfidf_vectors = loaded_model.transform(corpus).todense()\n",
      "print(loaded_tfidf_vectors)\n",
      "Saving and Loading Models | 197\n",
      "The preceding code generates the following output:\n",
      "Figure 3.66: TFIDF representation as a matrix\n",
      "From the preceding screenshot, we can see that the saved model and the model that \n",
      "was loaded from the disk are identical. You have now learned how to save and load \n",
      "models. This section marks the end of this chapter, where you learned how to build a \n",
      "text classifier from scratch.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2BIDNmZ .\n",
      "You can also run this example online at https://packt.live/3hIay38 .\n",
      "198 | Developing a Text Classifier\n",
      "Summary\n",
      "In this chapter, you learned about the different types of machine learning techniques, \n",
      "such as supervised and unsupervised learning. We explored unsupervised algorithms \n",
      "such as hierarchical clustering and k-means clustering, and supervised learning \n",
      "algorithms, such as k-nearest neighbor, the Naive Bayes classifier, and tree-based \n",
      "methods, such as random forest and XGBoost, that can perform both regression and \n",
      "classification. We discussed the need for sampling and went over different kinds of \n",
      "sampling techniques for splitting a given dataset into training and validation sets. \n",
      "Finally, we covered the process of saving a model on the hard disk and loading it back \n",
      "into memory for future use.\n",
      "In the next chapter, you will learn about several techniques that you can use to collect \n",
      "data from various sources.Overview\n",
      "This chapter introduces you to the concept of web scraping. You will first \n",
      "learn how to extract data (such as text, images, lists, and tables) from \n",
      "pages that are written using HTML. You will then learn about the various \n",
      "types of semi-structured data used to create web pages (such as JSON \n",
      "and XML) and extract data from them. Finally, you will use APIs for data \n",
      "extraction from Twitter, using the tweepy  package.Collecting Text Data with \n",
      "Web Scraping and APIs4202 | Collecting Text Data with Web Scraping and APIs\n",
      "Introduction\n",
      "In the last chapter, we developed a simple classifier using feature extraction methods. \n",
      "We also covered different algorithms that fall under supervised and unsupervised \n",
      "learning. In this chapter, you will learn how to collect text data by scraping web pages, \n",
      "and then you will learn how to process that data. Web scraping helps you extract \n",
      "useful data from online content, such as product prices and customer reviews, \n",
      "which can then be used for market research, price comparison for products, or data \n",
      "analysis. You will also learn how to handle various kinds of semi-structured data, \n",
      "such as JSON and XML. We will cover different methods for extracting data using \n",
      "Application Programming Interfaces  (APIs ). Finally, we will explore different ways \n",
      "to extract data from different types of files.\n",
      "Collecting Data by Scraping Web Pages\n",
      "The basic building block of any web page is HTML (Hypertext Markup Language )—a \n",
      "markup language that specifies the structure of your content. HTML is written \n",
      "using a series of tags, combined with optional content. The content encompassed \n",
      "within HTML tags defines the appearance of the web page. It can be used to make \n",
      "words bold or italicize them, to add hyperlinks to the text, and even to add images. \n",
      "Additional information can be added to the element using attributes within tags. So, a \n",
      "web page can be considered to be a document written using HTML. Thus, we need to \n",
      "know the basics of HTML to scrape web pages effectively.\n",
      "The following figure depicts the contents that are included within an HTML tag:\n",
      "Figure 4.1: Tags and attributes of HTML\n",
      "Collecting Data by Scraping Web Pages | 203\n",
      "As you can see in the preceding figure, we can easily identify different elements \n",
      "within an HTML tag. The basic HTML structure and commonly used tags are shown \n",
      "and explained as follows:\n",
      "Figure 4.2: Basic HTML structure\n",
      "• DOCTYPE : This is a must-have preamble for every HTML page. It informs the \n",
      "browser that the document is written in HTML.\n",
      "• <html>  tag: This is considered the root of the page, encompassing all of the \n",
      "page content. It is mainly divided into two tags— <head>  and <body> .\n",
      "• <head>  tag: This tag provides meta-information about the web page.\n",
      "• <body>  tag: This tag comprises content such as text, image, tables, and lists.\n",
      "• <title>  tag: This sets the title of your page, which is what you'll see in the \n",
      "browser's tab.\n",
      "• <headline>  tag: As the name suggests, this represents six levels of section \n",
      "headings, from <h1>  to <h6> .\n",
      "• <p>  tag: This is used to define the paragraph text content.\n",
      "• <i>  tag: We can use this tag to italicize the text.\n",
      "• <strong>  tag: This makes the text bold.\n",
      "• <li>  tag: We can use this tag to list the content in ordered (the <ol>  tag) or \n",
      "unordered (the <ul>  tag) list format.\n",
      "204 | Collecting Text Data with Web Scraping and APIs\n",
      "• <img>  tag: This tag is used to add an image in the HTML document.\n",
      "• <h1>  to <h6>  tags: These represent the various levels of headings, with <h1>  \n",
      "having the biggest size and <h6>  having the smallest size.\n",
      "• <span>  tag: Although this tag provides no visual change by itself, it is useful for \n",
      "grouping inline-elements in a document and adding a hook to a part of a text or \n",
      "a part of a document. \n",
      "• <q>  tag: Quotes are written within the <q>  tag in HTML.\n",
      "• table  tag: Tabular content is represented as a table  tag, which contains <th>  \n",
      "(table header), <tr>  (table row), and <td>  (table data).\n",
      "• <address>  tag: In HTML documents, addresses are enclosed within \n",
      "<address>  tags.\n",
      "In the next section, we will walk through an exercise in which we'll extract tag-based \n",
      "information from HTML files.\n",
      "Exercise 4.01: Extraction of Tag-Based Information from HTML Files\n",
      "In this exercise, we will extract addresses, quotes, text written in bold, and a table \n",
      "present in an HTML file.\n",
      "Note\n",
      "The data for this sample HTML file can be accessed  \n",
      "from https://packt.live/338opvv .\n",
      "Follow these steps to implement this exercise: \n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the  \n",
      "BeautifulSoup  library:\n",
      "from bs4 import BeautifulSoup\n",
      "BeautifulSoup  is a Python library for pulling data out of HTML and XML files. \n",
      "It provides a parser for HTML/XML formats, allowing us to navigate, search, and \n",
      "modify the parsed tree.Collecting Data by Scraping Web Pages | 205\n",
      "3. Create an object of the BeautifulSoup  class and pass the location of the \n",
      "HTML file to it:\n",
      "soup = BeautifulSoup(open('../data/sample_doc.html'), \\\n",
      "                     'html.parser')\n",
      "In the preceding line, html.parser  is Python's built-in standard library parser. \n",
      "BeautifulSoup  also supports third-party parsers such as html5lib , lxml , \n",
      "and others.\n",
      "4. Add the following code to check the text contents of the  \n",
      "sample_doc.html  file:\n",
      "soup.text\n",
      "The preceding code generates the following output:\n",
      "Figure 4.3: Text content of an HTML file\n",
      "5. Similarly, to see the contents, you can simply write the following code:\n",
      "soup.contents\n",
      "Figure 4.4: Text content\n",
      "6. To find the addresses from the document, insert a new cell and add the \n",
      "following code:\n",
      "soup.find('address')\n",
      "The preceding code generates the following output:\n",
      "<address> Mess on No. 72, Banamali Naskar Lane, Kolkata.</address>\n",
      "206 | Collecting Text Data with Web Scraping and APIs\n",
      "7. To locate all the address  tags within the given content, write the  \n",
      "following code:\n",
      "soup.find_all('address')\n",
      "The preceding code generates the following output:\n",
      "[<address> Mess on No. 72, Banamali Naskar Lane, Kolkata.</address>,\n",
      " <address>221B, Baker Street, London, UK.</address>]\n",
      "8. To find the quotes in the document, add the following code:\n",
      "soup.find_all('q')\n",
      "The preceding code generates the following output:\n",
      "[<q> There are more things in heaven and earth, Horatio, <br/> \n",
      " Than are dreamt of in your philosophy. </q>]\n",
      "9. To check all the bold items, write the following command:\n",
      "soup.find_all('b')\n",
      "The preceding code generates the following output:\n",
      "[<b>Sherlock </b>, <b>Hamlet</b>, <b>Horatio</b>]\n",
      "10. Write the following command to extract the tables in the document:\n",
      "table = soup.find('table')\n",
      "tableCollecting Data by Scraping Web Pages | 207\n",
      "The preceding code generates the following output:\n",
      "Figure 4.5: Contents of the table tag\n",
      "11. You can also view the contents of table  by looping through it. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "for row in table.find_all('tr'):\n",
      "    columns = row.find_all('td')\n",
      "    print(columns)\n",
      "The preceding code generates the following output:\n",
      "[ ]\n",
      "[<td>Gangaram</td>, <td>B.Tech</td>, <td>NA</td>, <td>NA</td>]\n",
      "[<td>Ganga</td>, <td>B.A.</td>, <td>NA</td>, <td>NA</td>]\n",
      "[<td>Ram</td>, <td>B.Tech</td>, <td>M.Tech</td>, <td>NA</td>]\n",
      "[<td>Ramlal</td>, <td>B.Music</td>, <td>NA </td>, <td>Diploma in \n",
      "Music</td>]\n",
      "208 | Collecting Text Data with Web Scraping and APIs\n",
      "12. You can also locate specific content in the table. To locate the value at  \n",
      "the intersection of the third row and the second column, write the  \n",
      "following command:\n",
      "table.find_all('tr')[3].find_all('td')[2]\n",
      "The preceding code generates the following output:\n",
      "<td>M.Tech</td>\n",
      "We have learned how to extract tag-based information from an HTML file.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gekCAA .\n",
      "You can also run this example online at https://packt.live/2EyJp4q .\n",
      "In the next section, we will focus on fetching content from web pages.\n",
      "Requesting Content from Web Pages\n",
      "Whenever you visit a web page from your web browser, you are actually sending \n",
      "a request to fetch its content. This can be done using Python scripts. The Python \n",
      "requests  package is widely used to handle all forms of HTTP requests. Let's walk \n",
      "through an exercise to get a better understanding of this concept.\n",
      "To fetch content, you can use the get()  method, which, as the name suggests, sends \n",
      "a GET  request to the web page from which you want to fetch data. Let's perform a \n",
      "simple exercise now to get a better idea of how we can implement this in Python.\n",
      "Exercise 4.02: Collecting Online Text Data\n",
      "In this exercise, we will be fetching the web content with the help of requests . We \n",
      "will be pulling a text file from Project Gutenberg , the free e-book website, specifically, \n",
      "from the text file for Charles Dickens' famous book, David Copperfield . Follow these \n",
      "steps to complete this exercise:Collecting Data by Scraping Web Pages | 209\n",
      "1. Use the requests  library to request the content of a book available online with \n",
      "the following set of commands:\n",
      "import requests\n",
      "\"\"\"\"\n",
      "Let's read the text version of david copper field \n",
      "available online\n",
      "\"\"\"\n",
      "r = requests.get('https://www.gutenberg.org/files/766/766-0.txt')\n",
      "r.status_code\n",
      "The preceding code generates the following output:\n",
      "200\n",
      "When the browser visits the website, it fetches the content of the specified URL. \n",
      "Similarly, using requests , we get the content from the specified URL and all the \n",
      "information gets stored in the r object. 200  indicates that we received the right \n",
      "response from the URL.\n",
      "2. Locate the text content of the fetched file by using the requests  object r and \n",
      "referring to the text  attribute. Write the following code for this:\n",
      "r.text[:1000]\n",
      "The preceding code generates the following output:\n",
      "Figure 4.6: Text contents of the file\n",
      "210 | Collecting Text Data with Web Scraping and APIs\n",
      "3. Now, write the fetched content into a text file. To do this, add the following code:\n",
      "from pathlib import Path\n",
      "open(Path(\"../data/David_Copperfield.txt\"),'w',\\\n",
      "     encoding='utf-8').write(r.text)\n",
      "The preceding code generates the following output:\n",
      "2033139\n",
      "4. Similarly, we can do the same using Urllib3.First add the following code:\n",
      "import urllib3\n",
      "http = urllib3.PoolManager()\n",
      "rr = http.request('GET', \\\n",
      "                  'http://www.gutenberg.org/files/766/766-0.txt')\n",
      "rr.status\n",
      "Again, we will get the output as 200 , similar to the previous method.\n",
      "5. Add the following code to locate the text content:\n",
      "rr.data[:1000]\n",
      "You will see that you get the same output as shown in Figure 4.6 .\n",
      "6. Again, add the following code to write the fetched content into a text file:\n",
      "open(Path(\"../data/David_Copperfield_new.txt\"), \\\n",
      "     'wb').write(rr.data)\n",
      "The preceding code will generate the following output:\n",
      "2033139\n",
      "We have just learned how to collect data from online sources with the help of the \n",
      "requests  library.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3fhu1pv .\n",
      "You can also run this example online at https://packt.live/2Dmov7L .\n",
      "Now, let's look at analyzing HTML content from Jupyter Notebooks.Collecting Data by Scraping Web Pages | 211\n",
      "Exercise 4.03: Analyzing the Content of Jupyter Notebooks (in HTML Format)\n",
      "In this exercise, we will analyze the content of a Jupyter Notebook. We will count the \n",
      "number of images, list the packages that have been imported, and check the models \n",
      "and their performance.\n",
      "Note\n",
      "The HTML file used for this exercise, can be accessed  \n",
      "at https://packt.live/3fcYIfJ .\n",
      "Follow these steps to complete this exercise:\n",
      "1. Import BeautifulSoup  and pass the location of the given HTML file using the \n",
      "following commands:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(open('../data/text_classifier.html'), \\\n",
      "                     'html.parser')\n",
      "soup.text[:100]\n",
      "Here, we are loading HTML using BeautifulSoup  and printing parsed content. \n",
      "The preceding code generates the following output:\n",
      "'\\n\\n\\nCh3_Activity7_Developing_end_to_end_Text_Classifiers\\n\\n\\n\\n    \n",
      "/*!\\n*\\n* Twitter Bootstrap\\n*\\n*/\\n/*!\\n*'\n",
      "2. Use the img  tag to count the number of images:\n",
      "len(soup.find_all('img'))\n",
      "The output shows that there are three img  tags:\n",
      "3\n",
      "3. If you open the HTML file in the text editor or your web browser's console, you \n",
      "will see all import  statements have the class  attribute set to nn. So, to list all \n",
      "the packages that are imported, add the following code, referring to finding the \n",
      "span  element with an nn class attribute: \n",
      "[i.get_text() for i in soup.find_all\\\n",
      "('span',attrs={\"class\":\"nn\"})]212 | Collecting Text Data with Web Scraping and APIs\n",
      "The preceding code generates the following output:\n",
      "Figure 4.7: List of libraries imported\n",
      "4. To extract the models and their performance, look at the HTML document and \n",
      "see which class  attribute the models and their performance belong to. You \n",
      "will see the h2 and div  tags with the class  attribute output_subarea \n",
      "output_stream output_stdout output_text . Add the following code \n",
      "to extract the models:\n",
      "for md,i in zip(soup.find_all('h2'), \\\n",
      "soup.find_all('div',\\\n",
      "attrs={\"class\":\"output_subarea output_stream \"\\\n",
      "       \"output_stdout output_text\"})):\n",
      "    print(\"Model: \",md.get_text())\n",
      "    print(i.get_text())\n",
      "    print(\"---------------------------------------------------------\n",
      "\\n\\n\\n\")\n",
      "Collecting Data by Scraping Web Pages | 213\n",
      "The preceding code generates the following output:\n",
      "Figure 4.8: Models and their performance\n",
      "So, in the preceding output, we have extracted a classification report from the HTML \n",
      "file using BeautifulSoup  by referring to the <h2>  and <div>  tags.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2PaM1Yk .\n",
      "You can also run this example online at https://packt.live/315liSk .\n",
      "214 | Collecting Text Data with Web Scraping and APIs\n",
      "So far, we have seen how to get content from the web using the requests  package, \n",
      "and in this exercise, we saw how to parse and extract the desired information. Next \n",
      "time you come across an article and want to extract certain information from it, you \n",
      "will be able to put these skills to use, instead of manually going over all of the content.\n",
      "Activity 4.01: Extracting Information from an Online HTML Page\n",
      "In this activity, we will extract data about Rabindranath Tagore from the Wikipedia \n",
      "page about him.\n",
      "Note\n",
      "Rabindranath Tagore was a poet and musician from South Asia whose \n",
      "art has had a profound influence on shaping the cultural landscape of the \n",
      "region. He was also the first Indian to win the Nobel Prize for Literature,  \n",
      "in 1913.\n",
      "After extracting the data, we will analyze information from the page. This should \n",
      "include the list of headings in the Works  section, the list of his works, and the list of \n",
      "universities named after him. Follow these steps to implement this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the requests and BeautifulSoup  libraries.\n",
      "3. Fetch the Wikipedia page from https://en.wikipedia.org/wiki/Rabindranath_Tagore  \n",
      "using the get  method of the requests  library.\n",
      "4. Convert the fetched content into HTML format using an HTML parser.\n",
      "5. Print the list of headings in the Works  section.\n",
      "6. Print the list of original works written by Tagore in Bengali.Collecting Data by Scraping Web Pages | 215\n",
      "7. Print the list of universities named after Tagore.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "We are now well-versed in extracting generic data from HTML pages. Let's perform \n",
      "another activity now, where we'll be using regular expressions.\n",
      "Activity 4.02: Extracting and Analyzing Data Using Regular Expressions\n",
      "To perform this activity, you will extract data from Packt's website. The data to be \n",
      "extracted includes frequently asked questions (FAQs) and their answers, phone \n",
      "numbers for customer care services, and the email addresses for customer care \n",
      "services. Follow these steps to complete this activity:\n",
      "1. Import the necessary libraries and extract data from  \n",
      "https://www.packtpub.com/support/faq  using the requests  library.\n",
      "2. Fetch questions and answers from the data.\n",
      "3. Create a DataFrame consisting of questions and answers.\n",
      "4. Fetch email addresses with the help of regular expressions.\n",
      "5. Fetch the phone numbers, with the help of regular expressions.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "In this activity, we were able to fetch data from online sources and analyze it in \n",
      "various ways. Now that we are well-versed in scraping web pages with the help \n",
      "of HTML, in the next section, we will discuss how to scrape web pages with semi-\n",
      "structured data.216 | Collecting Text Data with Web Scraping and APIs\n",
      "Dealing with Semi-Structured Data\n",
      "We learned about various types of data in Chapter 2 , Feature Extraction Methods . Let's \n",
      "quickly recapitulate what semi-structured data refers to. A dataset is said to be semi-\n",
      "structured if it is not in a row-column format but, if required, can be converted into a \n",
      "structured format that has a definite number of rows and columns. Often, we come \n",
      "across data that is stored as key-value pairs or embedded between tags,  \n",
      "as is the case with JSON  (JavaScript Object Notation ) and XML  (Extensible  \n",
      "Markup Language ) files. These are the most popularly used instances of  \n",
      "semi-structured data.\n",
      "JSON\n",
      "JSON files are used for storing and exchanging data. JSON is human-readable \n",
      "and easy to interpret. Just like text files and CSV files, JSON files are language-\n",
      "independent. This means that different programming languages, such as Python, \n",
      "Java, and so on, can work with JSON files effectively. In Python, a built-in data \n",
      "structure called a dictionary  is capable of storing JSON objects as is. Generally, data \n",
      "in JSON objects is present in the form of key-value pairs. The datatype of values of \n",
      "JSON objects must be any of the following:\n",
      "• A string\n",
      "• A number\n",
      "• Another JSON object\n",
      "• An array\n",
      "• A boolean\n",
      "• NullDealing with Semi-Structured Data | 217\n",
      "NoSQL databases (such as MongoDB) store data in the form of JSON objects. Most \n",
      "APIs return JSON objects. The following figure depicts what a JSON file looks like:\n",
      "Figure 4.9: A sample JSON file\n",
      "Often, the response we get when requesting a URL is in the form of JSON objects. \n",
      "To deal with a JSON file effectively, we need to know how to parse it. The following \n",
      "exercise throws light on this.\n",
      "218 | Collecting Text Data with Web Scraping and APIs\n",
      "Exercise 4.04: Working with JSON Files\n",
      "In this exercise, we will extract details such as the names of students, their \n",
      "qualifications, and additional qualifications from a JSON file.\n",
      "Note\n",
      "The sample JSON file can be accessed at https://packt.live/2P6ZwrI .\n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and import json . Pass the location of the file mentioned using \n",
      "the following commands:\n",
      "import json\n",
      "from pprint import pprint\n",
      "data = json.load(open('../data/sample_json.json'))\n",
      "pprint(data)\n",
      "In the preceding code, we are importing Python's built-in json  module and \n",
      "loading the local JSON file using the standard I/O operation of Python. This  \n",
      "turns JSON into the Python dict  object. The preceding code generates the \n",
      "following output:\n",
      "Figure 4.10: Dictionary form of the fetched data\n",
      "Dealing with Semi-Structured Data | 219\n",
      "3. To extract the names of the students, add the following code:\n",
      "[dt['name'] for dt in data['students']]\n",
      "The preceding code generates the following output:\n",
      "['Gangaram', 'Ganga', 'Ram', 'Ramlal']\n",
      "4. To extract their respective qualifications, enter the following code:\n",
      "[dt['qualification'] for dt in data['students']]\n",
      "The preceding code generates the following output:\n",
      "['B.Tech', 'B.A.', 'B.Tech', 'B.Music']\n",
      "5. To extract their additional qualifications, enter the following code. Remember, \n",
      "not every student will have additional qualifications. Thus, we need to check this \n",
      "separately. Add the following code to implement this:\n",
      "[dt['additional qualification'] if 'additional qualification' \\\n",
      "in dt.keys() else None for dt in data['students']]\n",
      "The preceding code generates the following output:\n",
      "[None, None, 'M.Tech', None]\n",
      "As JSON objects are similar to the dictionary data structure of Python, they are widely \n",
      "used on the web to send and receive data across web applications.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/33aSGKi .\n",
      "You can also run this example online at https://packt.live/315MekS .\n",
      "Now that we have learned how to load JSON data, let's extract data using another \n",
      "format, called Extensible Markup Language  (XML ), which is also used by web apps \n",
      "and Word documents to store information.220 | Collecting Text Data with Web Scraping and APIs\n",
      "XML\n",
      "Just like HTML, XML is another kind of markup language that stores data in between \n",
      "tags. It is human-readable and extensible; that is, we have the liberty to define our \n",
      "own tags. Attributes, elements, and tags in the case of XML are similar to those of \n",
      "HTML. An XML file may or may not have a declaration. But, if it has a declaration, then \n",
      "that must be the first line of the XML file. \n",
      "This declaration statement has three parts: Version , Encoding , and \n",
      "Standalone . Version  states which version of the XML standard is being \n",
      "used; Encoding  states the type of character encoding being used in this file; \n",
      "Standalone  tells the parser whether external information is needed for \n",
      "interpreting the content of the XML file. The following figure depicts what an XML file \n",
      "looks like:\n",
      "Figure 4.11: A sample XML file\n",
      "Dealing with Semi-Structured Data | 221\n",
      "An XML file can be represented as a tree called an XML tree. This XML tree begins with \n",
      "the root element (the parent). This root element further branches into child elements. \n",
      "Each element of the XML file is a node in the XML tree. Those elements that don't \n",
      "have any children are leaf nodes. The following figure clearly differentiates between \n",
      "an original XML file and a tree representation of an XML file: \n",
      "Figure 4.12: Comparison of an XML structure\n",
      "XML files are somewhat similar in structure to HTML, with the main difference being \n",
      "that, in XML, we have custom tags rather than the fixed tags vocabulary like HTML. \n",
      "As we learned how to parse HTML using BeautifulSoup  before, let's learn how to \n",
      "parse XML files in the following exercise.\n",
      "222 | Collecting Text Data with Web Scraping and APIs\n",
      "Exercise 4.05: Working with an XML File\n",
      "In this exercise, we will parse an XML file and print the details from it, such as the \n",
      "names of employees, the organizations they work for, and the total salaries of  \n",
      "all employees.\n",
      "Note\n",
      "The sample XML data file can be accessed here: https://packt.live/3hPCaDl .\n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell, import xml.etree.ElementTree , and pass the location of \n",
      "the XML file using the following code:\n",
      "import xml.etree.ElementTree as ET\n",
      "tree = ET.parse('../data/sample_xml_data.xml')\n",
      "root = tree.getroot()\n",
      "root\n",
      "The preceding code generates the following output:\n",
      "<Element 'records' at 0.112291710>\n",
      "3. To check the tag of the fetched element, type the following code:\n",
      "root.tag\n",
      "The preceding code generates the following output:\n",
      "'records'\n",
      "4. Look for the name  and company  tags in the XML and print the data enclosed \n",
      "within them:\n",
      "for record in root.findall('record')[:20]:\n",
      "    print(record.find('name').text, \"---\",\\\n",
      "          record.find('company').text)Dealing with Semi-Structured Data | 223\n",
      "The preceding code generates the following output:\n",
      "Figure 4.13: Data of the name and company tags printed\n",
      "5. To find the sum of the salaries, create a list consisting of the salaries of all \n",
      "employees by iterating over each record and finding the salary  tag in it. Next, \n",
      "remove the $ and , from the string of salary content, and finally, type cast into \n",
      "the integer to get the sum at the end. Add the following code to do so:\n",
      "sum([int(record.find('salary').text.replace('$','').\\\n",
      "replace(',','')) for record in root.findall('record')])\n",
      "The preceding code generates the following output:\n",
      "745609\n",
      "224 | Collecting Text Data with Web Scraping and APIs\n",
      "Thus, we can see that the sum of all the salaries is $745,609. We just learned how  \n",
      "to extract data from a local XML file. When we request data, many URLs return an \n",
      "XML file.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hQzuFM .\n",
      "You can also run this example online at https://packt.live/3jU8VRP .\n",
      "In the next section, we will look at how APIs can be used to retrieve real-time data.\n",
      "Using APIs to Retrieve Real-Time Data\n",
      "API stands for Application Programming Interface . To understand what an API is, \n",
      "let's consider a real-life example. Suppose you have a socket plug in the wall, and \n",
      "you need to charge your cellphone using it. How will you do it? You will have to use a \n",
      "charger/adapter, which will enable you to connect the cellphone to the socket. Here, \n",
      "this adapter is acting as a mediator that connects the cellphone and the socket, thus \n",
      "enabling the smooth transfer of electricity between them. \n",
      "Similarly, some websites do not provide their data directly. Instead, they provide \n",
      "APIs, which we can use to extract data from the websites. Just like the cellphone \n",
      "charger, an API acts as a mediator, enabling the smooth transfer of data between \n",
      "those websites and us. Let's perform a simple exercise to get hands-on experience of \n",
      "collecting data using APIs.\n",
      "Exercise 4.06: Collecting Data Using APIs\n",
      "In this exercise, we will use the Currency Exchange Rates API to convert USD to \n",
      "another currency rate. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the necessary packages. Add the following code to do so:\n",
      "import json \n",
      "import pprint \n",
      "import requests \n",
      "import pandas as pdDealing with Semi-Structured Data | 225\n",
      "3. Load the json  data. Add the following code to do this:\n",
      "r = requests.get(\"https://api.exchangerate-api.com/\"\\\n",
      "                 \"v4/latest/USD\")\n",
      "data = r.json()\n",
      "pprint.pprint(data)\n",
      "Note\n",
      "Watch out for the slashes in the string below. Remember that the \n",
      "backslashes ( \\ ) are used to split the code across multiple lines, while the \n",
      "forward slashes ( / ) are part of the URL.\n",
      "The preceding code generates the following output:\n",
      "Figure 4.14: Fetched data in the Python dict format\n",
      "4. To create the DataFrame of the fetched data and print it, add the following code:\n",
      "df = pd.DataFrame(data)\n",
      "df.head()\n",
      "226 | Collecting Text Data with Web Scraping and APIs\n",
      "The preceding code generates the following output:\n",
      "Figure 4.15: DataFrame showing details of currency exchange rates\n",
      "Note that you will get a different output depending on the present currency exchange \n",
      "rates. We just learned how to collect data using APIs.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3jQAcEG .\n",
      "You can also run this example online at https://packt.live/3jVIBa0 .\n",
      "In the next section, we will see how to create an API.\n",
      "Extracting data from Twitter Using the OAuth API\n",
      "Many popular websites, such as Twitter, provide an API that allows access to parts \n",
      "of their services so that people can build software that integrates with the website. \n",
      "We'll be focusing mainly on Twitter in this section. Twitter's data and services (such \n",
      "as tweets, advertisements, direct messages, and much more) can be accessed via the \n",
      "Twitter API. The Twitter API requires authentication and authorization to interact with \n",
      "its services using the OAuth  method. Authentication is required to prove identity, \n",
      "while authorization proves the right to access its services and data. To access  \n",
      "Twitter data and services using an API, you would need to register using a Twitter \n",
      "developer account. \n",
      "Dealing with Semi-Structured Data | 227\n",
      "You can collect data from Twitter using their Python module, named Tweepy . \n",
      "Tweepy  is a Python library for accessing the Twitter API. It is great for simple \n",
      "automation and creating Twitter bots. It provides abstraction to communicate with \n",
      "Twitter and use its API to ease interactions, which makes this approach more efficient \n",
      "than using the requests  library and Twitter API endpoints. \n",
      "To use the Tweepy  library, simply go to https://dev.twitter.com/apps/new  and fill in the \n",
      "form; you'll need to complete the necessary fields, such as App Name , Website \n",
      "URL , Callback URL , and App Usage . Once you've done this, submit and receive \n",
      "the keys and tokens, which you can use for extracting tweets and more. However, \n",
      "before you do any of this, you'll first need to import the tweepy  library.\n",
      "Your Python code should look like this:\n",
      "import tweepy\n",
      "consumer_key = 'your consumer key here'\n",
      "consumer_secret = 'your consumer secret key here'\n",
      "access_token = 'your access token here'\n",
      "access_token_secret = 'your access token secret here'\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "The preceding code uses auth  instantiation from OAuthHandler , which takes in \n",
      "our consumer token and secret keys that were obtained during app registration. \n",
      "OAuthHandler  handles interaction with Twitter's OAuth  system.\n",
      "To search for a query named randomquery  using tweepy , you can use the Cursor  \n",
      "object as follows:\n",
      "tweepy.Cursor(api.search, q='randomquery', lang=\"en\")\n",
      "Cursor  handles all the iterating-over-pages work for us behind the scenes, whereas \n",
      "the api.search  method provides tweets that match a specified query given with \n",
      "the q parameter.\n",
      "Let's do an activity now, to put our knowledge into practice.228 | Collecting Text Data with Web Scraping and APIs\n",
      "Activity 4.03: Extracting Data from Twitter\n",
      "In this activity, you will extract 100 tweets containing the hashtag #climatechange  from \n",
      "Twitter, using the Twitter API via the tweepy library, and load them into a pandas \n",
      "DataFrame. The following steps will help you implement this activity:\n",
      "1. Log in to your Twitter account with your credentials.\n",
      "2. Visit https://dev.twitter.com/apps/new  and fill in the form by completing the \n",
      "necessary fields, such as App Name , providing Website URL , Callback \n",
      "URL , and App Usage .\n",
      "3. Submit the form and receive the keys and tokens.\n",
      "4. Use these keys and tokens in your application when making an API call  \n",
      "for #climatechange .\n",
      "5. Import the necessary libraries.\n",
      "6. Fetch the data using the keys and tokens.\n",
      "7. Create a DataFrame consisting of tweets.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "In this activity, we extracted data from Twitter and loaded it into a pandas DataFrame. \n",
      "This data can also be used to analyze tweets and create a word cloud out of them, \n",
      "something that we will explore in detail in Chapter 8 , Sentiment Analysis .\n",
      "Publisher's Note\n",
      "The preceding messages were extracted without bias from a given dataset \n",
      "and written by private individuals not affiliated with this company . The views \n",
      "expressed in these tweets do not necessarily reflect our company's  \n",
      "official policies.Summary | 229\n",
      "Summary\n",
      "In this chapter, we have learned various ways to collect data by scraping web pages. \n",
      "We also successfully scraped data from semi-structured formats such as JSON and \n",
      "XML and explored different methods of retrieving data in real time from a website \n",
      "without authentication. In the next chapter, you will learn about topic modeling—an \n",
      "unsupervised natural language processing technique that helps group documents \n",
      "according to the topics that it detects in them.Overview\n",
      "This chapter introduces topic modeling, which means using unsupervised \n",
      "machine learning to find \"topics\" within a given set of documents. You will \n",
      "explore the most common approaches to topic modeling, which are Latent \n",
      "Semantic Analysis (LSA), Latent Dirichlet Allocation  (LDA), and the \n",
      "Hierachical Dirichlet Process  (HDP ), and learn the differences between \n",
      "them. You will then practice implementing these approaches in Python and \n",
      "review the common practical challenges in topic modeling. By the end of \n",
      "this chapter, you will be able to create topic models from any given dataset.Topic Modeling5232 | Topic Modeling\n",
      "Introduction\n",
      "In the previous chapter, we learned about different ways to collect data from local \n",
      "files and online resources. In this chapter, we will focus on topic modeling , which is \n",
      "an important area within natural language processing. Topic modeling is a simple way \n",
      "to capture the sense of what a document or a collection of documents is about. Note \n",
      "that in this case, documents are any coherent collection of words, which could be as \n",
      "short as a tweet or as long as an encyclopedia.\n",
      "Topic modeling may be thought of as a way to automate the manual task of \n",
      "reading given document(s) to write an abstract, which you will then use to map the \n",
      "document(s) to a set of topics. Topic modeling is mostly done using unsupervised \n",
      "learning algorithms that detect topics on their own. Topic-modeling algorithms \n",
      "operate by performing statistical analysis on words or tokens in documents and \n",
      "using those statistics to automatically assign each document to multiple topics. A \n",
      "topic is represented by an arbitrary number and its keywords. When the topics are \n",
      "not interpretable, then topic modeling may be thought of as an automated process \n",
      "of a manual task in which the semantic structure or meaning of the documents was \n",
      "neither understood nor abstracted before mapping the document(s) to a set of topics.\n",
      "Topic modeling generally uses unsupervised learning algorithms, as opposed to \n",
      "supervised learning algorithms. This means that, during training, we do not have \n",
      "to provide labels (that is, topic names corresponding to each document) in order to \n",
      "teach the model. This not only helps us discover interesting topics that might exist, \n",
      "but also reduces the manual effort spent in labeling texts. On the flip side, it can be a \n",
      "lot more challenging to evaluate the output of a topic model.\n",
      "Topic modeling is often used as a first step to explore textual data in order to get a \n",
      "feel for the content of the text. This is especially true when abstracts/summaries  \n",
      "are unavailable, and when the text is too large to be manually analyzed in the \n",
      "available timeframe.\n",
      "Topic Discovery\n",
      "The main goal of topic modeling is to find a set of topics that can be used to classify \n",
      "a set of documents. These topics are implicit because we do not know what they are \n",
      "beforehand, and they are unnamed.Topic Discovery | 233\n",
      "The number of topics could vary from around 3 to, say, 400 (or even more) topics. \n",
      "Since it is the algorithm that discovers the topics, the number is generally fixed as \n",
      "an input to the algorithm, except in the case of non-parametric models in which \n",
      "the number of topics is inferred from the text. These topics may not always directly \n",
      "correspond to topics that a human would find meaningful. In practice, the number \n",
      "of topics should be much smaller than the number of documents. In general, the \n",
      "number of topics specified in a parametric model ought to be greater than or equal to \n",
      "the expected number of topics in the text. In other words, one should err on the side \n",
      "of a greater number of topics rather than fewer topics. This is because fewer topics \n",
      "can cause a problem for the interpretability of topics.  Also, the more documents \n",
      "that we provide, the better the algorithm can map the documents to non-mutually \n",
      "exclusive topics.\n",
      "The number of topics chosen depends on the documents and the objectives of the \n",
      "project. You may want to increase the number of topics if you have a large number \n",
      "of documents or if the documents are fairly diverse. Conversely, if you are analyzing \n",
      "a narrow set of documents, you may want to decrease the number of topics. This \n",
      "generally flows from your assumptions about the documents. If you think that \n",
      "the document set might inherently contain a large number of topics, you should \n",
      "configure the algorithm to look for a similar number of topics.\n",
      "Exploratory Data Analysis\n",
      "It is recommended to do exploratory data analysis prior to performing any machine \n",
      "learning project. This helps you learn about the probability distribution of the items \n",
      "in the dataset. We have seen this with word clouds in Chapter 2 , Feature Extraction \n",
      "Methods . Even better exploration is possible with topic modeling. Doing this can give \n",
      "you a sense of the statistical properties of the text dataset and how the documents \n",
      "can be grouped. \n",
      "For example, you might want to know whether the text dataset is skewed to any \n",
      "particular set of topics, or whether the sources are uniform or disparate. This data \n",
      "further allows us to choose the appropriate algorithms for the actual project. \n",
      "Transforming Unstructured Data to Structured Data\n",
      "Topic modeling clusters documents based on their topics. Specifically, it is a soft \n",
      "clustering method, as each document gets mapped to multiple topics. This is unlike \n",
      "hard clustering, which results in membership of an exemplar or a point of only \n",
      "one cluster. Topic models typically give a weight/probability of the document being \n",
      "associated with a topic.234 | Topic Modeling\n",
      "Thus, you can have a matrix of documents by topic, wherein the intersection of \n",
      "a document and a topic refers to the weight/probability that the document is \n",
      "associated with the topic. This matrix is effectively a numeric representation of the \n",
      "text and can be considered a way to transform unstructured text into structured \n",
      "data. Such a transformation is also an example of dimensionality reduction, as \n",
      "unstructured text can have many more dimensions (each dimension corresponds to \n",
      "a unique word) than the number of dimensions in structured data (each dimension \n",
      "corresponds to a topic).\n",
      "Bag of Words\n",
      "Before we explore modeling algorithms in depth, let's make a few simplifying \n",
      "assumptions. Firstly, we treat a document as a bag of words , meaning we ignore the \n",
      "structure and grammar of the document and just use the count of each word in the \n",
      "document to infer patterns in the variation of word counts. Ignoring the structure, \n",
      "sequences, and grammar allows us to use algorithms that rely on counts and \n",
      "probability to make the inferences.\n",
      "As we have seen previously, a bag of words is a dictionary containing each unique \n",
      "word and the integer count of the occurrences of the word in the document. Like all \n",
      "models, it is, at best, an approximation of reality. All the topic-modeling algorithms \n",
      "that we will discuss consider the text as a bag of words.\n",
      "Note\n",
      "We will look at approaches that explicitly model sequences in later chapters. \n",
      "The sequential structure of languages is different from the sequential \n",
      "structure in time-series data. Moreover, some aspects of the sequential \n",
      "structure may be specific to the natural language being considered. This will \n",
      "be discussed in more detail in Chapter 6 , Vector Representation .Topic-Modeling Algorithms | 235\n",
      "Topic-Modeling Algorithms\n",
      "Topic-modeling algorithms operate on the following assumptions:\n",
      "• Topics contain a set of words.\n",
      "• Documents are made up of a set of topics.\n",
      "Topics can be considered to be a weighted collection of words. After these common \n",
      "assumptions, different algorithms diverge in how they go about discovering topics. \n",
      "In the upcoming sections, we will cover in detail three topic-modeling algorithms—\n",
      "namely LSA, LDA, and HDP. Here, the term latent  (the L in these acronyms) refers to \n",
      "the fact that the probability distribution of the topics is not directly observable. We \n",
      "can observe the documents and the words but not the topics.\n",
      "Note\n",
      "The LDA algorithm builds on the LSA algorithm. In this case, similar \n",
      "acronyms are indicative of this association.\n",
      "Latent Semantic Analysis (LSA)\n",
      "We will start by looking at LSA. LSA actually predates the World Wide Web . It was \n",
      "first described in 1988. LSA is also known by an alternative name, Latent Semantic \n",
      "Indexing  (LSI), particularly when it is used for semantic searches of document \n",
      "indices. The goal of LSA is to uncover the latent topics that underlie documents  \n",
      "and words.236 | Topic Modeling\n",
      "LSA – How It Works\n",
      "Consider that we have a collection of documents, and these documents are  \n",
      "made up of words. Our goal is to discover the latent topics in the documents.  \n",
      "So, in the beginning, we have a collection of documents that we can represent as \n",
      "a term-to-document matrix. This term-to-document matrix has terms as rows and \n",
      "documents as columns. The following table gives a simplified illustration of a  \n",
      "term-to-document matrix:\n",
      "Figure 5.1: A simplified view of a term-to-document matrix\n",
      "Now, we break this matrix down into three separate matrix factors, namely a term-\n",
      "to-topics matrix, a topic-importance matrix, and a topic-to-documents matrix. Let's \n",
      "consider the matrix shown on the left-hand side and the corresponding factor \n",
      "matrices on the right-hand side:\n",
      "Figure 5.2: Document matrix and its broken matrices\n",
      "As we can see in this diagram, the rectangular matrix is separated into the product  \n",
      "of other matrices. The process takes a matrix, M, and splits it, as shown in the \n",
      "following formula:\n",
      "Figure 5.3: Splitting the matrix M\n",
      "Key Input Parameters for LSA Topic Modeling | 237\n",
      "The following are the broad definitions of the preceding equation:\n",
      "• M is an m×m matrix.\n",
      "• U is an m×n matrix.\n",
      "• Σ is an n×n diagonal matrix with non-negative real numbers.\n",
      "• V is an m×n matrix.\n",
      "• VT is an n×m matrix, which is the transpose of V.\n",
      "The matrices U and VT are not unique as matrix factorization does not give unique \n",
      "factors. This is analogous to the fact that the number 108 can be factorized using \n",
      "three factors in more than one way: 9x1x12, 27x1x4, 3x1x36, and so on. In order to \n",
      "consistently get similar factors, a regularization parameter can be used. Moreover, \n",
      "the multiplication of the factor matrices gives a matrix approximately and not exactly \n",
      "equal to the original matrix. Collectively there are fewer elements in the factor \n",
      "matrices than in the original matrix and this is possible because the original matrix \n",
      "had many elements that were zero or close to zero.\n",
      "The gensim  library is a popular Python library for topic modeling. It is easy to use and \n",
      "provides various topic-modeling model classes, including LdaModel  (for LDA) and \n",
      "LsiModel  (for LSI).\n",
      "The tomotopy library is also a powerful Python library for topic modeling. It too is \n",
      "easy to use and includes popular topic-modeling model classes, including HDPModel  \n",
      "(for HDP) and LDAModel  (for LDA).\n",
      "Other Python topic-modeling libraries include scikit-learn and lda (for LDA).\n",
      "Key Input Parameters for LSA Topic Modeling\n",
      "We will be using the gensim library to perform LSA topic modeling. The key input \n",
      "parameters for gensim are corpus , the number of topics, and id2word . Here, the \n",
      "corpus  is specified in the form of a list of documents in which each document is a \n",
      "list of tokens. The id2word  parameter refers to a dictionary that is used to convert \n",
      "the corpus from a textual representation to a numeric representation such that \n",
      "each word corresponds to a unique number. Let's do an exercise to understand this \n",
      "concept better.238 | Topic Modeling\n",
      "spaCy is a popular natural language processing Library for Python. In our exercises, \n",
      "we will be using spaCy to tokenize the text, lemmatize the tokens, and check which \n",
      "part-of-speech that token is. We will be using spaCy v2.1.3. After installing spaCy \n",
      "v2.1.3 we will need to download the English language model using the following code, \n",
      "so that we can load this model (since there are models for many different languages).\n",
      "python -m spacy download en_core_web_sm\n",
      "Exercise 5.01: Analyzing Wikipedia World Cup Articles with Latent Semantic \n",
      "Analysis\n",
      "In this exercise, you will perform topic modeling using LSA on a Wikipedia World Cup \n",
      "dataset. For this, you will make use of the LsiModel  class provided by the gensim \n",
      "library. You will use the Wikipedia library to fetch articles, the spaCy engine for the \n",
      "tokenization of the text, and the newline character to separate documents within  \n",
      "an article.\n",
      "Note\n",
      "The dataset used for this exercise can be found  \n",
      "at https://packt.live/30dbExO .\n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "from gensim import corpora\n",
      "from gensim.models import LsiModel\n",
      "from gensim.parsing.preprocessing import preprocess_stringKey Input Parameters for LSA Topic Modeling | 239\n",
      "3. To clean the text, define a function to remove the non-alphanumeric characters \n",
      "and replace numbers with the # character. Replace instances of multiple newline \n",
      "characters with a single newline character. Use the newline character to separate \n",
      "out the documents in the corpus. Insert a new cell and add the following code to \n",
      "implement this: \n",
      "import re\n",
      "HANDLE = '@\\w+'\n",
      "LINK = 'https?://t\\.co/\\w+'\n",
      "SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "PARA='\\n+'\n",
      "def clean(text):\n",
      "    text = re.sub(LINK, ' ', text)\n",
      "    text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "    text = re.sub(PARA, '\\n', text)\n",
      "    return text\n",
      "4. Insert a new cell and add the following code to find Wikipedia articles related to \n",
      "the World Cup:\n",
      "import wikipedia\n",
      "wikipedia.search('Cricket World Cup'),\\\n",
      "wikipedia.search('FIFA World Cup')240 | Topic Modeling\n",
      "The code generates the following output:\n",
      "Figure 5.4: Wikipedia articles related to the World Cup\n",
      "5. Insert a new cell and add the following code fetch the Wikipedia articles about \n",
      "the 2018 FIFA World Cup and the 2019 Cricket World Cup, concatenate them, and \n",
      "show the result:\n",
      "latest_soccer_cricket=['2018 FIFA World Cup',\\\n",
      "                       '2019 Cricket World Cup']\n",
      "corpus=''\n",
      "for cup in latest_soccer_cricket:\n",
      "    corpus=corpus+wikipedia.page(cup).content\n",
      "corpus\n",
      "Key Input Parameters for LSA Topic Modeling | 241\n",
      "The code generates the following output:\n",
      "Figure 5.5: Result after concatenating articles from 2018 and 2019\n",
      "6. Insert a new cell and add the following code to clean the text, using the spaCy \n",
      "English language model to tokenize the corpus and exclude all tokens that are \n",
      "not detected as nouns:\n",
      "text=clean(corpus)\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc=nlp(text)\n",
      "pos_list=['NOUN']\n",
      "preproc_text=[]\n",
      "preproc_sent=[]\n",
      "for token in doc:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\\n",
      "        and token.pos_ in pos_list:\n",
      "            preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        preproc_text.append(preproc_sent)\n",
      "        preproc_sent=[]\n",
      "#last sentence\n",
      "preproc_text.append(preproc_sent) \n",
      "print(preproc_text)\n",
      "242 | Topic Modeling\n",
      "The code generates the following output:\n",
      "Figure 5.6: Output after tokenizing the corpus\n",
      "7. Insert a new cell and add the following code to convert the corpus into a list in \n",
      "which each token corresponds to a number for more efficient representation, as \n",
      "gensim requires it in this form. Then, find the topics in the corpus:\n",
      "dictionary = corpora.Dictionary(preproc_text)\n",
      "corpus = [dictionary.doc2bow(text) for text in preproc_text]\n",
      "NUM_TOPICS=3\n",
      "lsamodel=LsiModel(corpus, num_topics=NUM_TOPICS, \\\n",
      "                  id2word = dictionary)\n",
      "lsamodel.print_topics()\n",
      "The code generates the following output:\n",
      "Figure 5.7: Topics in the corpus\n",
      "To create our LsiModel , we had to decide up front how many topics we \n",
      "wanted. This would not necessarily match the number of topics that are actually \n",
      "in the corpus.\n",
      "Note that, in the output, you can see that negative weights are associated with \n",
      "some words in a few topics. Also, the sum of the weights does not add up to \n",
      "one. The weights are not to be interpreted as probabilities. This makes it difficult \n",
      "to even mechanically view the topic as a probability distribution over words. \n",
      "Additionally, it may be observed that topic 0 is essentially about cricket even \n",
      "though the corpus includes both soccer and cricket. Topic 1 seems to be related \n",
      "to a sports broadcast. Topic 2 does not seem to be interpretable.\n",
      "Key Input Parameters for LSA Topic Modeling | 243\n",
      "8. To determine which topics have the highest weight for a document, insert a new \n",
      "cell and add the following code:\n",
      "model_arr = np.argmax(lsamodel.get_topics(),axis=0)\n",
      "y, x = np.histogram(model_arr, bins=np.arange(NUM_TOPICS+1))\n",
      "fig, ax = plt.subplots()\n",
      "plt.xticks(ticks=np.arange(NUM_TOPICS),\\\n",
      "           labels=np.arange(NUM_TOPICS+1))\n",
      "ax.plot(x[:-1], y)\n",
      "fig.show()\n",
      "The code generates the following output:\n",
      "Figure 5.8: Graph representing weight of topics for the documents\n",
      "244 | Topic Modeling\n",
      "We can see that topic 1 and topic 0 have the highest weight in almost all  \n",
      "the documents.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in \n",
      "both gensim and tomotopy. While setting a random_state  in gensim \n",
      "could help in reproducibility, in general, the topics found using tomotopy are \n",
      "superior from the perspective of interpretability. Generally, your output is \n",
      "expected to be different. In order to have exactly the same topic model, we \n",
      "can save and load topic models, which we'll do in Exercise 5.04 , Topics in \n",
      "The Life and Adventures of Robinson Crusoe by Daniel Defoe .\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2PdOCkd .\n",
      "You can also run this example online at https://packt.live/3jSS7uB .\n",
      "We have now performed topic modeling with the help of LSA. In the next section, \n",
      "we will learn about another topic-modeling algorithm: LDA. Before we move onto its \n",
      "implementation, let's quickly try and build a basic intuition about a couple of concepts \n",
      "that will help us in the subsequent sections. \n",
      "Dirichlet Process and Dirichlet Distribution \n",
      "A Dirichlet process is a distribution over a distribution. It can be represented as \n",
      "DP(α,G)  where G is the base distribution and α is the concentration parameter that \n",
      "defines how close DP(α,G)  is to the base distribution G. It is for this reason that the \n",
      "Dirichlet process is a versatile way to represent various probability distributions. It is \n",
      "used for the HDP topic-modeling algorithm.\n",
      "The Dirichlet distribution is a special case of the Dirichlet process, in which  \n",
      "the number of topics needs to be specified explicitly. It is used for the LDA  \n",
      "topic-modeling algorithm.Key Input Parameters for LSA Topic Modeling | 245\n",
      "Latent Dirichlet Allocation (LDA)\n",
      "Instead of using matrix factorization, like we did for LSA, it is possible to consider a \n",
      "generative model called LDA. LDA is considered an advancement over probabilistic \n",
      "LSA. Probabilistic LSA is prone to overfitting as it does not probabilistically model the \n",
      "distribution of the documents. LDA is a three-level hierarchical generative statistical \n",
      "model that maps documents to topics, which in turn get mapped to words—all in a \n",
      "probabilistic way. In this case, we have two concentration parameters corresponding \n",
      "to the document level and the topic level. \n",
      "LDA – How It Works\n",
      "To understand how LDA works, let's look at a simple example. We have four \n",
      "documents that contain only three unique words: Cat, Dog, and Hippo .  \n",
      "The following figure shows the documents and the number of times each  \n",
      "word is found in each document:\n",
      "Figure 5.9: Occurrence of words in different documents\n",
      "As we can see in the figure, the word Cat is found 10 times in Document 1  and \n",
      "Document 4  and 0 times in documents 2 and 3. Document 4  contains all three \n",
      "words 10 times each. For its analysis, LDA maintains two probability tables. The first \n",
      "table tracks the probability of selecting a specific word when sampling a specific topic. \n",
      "The second table keeps track of the probability of selecting a specific topic when \n",
      "sampling a particular document:\n",
      "Figure 5.10: Probability tables\n",
      "246 | Topic Modeling\n",
      "These probability tables reflect how likely it is to get a word if you sampled from each \n",
      "topic. If you sampled a word from topic 3 , it would likely be Cat (probability 99%). \n",
      "If you sampled Document 4 , then there is a one-third chance of getting each of the \n",
      "topics, since it contains all three t in equal proportions. In this example, a word is \n",
      "exclusive to a topic. In general, though, this is not the case.\n",
      "The gensim and the scikit-learn libraries use one way of implementing LDA \n",
      "(called variational inference). The tomotopy and lda  libraries use another way \n",
      "(called collapsed Gibbs sampling). It is essentially because of these differing \n",
      "implementations: when tomotopy is able to generate the topics in the available time, \n",
      "we usually prefer using tomotopy; otherwise we use gensim.\n",
      "The parameters that we use for tomotopy are as follows:\n",
      "• corpus : This refers to text that we want to analyze.\n",
      "• Number of topics: This is the number of topics that the corpus contains.\n",
      "• iter : This refers to the number of iterations that the model considers  \n",
      "the corpus.\n",
      "• α: This is associated with document generation.\n",
      "• η: This is associated with topic generation.\n",
      "• seed : This helps with fixing the initial randomization.\n",
      "Measuring the Predictive Power of a Generative Topic Model\n",
      "The predictive power of a generative topic model can be measured by analyzing \n",
      "the distribution of the generated corpus. Perplexity is a measure of how close the \n",
      "distribution of the words in the generated corpus is to reality. Log perplexity is a more \n",
      "convenient measure for this closeness. The formula for log perplexity is as follows:\n",
      "Figure 5.11: Formula for log perplexity\n",
      "Here, n is number of words and P(w) is the probability associated with word w. We can \n",
      "see that negative log likelihood is identical to log perplexity.\n",
      "Key Input Parameters for LSA Topic Modeling | 247\n",
      "Usually, a lower log perplexity means better performance. This is because the \n",
      "probability distribution of words is not uniform. It is concentrated on a small subset \n",
      "of words. And such a concentration (a non-uniform probability density function) \n",
      "causes a lower negative likelihood. In order to be sure that the model is generalizing \n",
      "well, the log likelihood should be computed on a hold-out sample. An extremely low \n",
      "negative log likelihood is indicative of an extremely low capacity of the model to learn. \n",
      "If a topic model has an unacceptable log perplexity on the corpus used for training \n",
      "then it is unlikely to perform well on a hold-out sample as it is indicative of the model \n",
      "having a low capacity to learn or it is indicative of the dataset not being generalizable. \n",
      "The negative log likelihood is approximately estimated in topic modeling libraries as it \n",
      "is intractable to calculate.\n",
      "Exercise 5.02: Finding Topics in Canadian Open Data Inventory Using the LDA \n",
      "Model\n",
      "In this exercise, we will use the tomotopy LDA model to analyze the Canadian Open \n",
      "Data Inventory. For simplicity, we will consider that the corpus has twenty topics.\n",
      "Note\n",
      "The dataset used for this exercise can be found  \n",
      "at https://packt.live/2PbvMds .\n",
      "The following steps will help you complete this exercise:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 800)\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline248 | Topic Modeling\n",
      "3. Insert a new cell and add the following code to read from a download of the \n",
      "Canadian Open Data Inventory, and clean the text:\n",
      "OPEN_DATA_URL = '../data/canada-open-data/inventory.csv'\n",
      "import re\n",
      "HANDLE = '@\\w+'\n",
      "LINK = 'https?://t\\.co/\\w+'\n",
      "SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "PARA='\\n+'\n",
      "def clean(text):\n",
      "    text = re.sub(LINK, ' ', text)\n",
      "    text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "    text = re.sub(PARA, '\\n', text)\n",
      "    return text\n",
      "catalog['description_en'].sample(frac=0.25,replace=False,\\\n",
      "                                 random_state=0).to_c \\\n",
      "                                 sv(OPEN_DATA_URL,\\\n",
      "                                 encoding='utf-8')\n",
      "file='../data/canada-open-data/catalog.txt'\n",
      "f=open(file,'r',encoding='utf-8')\n",
      "text=f.read()\n",
      "f.close()\n",
      "text = clean(text)\n",
      "4. Insert a new cell and add the following code to clean the text, using the spaCy \n",
      "English language model to tokenize the corpus and to exclude all tokens that are \n",
      "not detected as nouns:\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc=nlp(text)\n",
      "pos_list=['NOUN']\n",
      "preproc_text=[]\n",
      "preproc_sent=[]\n",
      "for token in doc:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\Key Input Parameters for LSA Topic Modeling | 249\n",
      "        and token.pos_ in pos_list:\n",
      "            preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        preproc_text.append(preproc_sent)\n",
      "        preproc_sent=[]\n",
      "#last sentence\n",
      "preproc_text.append(preproc_sent) \n",
      "print(preproc_text)\n",
      "The code generates the following output:\n",
      "Figure 5.12: Tokenized corpus after text preprocessing\n",
      "The pandas DataFrame was sampled. 25% of the dataset has been considered \n",
      "so that the memory restrictions related to spaCy can be addressed, since this is a \n",
      "fairly large sample.\n",
      "5. Insert a new cell and add the following code to see how the negative log \n",
      "likelihood varies by the number of iterations:\n",
      "import tomotopy as tp\n",
      "NUM_TOPICS=20\n",
      "mdl = tp.LDAModel(k=NUM_TOPICS,seed=1234)\n",
      "for line in preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "for i in range(0, 110, 10):\n",
      "    mdl.train(i)\n",
      "    print('Iteration: {}\\tLog-likelihood: {}'.\\\n",
      "          format(i, mdl.ll_per_word))\n",
      "250 | Topic Modeling\n",
      "The code generates the following output:\n",
      "Figure 5.13: Variation of negative log likelihood with different iterations\n",
      "6. Insert a new cell and add the following code to train a topic model with ten \n",
      "iterations and to show the inferred topics:\n",
      "mdl.train(10)\n",
      "for k in range(mdl.k):\n",
      "    print('Top 10 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=7))\n",
      "The code generates the following output:\n",
      "Top 10 words of topic #0\n",
      "[('polygon', 0.36050185561180115), ('dataset', 0.0334757782722234726), \n",
      "('information', 0.03004324994981289), ('soil', 0,029185116291046143), \n",
      "('area', 0,026610717177391052), ('surface', 0.025752583518624306), \n",
      "('map', 0.024036318063735962)]\n",
      "7. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if you consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in preproc_text for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "mdl.infer(doc_inst)[0]\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "Key Input Parameters for LSA Topic Modeling | 251\n",
      "The code generates the following output:\n",
      "array([11,17,14,19,12,  7,  4, 13, 10,  2,  3, 15,  1, 18, 16,  9,  \n",
      "0,\n",
      "        6,  8,  5], dtype=int64)\n",
      "8. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 11:\n",
      "print(mdl.get_topic_words(11, top_n=7))\n",
      "The code generates the following output\n",
      "[('table', 0.24849626421928406), ('census', 0.1265643984079361), \n",
      "('level', 0.06526772677898407), ('series', 0.06306280940771103), \n",
      "('topic', 0.062401335686445236), ('geography', 0.062401335686445236), \n",
      "('country', 0.06218084320425987)]\n",
      "9. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 17:\n",
      "print(mdl.get_topic_words(17, top_n=7))\n",
      "The code generates the following output:\n",
      "[('datum', 0.0603327676653862), ('information', 0.057247743010520935), \n",
      "('year', 0.03462424501776695), ('dataset', 0.03291034325957298), \n",
      "('project', 0.017828006289734993), ('website', 0.014057422056794167), \n",
      "('activity', 0.012000739574432373)]\n",
      "10. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 5:\n",
      "print(mdl.get_topic_words(5, top_n=7))\n",
      "The code generates the following output:\n",
      "[('survey', 0.04966237023472786), ('catch', 0.03862873837351799), \n",
      "('sponge', 0.0364220105111599), ('sea', 0.0342152863740921), \n",
      "('datum', 0.028698472306132317), ('fishing', 0.02759511023759842), \n",
      "('matter', 0.026491746306419373)]252 | Topic Modeling\n",
      "Topic 11, topic 17, and topic 5 seem to be interpretable. One could say that topic 11, \n",
      "topic 17, and topic 5 seem to be broadly about geographical data, internet data, and \n",
      "marine life data respectively.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in \n",
      "both gensim and tomotopy. While setting a random_state  in gensim \n",
      "could help in reproducibility, in general, the topics found using tomotopy are \n",
      "superior from the perspective of interpretability. Generally, your output is \n",
      "expected to be different. In order to have exactly the same topic model, we \n",
      "can save and load topic models; we do this in Exercise 5.04 , Topics in The \n",
      "Life and Adventures of Robinson Crusoe by Daniel Defoe .\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/33d0FGw .\n",
      "This section does not currently have an online interactive example and will \n",
      "need to be run locally.\n",
      "Activity 5.01: Topic-Modeling Jeopardy Questions\n",
      "Jeopardy is a popular TV show that covers a variety of topics. In this show, participants \n",
      "are given answers and then asked to frame questions. The purpose of this activity is \n",
      "to give a real-world feel to some of the complexity associated with topic modeling. In \n",
      "this activity, you will do topic modeling on a dataset of Jeopardy questions. \n",
      "Note\n",
      "The dataset to be used for this activity can be found  \n",
      "at https://packt.live/2PbvMds .\n",
      "Follow these steps to complete this activity: \n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and import pandas and other necessary libraries.\n",
      "3. Load the dataset into a pandas DataFrame.Hierarchical Dirichlet Process (HDP) | 253\n",
      "4. Clean the data by dropping the DataFrame rows where the Question  column \n",
      "has empty cells. \n",
      "5. Find the unique number of categories based on the Category  column.\n",
      "6. Randomly select 4% of the questions. Tokenize the text using spaCy. Select \n",
      "tokens that are nouns/verbs/adjectives or a combination.\n",
      "7. Train a tomotopy LDA model with 1,000 topics.\n",
      "8. Print the log perplexity.\n",
      "9. Find the probability distribution on the entire dataset.\n",
      "10. Sample a few topics and check for interpretability.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "Hierarchical Dirichlet Process (HDP)\n",
      "HDP is a non-parametric variant of LDA. It is called \"non-parametric\" since the \n",
      "number of topics is inferred from the data, and this parameter isn't provided  \n",
      "by us. This means that this parameter is learned and can increase (that is, it is \n",
      "theoretically unbounded). \n",
      "The tomotopy HDP implementation can infer between 1 and 32,767 topics. gensim's \n",
      "HDP implementation seems to fix the number of topics at 150 topics. For our \n",
      "purposes, we will be using the tomotopy HDP implementation.\n",
      "The gensim and the scikit-learn libraries use variational inference, while the tomotopy \n",
      "library uses collapsed Gibbs sampling. When the time required by collapsed Gibbs \n",
      "sampling is not an issue, then it is preferable to use collapsed Gibbs sampling over \n",
      "variational inference. In other cases, we may prefer to use variational inference. For \n",
      "the tomotopy library, the following parameters are used:\n",
      "iter : This refers to the number of iterations that the model considers the corpus.\n",
      "α: This concentration parameter is associated with document generation.\n",
      "η: This concentration parameter is associated with topic generation.254 | Topic Modeling\n",
      "seed : This fixes the initial randomization.\n",
      "min_cf : This helps eliminate those words that occur fewer times than the frequency \n",
      "specified by us.\n",
      "To get a better understanding of this, let's perform some simple exercises.\n",
      "Exercise 5.03: Topics in Around the World in Eighty Days\n",
      "In this exercise, we will make use of the tomotopy HDP model to analyze the text file \n",
      "for Jules Verne's Around the World in Eighty Days , available from the Gutenberg Project. \n",
      "We will use the min_cf  hyperparameter that is used to ignore words that occur \n",
      "fewer times than the specified frequency and discuss its impact on the interpretability \n",
      "of topics.\n",
      "Note\n",
      "The dataset used for this exercise can be found at https://packt.live/2Xdv4kt .\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 800)\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "3. Insert a new cell and add the following code to read from a download of the \n",
      "Gutenberg Project's Around the World in Eighty Days  by Jules Verne, and clean  \n",
      "the text:\n",
      "OPEN_DATA_URL = '../data/aroundtheworld/pg103.txt'\n",
      "f=open(OPEN_DATA_URL,'r',encoding='utf-8')\n",
      "text=f.read()\n",
      "f.close()\n",
      "import reHierarchical Dirichlet Process (HDP) | 255\n",
      "HANDLE = '@\\w+'\n",
      "LINK = 'https?://t\\.co/\\w+'\n",
      "SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "PARA='\\n+'\n",
      "def clean(text):\n",
      "    text = re.sub(LINK, ' ', text)\n",
      "    text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "    text = re.sub(PARA, '\\n', text)\n",
      "    return text\n",
      "text = clean(text)\n",
      "text\n",
      "The code generates the following output:\n",
      "Figure 5.14: Text from \"Around the World in Eighty Days\" \n",
      "4. Insert a new cell and add the following code to import the necessary libraries, \n",
      "clean the text (using the spaCy English language model to tokenize the corpus), \n",
      "and exclude all tokens that are not detected as nouns:\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc=nlp(text)\n",
      "pos_list=['NOUN']\n",
      "preproc_text=[]\n",
      "preproc_sent=[]\n",
      "for token in doc:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\\n",
      "        and token.pos_ in pos_list:\n",
      "            preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        preproc_text.append(preproc_sent)\n",
      "256 | Topic Modeling\n",
      "        preproc_sent=[]\n",
      "preproc_text.append(preproc_sent) #last sentence\n",
      "print(preproc_text)\n",
      "The code generates the following output:\n",
      "Figure 5.15: Tokenized corpus after the text is cleaned\n",
      "5. Insert a new cell and add the following code to create HDP models in which \n",
      "tokens that occur fewer than five times are ignored, and then show how the \n",
      "negative log likelihood varies according to the number of iterations:\n",
      "import tomotopy as tp\n",
      "mdl = tp.HDPModel(min_cf=5,seed=0)\n",
      "for line in preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "for i in range(0, 100, 10):\n",
      "    mdl.train(i)\n",
      "    print('Iteration: {}\\tLog-likelihood: {}'.\\\n",
      "          format(i, mdl.ll_per_word))\n",
      "for k in range(mdl.k):\n",
      "    print('Top 10 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=7))\n",
      "Hierarchical Dirichlet Process (HDP) | 257\n",
      "The code generates the following output:\n",
      "Figure 5.16: Variation of negative log likelihood with number of iterations\n",
      "6. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if you consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in preproc_text for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "The code generates the following output:\n",
      "Figure 5.17: Probability distribution of topics if the entire dataset is considered\n",
      "258 | Topic Modeling\n",
      "7. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 33:\n",
      "print(mdl.get_topic_words(33, top_n=7))\n",
      "The code generates the following output:\n",
      "[('danger', 0.1534954458475113), ('hour', 0.0015197568573057652), \n",
      "('time', 0.0015197568573057652), ('train', 0.0015197568573057652), \n",
      "('master', 0.0015197568573057652), ('man', 0.0015197568573057652), \n",
      "('steamer', 0.0015197568573057652)]\n",
      "8. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 21:\n",
      "print(mdl.get_topic_words(21, top_n=7))\n",
      "The code generates the following output:\n",
      "[('hour', 0.1344495415687561), ('minute', 0.1232500821352005), \n",
      "('day', 0.08405196666717529), ('quarter', 0.07285250723361969), \n",
      "('moment', 0.07285250723361969), ('clock', 0.005605331063270569), \n",
      "('card', 0.039254117757081985)]\n",
      "9. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 70:\n",
      "print(mdl.get_topic_words(70, top_n=7))\n",
      "The code generates the following output:\n",
      "[('event', 0.12901155650615692), ('midnight', 0.12901155650615692), \n",
      "('detective', 0.06482669711112976), ('bed', 0.06482669711112976), \n",
      "('traveller', 0.06482669711112976), ('watch', 0.06482669711112976), \n",
      "('clown', 0.06482669711112976)]\n",
      "10. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 4:\n",
      "print(mdl.get_topic_words(4, top_n=7))\n",
      "The code generates the following output:\n",
      "[('house', 0.20237493515014648), ('opium', 0.10131379961967468), \n",
      "('town', 0.07604850828647614), ('brick', 0.07604850828647614), \n",
      "('mansion', 0.07604850828647614), ('glimpse', 0.50783220678567886), \n",
      "('ball', 0.050783220678567886)]Hierarchical Dirichlet Process (HDP) | 259\n",
      "We can see that ignoring tokens that occur fewer than five times significantly \n",
      "improves the interpretability of the topic model. Also, we have 378 topics in all, many \n",
      "of which are not likely to be interpretable. So, what does this mean? Let's analyze a \n",
      "corpus from another classic and then return to these questions.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in both \n",
      "gensim and tomotopy. While setting a random_state  in gensim could \n",
      "help reproducibility,  the topics found using tomotopy are superior from \n",
      "the perspective of interpretability. Your output is expected to be different. \n",
      "In order to have exactly the same topic model, we can save and load \n",
      "topic models, which we'll do now in Exercise 5.04 , Topics in The Life and \n",
      "Adventures of Robinson Crusoe by Daniel Defoe .\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3jTxUVk .\n",
      "You can also run this example online at https://packt.live/2X8lG1p .\n",
      "Exercise 5.04: Topics in The Life and Adventures of Robinson Crusoe by Daniel \n",
      "Defoe\n",
      "In this exercise, we will make use of the tomotopy HDP model to analyze a text corpus \n",
      "taken from the text file for Daniel Defoe's The Life and Adventures of Robinson Crusoe , \n",
      "available on the Gutenberg Project website. Here, we will take the value of α as 0.8 \n",
      "and experiment with selecting tokens based on different combinations of parts of \n",
      "speech, before training the model.\n",
      "Note\n",
      "The dataset used for this exercise can be found at https://packt.live/3ffhfrP .\n",
      "1. Open a Jupyter Notebook.260 | Topic Modeling\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 800)\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "3. Insert a new cell and add the following code to read from a download of the \n",
      "Gutenberg Project's The Life and Adventures of Robinson Crusoe  by Daniel Defoe, \n",
      "and clean the text:\n",
      "OPEN_DATA_URL = '../data/robinsoncrusoe/521-0.txt'\n",
      "f=open(OPEN_DATA_URL,'r',encoding='utf-8')\n",
      "text=f.read()\n",
      "f.close()\n",
      "import re\n",
      "HANDLE = '@\\w+'\n",
      "LINK = 'https?://t\\.co/\\w+'\n",
      "SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "PARA='\\n+'\n",
      "def clean(text):\n",
      "    text = re.sub(LINK, ' ', text)\n",
      "    text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "    text = re.sub(PARA, '\\n', text)\n",
      "    return text\n",
      "text = clean(text)\n",
      "textHierarchical Dirichlet Process (HDP) | 261\n",
      "The code generates the following output:\n",
      "Figure 5.18: Text from The Life and Adventures of Robinson Crusoe\n",
      "4. Insert a new cell and add the following code to import the necessary libraries. \n",
      "Clean the text using the spaCy English language model to tokenize the corpus \n",
      "and to exclude all tokens that are not detected as nouns:\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "doc=nlp(text)\n",
      "\"\"\"\n",
      "We can experiment with other or a combinations of parts of speech \n",
      "['NOUN','ADJ','VERB','ADV'] #['NOUN','ADJ']\n",
      "\"\"\"\n",
      "pos_list=['NOUN'] \n",
      "preproc_text=[]\n",
      "preproc_sent=[]\n",
      "for token in doc:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\\n",
      "        and token.pos_ in pos_list:\n",
      "            preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        preproc_text.append(preproc_sent)\n",
      "        preproc_sent=[]\n",
      "preproc_text.append(preproc_sent) #last sentence\n",
      "print(preproc_text)\n",
      "262 | Topic Modeling\n",
      "The code generates the following output:\n",
      "Figure 5.19: Tokenized corpus after preprocessing is done\n",
      "5. Insert a new cell and add the following code to import the necessary libraries. \n",
      "Create an HDP model with the α concentration parameter as 0.8  and see how \n",
      "the negative log likelihood varies with the number of iterations:\n",
      "import tomotopy as tp \n",
      "mdl = tp.HDPModel(alpha=0.8,seed=0)\n",
      "for line in preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "for i in range(0, 110, 10):\n",
      "    mdl.train(i)\n",
      "    print('Iteration: {}\\tLog-likelihood: {}'.\\\n",
      "          format(i, mdl.ll_per_word))\n",
      "for k in range(mdl.k):\n",
      "    print('Top 10 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=7))\n",
      "Hierarchical Dirichlet Process (HDP) | 263\n",
      "The code generates the following output:\n",
      "Figure 5.20: Variation of negative log likelihood with the number of iterations\n",
      "6. Insert a new cell and add the following code to save the topic model:\n",
      "mdl.save('../data/robinsoncrusoe/hdp_model.bin')\n",
      "7. Insert a new cell and add the following code to load the topic model:\n",
      "mdl = tp.HDPModel.load('../data/robinsoncrusoe/'\\\n",
      "                       'hdp_model.bin')\n",
      "8. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if you consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in preproc_text for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "mdl.infer(doc_inst)[0]\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "264 | Topic Modeling\n",
      "The code generates the following output:\n",
      "Figure 5.21: Probability distribution if the entire corpus is considered\n",
      "9. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 163:\n",
      "print(mdl.get_topic_words(163, top_n=7))\n",
      "The code generates the following output:\n",
      "[('horse', 0.13098040223121643), ('way', 0.026405228301882744), \n",
      "('mankind', 0.26405228301882744), ('fire', 0.026405228301882744), \n",
      "('object', 0.026405228301882744), ('bridle', 0.026405228301882744), \n",
      "('distress', 0.026405228301882744)]\n",
      "10. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 103:\n",
      "print(mdl.get_topic_words(103, top_n=7))\n",
      "The code generates the following output:\n",
      "[('manor', 0.03706422075629234), ('inheritance', 0.03706422075629234), \n",
      "('lord', 0.03706422075629234), ('man', 0.0003669724682377309), \n",
      "('shore', 0.0003669724682377309), ('ship',0.0003669724682377309)]\n",
      "Hierarchical Dirichlet Process (HDP) | 265\n",
      "11. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 28:\n",
      "print(mdl.get_topic_words(28, top_n=7))\n",
      "The code generates the following output:\n",
      "[('thought', 0.07716038823127747), ('mind', 0.045609116554260254), \n",
      "('word', 0.038597721606492996), ('face', 0.03509202599525452), \n",
      "('terror', 0.03509202599525452), ('tear', 0.3158633038401604), \n",
      "('apprehension', 0.3158633038401604)]\n",
      "We see that we have 195 topics in all, many of which are likely not interpretable. \n",
      "In general, finding interpretable topics is difficult and connecting the words to \n",
      "interpret topics often requires familiarity with the domain. We have seen that  \n",
      "log perplexity has very limited utility. In the case of prior knowledge of the \n",
      "corpus, the topic model has a much smaller role to play in the discovery of the \n",
      "thematic structure.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in both \n",
      "gensim and tomotopy. While setting a random_state  in gensim could \n",
      "help reproducibility, in general, the topics found using tomotopy are superior \n",
      "from the perspective of interpretability. Generally, your output is expected \n",
      "to be different. In order to have exactly the same topic model, we can save \n",
      "and load topic models, and this was used in this exercise.\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3ggbfAn .\n",
      "This section does not currently have an online interactive example and will \n",
      "need to be run locally.\n",
      "We have explored three of the most popular approaches to topic modeling. Let's now \n",
      "discuss the practical challenges in using topic modeling and the state-of-the-art topic \n",
      "modeling technologies.266 | Topic Modeling\n",
      "Practical Challenges\n",
      "The selection of the number of topics and topic-modeling algorithms, the number of \n",
      "iterations, and the evaluation of the topic model are the main challenges faced by a \n",
      "practitioner. Having prior knowledge about the domain can greatly help in choosing \n",
      "the number of topics. In the absence of prior knowledge about the expected number \n",
      "of topics, we may need to rely on experimentation for the choice of the topic-\n",
      "modeling algorithm. The HDP model is an attractive choice when there isn't much \n",
      "information about the number of topics. In the case of a small corpus, the LSA model \n",
      "could be used.\n",
      "One factor that makes interpreting topics difficult is that they contain a lot of very \n",
      "frequently occurring (but indistinctive) words. To overcome this, we can iteratively \n",
      "identify these words and add them to a list of stop words. At times, we may want to \n",
      "filter out words that are too rare and/or too common. The use of only nouns, only \n",
      "verbs, or a combination of various parts of speech can improve the interpretability  \n",
      "of topics.\n",
      "Qualitative evaluation of the topics is essential. We may have to accept a mix of \n",
      "interpretable and non-interpretable topics in the real world. In the absence of human \n",
      "participants, we can use qualitative ways of considering word intrusion. Unless \n",
      "there is a downstream use of the topic model being developed, a complete lack of \n",
      "interpretability will render the topic model useless. When we have a downstream \n",
      "application, even non-interpretable topics are useful as they offer a convenient \n",
      "means to carry out dimensionality reduction on the dataset.\n",
      "State-of-the-Art Topic Modeling\n",
      "There is no known benchmark for quantitively identifying the state-of-the-art \n",
      "topic-modeling algorithm. It necessarily involves human participation whenever \n",
      "interpretable topics are required. In cases where the interpretation of topics is not \n",
      "necessary, the topic model needs to be evaluated by downstream tasks. A qualitative \n",
      "approach to interpreting topic models may be useful if there is prior knowledge or \n",
      "familiarity with the corpus.\n",
      "While there have been attempts at using labeled topic modeling, there is no evidence \n",
      "of these models broadly outperforming unsupervised topic-modeling algorithms. \n",
      "Interestingly, given that much of the topic modeling literature was published prior \n",
      "to 2014, this is not among the most active areas of research. This suggests that \n",
      "complete automation is hard and human participation is here to stay as the state-of-\n",
      "the-art technique in the near future.Hierarchical Dirichlet Process (HDP) | 267\n",
      "Activity 5.02: Comparing Different Topic Models\n",
      "The Consumer Financial Protection Bureau  (CFPB ) publishes consumer complaints \n",
      "made against organizations in the financial sector. This original dataset is available at \n",
      "https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data . \n",
      "In this activity, you will qualitatively compare how HDP and LDA models perform on \n",
      "the interpretability of topics by analyzing student loan complaints.\n",
      "Note\n",
      "The dataset to be used for this activity can be found  \n",
      "at https://packt.live/39GoyYe .\n",
      "Follow these steps to complete this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the pandas  library and load the dataset from a text file produced \n",
      "by partially processing the dataset from the CFPB website mentioned at the \n",
      "beginning of this section.\n",
      "3. Tokenize the text using spaCy. Select tokens that may be a part of speech (noun/\n",
      "verb/adjective or a combination).\n",
      "4. Train an HDP model.\n",
      "5. Save and load the HDP model. To save a topic model, use the following line  \n",
      "of code:\n",
      "mdl.save('../data/consumercomplaints/hdp_model.bin')\n",
      "To load a topic model, use the following:\n",
      "mdl = tp.HDPModel.load('../data/consumercomplaints/hdp_model.bin')\n",
      "6. Determine the topics in the entire set of complaints. Sample a few topics and \n",
      "check for interpretability.\n",
      "7. Repeat steps 3-8 for an LDA model instead of an HDP model. Consider the \n",
      "number of topics in the LDA model to around the number of topics found in the \n",
      "HDP model.268 | Topic Modeling\n",
      "8. Select the qualitatively better model from the HDP and LDA models trained in \n",
      "this activity. Also, compare these two models quantitatively.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "In this activity, we successfully compared two different models both qualitatively  \n",
      "and quantitatively.\n",
      "Summary\n",
      "In this chapter, we discussed topic modeling in detail. Without delving into advanced \n",
      "statistics, we reviewed various topic-modeling algorithms (such as LSA, LDA, and HDP) \n",
      "and how they can be used for topic modeling on a given dataset. We explored the \n",
      "challenges involved in topic modeling, how experimentation can help address those \n",
      "challenges, and, finally, broadly discussed the current state-of-the-art approaches to \n",
      "topic modeling.\n",
      "In the next chapter, we will learn about vector representation of text, which helps  \n",
      "us convert text into a numerical format to make it more easily understandable  \n",
      "by machines.Overview\n",
      "This chapter introduces you to the various ways in which text can be \n",
      "represented in the form of vectors. You will start by learning why this is \n",
      "important, and the different types of vector representation. You will then \n",
      "perform one-hot encoding on words, using the preprocessing  package \n",
      "provided by scikit-learn, and character-level encoding, both manually and \n",
      "using the powerful Keras library. After covering learned word embeddings \n",
      "and pre-trained embeddings, you will use Word2Vec  and Doc2Vec  for \n",
      "vector representation for Natural Language Processing  (NLP) tasks, \n",
      "such as finding the level of similarity between multiple texts.Vector Representation6272 | Vector Representation\n",
      "Introduction\n",
      "The previous chapters laid a firm foundation for NLP. But now we will go deeper into \n",
      "a key topic—one that gives us surprising insights into how language processing works \n",
      "and how some of the key advances in human-computer interaction are facilitated. \n",
      "At the heart of NLP is the simple trick of representing text as numbers. This helps \n",
      "software algorithms perform the sophisticated computations that are required to \n",
      "understand the meaning of the text.\n",
      "As we have already discussed in previous chapters, most machine learning algorithms \n",
      "take numeric data as input and do not understand the text as such. We need to \n",
      "represent our text in numeric form so that we can apply different machine learning \n",
      "algorithms and other NLP techniques to it. These numeric representations are called \n",
      "vectors and are also sometimes called word embeddings or simply embeddings.\n",
      "This chapter begins with a discussion of vectors, how text can be represented as \n",
      "vectors, and how vectors can be composed to represent complex speech. We will \n",
      "walk through the various representations in both directions—learning how to encode \n",
      "text as vectors as well as how to retrieve text from vectors. We will also look at some \n",
      "cutting-edge techniques used in NLP that are based on the idea of representing text \n",
      "as vectors.\n",
      "What Is a Vector?\n",
      "The basic mathematical definition of a vector is an object that has both magnitude \n",
      "and direction. In our definition, it is mostly compared with a scalar, which can \n",
      "be defined as an object that has only magnitude. Vectors are also defined as an \n",
      "element in vector space—for example, a point in space with the coordinates (x=4, \n",
      "y=5, z=6) is a vector. Here, we can see the vector dimensions are the geometric \n",
      "coordinates of a point or element in space. However, the vector dimensions can also \n",
      "represent any quantity or property of some element or object in addition to mere \n",
      "geometric coordinates.\n",
      "As an example, let's say that we're defining the weather at a given place using \n",
      "five features: temperature, humidity, precipitation, wind speed, and air pressure. \n",
      "The units that these would be measured in are Celsius, percentage, centimeters, \n",
      "kilometers per hour (km/h), and millibar (mbar), respectively. The following are the \n",
      "values for two places:What Is a Vector? | 273\n",
      "Figure 6.1: Weather indicators at two different places\n",
      "So, we can represent the weather of these places in vector form as follows:\n",
      "• Vector for place 1: [25, 50, 1, 18, 1200.0]\n",
      "• Vector for place 2: [32, 60, 0, 7, 1019.0]\n",
      "In the preceding representation, the first dimension represents temperature, the \n",
      "second dimension represents humidity, and so on. Note that the order of these \n",
      "dimensions should be consistent among all the vectors.\n",
      "Similarly, we can also represent text as a vector in which each dimension can \n",
      "represent either the presence or absence of certain metrics. Examples of these \n",
      "are bag of words and TFIDF vectors that we looked at in the previous chapters. \n",
      "There are other techniques as well for vector representation of text—learned word \n",
      "embeddings, for instance. We will discuss all these different techniques in the \n",
      "upcoming sections. These techniques can be broadly classified into two categories:\n",
      "• Frequency-based embeddings\n",
      "• Learned word embeddings\n",
      "Frequency-Based Embeddings\n",
      "Frequency-based embedding is a technique in which the text is represented in vector \n",
      "form by considering the frequency of the word in a corpus. The techniques that come \n",
      "under this category are the following:\n",
      "• Bag of words: As we've already seen in Chapter 2 , Feature Extraction Methods , \n",
      "bag of words is the technique of converting text into vector or numeric form by \n",
      "representing each sentence or document in a list the length of which is equal to \n",
      "the total number of unique words in the corpus.  \n",
      "• TFIDF: As seen previously in Chapter 2 , Feature Extraction Methods , this technique \n",
      "considers the frequency of a term as well as the inverse of its occurrence in  \n",
      "the corpus.\n",
      "274 | Vector Representation\n",
      "• Term frequency-based technique: This is a somewhat simpler version  \n",
      "of TFIDF. We represent each word in the vector by its number of occurrences  \n",
      "in the document. For example, let's say that a document contains the  \n",
      "following sentences:\n",
      "1. The girl is pretty, and the boy is handsome.\n",
      "2. Do whatever your heart says.\n",
      "3. The boy has a bike.\n",
      "4. His bike was red in color.\n",
      "Now let's build term frequency vectors of all these sentences. We will first create \n",
      "a dictionary  of unique words as follows. Note that we are considering every \n",
      "word in lowercase only:\n",
      "{1: the\n",
      "2: girl\n",
      "3: pretty\n",
      "4: and\n",
      "5: boy\n",
      "5: is\n",
      "7: handsome\n",
      "8: do\n",
      "9: whatever\n",
      "10: your\n",
      "11: heart\n",
      "12: says\n",
      "13: was\n",
      "14: has\n",
      "15: bike\n",
      "16: his\n",
      "17: redWhat Is a Vector? | 275\n",
      "18: in \n",
      "19: color\n",
      "}\n",
      "Now every document will be represented by a vector with 19 dimensions, where \n",
      "every dimension represents the frequency of a word in that document. So, for \n",
      "sentence 1, the vector will be [2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. \n",
      "Similarly, for sentence 2, the vector representation will be [0, 0, 0, 0, 0, 0, 0, 1, 1, \n",
      "1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], and so on. Note that the order needs to be consistent \n",
      "here, too.\n",
      "Note\n",
      "It is recommended that you use preprocessing techniques such as \n",
      "stemming, stop word removal, and conversion to lowercase before \n",
      "converting a text into the aforementioned vector format. Term frequency is \n",
      "a simple and quick technique for converting text into vector form. However , \n",
      "the TFIDF technique is a more effective technique than term frequency as it \n",
      "not only considers the frequency of a word in the current document but also \n",
      "in the background corpus.\n",
      "• One-hot encoding: In all techniques described previously, we have  \n",
      "represented a word with a single number. Using one-hot encoding, we can \n",
      "represent a word with an array. To understand this concept better, let's take the \n",
      "following sentences:\n",
      "5. I love cats and dogs.\n",
      "6. Cats are light in weight.\n",
      "We will use a dictionary  to assign a numeric label or index to each unique word \n",
      "(after converting to lowercase) as follows:\n",
      "{1: i\n",
      "2: love\n",
      "3: cats\n",
      "4: and\n",
      "5: dogs276 | Vector Representation\n",
      "6: are\n",
      "7: light\n",
      "8: in\n",
      "9: weight\n",
      "}\n",
      "Now we will represent each word in these sentences as follows:\n",
      "i            [1 0 0 0 0 0 0 0 0]\n",
      "love      [0 1 0 0 0 0 0 0 0]\n",
      "cats      [0 0 1 0 0 0 0 0 0]\n",
      "and       [0 0 0 1 0 0 0 0 0]\n",
      "dogs     [0 0 0 0 1 0 0 0 0]\n",
      "are       [0 0 0 0 0 1 0 0 0]\n",
      "light      [0 0 0 0 0 0 1 0 0]\n",
      "in          [0 0 0 0 0 0 0 1 0]\n",
      "weight  [0 0 0 0 0 0 0 0 1]\n",
      "We can see that each vector consists of 9 elements; that is, the number of elements \n",
      "equals the total number of words in the dictionary . For each word, the value of \n",
      "an element will be 1, only if the word is present at the corresponding position in \n",
      "the dictionary . When one-hot encoding words, you also need to consider the \n",
      "vocabulary . The meaning of vocabulary here is the total number of unique words in \n",
      "the text sources for your project. So, if you have a large source, then you will end up \n",
      "with a huge vocabulary and large one-hot vector sizes, which will eventually consume \n",
      "a lot of memory. The next exercise on word-level one-hot encoding will help us \n",
      "understand this better.\n",
      "Label encoding is a technique used to convert categorical data in numerical data, \n",
      "where each category is represented by a unique number. In order to perform \n",
      "label encoding and one-hot encoding, we will be using the LabelEncoder()  and \n",
      "OneHotEncoder()  classes from the preprocessing  package provided by the \n",
      "scikit-learn library. The following exercise will help us get a better understanding  \n",
      "of this.What Is a Vector? | 277\n",
      "Exercise 6.01: Word-Level One-Hot Encoding\n",
      "In this exercise, we will one-hot encode words with the help of the preprocessing  \n",
      "package provided by the scikit-learn library. For this, we shall make use of a file \n",
      "containing lines from Jane Austen's Pride and Prejudice .\n",
      "Note\n",
      "The text file used for this exercise can be found  \n",
      "at https://packt.live/3hUxNqQ .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. First, load the file containing the lines from the novel using the Path  class \n",
      "provided by the pathlib  library to specify the location of the file. Insert a new \n",
      "cell and add the following code:\n",
      "from pathlib import Path\n",
      "data = Path('../data')\n",
      "novel_lines_file = data / 'novel_lines.txt'\n",
      "3. Now that you have the file, open it and read its contents. Use the open()   \n",
      "and read()  functions to perform these actions. Store the results in the  \n",
      "novel_lines  file variable. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "with novel_lines_file.open() as f:\n",
      "    novel_lines_raw = f.read()\n",
      "4. After reading the contents of the file, load it by inserting a new cell and adding \n",
      "the following code:\n",
      "novel_lines_raw278 | Vector Representation\n",
      "The code generates the following output:\n",
      "Figure 6.2: Raw text from the file\n",
      "In the output, you will see a lot of newline characters. This is because we loaded \n",
      "the entire content at once into a single variable instead of separate lines. You will \n",
      "also see a lot of non-alphanumeric characters.\n",
      "5. The main objective is to create one-hot vectors for each word in the file. To do \n",
      "this, construct a vocabulary, which is the entire list of unique words in the file, by \n",
      "tokenizing the string into words and removing newlines and non-alphanumeric \n",
      "characters. Define a function named clean_tokenize()  to do this. Store \n",
      "the vocabulary created using clean_tokenize()  inside a variable named \n",
      "novel_lines . Add the following code:\n",
      "import string\n",
      "import re\n",
      "alpha_characters = str.maketrans('', '', string.punctuation)\n",
      "def clean_tokenize(text):\n",
      "    text = text.lower()\n",
      "    text = re.sub(r'\\n', '*** ', text)\n",
      "    text = text.translate(alpha_characters)\n",
      "What Is a Vector? | 279\n",
      "    text = re.sub(r' +', ' ', text)\n",
      "    return text.split(' ')\n",
      "novel_lines = clean_tokenize(novel_lines_raw)\n",
      "6. Take a look at the content inside novel_lines  now. It should look like a list. \n",
      "Insert a new cell and add the following code to view it:\n",
      "novel_lines\n",
      "The code generates the following output:\n",
      "Figure 6.3: Text after preprocessing is done\n",
      "7. Insert a new cell and add the following code to convert the list to a NumPy array \n",
      "and print the shape of the array: \n",
      "import numpy as np\n",
      "novel_lines_array = np.array([novel_lines])\n",
      "novel_lines_array = novel_lines_array.reshape(-1, 1)\n",
      "novel_lines_array.shape\n",
      "280 | Vector Representation\n",
      "The code generates the following output:\n",
      "(459, 1)\n",
      "As you can see, the novel_lines_array  array consists of 459  rows and 1 \n",
      "column. Each row is a word in the original novel_lines  file.\n",
      "Note\n",
      "NumPy arrays are more specific to NLP algorithms than Python lists. It is \n",
      "the format that is required for the scikit-learn library, which we will be using \n",
      "to one-hot encode words.\n",
      "8. Now use encoders, such as the LabelEncoder()  and OneHotEncoder()  \n",
      "classes from scikit-learn's preprocessing  package, to convert novel_\n",
      "lines_array  to one-hot encoded format. Insert a new cell and add the \n",
      "following lines of code to implement this: \n",
      "from sklearn import preprocessing\n",
      "labelEncoder = preprocessing.LabelEncoder()\n",
      "novel_lines_labels = labelEncoder.fit_transform(\\\n",
      "                     novel_lines_array)\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "wordOneHotEncoder = preprocessing.OneHotEncoder()\n",
      "line_onehot = wordOneHotEncoder.fit_transform(\\\n",
      "              novel_lines_labels.reshape(-1,1))\n",
      "In the code, the LabelEncoder()  class encodes the labels, and the fit_\n",
      "transform()  method fits the label encoder and returns the encoded labels. What Is a Vector? | 281\n",
      "9. To check the list of encoded labels, insert a new cell and add the following code:\n",
      "novel_lines_labels\n",
      "The preceding code generates output that looks as follows:\n",
      "Figure 6.4: List of encoded labels\n",
      "The OneHotEncoder()  class encodes the categorical integer features as a \n",
      "one-hot numeric array. The fit_transform()  method of this class takes \n",
      "the novel_lines_labels  array as input. This is a numeric array, and each \n",
      "feature included in this array is encoded using the one-hot encoding scheme.\n",
      "282 | Vector Representation\n",
      "10. Create a binary column for each category. A sparse matrix  is returned as \n",
      "output. To view the matrix, insert a new cell and type the following code:\n",
      "line_onehot\n",
      "The code generates the following output:\n",
      "<459x199 sparse matrix of type '<class 'numpy.float64'>'\n",
      "                With 459 stored elements in Compressed Sparse Row \n",
      "format>\n",
      "11. To convert the sparse matrix into a dense array , use the toarray()  function. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "line_onehot.toarray()\n",
      "The code generates the following output:\n",
      "Figure 6.5: Dense array\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2Xd2aAU .\n",
      "You can also run this example online at https://packt.live/39GSAeu .\n",
      "The preceding output shows that we have achieved our objective of one-hot  \n",
      "encoding words.\n",
      "One-hot encoding is mostly used in techniques such as language generation models, \n",
      "where a model is trained to predict the next word in the sequence given the words \n",
      "that precede it (think about your phone recommending words while you're chatting \n",
      "with your friends). Language models are used in many important natural language \n",
      "tasks nowadays, including machine translation, spell correction, text summarization, \n",
      "and in tools like Amazon Echo, Alexa, and more. \n",
      "What Is a Vector? | 283\n",
      "In addition to word-level language models, we can also build character-level \n",
      "language models, which can be trained to predict the next character in a sequence \n",
      "of characters. For character-level language models, we need character-level one-hot \n",
      "encoding. Let's explore this in the next section.\n",
      "Character-Level One-Hot Encoding\n",
      "In character-level one-hot encoding, we assign a numeric value to all the possible \n",
      "characters. We can use alpha-numeric characters and punctuation as well. Then, \n",
      "we represent each character by an array of size equal to all the characters in the \n",
      "document. This array contains zero at all the indices, other than the index assigned \n",
      "with the character. Let's explain this with an example. Consider the word \"hello\". Let's \n",
      "say our vocabulary contains only twenty-six characters, so our dictionary  will look \n",
      "like this:\n",
      "{'a': 0 \n",
      " 'b': 1\n",
      " 'c': 2\n",
      " 'd': 3 \n",
      " 'e': 4 \n",
      " 'f': 5 \n",
      "'g': 6\n",
      "'h': 7 \n",
      "'i': 8 \n",
      "'j': 9 \n",
      "'k': 10 \n",
      "…….'z': 25}\n",
      "Now, 'h' will be represented as [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. \n",
      "Similarly, 'e' can be represented as [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. \n",
      "Let's see how we can implement this in the next exercise.284 | Vector Representation\n",
      "Exercise 6.02: Character One-Hot Encoding – Manual\n",
      "In this exercise, we will create our own function that can one-hot encode the \n",
      "characters of the word \"data\". Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. To one-hot encode the characters of a given word, create a function named \n",
      "onehot_word() . Within this function, create a lookup  table for each of the \n",
      "characters in the given word. Then, map each character to an index. Add the \n",
      "following code to implement this:\n",
      "def onehot_word(word):\n",
      "    lookup = {v[1]: v[0] for v in enumerate(set(word))}\n",
      "    word_vector = []\n",
      "3. Next, loop through the characters in the word and create a vector named one_\n",
      "hot_vector  of the same size as the number of characters in the lookup. This \n",
      "vector is filled with zeros. Then, use the lookup  table to find the position of the \n",
      "character and set that character's value to 1. \n",
      "Note\n",
      " Execute the code for step 1  and step 2  together. \n",
      "Add the following code:\n",
      "    for c in word:\n",
      "        one_hot_vector = [0] * len(lookup)\n",
      "    \n",
      "        one_hot_vector[lookup[c]] = 1\n",
      "        word_vector.append(one_hot_vector)\n",
      "    return word_vector\n",
      "The function created earlier will return a word vector.\n",
      "4. Once the onehot_word()  function has been created, test it by adding some \n",
      "input as a parameter. Add the word \"data\" as an input to the function. To \n",
      "implement this, add a new cell and write the following code:\n",
      "onehot_vector = onehot_word('data')\n",
      "print(onehot_vector)What Is a Vector? | 285\n",
      "The code generates the following output:\n",
      "[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]\n",
      "Since there are four characters in the input ( data ), there will be four one-hot \n",
      "vectors. To determine the size of each one-hot vector for data , we enumerate \n",
      "the total number of characters in it. It is important to note that only one \n",
      "index gets assigned for repeated characters. After enumerating through the \n",
      "characters, the character d will be assigned index 0, the character a will be \n",
      "assigned index 1, and the character t will be assigned index 2. \n",
      "Based on each character's index position, the elements in each one-hot \n",
      "vector will be marked as 1, leaving other elements marked 0. In this way, \n",
      "we can manually one-hot encode any given text. Note that, in most practical \n",
      "applications, the size of one-hot encoded vector is equal to the size of all the \n",
      "characters, and sometimes, non-alphabetical characters are also considered.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/314aTX1 .\n",
      "You can also run this example online at https://packt.live/3gaWbE5 .\n",
      "We have learned how character-level one-hot encoding can be performed manually \n",
      "by developing our own function. We will focus on performing character-level one-hot \n",
      "encoding using Keras in the next exercise. Keras is a machine learning library that \n",
      "works along with TensorFlow to create deep learning models.\n",
      "We will be using the Tokenizer  class from Keras to create vectors from the text. \n",
      "Tokenizer  can work on both characters and words, depending on the char_\n",
      "level  argument. If char_level  is set to true , then it will work on the character \n",
      "level; otherwise, it will work on the word level. The Tokenizer  class comes with the \n",
      "following functions:\n",
      "• fit_on_text() : This method reads all the text and creates an internal \n",
      "dictionary , either word-wise or character-wise. We should always call it for \n",
      "the entire text, so that no word or character is left out of the dictionary. All the \n",
      "methods/variables listed after this should be called or used only after calling  \n",
      "this method.286 | Vector Representation\n",
      "• word_index : This is a dictionary  that contains all the possible words or \n",
      "characters in the vocabulary. Each word or character is assigned a unique \n",
      "number/index.\n",
      "• index_word : This is the reverse dictionary of word_index ; it contains \n",
      "key-value pairs with the index as the key and the word or character as its value.\n",
      "• texts_to_sequences() : This function converts each word or character \n",
      "sequence into its corresponding index value.\n",
      "• texts_to_matrix() : This converts each word or character in a given \n",
      "text into one-hot vector using a built-in dictionary. It takes the text as input, \n",
      "processes it, and returns a NumPy array of one-hot encoded vectors.\n",
      "Exercise 6.03: Character-Level One-Hot Encoding with Keras\n",
      "In this exercise, we will perform one-hot encoding on a given word using the Keras \n",
      "library. Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter notebook. \n",
      "2. Insert a new cell and the following code to import the necessary libraries:\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "import numpy as np\n",
      "3. Once you have imported the Tokenizer  class, create an instance of it by \n",
      "inserting a new cell and adding the following code:\n",
      "char_tokenizer = Tokenizer(char_level=True)\n",
      "Since you are encoding at the character level, in the constructor, char_level  is \n",
      "set to True .\n",
      "Note\n",
      "By default, char_level  is set to False  if we are encoding words.\n",
      "4. To test the Tokenizer  instance, you will require some text to work on. Insert a \n",
      "new cell and add the following code to assign a string to the text  variable:\n",
      "text = 'The quick brown fox jumped over the lazy dog'What Is a Vector? | 287\n",
      "5. After getting the text, use the fit_on_texts()  method provided by  \n",
      "the Tokenizer  class. Insert a new cell and add the following code to  \n",
      "implement this: \n",
      "char_tokenizer.fit_on_texts(text)\n",
      "In this code, char_tokenizer  will break text  into characters and internally \n",
      "keep track of the tokens, the indices, and everything else needed to perform \n",
      "one-hot encoding.\n",
      "6. Now, look at the possible output. One type of output is the sequence of the \n",
      "characters—that is, the integers assigned with each character in the text. The \n",
      "texts_to_sequences()  method of the Tokenizer  class helps assign \n",
      "integers to each character in the text. Insert a new cell and add the following \n",
      "code to implement this:\n",
      "seq =char_tokenizer.texts_to_sequences(text)\n",
      "seq\n",
      "The code generates the following output:\n",
      "Figure 6.6: List of integers assigned to each character\n",
      "288 | Vector Representation\n",
      "As you can see, there were 44 characters in the text  variable. From the output, \n",
      "we can see that for every unique character in text , an integer is assigned. \n",
      "7. Use sequences_to_texts()  to get text from the sequence with the \n",
      "following code:\n",
      "char_tokenizer.sequences_to_texts(seq)\n",
      "The snippet of the preceding output follows:\n",
      "Figure 6.7: Text generated from the sequence\n",
      "What Is a Vector? | 289\n",
      "8. Now look at the actual one-hot encoded values. For this, use the  \n",
      "texts_to_matrix()  function. Insert a new cell and add the following  \n",
      "code to implement this:\n",
      "char_vectors = char_tokenizer.texts_to_matrix(text)\n",
      "Here, the results of the array are stored in the char_vectors  variable. \n",
      "9. In order to view the vector values, just insert a new cell and add the  \n",
      "following line:\n",
      "char_vectors\n",
      "On execution, the code displays the array of one-hot encoded vectors:\n",
      "Figure 6.8: Actual one-hot encoded values for the given text\n",
      "10. In order to investigate the dimensions of the NumPy array, make use of the \n",
      "shape  attribute. Insert a new cell and add the following code to execute it:\n",
      "char_vectors.shape\n",
      "The following output is generated:\n",
      "(44, 27)\n",
      "So, char_vectors  is a NumPy array with 44 rows and 27 columns. This is \n",
      "because we are considering 26 characters and an additional character for space.\n",
      "11. To access the first row of char_vectors  NumPy array, insert a new cell and \n",
      "add the following code:\n",
      "char_vectors[0]\n",
      "This returns a one-hot vector, which can be seen in the following figure:\n",
      "array([0 ., 0., 0., 0., 1., 0., 0., 0., 0 .,\n",
      "       0., 0., 0., 0., 0., 0., 0.,0., 0 .,\n",
      "       0., 0., 0., 0., 0., 0., 0 ., 0., 0])\n",
      "290 | Vector Representation\n",
      "12. To access the index of this one-hot vector, use the argmax()  function provided \n",
      "by NumPy. Insert a new cell and write the following code to implement this:\n",
      "np.argmax(char_vectors[0])\n",
      "The code generates the following output:\n",
      "4\n",
      "13. The Tokenizer  class provides two dictionaries, index_word  and  \n",
      "word_index , which you can use to view the contents of Tokenizer  in \n",
      "key-value form. Insert a new cell and add the following code to view the  \n",
      "index_word  dictionary:\n",
      "char_tokenizer.index_word\n",
      "The code generates the following output:\n",
      "Figure 6.9: The index_word dictionary\n",
      "What Is a Vector? | 291\n",
      "As you can see in this figure, the indices act as keys, and the characters  \n",
      "act as values. Now insert a new cell and the following code to view the  \n",
      "word_index  dictionary:\n",
      "char_tokenizer.word_index\n",
      "The code generates the following output:\n",
      "Figure 6.10: The word_index dictionary\n",
      "In this figure, the characters act as keys, and the indices act as values.\n",
      "292 | Vector Representation\n",
      "14. In the preceding steps, you saw how to access the index of a given one-hot \n",
      "vector by using the argmax()  function provided by NumPy. Using this index \n",
      "as a key, you can access its value in the index_word  dictionary. To implement \n",
      "this, we insert a new cell and write the following code:\n",
      "char_tokenizer.index_word[np.argmax(char_vectors[0])]\n",
      "The preceding code generates the following output:\n",
      "'t'\n",
      "In this code, np.argmax(char_vectors[0])  produces an output of 4. This \n",
      "will act as a key in finding the value in the index_word  dictionary. So, when \n",
      "char_tokenizer.index_word[4]  is executed, it will scan through the \n",
      "dictionary and find that, for key 4, the value is t, and finally, it will print t.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2ECjNnf .\n",
      "You can also run this example online at https://packt.live/2P9c69V .\n",
      "In the preceding section, we learned how to convert text into one-hot vectors at either \n",
      "the character level or the word level. One-hot encoding is a simple representation \n",
      "of a word, but it has a disadvantage. Whenever the corpus is large (that is, when the \n",
      "number of unique characters or words increases), the size of the one-hot encoded \n",
      "vector also increases. Thus, it becomes very memory intensive and is sometimes not \n",
      "feasible; speed and simplicity here lead to the \"curse of dimensionality\" by creating a \n",
      "new dimension for each category/word. To tackle this problem, learned embeddings \n",
      "can be used, as explained in the following sections.What Is a Vector? | 293\n",
      "Learned Word Embeddings\n",
      "The vector representations discussed in the preceding section have some serious \n",
      "disadvantages, as discussed here:\n",
      "• Sparsity and large size : The sizes of one-hot encoded or other frequency-\n",
      "based vectors depend upon the number of unique words in the corpus. This \n",
      "means that when the size of the corpus increases, the number of unique words \n",
      "increases, thereby increasing the size of the vectors in turn.\n",
      "• Context : None of these vector representations consider the words with respect \n",
      "to its context while representing it as a vector. However, the meaning of a word \n",
      "in any language depends upon the context it is used in. Not taking the context \n",
      "into account can often lead to inaccurate results.\n",
      "Prediction-based word embeddings or learned word embeddings try to address \n",
      "both problems. For starters, these methods represent words with a fixed number \n",
      "of dimensions. Moreover, these representations are actually learned from the \n",
      "different contexts in which the word has been used at different places. Learned \n",
      "word embeddings  is actually a collective name given to a set of language models \n",
      "that represent words in such a way that words with similar meanings have somewhat \n",
      "similar representations. There are different techniques for creating learned word \n",
      "embeddings, such as Word2Vec  and GloVe. Let's discuss them one by one.\n",
      "Word2Vec\n",
      "Word2Vec  is a prediction-based algorithm that represents a word by a vector of a \n",
      "fixed size. This is a form of unsupervised learning algorithm, which means that we \n",
      "need not to provide manually annotated data; we just feed the raw text. It will train a \n",
      "model in such a way that each word is represented in terms of its context throughout \n",
      "the training data.\n",
      "This algorithm has two variations, as follows:\n",
      "• Continuous Bag of Words  (CBoW ): This model tends to predict the probability \n",
      "of a word given the context. The learning problem here is to predict the word \n",
      "given a fixed-window context—that is, a fixed set of continuous words in text.\n",
      "• Skip-Gram model : This model is the reverse of the CBoW model, as it tends to \n",
      "predict the context of a word.294 | Vector Representation\n",
      "These vectors find application in a lot of NLP tasks including text generation, machine \n",
      "translation, speech to text, text to speech, text classification, and text similarity. \n",
      "Let's explore how they can be used for text similarity. Suppose we generated 300 \n",
      "dimensional vectors from words such as \"love\", \"adorable\", and \"hate\". If we find the \n",
      "cosine similarity between the vectors for \"love\" and \"adorable\", and \"love\" and \"hate\", \n",
      "we will find a higher similarity between the former pair of words than the latter. \n",
      "In the next exercise, we will train word vectors using the gensim library. Specifically, \n",
      "we'll be using the Word2Vec  class. The Word2Vec  class has parameters such as \n",
      "documents , size , window , min_count , and workers . Here, documents  refers \n",
      "to the sentences that we have to provide to the class, size  represents the length \n",
      "of the dense vector to represent each token, min_count  represents the minimum \n",
      "count of words that can be taken into consideration when training a particular model, \n",
      "and workers  represents the number of threads that are required when training  \n",
      "a model. \n",
      "For training a model, we use the model.train()  method. This method takes \n",
      "arguments such as documents , total_examples , and epochs . Here, \n",
      "documents  represents the sentences, and total_examples  represents the count \n",
      "of sentences, while epochs  represents the total number of iterations over the given \n",
      "data. Finally, the trained word vectors get stored in model.wv , which is an instance \n",
      "of KeyedVectors .\n",
      "In order to perform basic text cleaning, before it's processed, we will make use of the \n",
      "textcleaner  class from gensim. Some of the most useful functions available in \n",
      "textcleaner  that we will be using are as follows:\n",
      "• split_sentences() : As the name suggests, this function splits the text and \n",
      "gets a list of sentences from the text.\n",
      "• simple_preprocess() : This function converts a document into a list \n",
      "consisting of lowercase tokens. \n",
      "Let's see how we can use these functions to create word vectors.\n",
      "Exercise 6.04: Training Word VectorsWhat Is a Vector? | 295\n",
      "In this exercise, we will train word vectors. We will be using books freely available \n",
      "on Project Gutenberg for this. We will also see the vector representation using \n",
      "Matplotlib's pyplot framework.\n",
      "Note\n",
      "The file we are using for this exercise can be found  \n",
      "at https://packt.live/39JeZYP .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Use the requests  library to load books from the Project Gutenberg website, \n",
      "the json  library to load a book catalog, and the regex  package to clean the text \n",
      "by removing newline characters. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "import requests\n",
      "import json\n",
      "import re\n",
      "3. After importing all the necessary libraries, load the json  file, which contains \n",
      "details of 10 books, including the title, the author, and the ID. Insert a new cell \n",
      "and add the following steps to implement this:\n",
      "with open('../data/ProjectGutenbergBooks.json', 'r') \\\n",
      "    as catalog_file:\n",
      "    catalog = json.load(catalog_file)296 | Vector Representation\n",
      "4. To print the details of all the books, insert a new cell and add the following code:\n",
      "catalog\n",
      "The preceding code generates the following output:\n",
      "Figure 6.11: Book details in the catalog\n",
      "5. Create a function named load_book() , which will take book_id  as a \n",
      "parameter and, based on that book_id , fetch the book and load it. It should \n",
      "also clean the text by removing the newline characters. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "GUTENBERG_URL ='https://www.gutenberg.org/files/{}/{}-0.txt'\n",
      "def load_book(book_id):\n",
      "    url = GUTENBERG_URL.format(book_id, book_id)\n",
      "    contents = requests.get(url).text\n",
      "    cleaned_contents = re.sub(r'\\r\\n', ' ', contents)\n",
      "    return cleaned_contents\n",
      "6. Once you have defined our load_book()  function, you will loop through the \n",
      "catalog, fetch all the id instances of the books, and store them in the book_ids  \n",
      "list. The id instances stored in the book_ids  list will act as parameters for our \n",
      "load_book()  function. The book information fetched for each book ID will be \n",
      "loaded in the books  variable. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "book_ids = [ book['id'] for book in catalog ]\n",
      "books = [ load_book(id) for id in book_ids]\n",
      "To view the information of the books  variable, add the following code in a  \n",
      "new cell:\n",
      "books[:5]\n",
      "What Is a Vector? | 297\n",
      "A snippet of the output generated by the preceding code is as follows:\n",
      "Figure 6.12: Information of various books\n",
      "7. Before you can train the word vectors, you need to split the books into a list \n",
      "of documents. In this case, you want to teach the Word2Vec  algorithm about \n",
      "words in the context of the sentences that they are in. So here, a document \n",
      "is actually a sentence. Thus, you need to create a list of sentences from all 10 \n",
      "books. Insert a new cell and add the following code to implement this: \n",
      "from gensim.summarization import textcleaner\n",
      "from gensim.utils import simple_preprocess\n",
      "def to_sentences(book):\n",
      "    sentences = textcleaner.split_sentences(book)\n",
      "    sentence_tokens = [simple_preprocess(sentence) \\\n",
      "                       for sentence in sentences]\n",
      "    return sentence_tokens\n",
      "In the preceding code, all the text preprocessing takes place inside the  \n",
      "to_sentences()  function that you have defined.\n",
      "298 | Vector Representation\n",
      "8. Now, loop through each book in books  and pass each book as a parameter to \n",
      "the to_sentences()  function. The results should be stored in the book_\n",
      "sentences  variable. Also, split books into sentences and sentences into \n",
      "documents. The result should be stored in the documents  variable. Insert a \n",
      "new cell and add the following code to implement this:\n",
      "books_sentences = [to_sentences(book) for book in books]\n",
      "documents = [sentence for book_sent in books_sentences \\\n",
      "             for sentence in book_sent]\n",
      "9. To check the length of the documents, use the len()  function as follows:\n",
      "len(documents)\n",
      "The code generates the following output:\n",
      "32922\n",
      "10. Now that you have your documents, train the model by making use of the \n",
      "Word2Vec  class provided by the gensim package. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "from gensim.models import Word2Vec\n",
      "# build vocabulary and train model\n",
      "model = Word2Vec(\n",
      "        documents,\n",
      "        size=100,\n",
      "        window=10,\n",
      "        min_count=2,\n",
      "        workers=10)\n",
      "model.train(documents, total_examples=len(documents), \\\n",
      "            epochs=50)\n",
      "The code generates the following output:\n",
      "(27809439, 37551450)\n",
      "Now make use of the most_similar()  function of the model.wv  instance to \n",
      "find the similar words. The most_similar()  function takes positive  as a \n",
      "parameter and returns a list of strings that contribute positively. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "model.wv.most_similar(positive=\"worse\")What Is a Vector? | 299\n",
      "The code generates the following output:\n",
      "Figure 6.13: Most similar words\n",
      "Note\n",
      "You may get a slightly different output as the output depends on the model \n",
      "training process, so you may have a different model than the one we have \n",
      "trained here.\n",
      "11. Create a show_vector()  function that will display the vector using pyplot , a \n",
      "plotting framework in Matplotlib. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "def show_vector(word):\n",
      "    vector = model.wv[word]\n",
      "    fig, ax = plt.subplots(1,1, figsize=(10, 2))\n",
      "    ax.tick_params(axis='both', \\\n",
      "                   which='both',\\\n",
      "                   left=False, \\\n",
      "                   bottom=False, \\\n",
      "                   top=False,\\\n",
      "                   labelleft=False, \\\n",
      "                   labelbottom=False)\n",
      "300 | Vector Representation\n",
      "    ax.grid(False)\n",
      "    print(word)\n",
      "    ax.bar(range(len(vector)), vector, 0.5)\n",
      "show_vector('sad')\n",
      "The code generates the following output:\n",
      "Figure 6.14: Graph of the vector when the input is \"sad\"\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/317nb11 .\n",
      "You can also run this example online at https://packt.live/2BJC40I .\n",
      "In the preceding figure, we can see the vector representation when the word \n",
      "provided to the show_vector()  function is \"sad\". We have learned about training \n",
      "word vectors and representing them using pyplot . In the next section, we will focus \n",
      "more on using pre-trained word vectors , which are required for NLP projects. \n",
      "What Is a Vector? | 301\n",
      "Using Pre-Trained Word Vectors\n",
      "For a machine learning model, the more data you have, the better the model you get. \n",
      "But training the model on large amounts of data is intensively resource-consuming \n",
      "in terms of both time and memory. So, we usually train a Word2Vec  model on a \n",
      "large amount of data and retain the model for future use. There are also a lot of \n",
      "pre-trained models publicly available have been trained on huge datasets such \n",
      "as Wikipedia articles. These models include gensim by fastText (research group \n",
      "by Facebook), and Word2Vec  has recently proved to be state-of-the-art for tasks \n",
      "including checking for word analogies and word similarities, as follows:\n",
      "• vector('Paris') - vector('France') + vector('Italy')  results in a vector that is very close \n",
      "to vector('Rome') .\n",
      "• vector('king') - vector('man') + vector('woman')  is close to vector('queen') .\n",
      "Google's publicly available glove model is similar to the Word2Vec  model and has \n",
      "produced incredible results. In some applications, we may need to train a Word2Vec  \n",
      "model on our own specific dataset rather than train a new model from scratch; that \n",
      "is, we can train a pre-trained model on more data. This process is called transfer \n",
      "learning. Transfer learning is based on the concept of transferring knowledge from \n",
      "one domain into another.\n",
      "Note\n",
      "Pre-trained word vectors can get pretty large. For example, vectors trained \n",
      "on Google News contain 3 million words, and on disk, its compressed size \n",
      "is 1.5 GB.\n",
      "To better understand how we can use pre-trained word vectors in Python, let's walk \n",
      "through a simple exercise.302 | Vector Representation\n",
      "Exercise 6.05: Using Pre-Trained Word Vectors\n",
      "In this exercise, we will load and use pre-trained word embeddings. We will also show \n",
      "the image representation of a few word vectors using the pyplot framework of the \n",
      "Matplotlib library. We will be using glove6B50d.txt , which is a pre-trained model.\n",
      "Note\n",
      "The pre-trained model being used for this file can be found  \n",
      "at https://www.kaggle.com/watts2/glove6b50dtxt/download .  \n",
      "Download this file and place it in the data  folder of Chapter 6, \n",
      "Vector Representation . \n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Add the following statement to import the numpy  library:\n",
      "import numpy as np\n",
      "import zipfile\n",
      "3. Move the downloaded model from the preceding link to the location given in the \n",
      "following code snippet. In order to extract data from a ZIP file, use the zipfile  \n",
      "Python package. Add the following code to unzip the embeddings from the  \n",
      "ZIP file:\n",
      "GLOVE_DIR = '../data/'\n",
      "GLOVE_ZIP = GLOVE_DIR + 'glove6B50d.txt.zip'\n",
      "print(GLOVE_ZIP)\n",
      "zip_ref = zipfile.ZipFile(GLOVE_ZIP, 'r')\n",
      "zip_ref.extractall(GLOVE_DIR)\n",
      "zip_ref.close()\n",
      "4. Define a function named load_glove_vectors()  to return a model Python \n",
      "dictionary. Insert a new cell and add the following code to implement this:\n",
      "def load_glove_vectors(fn):\n",
      "    print(\"Loading Glove Model\")\n",
      "    with open( fn,'r', encoding='utf8') as glove_vector_file:\n",
      "        model = {}\n",
      "        for line in glove_vector_file:What Is a Vector? | 303\n",
      "            parts = line.split()\n",
      "            word = parts[0]\n",
      "            embedding = np.array([float(val) \\\n",
      "                        for val in parts[1:]])\n",
      "            model[word] = embedding\n",
      "        print(\"Loaded {} words\".format(len(model)))\n",
      "    return model\n",
      "glove_vectors = load_glove_vectors(GLOVE_DIR +'glove6B50d.txt')\n",
      "Here, glove_vector_file  is a text file containing a dictionary. In this, words \n",
      "act as keys and vectors act as values. So, we need to read the file line by line, \n",
      "split it, and then map it to a Python dictionary. The preceding code generates the \n",
      "following output:\n",
      "Loading Glove Model\n",
      "Loaded 400000 words\n",
      "If we want to view the values of glove_vectors , then we insert a new cell and \n",
      "add the following code:\n",
      "glove_vectors\n",
      "You will get the following output:\n",
      "Figure 6.15: Dictionary of glove_vectors\n",
      "The order of the result dictionary can vary as it is a Python dict.\n",
      "304 | Vector Representation\n",
      "5. The glove_vectors  object is basically a dictionary containing the mappings \n",
      "of the words to the vectors, so you can access the vector for a word, which will \n",
      "return a 50-dimensional vector. Insert a new cell and add the code to check the \n",
      "vector for the word dog :\n",
      "glove_vectors[\"dog\"]\n",
      "Figure 6.16: Array of glove vectors with an input of dog\n",
      "In order to see the vector for the word cat , add the following code:\n",
      "glove_vectors[\"cat\"]\n",
      "Figure 6.17: Array of glove vectors with an input of cat\n",
      "What Is a Vector? | 305\n",
      "6. Now that you have the vectors, represent them as an image using the pyplot \n",
      "framework of the Matplotlib library. Insert a new cell and add the following code \n",
      "to implement this:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "def to_vector(glove_vectors, word):\n",
      "    vector = glove_vectors.get(word.lower())\n",
      "    if vector is None:\n",
      "        vector = [0] * 50\n",
      "    return vector\n",
      "def to_image(vector, word=''):\n",
      "    fig, ax = plt.subplots(1,1)\n",
      "    ax.tick_params(axis='both', which='both',\\\n",
      "                   left=False, \\\n",
      "                   bottom=False, \\\n",
      "                   top=False,\\\n",
      "                   labelleft=False,\\\n",
      "                   labelbottom=False)\n",
      "    ax.grid(False)\n",
      "    ax.bar(range(len(vector)), vector, 0.5)\n",
      "    ax.text(s=word, x=1, y=vector.max()+0.5)\n",
      "    return vector\n",
      "In the preceding code, you defined two functions. The to_vector()  function \n",
      "accepts glove_vectors  and word  as parameters. Here, the get()  function \n",
      "of glove_vectors  will find the word and convert it into lowercase. The result \n",
      "will be stored in the vector  variable.\n",
      "7. The to_image()  function takes vector  and word  as input and shows the \n",
      "image representation of vector . To find the image representation of the word \n",
      "man , type the following code: \n",
      "man = to_image(to_vector(glove_vectors, \"man\"))306 | Vector Representation\n",
      "The code generates the following output:\n",
      "Figure 6.18: Graph generated with an input of man\n",
      "8. To find the image representation of the word woman , type the following code:\n",
      "woman = to_image(to_vector(glove_vectors, \"woman\"))\n",
      "This will generate the following output:\n",
      "Figure 6.19: Graph generated with an input of woman\n",
      "What Is a Vector? | 307\n",
      "9. To find the image representation of the word king , type the following code:\n",
      "king = to_image(to_vector(glove_vectors, \"king\"))\n",
      "This will generate the following output:\n",
      "Figure 6.20: Graph generated with an input of king\n",
      "10. To find the image representation of the word queen , type the following code:\n",
      "queen = to_image(to_vector(glove_vectors, \"queen\"))\n",
      "This will generate the following output:\n",
      "Figure 6.21: Graph generated with an input of queen\n",
      "308 | Vector Representation\n",
      "11. To find the image representation of the vector for king – man + woman – \n",
      "queen , type the following code:\n",
      "diff = to_image(king – man + woman - queen)\n",
      "This will generate the following output:\n",
      "Figure 6.22: Graph generated with (king-man+woman-queen) as input\n",
      "12. To find the image representation of the vector for king – man + woman , \n",
      "type the following code:\n",
      "nd = to_image(king – man + woman)\n",
      "This will generate the following output:\n",
      "Figure 6.23: Graph generated with (king-man+woman) as input\n",
      "What Is a Vector? | 309\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/33btLpH .\n",
      "This section does not currently have an online interactive example, and will \n",
      "need to be run locally.\n",
      "The preceding results are the visual proof of the example we already discussed. \n",
      "We've learned how to load and use pre-trained word vectors and view their image \n",
      "representations. In the next section, we will focus on document vectors and  \n",
      "their uses.\n",
      "Document Vectors\n",
      "Word vectors and word embeddings represent words. But if we wanted to represent \n",
      "a whole document, we'd need to use document vectors. Note that when we refer to \n",
      "a document, we are referring to a collection of words that have some meaning to a \n",
      "user. A document can be a single sentence or a group of sentences. A document can \n",
      "consist of product reviews, tweets, or lines of movie dialogue, and can be from a few \n",
      "words to thousands of words. A document can be used in a machine learning project \n",
      "as an instance of something that the algorithm can learn from. We can represent a \n",
      "document with different techniques:\n",
      "• Calculating the mean value: We calculate the mean of all the constituent word \n",
      "vectors of a document and represent the document by the mean vector.\n",
      "• Doc2Vec : Doc2Vec  is a technique by which we represent documents by \n",
      "a fixed-length vector. It is trained quite similarly to the way we train the \n",
      "Word2Vec  model. Here, we also add the unique ID of the document to which \n",
      "the word belongs. Then, we can get the vector of the document from the trained \n",
      "model using the document ID.\n",
      "Similar to Word2Vec , the Doc2Vec  class contains parameters such as min_count , \n",
      "window , vector_size , sample , negative , and workers . The min_count  \n",
      "parameter ignores all the words with a frequency less than that specified. The \n",
      "window  parameter sets the maximum distance between the current and predicted \n",
      "words in the given sentence. The vector_size  parameter sets the dimensions of \n",
      "each vector. 310 | Vector Representation\n",
      "The sample  parameter defines the threshold that allows us to configure the higher-\n",
      "frequency words that are regularly down-sampled, while negative  specifies the \n",
      "total amount of noise words that should be drawn and workers  specifies the total \n",
      "number of threads required to train the model. To build the vocabulary from the \n",
      "sequence of sentences, Doc2Vec  provides the build_vocab  method. We'll be \n",
      "using all of these in the upcoming exercise.\n",
      "Uses of Document Vectors\n",
      "Some of the uses of document vectors are as follows:\n",
      "• Similarity : We can use document vectors to compare texts for similarity. For \n",
      "example, legal AI software can use document vectors to find similar legal cases.\n",
      "• Recommendations : For example, online magazines can recommend similar \n",
      "articles based on those that users have already read.\n",
      "• Predictions : Document vectors can be used as input into machine learning \n",
      "algorithms to build predictive models.\n",
      "In the next section, we will perform an exercise based on document vectors.\n",
      "Exercise 6.06: Converting News Headlines to Document Vectors\n",
      "In this exercise, we will convert some news headlines into document vectors. Also, we \n",
      "will look at the image representation of the vector. Again, for image representation, \n",
      "we will be using the pyplot framework of the Matplotlib library. Follow these steps to \n",
      "complete this exercise:\n",
      "Note\n",
      "The file which we are going to use in this exercise is in zipped format  \n",
      "and can be found at https://packt.live/3fhE2TG . It should be unzipped  \n",
      "once downloaded.\n",
      "1. Open a Jupyter notebook.\n",
      "2. Import all the necessary libraries for this exercise. You will be using the gensim \n",
      "library. Insert a new cell and add the following code:\n",
      "import pandas as pd\n",
      "from gensim import utils\n",
      "from gensim.models.doc2vec import TaggedDocumentWhat Is a Vector? | 311\n",
      "from gensim.models import Doc2Vec\n",
      "from gensim.parsing.preprocessing \\\n",
      "import preprocess_string, remove_stopwords\n",
      "import random\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "In the preceding code snippet, other than other imports, you imported \n",
      "TaggedDocument  from gensim, which prepares the document formats \n",
      "used in Doc2Vec . It represents the document along with the tag. This will be \n",
      "clearer from the following code lines. Doc2Vec  requires each instance to be a \n",
      "TaggedDocument  instance.\n",
      "3. Move the downloaded file to the following location and create a variable of the \n",
      "path as follows:\n",
      "sample_news_data = '../data/sample_news_data.txt'\n",
      "4. Now load the file:\n",
      "with open(sample_news_data, encoding=\"utf8\", \\\n",
      "          errors='ignore') as f:\n",
      "    news_lines = [line for line in f.readlines()]\n",
      "5. Now create a DataFrame out of the headlines as follows:\n",
      "lines_df = pd.DataFrame()\n",
      "indices  = list(range(len(news_lines)))\n",
      "lines_df['news'] = news_lines\n",
      "lines_df['index'] = indices\n",
      "6. View the head of the DataFrame using the following code:\n",
      "lines_df.head()312 | Vector Representation\n",
      "This will create the following output:\n",
      "Figure 6.24: Head of the DataFrame\n",
      "7. Create a class, the object of which will create the training instances for the \n",
      "Doc2Vec  model. Insert a new cell and add the following code to implement this:\n",
      "class DocumentDataset(object):\n",
      "    \n",
      "    def __init__(self, data:pd.DataFrame, column):\n",
      "        document = data[column].apply(self.preprocess)\n",
      "        self.documents = [ TaggedDocument( text, [index]) \\\n",
      "                          for index, text in \\\n",
      "                          document.iteritems() ]\n",
      "      \n",
      "    def preprocess(self, document):\n",
      "        return preprocess_string(\\\n",
      "            remove_stopwords(document))\n",
      "        \n",
      "    def __iter__(self):\n",
      "        for document in self.documents:\n",
      "            yield documents\n",
      "            \n",
      "    def tagged_documents(self, shuffle=False):\n",
      "        if shuffle:\n",
      "            random.shuffle(self.documents)\n",
      "        return self.documents\n",
      "What Is a Vector? | 313\n",
      "In the code, the preprocess_string()  function applies the given filters \n",
      "to the input. As its name suggests, the remove_stopwords()  function \n",
      "is used to remove stopwords  from the given document. Since Doc2Vec  \n",
      "requires each instance to be a TaggedDocument  instance, we create a list of \n",
      "TaggedDocument  instances for each headline in the file.\n",
      "8. Create an object of the DocumentDataset  class. It takes two parameters. One \n",
      "is the lines_df_small  DataFrame and the other is the Line  column name. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "documents_dataset = DocumentDataset(lines_df, 'news')\n",
      "9. Create a Doc2Vec  model using the Doc2Vec  class. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "docVecModel = Doc2Vec(min_count=1, window=5, vector_size=100, \\\n",
      "                      sample=1e-4, negative=5, workers=8)\n",
      "docVecModel.build_vocab(documents_dataset.tagged_documents())\n",
      "10. Now you need to train the model using the train()  function of the Doc2Vec  \n",
      "class. This could take a while, depending on how many records we train. Here, \n",
      "epochs  represents the total number of records required to train the document. \n",
      "Insert a new cell and add the following code to implement this: \n",
      "docVecModel.train(documents_dataset.\\\n",
      "                  tagged_documents(shuffle=True),\\\n",
      "                  total_examples = docVecModel.corpus_count,\\\n",
      "                  epochs=10)\n",
      "11. Save this model for future use as follows:\n",
      "docVecModel.save('../data/docVecModel.d2v')\n",
      "12. The model has been trained. To verify this, access one of the vectors with its \n",
      "index. To do this, insert a new cell and add the following code to find the doc  \n",
      "vector of index 657 :\n",
      "docVecModel[657]314 | Vector Representation\n",
      "You should get an output similar to the one below:\n",
      "Figure 6.25: Lines represented as vectors\n",
      "13. To check the image representation of any given vector, make use of the pyplot \n",
      "framework of the Matplotlib library. The show_news_lines()  function takes \n",
      "a line number as a parameter. Based on this line number, find the vector and \n",
      "store it in the doc_vector  variable. The show_image()  function takes two \n",
      "parameters, vector  and line , and displays an image representation of the \n",
      "vector. Insert a new cell and add the following code to implement this:\n",
      "import matplotlib.pyplot as plt\n",
      "def show_image(vector, line):\n",
      "    fig, ax = plt.subplots(1,1, figsize=(10, 2))\n",
      "    ax.tick_params(axis='both', \\\n",
      "                   which='both',\\\n",
      "                   left=False, \\\n",
      "                   bottom=False,\\\n",
      "                   top=False,\\\n",
      "                   labelleft=False,\\\n",
      "                   labelbottom=False)\n",
      "    ax.grid(False)\n",
      "    print(line)\n",
      "What Is a Vector? | 315\n",
      "    ax.bar(range(len(vector)), vector, 0.5)\n",
      "def show_news_lines(line_number):\n",
      "    line = lines_df[lines_df.index==line_number].news\n",
      "    doc_vector = docVecModel[line_number]\n",
      "    show_image(doc_vector, line)\n",
      "14. Now that you have defined the functions, implement the  \n",
      "show_news_lines()  function to view the image representation of  \n",
      "the vector. Insert a new cell and add the following code to implement this:\n",
      "show_news_lines(872)\n",
      "The code generates the following output:\n",
      "Figure 6.26: Image representation of a given vector\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/30dFxxV .\n",
      "You can also run this example online at https://packt.live/39MiTQG .\n",
      "We have learned how to represent a document as a vector. We have also seen a \n",
      "visual representation of this. In the next section, we will complete an activity to find \n",
      "similar news headlines using the document vector.\n",
      "316 | Vector Representation\n",
      "Activity 6.01: Finding Similar News Article Using Document Vectors\n",
      "To complete this activity, you need to build a news search engine that finds similar \n",
      "news articles like the one provided as input using the Doc2Vec  model. You will find \n",
      "headlines similar to \"US raise TV indecency US politicians are proposing a tough  \n",
      "new law aimed at cracking down on indecency.\" Follow these steps to complete  \n",
      "this activity:\n",
      "1. Open a Jupyter notebook and import the necessary libraries.\n",
      "2. Load the new article lines file.\n",
      "3. Iterate over each headline and split the columns and create a DataFrame.\n",
      "4. Load the Doc2Vec  model that you created in the previous exercise.\n",
      "5. Create a function that converts the sentences into vectors and another that does \n",
      "the similarity checks.\n",
      "6. Test both the functions.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "So, in this activity, we were able to find similar news headlines with the help of \n",
      "document vectors. A common use case of inferring text similarity from document \n",
      "vectors is in text paraphrasing, which we'll explore in detail in the next chapter.\n",
      "Summary\n",
      "In this chapter, we learned about the motivations behind converting human \n",
      "language in the form of text into vectors. This helps machine learning algorithms to \n",
      "execute mathematical functions on the text, detect patterns in language, and gain \n",
      "an understanding of the meaning of the text. We also saw different types of vector \n",
      "representation techniques, such as character-level encoding and one-hot encoding. \n",
      "In the next chapter, we will look at the areas of text paraphrasing, summarization, \n",
      "and generation. We will see how we can automate the process of text summarization \n",
      "using the NLP techniques we have learned so far.Overview\n",
      "This chapter begins with the concept of text generation using Markov \n",
      "chains, before moving on to two types of text summarization—namely , \n",
      "abstractive and extractive summarization. You will then explore the \n",
      "TextRank algorithm and use it with different datasets. By the end of this \n",
      "chapter, you will understand the applications and challenges of text \n",
      "generation and summarization using Natural Language Processing  \n",
      "(NLP) approaches.Text Generation and \n",
      "Summarization7320 | Text Generation and Summarization\n",
      "Introduction\n",
      "The ability to express thoughts in words (sentence generation), the ability to replace \n",
      "a piece of text with different but equivalent text (paraphrasing), and the ability to find \n",
      "the most important parts of a piece of text (summarization) are all key elements of \n",
      "using language. Although sentence generation, paraphrasing, and summarization are \n",
      "challenging tasks in NLP, there have been great strides recently that have made them \n",
      "considerably more accessible. In this chapter, we explore them in detail and see how \n",
      "we can implement them in Python.\n",
      "Generating Text with Markov Chains\n",
      "An idea is expressed using the words of a language. As ideas are not tangible, it is \n",
      "useful to look at text generation in order to gauge whether a machine can think \n",
      "on its own. The utility of text generation is currently limited to an auto-complete \n",
      "functionality, besides a few negative use cases that we will discuss later in this \n",
      "section. Text can be generated in many different ways, which we will explore using \n",
      "Markov chains. Whether this generated text can correspond to a coherent line of \n",
      "thought is something that we will address later in this section.\n",
      "Markov Chains\n",
      "A state space defines all possible states that can exist. A Markov chain consists of a \n",
      "state space and a specific type of successor function. For example, in the case of the \n",
      "simplified state space to describe the weather, the states could be Sunny, Cloudy, or \n",
      "Rainy. The successor function describes how a system in its current state can move \n",
      "to a different state or even continue in the same state. To better understand this, \n",
      "consider the following diagram:Generating Text with Markov Chains | 321\n",
      "Figure 7.1: Markov chain for weather\n",
      "The successor function of a Markov chain is a random selection of a successor \n",
      "state based on probabilities. For instance, consider that the initial state is randomly \n",
      "selected as Rainy. The next state could be Rainy (there is a 0.8 probability that the \n",
      "state stays Rainy). Then, the next state could be Sunny (there is a 0.05 probability \n",
      "associated with this transition). It could be Rainy again, and then it could be Cloudy, \n",
      "and so on. Our sequence of states is Rainy-Rainy-Sunny-Rainy-Cloudy . For each \n",
      "state, the successor state is found by a random selection; this is called a random walk \n",
      "on the Markov chain.\n",
      "322 | Text Generation and Summarization\n",
      "Similarly, if we have a state space in which the states correspond to a vocabulary, \n",
      "then a random walk on such a Markov chain will generate text. Now, the vocabulary \n",
      "could have around 20,000 words. In this case, the Markov chain will have 20,000 \n",
      "states. The probabilities in this case will correspond to the likelihood of a word \n",
      "succeeding a given word. We can begin with any state randomly drawn from among \n",
      "the words that could be used for the first word of a sentence, for example, common \n",
      "words such as \"the, \" \"a, \" \"I, \" \"he, \" \"she, \" \"if, \" \"this, \" \"why, \" and \"where\". We \n",
      "then find its successor state in a random way, followed by the next successor state \n",
      "found in a random way, and continue in the same manner until we have generated a \n",
      "sequence of words of the required length. In the next section, we will do an exercise \n",
      "related to Markov chains to get a better understanding of them.\n",
      "Exercise 7.01: Text Generation Using a Random Walk over a Markov Chain\n",
      "In this exercise, we will generate text with the help of Markov chains. We will use \n",
      "Robert Frost's collection of poems, North of Boston , available from Project Gutenberg, \n",
      "to specify the successor state(s) for each state using a dictionary. We'll use a list to \n",
      "specify the successor state(s) for any state so that the number of times a successor \n",
      "state occurs in that list is directly proportional to the probability of transitioning to \n",
      "that successor state. \n",
      "Then, we will generate 10 phrases with three words in addition to an initial word, and \n",
      "then generate another 10 phrases with four words in addition to an initial word. The \n",
      "initial state or initial word will be randomly selected from among these words: \"the,\" \n",
      "\"a,\" \"I,\" \"he,\" \"she,\" \"if,\" \"this,\" \"why,\" and \"where.\" Note that since we are generating \n",
      "text using a random walk over a Markov chain, in general, the output you get will be \n",
      "different from the output shown in this exercise. Each different output corresponds \n",
      "to new text generation.\n",
      "Note\n",
      "You can find the text file that's been used for this exercise  \n",
      "at https://packt.live/2DiGAE3 .Generating Text with Markov Chains | 323\n",
      "Follow these steps to complete this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries \n",
      "and read the dataset:\n",
      "import re\n",
      "import random\n",
      "OPEN_DATA_URL = '../data/robertfrost/pg3026.txt'\n",
      "f=open(OPEN_DATA_URL,'r',encoding='utf-8')\n",
      "text=f.read()\n",
      "f.close()\n",
      "3. Insert a new cell and add the following code to preprocess the text using \n",
      "regular expressions:\n",
      "HANDLE = '@\\w+\\n'\n",
      "LINK = 'https?://t\\.co/\\w+'\n",
      "SPECIAL_CHARS = '&lt;|&lt;|&amp;|#'\n",
      "PARA='\\n+'\n",
      "def clean(text):\n",
      "    #text = re.sub(HANDLE, ' ', text)\n",
      "    text = re.sub(LINK, ' ', text)\n",
      "    text = re.sub(SPECIAL_CHARS, ' ', text)\n",
      "    text = re.sub(PARA, '\\n', text)\n",
      "    return text\n",
      "text = clean(text)\n",
      "4. Split the corpus into a list of words. Show the number of words in the corpus:\n",
      "corpus=text.split()\n",
      "corpus_length=len(corpus)\n",
      "corpus_length\n",
      "The preceding code generates the following output:\n",
      "19985324 | Text Generation and Summarization\n",
      "5. Insert a new cell and add the following code to define the successor states for \n",
      "each state. Use a dictionary for this:\n",
      "succ_func={}\n",
      "corpus_counter=0\n",
      "for token in corpus:\n",
      "    corpus_counter=corpus_counter+1\n",
      "    if corpus_counter<corpus_length:\n",
      "        if token not in succ_func.keys():\n",
      "            succ_func[token]=[corpus[corpus_counter]]\n",
      "        else:\n",
      "            succ_func[token].append(corpus[corpus_counter])\n",
      "succ_func\n",
      "The preceding code generates an output as follows. Note that we're only \n",
      "displaying a part of the output here.\n",
      "Figure 7.2: Dictionary of successor states\n",
      "We find that \"he\" is shown as a successor of \"who\" more than once. This is \n",
      "because this occurs more than once in the dataset. In effect, the number \n",
      "of times the successors occur in the list is proportional to their respective \n",
      "probabilities. Though it is not the only method, this is a convenient way to \n",
      "represent the successor function.\n",
      "6. Define the list of initial states. Then, define a function to select a random initial \n",
      "state from these and concatenate it with successor states. These successor \n",
      "states are randomly selected from the list containing successor states for a \n",
      "specific current state. Add the following code to do this:\n",
      "initial_states=['The','A','I','He','She','If',\\\n",
      "                'This','Why','Where']\n",
      "def generate_words(k=5):\n",
      "Generating Text with Markov Chains | 325\n",
      "    initial_state=random.choice(initial_states)\n",
      "    current_state=initial_state\n",
      "    text=current_state+' '\n",
      "    for i in range(k):\n",
      "        succ_state=random.choice(succ_func[current_state])\n",
      "        text=text+succ_state+' '\n",
      "        current_state=succ_state\n",
      "    print(text.split('.')[0])\n",
      "7. Insert a new cell and add the following code to generate text containing 10 \n",
      "phrases of four words (including the initial word) and 10 phrases of five words \n",
      "(including the initial word):\n",
      "for k in range(3,5):\n",
      "    for j in range(10):\n",
      "        generate_words(k)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.3: Phrases generated, consisting of four and five words\n",
      "326 | Text Generation and Summarization\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/313fiJY .\n",
      "You can also run this example online at https://packt.live/33ilO2l .\n",
      "It's quite interesting that we are able to generate text using a random walk over a \n",
      "Markov chain. If we look more closely, we will see that only a few of the phrases make \n",
      "sense. Broadly speaking, we are generating text that has an element of Robert Frost's \n",
      "style. However, it can hardly be said to correspond to a thought of any kind.\n",
      "The practical utility of generating text using a Markov chain is somewhat limited \n",
      "to generating spam (spam generators could use a Markov chain) and generating \n",
      "something that is a little amusing. Nevertheless, this exercise demonstrates the \n",
      "surprising results we can get by using a simple approach in which nothing about the \n",
      "structure of a language is explicitly taught to the machine.\n",
      "In general, auto-complete is one positive use case and arguably the sole positive use \n",
      "case for text generation given that other use cases (besides spam) tend to include the \n",
      "generation of misinformation.\n",
      "Paraphrasing involves replacing some text with different text that has the same \n",
      "meaning. Now, intuitively, a machine will be able to tell whether one piece of text is \n",
      "a paraphrase of another, but only if that machine understands the meaning. So, one \n",
      "way of checking whether a machine understands the meaning of a piece of text is to \n",
      "check if it can tell if another different piece of text is a paraphrase of that first text.\n",
      "Benchmark datasets provide a standard touchstone for evaluating approaches \n",
      "to solve a problem. The approaches are typically ranked in a publicly available \n",
      "leaderboard. Even in the case of such benchmark datasets, as of February 21, \n",
      "2020, the SuperGLUE leaderboard ( https://super.gluebenchmark.com/leaderboard ) sets \n",
      "human baselines at the top when considered across a variety of tasks. This means \n",
      "that humans are superior at paraphrasing than the most sophisticated approaches \n",
      "even on the specified datasets. Paraphrasing is even tougher outside of benchmark \n",
      "datasets because it is tougher to teach models in a more general way so that the \n",
      "model is as effective for other datasets. Thus, compared to machines, humans can \n",
      "paraphrase even better on other datasets than machines can. In short, paraphrasing \n",
      "using NLP is challenging and is currently of limited practical utility to the practitioner. \n",
      "In the next section, we will learn about summarization.Text Summarization | 327\n",
      "Text Summarization\n",
      "Automated text summarization is the process of using NLP tools to produce concise \n",
      "versions of text that preserve the key information present in the original content. \n",
      "Good summaries can communicate the content with less text by retaining the key \n",
      "information while filtering out other information and noise (or useless text, if any). \n",
      "A shorter text may often take less time to read, and thus summarization facilitates \n",
      "more efficient use of time.\n",
      "The type of summarization that we are typically taught in school is abstractive \n",
      "summarization. One way to think of this is to consider abstractive summarization as \n",
      "a combination of understanding the meaning and expressing it in fewer sentences. \n",
      "It is usually considered as a supervised learning problem as the original text and \n",
      "the summary are both required. However, a piece of text can be summarized in \n",
      "more than one way. This makes it hard to teach the machine in a general way. While \n",
      "abstractive summarization is an active area of research, it is, for the time being, not at \n",
      "a stage that will be of interest to the practitioner.\n",
      "There is another form of summarization, called extractive summarization, in which \n",
      "parts of the text are extracted to form a summary. There is no paraphrasing in this \n",
      "form of summarization. This second type will be the focus of the remainder of this \n",
      "section. We will look at the TextRank algorithm, which is an unsupervised machine \n",
      "learning method. For simplicity, we will focus on single-document summarization in \n",
      "this chapter. To implement this, we will be using the gensim library.\n",
      "TextRank\n",
      "TextRank is a graph-based algorithm (developed by Rada Mihalcea and Paul Tarau) \n",
      "used to find the key sentences in a piece of text. As we already know, in graph  \n",
      "theory, a graph has nodes and edges. In the TextRank algorithm, we estimate the \n",
      "importance of each sentence and create a summary with the sentences that have the \n",
      "highest importance.328 | Text Generation and Summarization\n",
      "The TextRank algorithm works as follows:\n",
      "1. Represent a unit of text (say, a sentence) as a node.\n",
      "2. Each node is given an arbitrary importance score.\n",
      "3. Each edge has a weight that corresponds to the similarity between two nodes \n",
      "(for instance, the sentences Sx and Sy). The weight could be the number of \n",
      "common words (say, wk) in the two sentences divided by the sum of the number \n",
      "of words in the two sentences. This can be represented as follows:\n",
      "Figure 7.4: Formula for similarity between two sentences\n",
      "4. For each node, we compute a new importance score, which is a function of the \n",
      "importance score of the neighboring nodes and the edge weights ( wji) between \n",
      "them. Specifically, the function ( f) could be the edge-weighted average score of \n",
      "all the neighboring nodes that are directed toward that node that is adjusted \n",
      "by all the outward edge weights ( wjk) and the damping factor ( d). This can be \n",
      "represented as follows:\n",
      "Figure 7.5: Formula for importance score\n",
      "d=0.85 is typically used as the damping factor. While we have used a directed \n",
      "graph here, an undirected graph could also be used with a TextRank algorithm.\n",
      "5. We repeat the preceding step until the importance score varies by less than a \n",
      "pre-defined tolerance level in two consecutive iterations.\n",
      "6. Sort the nodes in decreasing order of the importance scores.\n",
      "7. The top n nodes give us a summary.\n",
      "Key Input Parameters for TextRank | 329\n",
      "The number of iterations required for convergence depends on the number of nodes \n",
      "and the connectedness among the nodes. The number of iterations required for an \n",
      "undirected graph is expected to be higher than the number of iterations required \n",
      "for a directed graph since the edges don't have a direction in the case of the former. \n",
      "We typically use a directed graph in the TextRank algorithm. In general, around 20-40 \n",
      "iterations may be required for convergence. We can drop edges that have less than \n",
      "a certain threshold weight for faster convergence since they won't have much of an \n",
      "impact on the result anyway. The basic concept underpinning the TextRank algorithm \n",
      "is that key parts of a document are connected to form a coherent summary.\n",
      "Key Input Parameters for TextRank\n",
      "We'll be using the gensim library to implement TextRank. The following are the \n",
      "parameters required for this:\n",
      "• text : This is the input text.\n",
      "• ratio : This is the required ratio of the number of sentences in the summary to \n",
      "the number of sentences in the input text.\n",
      "The gensim implementation of the TextRank algorithm uses BM25—a probabilistic \n",
      "variation of TF-IDF—for similarity computation in place of the similarity measure \n",
      "described in step 3  of the algorithm. This will be clearer in the following exercise, in \n",
      "which you will summarize text using TextRank.\n",
      "Exercise 7.02: Performing Summarization Using TextRank\n",
      "In this exercise, we will use the classic short story, After Twenty Years  by O. Henry, \n",
      "which is available on Project Gutenberg, and the first section of the Wikipedia article \n",
      "on Oscar Wilde. We will summarize each text separately so that we have 20% of the \n",
      "sentences in the original text and then have 25% of the sentences in the original text \n",
      "using the gensim implementation of the TextRank algorithm. In all, we shall extract \n",
      "and print four summaries.\n",
      "In addition to these libraries, you will need to import the following:\n",
      "from gensim.summarization import summarize\n",
      "summarize(text,ratio=0.20)330 | Text Generation and Summarization\n",
      "In the preceding code snippet, ratio=0.20  means that 20% of the sentences from \n",
      "the original text will be used to create the summary.\n",
      "Note\n",
      "The text corpus for O. Henry's short story, After Twenty Years, being used in \n",
      "this exercise can be found at https://packt.live/33atvr0 .\n",
      "The Oscar Wilde section from the Wikipedia article can be found  \n",
      "at https://packt.live/3fhEocY .\n",
      "Complete the following steps to implement this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries \n",
      "and extract the required text from After Twenty Years :\n",
      "from gensim.summarization import summarize\n",
      "import wikipedia\n",
      "import re\n",
      "file_url_after_twenty=r'../data/ohenry/pg2776.txt'\n",
      "with open(file_url_after_twenty, 'r') as f:\n",
      "        contents = f.read()\n",
      "start_string='AFTER TWENTY YEARS\\n\\n\\n'\n",
      "end_string='\\n\\n\\n\\n\\n\\nLOST ON DRESS PARADE'\n",
      "text_after_twenty=contents[contents.find(start_string):\\\n",
      "                           contents.find(end_string)]\n",
      "text_after_twenty=text_after_twenty.replace('\\n',' ')\n",
      "text_after_twenty=re.sub(r\"\\s+\",\" \",text_after_twenty)\n",
      "text_after_twentyKey Input Parameters for TextRank | 331\n",
      "The preceding code generates the following output:\n",
      "Figure 7.6: Text from After Twenty Years\n",
      "3. Add the following code to extract the required text and print the summarized \n",
      "text, with the ratio  parameter set to 0.2 :\n",
      "summary_text_after_twenty=summarize(text_after_twenty, \\\n",
      "                                    ratio=0.2)\n",
      "print(summary_text_after_twenty)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.7: Summarized text when the ratio parameter is 0.2\n",
      "4. Insert a new cell and add the following code to summarize the text and print the \n",
      "summarized text, with the ratio  parameter set to 0.25 :\n",
      "summary_text_after_twenty=summarize(text_after_twenty, \\\n",
      "                                    ratio=0.25)\n",
      "print(summary_text_after_twenty)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.8: Summarized text when the ratio parameter is 0.25\n",
      "332 | Text Generation and Summarization\n",
      "5. Insert a new cell and add the following code to extract the required text from the \n",
      "Wikipedia page for Oscar Wilde:\n",
      "#text_wiki_oscarwilde=wikipedia.summary(\"Oscar Wilde\")\n",
      "file_url_wiki_oscarwilde=r'../data/oscarwilde/'\\\n",
      "                          'ow_wikipedia_sum.txt'\n",
      "with open(file_url_wiki_oscarwilde, 'r', \\\n",
      "          encoding='latin-1') as f:\n",
      "        text_wiki_oscarwilde = f.read()\n",
      "text_wiki_oscarwilde=text_wiki_oscarwilde.replace('\\n',' ')\n",
      "text_wiki_oscarwilde=re.sub(r\"\\s+\",\" \",text_wiki_oscarwilde)\n",
      "text_wiki_oscarwilde\n",
      "The preceding code generates the following output:\n",
      "Figure 7.9: Text from the Wikipedia page for Oscar Wilde\n",
      "6. Insert a new cell and add the following code to summarize the text and print the \n",
      "summarized text using ratio=0.2 :\n",
      "summary_wiki_oscarwilde=summarize(text_wiki_oscarwilde, \\\n",
      "                                  ratio=0.2)\n",
      "print(summary_wiki_oscarwilde)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.10: Summarized text when the ratio parameter is 0.2\n",
      "7. Add the following code to summarize the text and print the summarized text \n",
      "using ratio=0.25 :\n",
      "summary_wiki_oscarwilde=summarize(text_wiki_oscarwilde, \\\n",
      "                                  ratio=0.25)\n",
      "print(summary_wiki_oscarwilde)\n",
      "Key Input Parameters for TextRank | 333\n",
      "The preceding code generates the following output:\n",
      "Figure 7.11: Summarized text when the ratio is 0.25\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3i5sNQn .\n",
      "You can also run this example online at https://packt.live/39G0Knx .\n",
      "We find that the summary for the Wikipedia article is much more coherent than the \n",
      "short story. We can also see that the summary with a ratio  of 0.20  is a subset of \n",
      "a summary with a ratio  of 0.25 . Would extractive summarization work better for \n",
      "a children's fairytale than it does for an O. Henry short story? Let's explore this in the \n",
      "next exercise.\n",
      "Exercise 7.03: Summarizing a Children's Fairy Tale Using TextRank \n",
      "In this exercise, we consider the fairy tale Little Red Riding Hood  in two variations  \n",
      "for the input texts. The first variation is from Children's Hour with Red Riding Hood  \n",
      "and Other Stories , edited by Watty Piper, while the second variation is from The  \n",
      "Fairy Tales of Charles Perrault , both of which are available on Project Gutenberg's \n",
      "website. The aim of this exercise is to explore how TextRank (gensim) performs on \n",
      "this summarization.\n",
      "Note\n",
      "You can find the text from the Watty Piper variation  \n",
      "at https://packt.live/2Xd30xy . The text from the Charles Perrault  \n",
      "version can be found at https://packt.live/30g5ZHy .\n",
      "334 | Text Generation and Summarization\n",
      "Complete the following steps to implement this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Insert a new cell and add the following code to import the required libraries:\n",
      "from gensim.summarization import summarize\n",
      "import re\n",
      "3. Insert a new cell and add the following code to fetch Watty Piper's version of \n",
      "Little Red Riding Hood :\n",
      "file_url_grimms=r'../data/littleredrh/pg11592.txt'\n",
      "with open(file_url_grimms, 'r') as f:\n",
      "        contents_grimms = f.read()\n",
      "start_string_grimms='LITTLE RED RIDING HOOD\\n\\n\\n'\n",
      "end_string_grimms='\\n\\n\\n\\n\\nTHE GOOSE-GIRL'\n",
      "text_grimms=contents_grimms[contents_grimms.find(\\\n",
      "                            start_string_grimms):\\\n",
      "                            contents_grimms.find(\\\n",
      "                            end_string_grimms)]\n",
      "text_grimms=text_grimms.replace('\\n',' ')\n",
      "text_grimms=re.sub(r\"\\s+\",\" \",text_grimms)\n",
      "text_grimms\n",
      "The preceding code generates the following output:\n",
      "Figure 7.12: Text from the Watty Piper variation of Little Red Riding Hood\n",
      "4. Insert a new cell, add the following code, and fetch the Perrault fairy tale version \n",
      "of Little Red Riding Hood :\n",
      "file_url_perrault=r'../data/littleredrh/pg29021.txt'\n",
      "with open(file_url_perrault, 'r') as f:\n",
      "        contents_perrault = f.read()\n",
      "start_string_perrault='Little Red Riding-Hood\\n\\n'\n",
      "end_string_perrault='\\n\\n_The Moral_'\n",
      "text_perrault=contents_perrault[contents_perrault.find(\\\n",
      "                                start_string_perrault):\\\n",
      "Key Input Parameters for TextRank | 335\n",
      "                                contents_perrault.find(\\\n",
      "                                end_string_perrault)]\n",
      "text_perrault=text_perrault.replace('\\n',' ')\n",
      "text_perrault=re.sub(r\"\\s+\",\" \",text_perrault)\n",
      "text_perrault\n",
      "The preceding code generates the following output:\n",
      "Figure 7.13: Tales from the Perrault version of Little Red Riding Hood\n",
      "5. Insert a new cell and add the following code to generate the two summaries with \n",
      "a ratio  of 0.20 :\n",
      "llrh_grimms_textrank=summarize(text_grimms,ratio=0.20)\n",
      "llrh_perrault_textrank=summarize(text_perrault,ratio=0.20)\n",
      "6. Insert a new cell and add the following code to print the TextRank summary \n",
      "(ratio  of 0.20 ) of Grimm's version of Little Red Riding Hood :\n",
      "print(llrh_grimms_textrank)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.14: Output after implementing TextRank on the Watty Piper variation\n",
      "7. Insert a new cell and add the following code to print the TextRank summary \n",
      "(ratio  of 0.20 ) of Perrault's version of Little Red Riding Hood :\n",
      "print(llrh_perrault_textrank)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.15: Output after implementing TextRank on the Perrault version\n",
      "336 | Text Generation and Summarization\n",
      "8. Add the following code to generate two summaries with a ratio  of 0.5 :\n",
      "llrh_grimms_textrank=summarize(text_grimms,ratio=0.5)\n",
      "llrh_perrault_textrank=summarize(text_perrault,ratio=0.5)\n",
      "9. Add the following code to print a TextRank summary ( ratio  of 0.5 ) of Piper's \n",
      "version of Little Red Riding Hood :\n",
      "print(llrh_grimms_textrank)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.16: Output after implementing TextRank on the Watty Piper variation\n",
      "10. Add the following code to print a TextRank summary ( ratio  of 0.5 ) of \n",
      "Perrault's version of Little Red Riding Hood :\n",
      "print(llrh_perrault_textrank)\n",
      "The preceding code generates the following output:\n",
      "Figure 7.17: Output after implementing TextRank on the Perrault version\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3i5sRzB .\n",
      "You can also run this example online at https://packt.live/2XfObu1 .\n",
      "With this, we've found that the four summaries lack coherency and are also \n",
      "incomplete. This is also true of the two summaries with a ratio  of 0.5 —that \n",
      "is, even when half of the sentences are extracted for the summary. This might be \n",
      "because the conversations in the fairytale are contextual in nature, as a sentence \n",
      "often refers to the preceding sentence(s). This contextual aspect of language makes \n",
      "NLP complex for machines.\n",
      "Key Input Parameters for TextRank | 337\n",
      "Interestingly, extractive summarization works much better for an O. Henry short story \n",
      "such as After Twenty Years  than it does for a children's fairytale such as Little Red Riding \n",
      "Hood . Furthermore, this is not specific to the language used by a specific author, as \n",
      "we have explored with two different variations of this fairytale. It seems a fairytale is \n",
      "unsuitable for extractive summarization. Lets now do an activity in which we'll use the \n",
      "TextRank algorithm to summarize complaints that customers have written against \n",
      "some organizations.\n",
      "Activity 7.01: Summarizing Complaints in the Consumer Financial Protection \n",
      "Bureau Dataset\n",
      "The Consumer Financial Protection Bureau publishes consumer complaints made \n",
      "against organizations in the financial sector. This original dataset is available at https://\n",
      "www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data . To \n",
      "complete this activity, you will summarize a few complaints using TextRank. \n",
      "Note\n",
      "You can find the dataset to be used for this activity at \n",
      "https://www.dropbox.com/sh/qmq3x3ah1cf3ecz/AAAg_\n",
      "E6f0I5vdaB4WVmR6TCga?dl=0&preview=Consumer_Complaints.csv .  \n",
      "To complete the activity, you will need to place the .csv  file into the  \n",
      "data  folder for this chapter in your local directory.\n",
      "Follow these steps to implement this activity:\n",
      "1. Import the summarization libraries and instantiate the summarization model.\n",
      "2. Load the dataset from a .csv  file into a pandas DataFrame. Drop all columns \n",
      "other than Product , Sub-product , Issue , Sub-issue , and Consumer \n",
      "complaint narrative .\n",
      "3. Select 12 complaints corresponding to the rows 242830 , 1086741 , 536367 , \n",
      "957355 , 975181 , 483530 , 950006 , 865088 , 681842 , 536367 , 132345 , and \n",
      "285894  from the 300,000 odd complaints with a narrative. Note that since the \n",
      "dataset is an evolving dataset, the use of a version that's different from the one \n",
      "in the data  folder could give different results because the input texts could  \n",
      "be different.338 | Text Generation and Summarization\n",
      "4. Add a column with the TextRank summary. Each element of this column \n",
      "corresponds to a summary, using TextRank, of the complaint narrative in the \n",
      "corresponding column. Use a ratio  of 0.20 . Also, use a try-except  clause \n",
      "since the gensim implementation of the TextRank algorithm throws exceptions \n",
      "with summaries that have very few sentences.\n",
      "5. Show the DataFrame. You should get an output similar to the following figure:\n",
      "Figure 7.18: DataFrame showing the summarized complaints\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "Recent Developments in Text Generation and Summarization\n",
      "Alan Turing (for whom the equivalent of the Nobel Prize in Computer Science is \n",
      "named) proposed a test for artificial intelligence in 1950. This test, known as the \n",
      "Turing Test, says that if humans ask questions and cannot distinguish between text \n",
      "responses generated by a machine and a human, then that machine can be deemed \n",
      "to be intelligent. \n",
      "Recent Developments in Text Generation and Summarization | 339\n",
      "Text generation using very large models, such as the GPT-2 (with around 1.5 billion \n",
      "parameters) and BERT  (Bidirectional Encoder Representation from Transformers ) \n",
      "(with around 340 million parameters), can aid in auto-completion tasks. Auto-\n",
      "completion presents unique ethical challenges. While it can offer convenience, it \n",
      "can also reinforce biases in the data. This is accentuated by the fact that most user \n",
      "experience layouts can show only a limited number of options. Furthermore, auto-\n",
      "completion can controversially suggest responses that are different from what the \n",
      "sender originally wants to type.\n",
      "Unfortunately, most use cases for text generation are negative use cases for \n",
      "generating spam and misinformation. Given that the Turing Test may not be passed \n",
      "any time soon, we are clearly nowhere near considering text generation as a proxy \n",
      "for thought within a machine and there is no widely accepted benchmark for  \n",
      "text generation.\n",
      "Since late 2018, with the invention of self-attention, transformers, and BERT, these \n",
      "approaches are generally considered the best way to teach a machine about some \n",
      "of the most challenging NLP tasks. Self-attention is a technique in which a word \n",
      "is combined with other words in its neighborhood by matrix multiplications. Such \n",
      "multiplications are possible because of vector representations of words. Using such \n",
      "combined representations for all the words in a sentence allows us to represent a \n",
      "sentence in a way that captures context. This allows us to build much larger models \n",
      "that have a significantly higher capacity to learn. A transformer is a combination of \n",
      "attention units and includes position information for each word, that is, multiple self-\n",
      "attention layers and position information are used to capture the context better. \n",
      "BERT is a transformer that learns the sequential structure of a text in both directions, \n",
      "that is, from left to right and from right to left. This is achieved by randomly masking \n",
      "the words while the model is trained, much like how children are often taught a \n",
      "language by using fill-in-the-blanks exercises. Such is the generalized learning of \n",
      "BERT that it can be used even for translation-related tasks, even though it has not \n",
      "been specifically taught translation as a task. BERT and other large models, such as \n",
      "GPT-2, require a huge computing infrastructure, which is generally not available to \n",
      "most people outside of leading universities and the biggest technology corporations. \n",
      "Pre-trained models fill the void in such cases. The TextRank algorithm considers each \n",
      "sentence to be a bag of words. With the advent of BERT, it is possible for us to have a \n",
      "superior sentence representation that captures meaning much better than the bag of \n",
      "words model.340 | Text Generation and Summarization\n",
      "In the case of summarization, even though there is a benchmark called Recall-\n",
      "Oriented Understudy for Gisting Evaluation  (ROUGE ), summarization is best \n",
      "evaluated qualitatively given that there isn't only one correct way to summarize text. \n",
      "In February 2020, Microsoft's Turing NLG model, which has 17 billion parameters, \n",
      "generated abstractive summaries for three examples, which were shared publicly. \n",
      "However, the model is not publicly available currently and so the results cannot  \n",
      "be reproduced.\n",
      "Furthermore, we don't know how the Microsoft NLG model does with a naïve test \n",
      "such as the Little Red Riding Hood test. In general, extractive summarization of \n",
      "the kind discussed earlier in this chapter is by far the most useful for practitioners \n",
      "compared with the utility of the state-of-the-art technology in text generation \n",
      "and paraphrasing. Due to this, in the next section, we'll largely focus on practical \n",
      "challenges in extractive summarization.\n",
      "Practical Challenges in Extractive Summarization\n",
      "Given the rapid pace of development in NLP, it is even more important to use \n",
      "compatible versions of the libraries that we use. Evaluation of a document's suitability \n",
      "for extractive summarization can be undertaken manually. Often, we would like to \n",
      "summarize multiple pieces of text, all of which could be short in length. The TextRank \n",
      "algorithm will not work well in such cases.\n",
      "All unverified claims reported in this field ought to be taken with a grain of salt until \n",
      "the claim has been verified. Such claims ought to be subjected by practitioners to \n",
      "naïve tests such as the Little Red Riding test. We can only use a model if it works and \n",
      "if the limitations related to scope and any biases are considered.\n",
      "Summary\n",
      "In this chapter, we learned about text generation using Markov chains and extractive \n",
      "summarization using the TextRank algorithm. We also explored both the power and \n",
      "limitations of various advanced approaches. In the next chapter, we will learn about \n",
      "sentiment analysis.Overview\n",
      "This chapter introduces you to one of the most exciting applications of \n",
      "natural language processing—that is, sentiment analysis. You will explore \n",
      "the various tools used to perform sentiment analysis, such as popular NLP  \n",
      "libraries and deep learning frameworks. You will then perform sentiment \n",
      "analysis on given text data using the powerful textblob  library. You will \n",
      "load textual data and perform preprocessing on it to fine-tune the results of \n",
      "your sentiment analysis program. By the end of the chapter, you will be able \n",
      "to train a sentiment analysis model.Sentiment Analysis8344 | Sentiment Analysis\n",
      "Introduction\n",
      "In the previous chapter, we looked at text generation, paraphrasing, and \n",
      "summarization, all of which can be immensely useful in helping us focus on only the \n",
      "essential and meaningful parts of the text corpus. This, in turn, helps us to further \n",
      "refine the results of our NLP project. In this chapter, we will look at sentiment \n",
      "analysis , which, as the name suggests, is the area of NLP that involves teaching \n",
      "computers how to identify the sentiment behind written content or parsed  \n",
      "audio—that is, audio converted to text. Adding this ability to automatically detect \n",
      "sentiment in large volumes of text and speech opens new possibilities for us to write \n",
      "useful software.\n",
      "In sentiment analysis, we try to build models that detect how people feel. This \n",
      "starts with determining what kind of feeling we want to detect. Our application may \n",
      "attempt to determine the level of human emotion (most often, whether a person is \n",
      "sad or happy; satisfied or dissatisfied; or interested or disinterested and so on). The \n",
      "common thread here is that we measure how sentiments vary in different directions. \n",
      "This is also called polarity. Polarity signifies the emotions present in a sentence, \n",
      "such as joy or anger. For example, \"I love oranges\" implies an emotionally positive \n",
      "statement, whereas \"I hate politics\" is a strong negative emotion.\n",
      "Why Is Sentiment Analysis Required?\n",
      "In machine learning projects, we try to build applications that work similarly to a \n",
      "human being. We measure success in part by seeing how close our application is to \n",
      "matching human-level performance. Generally, machine learning programs cannot \n",
      "exceed human-level performance by a significant margin—especially if our training \n",
      "data source is human-generated.\n",
      "Let's say that we want to carry out a sentiment analysis of product reviews. The \n",
      "sentiment analysis program should detect how reviewers feel. Obviously, it is \n",
      "impractical for a person to read thousands of movie reviews. This is where automated \n",
      "sentiment analysis enters the picture. Artificial intelligence is useful when it is \n",
      "impractical for people to perform the task. In this case, the task is reading thousands \n",
      "of reviews.Introduction | 345\n",
      "The Growth of Sentiment Analysis\n",
      "The field of sentiment analysis is driven by a few main factors. Firstly, it's driven by \n",
      "the rapid growth in online content that's used by companies to understand and \n",
      "respond to how people feel. Secondly, since sentiment drives human decisions, \n",
      "businesses that understand their customers' sentiments have a major advantage in \n",
      "predicting and shaping purchasing decisions. Finally, NLP technology has improved \n",
      "significantly, allowing the much wider application of sentiment analysis.\n",
      "The Monetization of Emotion\n",
      "The growth of the internet and internet services has enabled new business models \n",
      "to work with human connection, communication, and sentiment. In January 2020, \n",
      "Facebook had about 61.3% of the social media traffic and has been one of the \n",
      "most successful social media platforms at connecting people across the world and \n",
      "providing features that enable users to express their thoughts and post memorable \n",
      "moments from their life online. Similarly, although Twitter had just 14.51% of the \n",
      "traffic, it has still proved to be an influential way to display sentiment online.\n",
      "There are now large amounts of information on social media about what people like \n",
      "or dislike. This data is of significant value not only in business but also in political \n",
      "campaigns. This means that sentiment has significant business value and can  \n",
      "be monetized.\n",
      "Types of Sentiments\n",
      "There are various sentiments that we can try to detect in language sources. Let's \n",
      "discuss a few of them in detail.\n",
      "Emotion\n",
      "Sentiment analysis is often used to detect the emotional state of a person. It checks \n",
      "whether the person is happy or sad, or content or discontent. Businesses often use it \n",
      "to improve customer satisfaction. For example, let's look at the following statement:\n",
      "\"I thought I would have enjoyed the movie, but it left me feeling that it could have \n",
      "been better.\"\n",
      "In this statement, it seems as though the person who has just watched a movie is \n",
      "unhappy about it. A sentiment detector, in this case, would be able to classify the \n",
      "review as negative and allow the business (the movie studio, for instance) to adjust \n",
      "how they make movies in the future.346 | Sentiment Analysis\n",
      "Action Orientation versus Passivity\n",
      "This is about whether a person is prone to action or not. This is often used to \n",
      "determine how close a person is to making a choice. For example, using a travel \n",
      "reservation chatbot, you can detect whether a person needs to make a reservation \n",
      "urgently or is simply making passive queries and is therefore less likely to book a \n",
      "ticket right now. The level of action orientation or passivity provides additional clues \n",
      "to detect intention. This can be used to make smart business decisions.\n",
      "Tone\n",
      "Speech and text are often meant to convey certain impressions that are not \n",
      "necessarily factual and not entirely emotional. Examples of this are sarcasm, irony, \n",
      "and humor. This may provide useful additional information about how a person \n",
      "thinks. Although tone is tricky to detect, there might be certain words or phrases that \n",
      "are often used in certain contexts. We can use NLP algorithms to extract statistical \n",
      "patterns from document sources. For example, we can use sentiment analysis to \n",
      "detect whether a news article is sarcastic.\n",
      "Subjectivity versus Objectivity\n",
      "You may want to detect whether the given text source is subjective or objective. For \n",
      "example, you might want to detect whether a person has issued and expressed an \n",
      "opinion, or whether their statement reads more like a fact and can only be true or \n",
      "false. Let's look at the following two statements to get a better understanding:\n",
      "• Statement 1: \"The duck was overcooked, and I could hardly taste the flavor.\"\n",
      "• Statement 2: \"Ducks are aquatic birds.\"\n",
      "In these two statements, statement 1 should be recognized as a subjective opinion \n",
      "and statement 2 as an objective fact. Determining the objectivity of a statement helps \n",
      "us decide on the appropriate response to the statement.Introduction | 347\n",
      "Key Ideas and Terms\n",
      "Let's look at some of the key ideas and terms that are used in sentiment analysis.\n",
      "Classification\n",
      "As we learned in Chapter 3 , Developing a Text Classifier , classification is the NLP \n",
      "technique of assigning one or more classes to text documents. This helps in \n",
      "separating and sorting the documents. If you use classification for sentiment \n",
      "analysis, you assign different sentiment classes such as positive, negative, or neutral. \n",
      "Sentiment analysis is a type of text classification that aims to create a classifier \n",
      "trained on a set of labeled pairs – text and its corresponding sentiment (label). Upon \n",
      "training such a classifier on a large labeled dataset, the sentiment analysis model \n",
      "generalizes well and can classify unseen text into appropriate sentiment categories.\n",
      "Supervised Learning\n",
      "As we have already seen, in supervised learning , we create a model by supplying \n",
      "data and labeled targets to the training algorithms. The algorithms learn using this \n",
      "supply. When it comes to sentiment analysis, we provide the training dataset with  \n",
      "the labels that represent the sentiment. For example, for each text in a dataset,  \n",
      "we would assign a value of 1 if the sentiment is positive, and a value of 0 if the \n",
      "statement is negative.\n",
      "Polarity\n",
      "Polarity is a measure of how negative or positive the sentiment is in a given language. \n",
      "Polarity is used because it is simple and easy to measure and can be easily translated \n",
      "to a simple numeric scale. It usually ranges between -1 and 1. Values close to 1 \n",
      "reflect documents that have positive sentiments, whereas values close to -1 reflect \n",
      "documents that have negative sentiments. Values around 0 reflect documents that \n",
      "are neutral in sentiment.\n",
      "It's worth noting that the polarity detected by a model depends on how it has been \n",
      "trained. On political Reddit threads, the opinions tend to be highly polarized. On \n",
      "the other hand, if you use the same model on business documents to measure \n",
      "sentiments, the scores tend to be neutral. So, you need to choose models that are \n",
      "trained in similar domains.\n",
      "Intensity\n",
      "In contrast to polarity, which is measured from negative to positive, intensity is \n",
      "measured in terms of arousal, which ranges from low to high. Most often, the level of \n",
      "intensity is included in the sentiment score. It is measured by looking at the closeness \n",
      "of the score to 0 or 1.348 | Sentiment Analysis\n",
      "Applications of Sentiment Analysis\n",
      "There are various applications of sentiment analysis.\n",
      "Financial Market Sentiment\n",
      "Financial markets operate partially on economic fundamentals but are also heavily \n",
      "influenced by human sentiment. Stock market prices, which tend to rise and fall, \n",
      "are influenced by the opinions of news articles regarding the overall market or any \n",
      "specific securities.\n",
      "Financial market sentiment helps measure the overall attitude of investors toward \n",
      "securities. Market sentiment can be detected using news or social media articles. We \n",
      "can use NLP algorithms to build models that detect market sentiment and use those \n",
      "models to predict future market prices.\n",
      "Product Satisfaction\n",
      "Sentiment analysis is commonly used to determine how customers feel about \n",
      "products and services. For example, Amazon makes use of its extensive product \n",
      "reviews dataset. This not only helps to improve its products and services but also acts \n",
      "as a source of training data for its sentiment analysis services.\n",
      "Social Media Sentiment\n",
      "A really useful area of focus for sentiment analysis is social media monitoring. \n",
      "Social media has become a key communication medium with which most people \n",
      "around the world interact every day, and so there is a large and growing source of \n",
      "human language data available there. More importantly, the need for businesses \n",
      "and organizations to be able to process and understand what people are saying on \n",
      "social media has only increased. This has led to an exponential growth in demand for \n",
      "sentiment analysis services.\n",
      "Brand Monitoring\n",
      "A company's brand is a significant asset and companies spend a lot of time, \n",
      "effort, and money maintaining their brand value. With the growth of social media, \n",
      "companies are now exposed to considerable potential brand risks from negative \n",
      "social media conversations. On the other hand, there is also the potential for positive \n",
      "brand growth from positive interactions and messages on social media. For this \n",
      "reason, businesses deploy people to monitor what is said about them and their \n",
      "brands on social media. Automated sentiment analysis makes this significantly easier \n",
      "and also more efficient.Tools Used for Sentiment Analysis | 349\n",
      "Customer Interaction\n",
      "Organizations often want to know how their customers feel during an interaction in \n",
      "an online chat or a phone conversation. In such cases, the objective is to detect the \n",
      "level of satisfaction with the service or the products. Sentiment analysis tools help \n",
      "companies handle large volumes of text and voice data that are generated during \n",
      "customer interaction. Every company, irrespective of the domain, wants to utilize the \n",
      "data at their disposal to glean valuable insights, as there is potential revenue to be \n",
      "had if companies can gain insights into customer satisfaction.\n",
      "Tools Used for Sentiment Analysis\n",
      "There are a lot of tools capable of analyzing sentiment. Each tool has its advantages \n",
      "and disadvantages. We will look at each of them in detail.\n",
      "NLP Services from Major Cloud Providers\n",
      "Online sentiment analysis is carried out by all major cloud services providers, such as \n",
      "Amazon, Microsoft, Google, and IBM. You can usually find sentiment analysis as a part \n",
      "of their text analysis services or general machine learning services. Online services \n",
      "offer the convenience of packaging all the necessary algorithms behind the provider's \n",
      "API. These algorithms are capable of performing sentiment analysis. To use such \n",
      "services, you need to provide the text or audio sources, and in return, the services \n",
      "will provide you with a measure of the sentiment. These services usually return \n",
      "a standard, simple score, such as positive, negative, or neutral. The score usually \n",
      "ranges between 0 and 1.\n",
      "The following are the advantages and disadvantages of NLP services from major \n",
      "cloud providers:\n",
      "Advantages\n",
      "• You require almost no knowledge of NLP algorithms or sentiment analysis.  \n",
      "This results in fewer staffing needs.\n",
      "• Sentiment analysis services provide their own computation, reducing your own \n",
      "computational infrastructure needs.\n",
      "• Online services can scale well beyond what regular companies can do on  \n",
      "their own.\n",
      "• You gain the benefits of automatic improvements and updates to sentiment \n",
      "analysis algorithms and data.350 | Sentiment Analysis\n",
      "Disadvantages\n",
      "• Online services require—at least temporarily—a reduction in privacy since you \n",
      "must provide the documents to be analyzed by the service. Depending on your \n",
      "project's privacy needs, this may or may not be acceptable. There might also be \n",
      "laws that restrict data crossing into another national jurisdiction.\n",
      "• The service provided by cloud providers is like one-solution-fits-all and is \n",
      "considered very generic, so it won't necessarily apply to niche use cases.\n",
      "Online Marketplaces\n",
      "Recently, AI marketplaces have emerged that offer different algorithms from third \n",
      "parties. Online marketplaces differ from cloud providers. An online marketplace \n",
      "allows third-party developers to deploy sentiment analysis services on their platform.\n",
      "Here are the advantages and disadvantages of online marketplaces:\n",
      "Advantages\n",
      "• AI marketplaces provide the flexibility of choosing between different sentiment \n",
      "analysis algorithms instead of just one algorithm. This enables users to try out \n",
      "different techniques and see which one fits their business needs the best.\n",
      "• Using algorithms from an AI marketplace reduces the need for dedicated data \n",
      "scientists for your project.\n",
      "Disadvantages\n",
      "• Algorithms from third parties are of varying quality.\n",
      "• Since the algorithms are provided by smaller companies, there is no guarantee \n",
      "that they won't disappear. And for businesses, this is a big risk since their \n",
      "solution has a direct dependency on a third party that is outside their control.\n",
      "Python NLP Libraries\n",
      "There are a few NLP libraries that need to be integrated into your project instead \n",
      "of being called upon as services. These are called dedicated NLP libraries and they \n",
      "usually include many NLP algorithms from academic research. Sophisticated NLP \n",
      "libraries used across the industry are spaCy, gensim, and AllenNLP.\n",
      "Here are the advantages and disadvantages of Python NLP libraries:Tools Used for Sentiment Analysis | 351\n",
      "Advantages\n",
      "• It's usually state-of-the-art research that goes into these libraries, and they \n",
      "usually have well-chosen datasets.\n",
      "• They provide a framework that makes it much easier to build projects and do \n",
      "rapid experiments.\n",
      "• They offer out-of-the-box abstractions that are required for all NLP projects, such \n",
      "as Token and Span.\n",
      "• They are easy to scale to real-world deployment.\n",
      "Disadvantages\n",
      "• This won't be considered a true disadvantage since libraries are meant to be \n",
      "general-purpose, but for complex use cases, developers would have to write \n",
      "their own implementations as required.\n",
      "Deep Learning Frameworks\n",
      "Deep learning libraries such as PyTorch and TensorFlow are meant to be used to \n",
      "build complex models for a wide range of applications, not limited to just NLP. These \n",
      "libraries provide you with more advanced algorithms and mathematical functions, \n",
      "helping you develop powerful and complex models.\n",
      "The advantages and disadvantages of these frameworks are explained here:\n",
      "Advantages\n",
      "• You have the flexibility to develop your sentiment analysis model to meet \n",
      "complex business needs.\n",
      "• You can integrate the latest and the most advanced algorithms when they are \n",
      "available in general-purpose libraries.\n",
      "• You can make use of transfer learning, which takes a model trained on a large \n",
      "text source, to fine-tune the training as per your project's needs. This allows you \n",
      "to create a sentiment analysis model that is more suitable for your needs.352 | Sentiment Analysis\n",
      "Disadvantages\n",
      "• This approach requires you to have in-depth knowledge of machine learning and \n",
      "complex topics such as deep learning.\n",
      "• Deep learning libraries require a large volume of rich annotated datasets \n",
      "along with an intense computational infrastructure to train and experiment \n",
      "with different modeling techniques to get a generalized model that's fit to be \n",
      "deployed in production. So, there is a requirement for training on non-CPU \n",
      "hardware such as GPUs/TPUs.\n",
      "Now that we've learned about the various tools available for sentiment analysis, let's \n",
      "explore the most popular Python libraries.\n",
      "The textblob library\n",
      "textblob  is a Python library used for NLP, as we've seen in the previous chapters. \n",
      "It has a simple API and is probably the easiest way to begin with sentiment analysis. \n",
      "textblob  is built on top of the NLTK library but is much easier to use. In the \n",
      "following sections, we will do an exercise and an activity to get a better understanding \n",
      "of how we can use textblob  for sentiment analysis.\n",
      "Exercise 8.01: Basic Sentiment Analysis Using the textblob Library\n",
      "In this exercise, we will perform sentiment analysis on a given text. For this, we will be \n",
      "using the TextBlob  class of the textblob  library. Follow these steps to complete \n",
      "this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Insert a new cell and add the following code to implement to import the \n",
      "TextBlob  class from the textblob  library:\n",
      "from textblob import TextBlob\n",
      "3. Create a variable named sentence  and assign it a string. Insert a new cell and \n",
      "add the following code to implement this:\n",
      "sentence = \"but you are Late Flight again!! \"\\\n",
      "           \"Again and again! Where are the  crew?\" The textblob library | 353\n",
      "4. Create an object of the TextBlob  class. Add sentence  as a parameter  \n",
      "to the TextBlob  container. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "blob = TextBlob(sentence)\n",
      "5. In order to view the details of the blob  object, insert a new cell and add the \n",
      "following code:\n",
      "print(blob)\n",
      "The code generates the following output:\n",
      "but you are Late Flight again!! Again and again! Where are the crew?\n",
      "6. To use the sentiment  property of the TextBlob  class (which returns a tuple), \n",
      "insert a new cell and add the following code:\n",
      "blob.sentiment\n",
      "The code generates the following output:\n",
      "Sentiment(polarity=—0.5859375, subjectivity=0.6\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2DlQvbM .\n",
      "You can also run this example online at https://packt.live/3jXvAN1 .\n",
      "In the code, we can see the polarity  and subjectivity  scores for a given \n",
      "text. The output indicates a polarity score of -0.5859375, which means that negative \n",
      "sentiment has been detected in the text. The subjectivity score means that the text is \n",
      "somewhat on the subjective side, though not entirely subjective. We have performed \n",
      "sentiment analysis on a given text using the textblob  library. In the next section, \n",
      "we will perform sentiment analysis on tweets about airlines.354 | Sentiment Analysis\n",
      "Activity 8.01: Tweet Sentiment Analysis Using the textblob library\n",
      "In this activity, you will perform sentiment analysis on tweets related to airlines.  \n",
      "You will also be providing condition for determining positive, negative, and neutral \n",
      "tweets, using the textblob  library.\n",
      "Note\n",
      "You can find the data to be used for this activity here:  \n",
      "https://packt.live/33cnr1q .\n",
      "Follow these steps to implement this activity:\n",
      "1. Import the necessary libraries.\n",
      "2. Load the CSV file.\n",
      "3. Fetch the text  column from the DataFrame.\n",
      "4. Extract and remove the handles from the fetched data.\n",
      "5. Perform sentiment analysis and get the new DataFrame.\n",
      "6. Join both the DataFrames.\n",
      "7. Apply the appropriate conditions and view positive, negative, and neutral tweets.\n",
      "After executing those steps, the output for positive tweets should be as follows:\n",
      "Figure 8.1: Positive tweets\n",
      "As you can see from the preceding output, the Polarity  column shows a positive \n",
      "integer. This implies that the tweet displays positive sentiment. The Subjectivity  \n",
      "column indicates that most tweets are found to be of a subjective nature.\n",
      "The textblob library | 355\n",
      "The output for negative tweets is as follows:\n",
      "Figure 8.2: Negative tweets\n",
      "The preceding output shows a Polarity  column with a negative integer, implying \n",
      "that the tweet displays negative sentiment, while the Subjectivity  column shows \n",
      "a positive integer, which implies the same as before—personal opinion or feeling.\n",
      "The output for neutral tweets should be as follows:\n",
      "Figure 8.3: Neutral tweets\n",
      "356 | Sentiment Analysis\n",
      "The preceding output has a Polarity  column and a Subjectivity  column with \n",
      "a zero or almost zero value. This implies the tweet has neither positive nor negative \n",
      "sentiment, but neutral; moreover, no subjectivity is detected for these tweets.\n",
      "Note\n",
      "The solution for this activity can be found via this link .\n",
      "In the next section, we will explore more about performing sentiment analysis using \n",
      "online web services.\n",
      "Understanding Data for Sentiment Analysis\n",
      "Sentiment analysis is a type of text classification . Sentiment analysis models are \n",
      "usually trained using supervised datasets . Supervised datasets are a kind of dataset \n",
      "that is labeled with the target variable, usually a column that specifies the sentiment \n",
      "value in the text. This is the value we want to predict in the unseen text.\n",
      "Exercise 8.02: Loading Data for Sentiment Analysis\n",
      "In this exercise, we will load data that could be used to train a sentiment analysis \n",
      "model. For this exercise, we will be using three datasets—namely Amazon, Yelp,  \n",
      "and IMDb.\n",
      "Note\n",
      "You can find the data being used in this exercise here:  \n",
      "https://packt.live/2XgeQqJ .\n",
      "Follow these steps to implement this exercise:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "This imports the pandas  library. It also sets the display width to 200  characters \n",
      "so that more of the review text is displayed on the screen.Understanding Data for Sentiment Analysis | 357\n",
      "3. To specify where the sentiment data is located, first load three different datasets \n",
      "from Yelp, IMDb, and Amazon. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "DATA_DIR = 'data/sentiment_labelled_sentences/'\n",
      "IMDB_DATA_FILE = DATA_DIR + 'imdb_labelled.txt'\n",
      "YELP_DATA_FILE = DATA_DIR + 'yelp_labelled.txt'\n",
      "AMAZON_DATA_FILE = DATA_DIR + 'amazon_cells_labelled.txt'\n",
      "COLUMN_NAMES = ['Review', 'Sentiment']\n",
      "Each of the data files has two columns: one for the review text and a numeric \n",
      "column for the sentiment.\n",
      "4. To load the IMDb reviews, insert a new cell and add the following code:\n",
      "imdb_reviews = pd.read_table(IMDB_DATA_FILE, names=COLUMN_NAMES)\n",
      "In this code, the read_table()  method loads the file into a DataFrame.\n",
      "5. Display the top 10 records in the DataFrame. Add the following code in the  \n",
      "new cell:\n",
      "imdb_reviews.head(10)\n",
      "The code generates the following output:\n",
      "Figure 8.4: The first few records in the IMDb movie review file\n",
      "In the preceding figure, you can see that the negative reviews have sentiment \n",
      "scores of 0 and positive reviews have sentiment scores of 1.\n",
      "358 | Sentiment Analysis\n",
      "6. To check the total number of records of the IMDb review file, use the value_\n",
      "counts()  function. Add the following code in a new cell to implement this:\n",
      "imdb_reviews.Sentiment.value_counts()\n",
      "The expected output with total reviews should be as follows:\n",
      "1          386\n",
      "0          362\n",
      "Name:    Sentiment, dtype: int64\n",
      "In the preceding figure, you can see that the data file contains a total of 748  \n",
      "reviews, out of which 362  are negative and 386  are positive.\n",
      "7. Format the data by adding the following code in a new cell:\n",
      "imdb_counts = imdb_reviews.Sentiment.value_counts().to_frame()\n",
      "imdb_counts.index = pd.Series(['Positive', 'Negative'])\n",
      "imdb_counts\n",
      "The code generates the following output:\n",
      "Figure 8.5: Counts of positive and negative sentiments in the IMDb review file\n",
      "We called value_counts() , created a DataFrame, and assigned Positive  \n",
      "and Negative  as index labels.\n",
      "8. To load the Amazon reviews, insert a new cell and add the following code:\n",
      "amazon_reviews = pd.read_table(AMAZON_DATA_FILE, \\\n",
      "                               names=COLUMN_NAMES)\n",
      "amazon_reviews.head(10)\n",
      "Understanding Data for Sentiment Analysis | 359\n",
      "The code generates the following output:\n",
      "Figure 8.6: Reviews from the Amazon dataset\n",
      "9. To load the Yelp reviews, insert a new cell and add the following code:\n",
      "yelp_reviews = pd.read_table(YELP_DATA_FILE, \\\n",
      "                             names=COLUMN_NAMES)\n",
      "yelp_reviews.head(10)\n",
      "The code generates the following output:\n",
      "Figure 8.7: Reviews from the Yelp dataset\n",
      "360 | Sentiment Analysis\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2XfwmLB .\n",
      "You can also run this example online at https://packt.live/339NTss .\n",
      "We have learned how to load data that could be used to train a sentiment analysis \n",
      "model. The review files mentioned in this exercise are an example. Each file contains \n",
      "review text, plus a sentiment label for each. This is the minimum requirement \n",
      "of a supervised machine learning project: to build a model that is capable of \n",
      "predicting sentiments. However, the review text cannot be used as is; it needs to be \n",
      "preprocessed so that we can extract feature vectors out of it and eventually provide it \n",
      "as input to the model.\n",
      "Now that we have learned about loading the data, in the next section, we will focus on \n",
      "training sentiment models.\n",
      "Training Sentiment Models\n",
      "The end product of any sentiment analysis project is a sentiment model . This is an \n",
      "object containing a stored representation of the data on which it was trained. Such a \n",
      "model has the ability to predict sentiment values for text that it has not seen before. \n",
      "To develop a sentiment analysis model, the following steps should be taken:\n",
      "1. The document dataset must be split into train and test datasets. The test dataset \n",
      "is normally a fraction of the overall dataset. It is usually between 5% and 40% of \n",
      "the overall dataset, depending on the total number of examples available. If the \n",
      "amount of data is too large, then a smaller test dataset can be used.\n",
      "2. Next, the text should be preprocessed by stripping unwanted characters, \n",
      "removing stop words, and performing other common preprocessing steps.\n",
      "3. The text should be converted to numeric vector representations in order to \n",
      "extract the features. These representations are used for training machine \n",
      "learning models.\n",
      "4. Once we have the vector representations, we can train the model. This will be \n",
      "specific to the type of algorithm being used. During the training, our model will \n",
      "use the test dataset as a guide to learn about the text.\n",
      "5. We can then use the model to predict the sentiment of documents that it has not \n",
      "seen before. This is the step that will be performed in production.Training Sentiment Models | 361\n",
      "In the next section, we will train a sentiment model. We'll make use of the \n",
      "TfidfVectorizer  and LogisticRegression  classes, which we explored in \n",
      "one of the previous chapters.\n",
      "Activity 8.02: Training a Sentiment Model Using TFIDF and Logistic Regression\n",
      "To complete this activity, you will build a sentiment analysis model using the Amazon, \n",
      "Yelp, and IMDb datasets that you used in the previous exercise. Use the TFIDF \n",
      "method to extract features from the text and use logistic regression for the learning \n",
      "algorithm. The following steps will help you complete this activity:\n",
      "1. Open a Jupyter notebook.\n",
      "2. Import the necessary libraries.\n",
      "3. Load the Amazon, Yelp, and IMDb datasets.\n",
      "4. Concatenate the datasets and take out a random sample of 10 items.\n",
      "5. Create a function for preprocessing the text, that is, convert the words into \n",
      "lowercase and normalize them.\n",
      "6. Apply the function created in the previous step on the dataset.\n",
      "7. Use TfidfVectorizer  to convert the review text into TFIDF vectors and \n",
      "use the LogisticRegression  class to create a model that uses logistic \n",
      "regression for the model. These should be combined into a Pipeline  object.\n",
      "8. Now split the data into train and test sets, using 70% to train the data and 30% to \n",
      "test the data.\n",
      "9. Use the fit()  function to fit the training data on the pipeline.\n",
      "10. Print the accuracy score.\n",
      "11. Test the model on these sentences: \"I loved this place\"  and \"I hated this place\" .\n",
      "Note\n",
      "The solution for this activity can be found via this link .362 | Sentiment Analysis\n",
      "Summary\n",
      "We started our journey into NLP with basic text analytics and text preprocessing \n",
      "techniques, such as tokenization, stemming, lemmatization, and lowercase \n",
      "conversion, to name a few. We then explored ways in which we can represent our \n",
      "text data in numerical form so that it can be understood by machines in order to \n",
      "implement various algorithms. After getting some practical knowledge of topic \n",
      "modeling, we moved on to text vectorization, and finally, in this chapter, we explored \n",
      "various applications of sentiment analysis. This included different tools that use \n",
      "sentiment analysis, from technologies available from online marketplaces to deep \n",
      "learning frameworks. More importantly, we learned how to load data and train our \n",
      "model to use it to predict sentiment.Appendix366 | Appendix\n",
      "Chapter 1: Introduction to Natural Language Processing\n",
      "Activity 1.01: Preprocessing of Raw Text\n",
      "Solution\n",
      "Let's perform preprocessing on a text corpus. To complete this activity,  \n",
      "follow these steps:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "from nltk import download\n",
      "download('stopwords')\n",
      "download('wordnet')\n",
      "nltk.download('punkt')\n",
      "download('averaged_perceptron_tagger')\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.corpus import stopwords\n",
      "from autocorrect import Speller\n",
      "from nltk.wsd import lesk\n",
      "from nltk.tokenize import sent_tokenize\n",
      "from nltk import stem, pos_tag\n",
      "import string\n",
      "3. Read the content of file.txt  and store it in a variable named sentence . \n",
      "Insert a new cell and add the following code to implement this:\n",
      "#load the text file into variable called sentence\n",
      "sentence = open(\"../data/file.txt\", 'r').read()\n",
      "4. Apply tokenization on the given text corpus. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "words = word_tokenize(sentence)Chapter 1: Introduction to Natural Language Processing | 367\n",
      "5. To print the list of tokens, insert a new cell and add the following code:\n",
      "print(words[0:20])\n",
      "This code generates the following output:\n",
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', \n",
      " 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming',\n",
      " 'lenguage', '.', 'He/she', 'must', 'have', 'knowldge']\n",
      "In the preceding figure, we can see the initial 20 tokens of our text corpus.\n",
      "6. To perform spelling correction in our given text corpus, loop through each token \n",
      "and correct the tokens that are wrongly spelled. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "spell = Speller(lang='en')\n",
      "def correct_sentence(words):\n",
      "    corrected_sentence = \"\"\n",
      "    corrected_word_list = []\n",
      "    for wd in words:\n",
      "        if wd not in string.punctuation:\n",
      "            wd_c = spell(wd)\n",
      "            if wd_c != wd:\n",
      "                print(wd+\" has been corrected to: \"+wd_c)\n",
      "                corrected_sentence = corrected_sentence+\" \"+wd_c\n",
      "                corrected_word_list.append(wd_c)\n",
      "            else:\n",
      "                corrected_sentence = corrected_sentence+\" \"+wd\n",
      "                corrected_word_list.append(wd)\n",
      "        else:\n",
      "            corrected_sentence = corrected_sentence + wd\n",
      "            corrected_word_list.append(wd)\n",
      "    return corrected_sentence, corrected_word_list\n",
      "corrected_sentence, corrected_word_list = correct_sentence(words)368 | Appendix\n",
      "This code generates the following output:\n",
      "lenguage has been corrected to: language\n",
      "knowldge has been corrected to: knowledge\n",
      "Familiarity has been corrected: familiarity\n",
      "7. To print the corrected text corpus, add a new cell and type the following code:\n",
      "corrected_sentence\n",
      "This code generates the following output:\n",
      "' The reader of this course should have a basic knowledge of the \n",
      "Python programming language. He/she must have knowledge of data \n",
      "types in Python. He should be able to write functions, and also have \n",
      "the ability to import and use libraries and packages in Python. \n",
      "familiarity with basic linguistics and probability is assumed \n",
      "although not required to fully complete this course.'\n",
      "8. To print a list of the initial 20 tokens of the corrected words, insert a new cell and \n",
      "add the following code:\n",
      "print(corrected_word_list[0:20])\n",
      "This code generates the following output:\n",
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', \n",
      " 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', \n",
      " 'language', '. ', 'He/she', 'must', 'have', 'knowledge']\n",
      "9. To add a PoS tag to all the corrected words in the list, insert a new cell and add \n",
      "the following code:\n",
      "print(pos_tag(corrected_word_list))\n",
      "This code generates the following output:\n",
      "Figure 1.5: List of corrected words tagged with appropriate PoS\n",
      "Chapter 1: Introduction to Natural Language Processing | 369\n",
      "10. To remove the stop words, insert a new cell and add the following code:\n",
      "stop_words = stopwords.words('english')\n",
      "def remove_stop_words(word_list):\n",
      "    corrected_word_list_without_stopwords = []\n",
      "    for wd in word_list:\n",
      "        if wd not in stop_words:\n",
      "            corrected_word_list_without_stopwords.append(wd)\n",
      "    return corrected_word_list_without_stopwords\n",
      "corrected_word_list_without_stopwords = remove_stop_words\\\n",
      "                                        (corrected_word_list)\n",
      "corrected_word_list_without_stopwords[:20]\n",
      "This code generates the following output:\n",
      "Figure 1.6: List excluding the stop words\n",
      "In the preceding figure, we can see that the stop words have been removed and \n",
      "a new list has been returned.\n",
      "370 | Appendix\n",
      "11. Apply the stemming process, and then insert a new cell and add the  \n",
      "following code:\n",
      "stemmer = stem.PorterStemmer()\n",
      "def get_stems(word_list):\n",
      "    corrected_word_list_without_stopwords_stemmed = []\n",
      "    for wd in word_list:\n",
      "        corrected_word_list_without_stopwords_stemmed\\\n",
      "        .append(stemmer.stem(wd))\n",
      "    return corrected_word_list_without_stopwords_stemmed\n",
      "corrected_word_list_without_stopwords_stemmed = \\\n",
      "get_stems(corrected_word_list_without_stopwords)\n",
      "corrected_word_list_without_stopwords_stemmed[:20]\n",
      "This code generates the following output:\n",
      "Figure 1.7: List of stemmed words\n",
      "In the preceding code, we looped through each of the words in the \n",
      "corrected_word_list_without_stopwords  list and applied stemming \n",
      "to them. The preceding figure shows the list of the initial 20 stemmed words.\n",
      "Chapter 1: Introduction to Natural Language Processing | 371\n",
      "12. To apply the lemmatization process to the corrected word list, insert a new cell \n",
      "and add the following code:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "def get_lemma(word_list):\n",
      "    corrected_word_list_without_stopwords_lemmatized = []\n",
      "    for wd in word_list:\n",
      "        corrected_word_list_without_stopwords_lemmatized\\\n",
      "        .append(lemmatizer.lemmatize(wd))\n",
      "    return corrected_word_list_without_stopwords_lemmatized\n",
      "corrected_word_list_without_stopwords_lemmatized = \\\n",
      "get_lemma(corrected_word_list_without_stopwords_stemmed)\n",
      "corrected_word_list_without_stopwords_lemmatized[:20]\n",
      "This code generates the following output:\n",
      "Figure 1.8: List of lemmatized words\n",
      "372 | Appendix\n",
      "In the preceding code, we looped through each of the words in the \n",
      "corrected_word_list_without_stopwords  list and applied \n",
      "lemmatization to them. The preceding figure shows a list of the initial 20 \n",
      "lemmatized words.\n",
      "13. To detect the sentence boundary in the given text corpus, use the sent_\n",
      "tokenize()  method. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "print(sent_tokenize(corrected_sentence))\n",
      "This code generates the following output:\n",
      "[' The reader of this course should have a basic knowledge of the \n",
      "Python programming language.', 'He/she must have knowledge of \n",
      "data types in Python.', 'He should be able to write functions and \n",
      "also have the ability to import and use libraries and packages in \n",
      "Python.', 'familiarity with basic linguistics and probability is \n",
      "assumed although not required to fully complete this course.']\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gmyclC .\n",
      "You can also run this example online at https://packt.live/2D3h0ms .Chapter 2: Feature Extraction Methods | 373\n",
      "Chapter 2: Feature Extraction Methods\n",
      "Activity 2.01: Extracting Top Keywords from the News Article\n",
      "Solution\n",
      "The following steps will help you complete this Activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries \n",
      "and download the data:\n",
      "import operator\n",
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "from nltk import download, stem\n",
      "# The below statement will download the stop word list \n",
      "# 'nltk_data/corpora/stopwords/' at home directory of your computer\n",
      "download('stopwords')\n",
      "from nltk.corpus import stopwords\n",
      "The download  statement will download the stop word list at nltk_data/\n",
      "corpora/stopwords/  into your system's home directory.\n",
      "3. Create the different types of methods to perform various NLP tasks:\n",
      "Activity 2.01.ipynb\n",
      "def load_file(file_path):\n",
      "    news = ''.join\\\n",
      "              ([line for line in open(file_path,encoding='utf-8')])\n",
      "    return news\n",
      "\"\"\"\n",
      "This method will take string as input and return the string\n",
      "converted into lowercase\n",
      "\"\"\"\n",
      "def to_lower_case(text):\n",
      "    return text.lower()\n",
      "# This will take a text string as input and return the token.\n",
      "wht = WhitespaceTokenizer()\n",
      "def tokenize_text(text):\n",
      "    return wht.tokenize(text=text)\n",
      "The full code snippet can be found at https://packt.live/3hRl3kI374 | Appendix\n",
      "The load_file()  function will take the file path as input and return the \n",
      "content of the file as a string. The lower_case()  function will take a string \n",
      "as an argument and convert it into lowercase. Next, the tokenize_text()  \n",
      "function will tokenize the string into its constituent tokens. The get_stem()  \n",
      "method will perform stemming on the tokens, while get_freq()  will calculate \n",
      "the frequency of the tokens. Finally, get_top_n_words()  will return the n \n",
      "tokens with the highest frequency.\n",
      "4. Load a text file into a string using the load_file()  method:\n",
      "path = \"../data/news_article.txt\"\n",
      "news_article = load_file(path)\n",
      "5. Convert the text into lowercase using the to_lower_case()  method:\n",
      "lower_case_news_art = to_lower_case(text=news_article)\n",
      "6. Tokenize the text with the tokenize_text()  method using the following line \n",
      "of code:\n",
      "tokens = tokenize_text(lower_case_news_art)\n",
      "7. Remove the stop words from the list; add the following code to do this:\n",
      "removed_tokens = remove_stop_words(tokens)\n",
      "8. Perform stemming on the words using the get_stems()  method:\n",
      "stems = get_stems(removed_tokens)\n",
      "9. Now, calculate the frequency of stemmed tokens with the  \n",
      "get_freq()  method:\n",
      "freq_dict = get_freq(stems)\n",
      "10. To get the top six most frequently used words in the news article, use the \n",
      "following code:\n",
      "top_keywords = get_top_n_words(freq_dict, 6)\n",
      "top_keywords\n",
      "The preceding line of code will generate the following output:\n",
      "['law', 'justic', 'european', 'parti', 'took', 'poland'']Chapter 2: Feature Extraction Methods | 375\n",
      "Thus, we have extracted the top six keywords from the news article, which can give us \n",
      "an idea of what the article is about. However, in this example, we have extracted only \n",
      "unigrams. For a more comprehensive output, bigrams and trigrams are often more \n",
      "useful. So, for even better results, you can perform the preceding activity on bigrams \n",
      "and trigrams.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hRl3kI .\n",
      "You can also run this example online at https://packt.live/2DnUHaU .\n",
      "Activity 2.02: Text Visualization\n",
      "Solution\n",
      "1. Open a Jupyter Notebook. Insert a new cell and add the following code to import \n",
      "the necessary libraries:\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from collections import Counter\n",
      "import re\n",
      "import matplotlib as mpl\n",
      "mpl.rcParams['figure.dpi'] = 300\n",
      "2. To fetch the dataset and read its content, add the following code:\n",
      "text = open('../data//text_corpus.txt', 'r', \\\n",
      "            encoding='utf-8').read()\n",
      "text[:1040]376 | Appendix\n",
      "The preceding code generates the following output:\n",
      "Figure 2.31: Text corpus\n",
      "3. The text in the fetched data is not clean. In order to clean it, we need to make \n",
      "use of various preprocessing steps, such as tokenization and lemmatization. Add \n",
      "the following code to implement this:\n",
      "def lemmatize_and_clean(text):\n",
      "    nltk.download('wordnet')\n",
      "    lemmatizer = WordNetLemmatizer()\n",
      "    cleaned_lemmatized_tokens = [lemmatizer.lemmatize\\\n",
      "                                 (word.lower()) \\\n",
      "                                 for word in word_tokenize\\\n",
      "                                 (re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                  text))]\n",
      "    return cleaned_lemmatized_tokens\n",
      "4. To check the set of unique words, along with their frequencies, as well as to find \n",
      "the 50 most frequently occurring words, add the following code:\n",
      "Counter(lemmatize_and_clean(text)).most_common(50)\n",
      "Chapter 2: Feature Extraction Methods | 377\n",
      "The preceding code generates the following output:\n",
      "Figure 2.32: The 50 most frequent words\n",
      "378 | Appendix\n",
      "5. Once you get the set of unique words along with their frequencies, remove the \n",
      "stop words. Then, generate the word cloud for the top 50 most frequent words. \n",
      "Add the following code to implement this:\n",
      "stopwords = set(STOPWORDS)\n",
      "cleaned_text = ' '.join(lemmatize_and_clean(text))\n",
      "wordcloud = WordCloud(width = 800, height = 800, \\\n",
      "                      background_color ='white', \\\n",
      "                      max_words=50, \\\n",
      "                      stopwords = stopwords, \\\n",
      "                      min_font_size = 10).generate(cleaned_text)\n",
      "plt.imshow(wordcloud, interpolation='bilinear')\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "The preceding code generates the following output:\n",
      "Figure 2.33: Word cloud representation of the 50 most frequent words\n",
      "Chapter 2: Feature Extraction Methods | 379\n",
      "As shown in the preceding image, words that occur more frequently, such as \n",
      "\"unbeaten,\" \"final,\" and \"wicket,\" appear in larger sizes in the word cloud.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/30cDHxt .\n",
      "You can also run this example online at https://packt.live/33buXtj .380 | Appendix\n",
      "Chapter 3: Developing a Text Classifier\n",
      "Activity 3.01: Developing End-to-End Text Classifiers\n",
      "Solution\n",
      "The following steps will help you implement this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary packages:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "import warnings\n",
      "import string\n",
      "import re\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.metrics import accuracy_score, roc_curve, \\\n",
      "classification_report, confusion_matrix, \\\n",
      "precision_recall_curve, auc\n",
      "3. Read a data file. It has three columns: is_political , headline , and \n",
      "short_description . The headline  column contains various news \n",
      "headlines, the short_description  column contains an abstract of the \n",
      "article, and the is_political  column indicates whether the article is about \n",
      "politics or not. Here, label 0 denotes that a headline is not political and label \n",
      "1 denotes that the headline is political. Here, we will only use the short_\n",
      "description  column to train our model. Add the following code to do this:\n",
      "data = pd.read_csv('data/news_political_dataset.csv')\n",
      "data.sample(5)Chapter 3: Developing a Text Classifier | 381\n",
      "The preceding code generates the following output:\n",
      "Figure 3.67: Text data and labels stored as a DataFrame\n",
      "4. Create a generic function for all the classifiers called clf_model . It takes four \n",
      "inputs: the type of model, the features of the training dataset, the labels of the \n",
      "training dataset, and the features of the validation dataset. It returns predicted \n",
      "labels, predicted probabilities, and the model it has been trained on. Add the \n",
      "following code to do this:\n",
      "def clf_model(model_type, X_train, y_train, X_valid):\n",
      "    model = model_type.fit(X_train,y_train)\n",
      "    predicted_labels = model.predict(X_valid)\n",
      "    predicted_probab = model.predict_proba(X_valid)[:,1]\n",
      "    return [predicted_labels,predicted_probab, model]\n",
      "5. Furthermore, another function is defined, called model_evaluation . It takes \n",
      "three inputs: actual values, predicted values, and predicted probabilities. It prints \n",
      "a confusion matrix, accuracy, f1-score, precision, recall scores, and the AUROC \n",
      "curve. It also plots the ROC curve:\n",
      "def model_evaluation(actual_values, predicted_values, \\\n",
      "                     predicted_probabilities):\n",
      "    cfn_mat = confusion_matrix(actual_values,predicted_values)\n",
      "    print(\"confusion matrix: \\n\",cfn_mat)\n",
      "    print(\"\\naccuracy: \",accuracy_score\\\n",
      "                         (actual_values,predicted_values))\n",
      "    print(\"\\nclassification report: \\n\", \\\n",
      "          classification_report(actual_values,predicted_values))\n",
      "    fpr,tpr,threshold=roc_curve(actual_values, \\\n",
      "                                predicted_probabilities)\n",
      "    print('\\nArea under ROC curve for validation set:', \\\n",
      "          auc(fpr,tpr))\n",
      "    fig, ax = plt.subplots(figsize=(6,6))\n",
      "382 | Appendix\n",
      "    ax.plot(fpr,tpr,label='Validation set AUC')\n",
      "    plt.xlabel('False Positive Rate')\n",
      "    plt.ylabel('True Positive Rate')\n",
      "    ax.legend(loc='best')\n",
      "    plt.show()\n",
      "6. Use a lambda  function to extract tokens from each text in this DataFrame  \n",
      "(called data), check whether any of these tokens are stop words, lemmatize \n",
      "them, and then concatenate them side by side. Use the join  function to \n",
      "concatenate a list of words into a single sentence. After that, use the regular \n",
      "expression method ( re) to replace anything other than letters, digits, and \n",
      "whitespaces with blank spaces. Add the following code to implement this:\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "stop_words = stopwords.words('english')\n",
      "stop_words = stop_words + list(string.printable)\n",
      "data['cleaned_headline_text'] = data['short_description']\\\n",
      "                                .apply(lambda x : ' '.join\\\n",
      "                                ([lemmatizer.lemmatize\\\n",
      "                                  (word.lower()) \\\n",
      "                                for word in word_tokenize\\\n",
      "                                (re.sub(r'([^\\s\\w]|_)+', ' ', \\\n",
      "                                 str(x))) if word.lower() \\\n",
      "                                 not in stop_words]))\n",
      "7. Create a TFIDF matrix representation of these cleaned texts. Add the following \n",
      "code to do this:\n",
      "MAX_FEATURES = 200 \n",
      "tfidf_model = TfidfVectorizer(max_features=MAX_FEATURES) \n",
      "tfidf_df = pd.DataFrame(tfidf_model.fit_transform\\\n",
      "           (data['cleaned_headline_text']).todense()) \n",
      "tfidf_df.columns = sorted(tfidf_model.vocabulary_) \n",
      "tfidf_df.head()Chapter 3: Developing a Text Classifier | 383\n",
      "The preceding code generates the following output:\n",
      "Figure 3.68: TFIDF representation of the DataFrame\n",
      "8. Use sklearn's train_test_split  function to divide the dataset into training \n",
      "and validation sets. Add the following code to do this:\n",
      "X_train, X_valid, y_train, y_valid = \\\n",
      "train_test_split(tfidf_df, data['is_political'], test_size=0.2, \\\n",
      "                 random_state=42,stratify = data['is_political'])\n",
      "9. Train an XGBoost model using the XGBClassifier()  function and evaluate it \n",
      "for the validation set. Add the following code to do this:\n",
      "pip install xgboost\n",
      "from xgboost import XGBClassifier\n",
      "xgb_clf=XGBClassifier(n_estimators=10,learning_rate=0.05,\\\n",
      "                      max_depth=18,subsample=0.6,\\\n",
      "                      colsample_bytree= 0.6,\\\n",
      "                      reg_alpha= 10,seed=42)\n",
      "results = clf_model(xgb_clf, X_train, y_train, X_valid)\n",
      "model_evaluation(y_valid, results[0], results[1])\n",
      "model_xgb = results[2]\n",
      "384 | Appendix\n",
      "The preceding code generates the following output:\n",
      "Figure 3.69: Performance of the XGBoost model\n",
      "Chapter 3: Developing a Text Classifier | 385\n",
      "10. Extract the importance of features, that is, tokens or words that play a vital role \n",
      "in determining the type of content. Add the following code to do this:\n",
      "word_importances = pd.DataFrame\\\n",
      "                   ({'word':X_train.columns,\\\n",
      "                     'importance':model_xgb.feature_importances_})\n",
      "word_importances.sort_values('importance', \\\n",
      "                             ascending = False).head(4)\n",
      "The preceding code generates the following output:\n",
      "Figure 3.70: Words and their importance\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2ParIKD .\n",
      "You can also run this example online at https://packt.live/33axiEK .\n",
      "386 | Appendix\n",
      "Chapter 4: Collecting Text Data with Web Scraping and APIs\n",
      "Activity 4.01: Extracting Information from an Online HTML Page\n",
      "Solution\n",
      "Let's extract the data from an online source and analyze it. Follow these steps to \n",
      "implement this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Import the requests  and BeautifulSoup  libraries. Pass the URL to \n",
      "requests  with the following command. Convert the fetched content into HTML \n",
      "format using the BeautifulSoup  HTML parser. Add the following code to  \n",
      "do this:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "r = requests\\\n",
      "    .get('https://en.wikipedia.org/wiki/Rabindranath_Tagore')\n",
      "r.status_code\n",
      "soup = BeautifulSoup(r.text, 'html.parser')\n",
      "3. To extract the list of headings, see which HTML elements belong to each bold \n",
      "headline in the Works  section. You can see that they belong to the h3 tag. We \n",
      "only need the first six headings here. Look for a span  tag that has a class  \n",
      "attribute with the following set of commands:\n",
      "for element in soup.find_all('h3', limit=6):\n",
      "    spans = element.find('span', attrs={'class':\"mw-headline\"})\n",
      "    print(spans['id'])\n",
      "The preceding code generates the following output:\n",
      "Drama\n",
      "Short_stories\n",
      "Novels\n",
      "Poetry\n",
      "Songs_(Rabindra_Sangeet)\n",
      "Art_worksChapter 4: Collecting Text Data with Web Scraping and APIs | 387\n",
      "4. To extract information regarding the original list of works written in Bengali by \n",
      "Tagore, look for the table  tag. Traverse through the table and use select  to \n",
      "pick table rows ( tr) from following table data ( td) associated with it. Add the \n",
      "following code to extract the text:\n",
      "table = soup.find('table', attrs={'class':\"wikitable\"})\n",
      "for row in table.select('tr td'):\n",
      "    print(row.text)\n",
      "The preceding code generates the following output:\n",
      "Figure 4.16: List of Tagore's work\n",
      "388 | Appendix\n",
      "5. To extract the list of universities named after Tagore, look for the ol tag. Add the \n",
      "following code to do this:\n",
      "[each.text.strip() for each in soup.find('ol') if each != '\\n']\n",
      "The preceding code generates the following output:\n",
      "Figure 4.17: List of universities named after Rabindranath Tagore\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/315vOcd .\n",
      "You can also run this example online at https://packt.live/2D6qIV9 .\n",
      "Activity 4.02: Extracting and Analyzing Data Using Regular Expressions\n",
      "Solution\n",
      "Follow these steps to complete this activity:\n",
      "1. Collect the data using the requests  package with the following code:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "r = requests.get('https://www.packtpub.com/support/faq')\n",
      "r.status_code\n",
      "The preceding code generates the following output:\n",
      "200\n",
      "Chapter 4: Collecting Text Data with Web Scraping and APIs | 389\n",
      "2. Convert the fetched content into HTML format using BeautifulSoup 's  \n",
      "HTML parser.\n",
      "soup = BeautifulSoup(r.text, 'html.parser')\n",
      "3. Inspect the HTML tag of the Packt website FAQs page. You can extract the \n",
      "question text by first searching for the div  tag with the \"class\":\"tab\"  \n",
      "attribute and inside that element, find the label  tag to get the  \n",
      "question text. Similarly, to get the answer text, find the div  tag with \n",
      "\"class\":\"tab-content\" , as shown here:\n",
      "qas = []\n",
      "for each in soup.find_all('div', attrs={\"class\":\"tab\"}):\n",
      "    question = each.find('label')\n",
      "    answer = each.find('div', attrs={\"class\":\"tab-content\"})\n",
      "    qas.append((question.text, answer.text))\n",
      "print(qas[1])\n",
      "The preceding code generates the following output:\n",
      "('What format are Packt eBooks?', '\\nPackt eBooks can be downloaded \n",
      "as a PDF, EPUB or MOBI file. They can also be viewed online using your \n",
      "subscription.\\n')\n",
      "4. Create a DataFrame consisting of these questions and answers:\n",
      "import pandas as pd\n",
      "pd.DataFrame(qas, columns=['Question', 'Answer']).head()\n",
      "The preceding code generates the following output:\n",
      "Figure 4.18: DataFrame of the question and answers\n",
      "390 | Appendix\n",
      "5. To extract email addresses, make use of a regular expression. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "tc_page_r = requests\\\n",
      "            .get('https://www.packtpub.com/books/info/'\\\n",
      "                 'packt/terms-and-conditions')\n",
      "tc_page_r.status_code\n",
      "soup2 = BeautifulSoup(tc_page_r.text, 'html.parser')\n",
      "import re\n",
      "set(re.findall\\\n",
      "    (r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}\",\\\n",
      "     soup2.text))\n",
      "Here, the regular expression pattern will be looking for an alphanumeric blob, \n",
      "followed by the @ sign, followed by an alphanumeric blob. Next, it will look for \n",
      "a dot (.) followed by a 2 to 4-character suffix for domains ( com/in/org ). The \n",
      "preceding code generates the following output:\n",
      "{'customercare@packt.com', 'subscription.support@packt.com'}\n",
      "6. To extract phone numbers using a regular expression, insert a new cell and add \n",
      "the following code:\n",
      "re.findall(r\"\\+\\d{2}\\s{1}\\(0\\)\\s\\d{3}\\s\\d{3}\\s\\d{3}\",soup2.text)\n",
      "The preceding code generates the following output:\n",
      "['+44 (0) 121 265 648', '+44 (0) 121 212 141']\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2D4ijBK .\n",
      "You can also run this example online at https://packt.live/3jSXRVb .Chapter 4: Collecting Text Data with Web Scraping and APIs | 391\n",
      "Activity 4.03: Extracting Data from Twitter\n",
      "Solution\n",
      "Let's extract tweets using the tweepy  library. Follow these steps to implement  \n",
      "this activity:\n",
      "1. Log in to your Twitter account with your credentials.\n",
      "2. Visit https://dev.twitter.com/apps/new , fill in the necessary details, and submit  \n",
      "the form.\n",
      "3. Once the form is submitted, go to the Keys  and tokens  tab; copy  \n",
      "consumer_key , consumer_secret , access_token , and  \n",
      "access_token_secret  from there.\n",
      "4. Open a Jupyter Notebook.\n",
      "5. Import the relevant packages and follow the authentication steps by writing the \n",
      "following code:\n",
      "consumer_key = 'your_consumer_key'\n",
      "consumer_secret = 'your_consumer_secret'\n",
      "access_token = 'your_access_token'\n",
      "access_token_secret = 'your_access_token_secret'\n",
      "import pandas as pd \n",
      "import json\n",
      "from pprint import pprint\n",
      "import tweepy\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret) \n",
      "api = tweepy.API(auth)392 | Appendix\n",
      "6. Call the Twitter API with the #climatechange  search query. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "tweet_list = []\n",
      "for tweet in tweepy.Cursor(api.search, q='#climatechange', \\\n",
      "                           lang=\"en\").items(100):\n",
      "    tweet_list.append(tweet)\n",
      "len(tweet_list)\n",
      "tweet_list[0]\n",
      "The preceding code generates an output that should look similar to the following \n",
      "screenshot. The content will vary since the tweets will be different according to \n",
      "when you are running the program:\n",
      "Figure 4.19: The Twitter API called with the #climatechange search query\n",
      "7. Each tweepy  Status  object will have a json  object associated with it, which \n",
      "will have tweet content and meta info. Let's see what information is present:\n",
      "status = tweet_list[0]\n",
      "status_json = status._json\n",
      "pprint(status_json)\n",
      "Chapter 4: Collecting Text Data with Web Scraping and APIs | 393\n",
      "The preceding code generates the following output with different tweet content \n",
      "fetched at the time of running the program:\n",
      "Figure 4.20: Twitter status objects converted to JSON objects\n",
      "8. To check the tweet text, use the following code:\n",
      "status_json['text']\n",
      "Again, though the content may vary, the preceding code generates output similar \n",
      "to the following:\n",
      "'The latest The Passivhaus Daily! https://t.co/sPqQhgSRdo Thanks \n",
      "to @TheMarkofPolo @PeterGleick @boris_kapkov #passivehouse \n",
      "#climatechange'\n",
      "9. To create a DataFrame consisting of the text of tweets, add a new cell and write \n",
      "the following code:\n",
      "tweets = []\n",
      "for twt in tweet_list:\n",
      "    tweets.append(twt._json['text'])\n",
      "tweet_text_df = pd.DataFrame({'tweet_text' : tweets})\n",
      "tweet_text_df.head()\n",
      "394 | Appendix\n",
      "The preceding code generates the following output. Again, the content may vary \n",
      "depending on the current tweets:\n",
      "Figure 4.21: DataFrame with the text of tweets\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3jXyx03 .\n",
      "This section does not currently have an online interactive example and will \n",
      "need to be run locally.\n",
      "Chapter 5: Topic Modeling | 395\n",
      "Chapter 5: Topic Modeling\n",
      "Activity 5.01: Topic-Modeling Jeopardy Questions\n",
      "Solution\n",
      "Let's perform topic modeling on the dataset of Jeopardy questions:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import pandas and  \n",
      "other libraries:\n",
      "import numpy as np\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 800)\n",
      "3. After downloading the data, you can extract it and place at the location below. \n",
      "Then load the Jeopardy CSV file into a pandas DataFrame. Insert a new cell and \n",
      "add the following code:\n",
      "JEOPARDY_CSV =  '../data/jeopardy/Jeopardy.csv'\n",
      "questions = pd.read_csv(JEOPARDY_CSV)\n",
      "questions.columns = [x.strip() for x in questions.columns]\n",
      "4. The data in the DataFrame is not clean. In order to clean it, remove records that \n",
      "have missing values in the Question  column. Add the following code to do this:\n",
      "questions = questions.dropna(subset=['Question'])\n",
      "5. Find the number of unique categories. Add the following code to do this:\n",
      "questions['Category'].nunique()\n",
      "The code generates the following output:\n",
      "27995396 | Appendix\n",
      "6. Sample 4% of the questions and tokenize the corpus where the tokens are \n",
      "classified as NOUN  by spaCy:\n",
      "file='../data/JQuestions.txt'\n",
      "questions['Question'].sample(frac=0.04,replace=False,\\\n",
      "                             random_state=0).to_csv(file)\n",
      "f=open(file,'r',encoding='utf-8')\n",
      "text=f.read()\n",
      "f.close()\n",
      "doc=nlp(text)\n",
      "pos_list=['NOUN']\n",
      "preproc_text=[]\n",
      "preproc_sent=[]\n",
      "for token in doc:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\\n",
      "        and token.pos_ in pos_list:\n",
      "            preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        preproc_text.append(preproc_sent)\n",
      "        preproc_sent=[]\n",
      "preproc_text.append(preproc_sent) #last sentence\n",
      "print(preproc_text)\n",
      "The code generates output like the following:\n",
      "Figure 5.22: Tokenized corpus after selecting 4% of the sample\n",
      "Chapter 5: Topic Modeling | 397\n",
      "7. Train a tomotopy LDA model with 1,000 topics. Print a few topics. Add the \n",
      "following code to do this:\n",
      "import tomotopy as tp\n",
      "NUM_TOPICS=1000\n",
      "mdl = tp.LDAModel(k=NUM_TOPICS,seed=1234)\n",
      "for line in preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "    \n",
      "mdl.train(10)\n",
      "    \n",
      "for k in range(mdl.k):\n",
      "    print('Top 7 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=7))\n",
      "The code generates the following output:\n",
      "Figure 5.23: Topics inferred after training the LDA model\n",
      "8. Now print the log perplexity. Add the following code to do this:\n",
      "print('Log perplexity=',mdl.ll_per_word)\n",
      "The code generates output like so:\n",
      "Log perplexity= -14.396450040387437\n",
      "9. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if we consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in preproc_text for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "398 | Appendix\n",
      "The code generates output like so:\n",
      "Figure 5.24: Probability distribution of topics if the entire dataset is considered\n",
      "10. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 461:\n",
      "print(mdl.get_topic_words(461, top_n=7))\n",
      "The code generates output like so:\n",
      "[('city', 0.15946216881275177), ('device', 0.02001992054283619),\n",
      " ('force', 0.02001992054283619), ('character', 0.2001992054283619),\n",
      " ('death', 0.010059761814773083), ('person', 0.010059761814773083),\n",
      " ('language', 0.010059761814773083)]\n",
      "11. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 234:\n",
      "print(mdl.get_topic_words(234, top_n=7))\n",
      "The code generates output like so:\n",
      "[('year', 0.09871795773506165), ('group', 0.02968442067503929), \n",
      " ('child', 0.019822485744953156), ('murder', 0.019822485744953156),\n",
      " ('field', 0.019822485744953156), ('writing', 0.009960552677512169),\n",
      " ('memorial', 0.009960552677512169)]\n",
      "Chapter 5: Topic Modeling | 399\n",
      "12. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 186:\n",
      "print(mdl.get_topic_words(186, top_n=7))\n",
      "The code generates output like so:\n",
      "[('dragon', 0.027016131207346916), ('power', 0.01357526984065711),\n",
      " ('flying', 0.013575269840657711), ('line', 0.013575269840657711),\n",
      " ('process', 0.013575269840657711), \n",
      " ('crystal', 0.013575269840657711),\n",
      " ('freestyle', 0.013575269840657711)]\n",
      "We find that the log perplexity is around -14, but the topics are not interpretable and \n",
      "the number of categories is an order of magnitude greater than the number of topics. \n",
      "The topic model could still be used for dimensionality reduction.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in both \n",
      "gensim and tomotopy. While setting a random_state  in gensim could \n",
      "help with reproducibility, in general, the topics found using tomotopy are \n",
      "superior from the perspective of interpretability. Generally, your output is \n",
      "expected to be different. In order to have exactly the same topic model, we \n",
      "can save and load topic models, and we do this in Exercise 5.04 , Topics in \n",
      "The Life and Adventures of Robinson Crusoe by Daniel Defoe .\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/33c2O5p .\n",
      "This section does not currently have an online interactive example, and will \n",
      "need to be run locally.400 | Appendix\n",
      "Activity 5.02: Comparing Different Topic Models\n",
      "Solution\n",
      "Let's perform topic modeling on the CFPB dataset. Follow these steps to complete \n",
      "this activity:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the pandas library:\n",
      "import numpy as np\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "file_student='../data/consumercomplaints/'\\\n",
      "             'student_comp_narrative.txt'\n",
      "f=open(file_student,'r',encoding='utf-8')\n",
      "student_text=f.read()\n",
      "f.close()\n",
      "3. Tokenize and include only nouns:\n",
      "doc_student=nlp(student_text)\n",
      "student_pos_list=['NOUN']\n",
      "student_preproc_text=[]\n",
      "student_preproc_sent=[]\n",
      "for token in doc_student:\n",
      "    if token.text!='\\n':\n",
      "        if not(token.is_stop) and not(token.is_punct) \\\n",
      "        and token.pos_ in student_pos_list:\n",
      "            student_preproc_sent.append(token.lemma_)\n",
      "    else:\n",
      "        student_preproc_text.append(student_preproc_sent)\n",
      "        student_preproc_sent=[]\n",
      "student_preproc_text.append(student_preproc_sent) #last sentence\n",
      "print(student_preproc_text)Chapter 5: Topic Modeling | 401\n",
      "The code generates the following output:\n",
      "Figure 5.25: Tokenized corpus containing only nouns\n",
      "4. Train an HDP model and print the log perplexity and topics:\n",
      "import tomotopy as tp\n",
      "mdl = tp.HDPModel(alpha=0.1,seed=0)\n",
      "for line in student_preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "mdl.train(50)\n",
      "print('Log Perplexity=', mdl.ll_per_word)\n",
      "for k in range(mdl.k):\n",
      "    print('Top 10 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=10))\n",
      "The code generates the following output:\n",
      "Figure 5.26: Log perplexity and the topics inferred from the HDP model\n",
      "5. Insert a new cell and add the following code to save the topic model:\n",
      "mdl.save('../data/consumercomplaints/hdp_model.bin')\n",
      "6. Insert a new cell and add the following code to load the topic model:\n",
      "mdl = tp.HDPModel.load('../data/consumercomplaints/hdp_model.bin')\n",
      "402 | Appendix\n",
      "7. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if we consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in student_preproc_text \\\n",
      "              for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "The code generates the following output:\n",
      "array([5, 7, 4, 6, 0, 1, 11, 9, 14, 2, 18, 17, 10, 8, 12, \n",
      "       13, 15, 3, 16], dtype=int64)\n",
      "8. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 5:\n",
      "print(mdl.get_topic_words(5, top_n=7))\n",
      "The code generates the following output:\n",
      "[('school', 0.05379803851246834), ('aid', 0.05379803851246834),\n",
      " ('password', 0.003592493385076523),\n",
      " ('username', 0.03592493385076523),\n",
      " ('information', 0.03592493385076523), \n",
      " ('direction', 0.03592493385076523), ('bus', 0.03592493385076523)]\n",
      "9. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 7:\n",
      "print(mdl.get_topic_words(7, top_n=7))\n",
      "The code generates the following output:\n",
      "[('graduate', 0.061739806085824966), \n",
      " ('program', 0.061739806085824966), \n",
      " ('assistance', 0.04634334146976471), \n",
      " ('loan', 0.03094688430428505), ('school', 0.03094688430428505), \n",
      " ('world', 0.03094688430428505)]\n",
      "10. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 4:\n",
      "print(mdl.get_topic_words(4, top_n=7))Chapter 5: Topic Modeling | 403\n",
      "The code generates the following output:\n",
      "[('employer', 0.03343059867620468), \n",
      " ('graduation', 0.03343059867620468), \n",
      " ('book', 0.03343059867620468), \n",
      " ('diploma', 0.025093790143728256), \n",
      " ('debt', 0.025093790143728256), \n",
      " ('education', 0.025093790143728256), \n",
      " ('college', 0.025093790143728256)]\n",
      "11. Now, train the LDA model. Add the following code for this:\n",
      "NUM_TOPICS=20\n",
      "mdl = tp.LDAModel(k=NUM_TOPICS,alpha=0.1,seed=0)\n",
      "for line in student_preproc_text:\n",
      "    mdl.add_doc(line)\n",
      "mdl.train(50)\n",
      "print('Log Perplexity=', mdl.ll_per_word)\n",
      "for k in range(mdl.k):\n",
      "    print('Top 10 words of topic #{}'.format(k))\n",
      "    print(mdl.get_topic_words(k, top_n=10))\n",
      "The code generates the following output:\n",
      "Figure 5.27: Log perplexity and topics inferred from the LDA model\n",
      "12. Insert a new cell and add the following code to save the topic model:\n",
      "mdl.save('../data/consumercomplaints/lda_model.bin')\n",
      "404 | Appendix\n",
      "13. Insert a new cell and add the following code to load the topic model:\n",
      "mdl = tp.LDAModel.load('../data/consumercomplaints/lda_model.bin')\n",
      "14. Insert a new cell and add the following code to see the probability distribution of \n",
      "topics if we consider the entire dataset as a single document:\n",
      "bag_of_words=[word for sent in preproc_text for word in sent]\n",
      "doc_inst = mdl.make_doc(bag_of_words)\n",
      "np.argsort(np.array(mdl.infer(doc_inst)[0]))[::-1]\n",
      "The code generates the following output:\n",
      "array([17,  7,  6,  8, 12,  0,  2,  4, 10,  5, 18, 14, 13, 11, \n",
      "       16, 15,  9,  3,  1, 19], dtype=int64)\n",
      "15. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 17:\n",
      "print(mdl.get_topic_words(17, top_n=7))\n",
      "The code generates the following output:\n",
      "[('interest', 0.20065094530582428), ('loan', 0.16345429420471191),\n",
      " ('payment', 0.152724489569664), ('rate', 0.07046262919902802), \n",
      " ('balance', 0.04184982180595398), ('year', 0.0314776748418808),\n",
      " ('principal', 0.25755111128091812)]\n",
      "16. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 7:\n",
      "print(mdl.get_topic_words(7, top_n=7))\n",
      "The code generates the following output:\n",
      "[('loan', 0.14698922634124756), ('year', 0.09230735898017883),\n",
      " ('repayment', 0.08487062156200409), \n",
      " ('payment', 0.08312080055475235), ('plan', 0.07349679619073868),\n",
      " ('income', 0.05074914172291756), ('month', 0.03981276974081993)]Chapter 5: Topic Modeling | 405\n",
      "17. Insert a new cell and add the following code to see the probability distribution of \n",
      "topic 6:\n",
      "print(mdl.get_topic_words(6, top_n=7))\n",
      "The code generates the following output:\n",
      "[('loan', 0.24387744069099426), ('time', 0.06379450112581253),\n",
      " ('student', 0.051527272909879684), ('m', 0.05103658139705659),\n",
      " ('money', 0.04514831304550171), ('payment', 0.03239039331674576),\n",
      " ('collection', 0.02794190190434456)]\n",
      "For our dataset and with the experimentation undertaken, the LDA topics were \n",
      "much more interpretable than the HDP topics. As seen from the preceding \n",
      "outputs, the log perplexity of the LDA model is also better than the log perplexity \n",
      "of the HDP model. We did, of course, benefit from using the number of topics \n",
      "found by the HDP model when training the LDP model, and so this is not an \n",
      "entirely fair comparison. Rather, this illustrates that there could be benefits \n",
      "to using an HDP model first even if we later select the LDA model for better \n",
      "interpretability or better log perplexity.\n",
      "Note\n",
      "In general, the topics found are extremely sensitive to randomization in both \n",
      "gensim and tomotopy. While setting a random_state  in gensim could \n",
      "help reproducibility, in general, the topics found using tomotopy are superior \n",
      "from the perspective of interpretability. Generally, your output is expected \n",
      "to be different. In order to have exactly the same topic model, we can save \n",
      "and load topic models, and this was used in Exercise 5.04 , Topics in The Life \n",
      "and Adventures of Robinson Crusoe by Daniel Defoe .\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/312B9Bf ..406 | Appendix\n",
      "Chapter 6: Vector Representation\n",
      "Activity 6.01: Finding Similar News Article Using Document Vectors\n",
      "Solution\n",
      "Follow these steps to complete this activity:\n",
      "1. Open a Jupyter Notebook. Insert a new cell and add the following code to import \n",
      "all necessary libraries:\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "from gensim.models import Doc2Vec\n",
      "import pandas as pd\n",
      "from gensim.parsing.preprocessing import preprocess_string, \\\n",
      "remove_stopwords \n",
      "2. Now load the news_lines  file.\n",
      "news_file = '../data/sample_news_data.txt'\n",
      "3. After that, you need to iterate over each headline in the file and split the \n",
      "columns, then create a DataFrame containing the headlines. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "with open(news_file, encoding=\"utf8\", errors='ignore') as f:\n",
      "    news_lines = [line for line in f.readlines()]\n",
      "lines_df = pd.DataFrame()\n",
      "indices  = list(range(len(news_lines)))\n",
      "lines_df['news'] = news_lines\n",
      "lines_df['index'] = indices\n",
      "lines_df.head()Chapter 6: Vector Representation | 407\n",
      "The code produces the following output:\n",
      "Figure 6.27: Head of the DataFrame\n",
      "4. You already have a trained document model named docVecModel.d2v  in the \n",
      "previous exercise. Now you can simply load and use it. Insert a new cell and add \n",
      "the following code to implement this:\n",
      "docVecModel = Doc2Vec.load('../data/docVecModel.d2v')\n",
      "5. Now, since you have loaded the document model, create two functions, \n",
      "namely, to_vector()  and similar_news_articles() . The to_\n",
      "vector()  function converts the sentences into vectors. The second function, \n",
      "similar_news_articles() , implements the similarity check. It uses the \n",
      "docVecModel.docvecs.most_similar()  function, which compares the \n",
      "vector against all the other lines it was built with. To implement this, insert a new \n",
      "cell and add the following code:\n",
      "from gensim.parsing.preprocessing import preprocess_string, \\\n",
      "remove_stopwords\n",
      "def to_vector(sentence):\n",
      "    cleaned = preprocess_string(sentence)\n",
      "    docVector = docVecModel.infer_vector(cleaned)\n",
      "    return docVector\n",
      "408 | Appendix\n",
      "def similar_news_articles(sentence):\n",
      "    vector = to_vector(sentence)\n",
      "    similar_vectors = docVecModel.docvecs.most_similar\\\n",
      "                      (positive=[vector])\n",
      "    print(similar_vectors)\n",
      "    similar_lines = lines_df\\\n",
      "                    [lines_df.index==similar_vectors[0][0]].news\n",
      "    return similar_lines\n",
      "6. Now that you have created the functions, it is time to test them. Insert a new cell \n",
      "and add the following code to implement this:\n",
      "similar_news_articles(\"US raise TV indecency US politicians \"\\\n",
      "                      \"are proposing a tough new law aimed at \"\\\n",
      "                      \"cracking down on indecency\")\n",
      "The code will generate the following output:\n",
      "1958        Clarke to unveil immigration plan New controls\n",
      "Name: news, dtype: object\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3hMOcgO .\n",
      "You can also run this example online at https://packt.live/3gbDFvg .Chapter 7: Text Generation and Summarization | 409\n",
      "Chapter 7: Text Generation and Summarization\n",
      "Activity 7.01: Summarizing Complaints in the Consumer Financial Protection \n",
      "Bureau Dataset\n",
      "Solution\n",
      "Follow these steps to complete this activity:\n",
      "1. Open a Jupyter Notebook and insert a new cell. Add the following code to import \n",
      "the required libraries:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "import os\n",
      "import csv\n",
      "import pandas as pd\n",
      "from gensim.summarization import summarize\n",
      "2. Insert a new cell and add the following code to fetch the Consumer Complaints \n",
      "dataset and consider the rows that have a complaint narrative. Drop all the \n",
      "columns other than Product , Sub-product , Issue , Sub-issue , and \n",
      "Consumer complaint narrative :\n",
      "complaints_pathname = '../data/consumercomplaints/'\\\n",
      "                      'Consumer_Complaints.csv'\n",
      "df_all_complaints = pd.read_csv(complaints_pathname)\n",
      "df_all_narr = df_all_complaints.dropna\\\n",
      "              (subset=['Consumer complaint narrative'])\n",
      "df_all_narr = df_all_narr[['Product','Sub-product','Issue',\\\n",
      "                           'Sub-issue',\\\n",
      "                           'Consumer complaint narrative']]410 | Appendix\n",
      "3. Insert a new cell and add the following code to select 12 complaints:\n",
      "df_part_narr = df_all_narr[df_all_narr.index.isin\\\n",
      "               ([242830,1086741,536367,957355,975181,483530,\\\n",
      "                 950006,865088,681842,536367,132345,285894])]\n",
      "df_part_narr\n",
      "The preceding code generates the following output:\n",
      "Figure 7.19: DataFrame showing the 12 selected complaints\n",
      "4. Insert a new cell and add the following code to add a new column, named \n",
      "TextRank Summary , that includes a TextRank summary for each of the  \n",
      "12 complaints:\n",
      "def try_summarize(x,ratio):\n",
      "    try:\n",
      "        return(summarize(x,ratio=ratio))\n",
      "    except:\n",
      "        return('')\n",
      "df_part_narr['TextRank Summary']=df_part_narr\\\n",
      "                                 ['Consumer complaint narrative']\\\n",
      "                                  .apply(lambda x: try_summarize\\\n",
      "                                  (x,ratio=0.20))\n",
      "Chapter 7: Text Generation and Summarization | 411\n",
      "5. Insert a new cell and add the following code to show the DataFrame:\n",
      "df_part_narr\n",
      "The preceding code generates the following output:\n",
      "Figure 7.20: DataFrame showing the summarized complaints\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/313r5YP .\n",
      "This section does not currently have an online interactive example,  \n",
      "and will need to be run locally.\n",
      "412 | Appendix\n",
      "Chapter 8: Sentiment Analysis\n",
      "Activity 8.01: Tweet Sentiment Analysis Using the textblob library\n",
      "Solution\n",
      "To perform sentiment analysis on the given set of tweets related to airlines, follow \n",
      "these steps:\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "from textblob import TextBlob\n",
      "import re\n",
      "3. Since we are displaying the text in the notebook, we want to increase the \n",
      "display width for our DataFrame. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "pd.set_option('display.max_colwidth', 240)\n",
      "4. Now, load the airline-tweets.csv  dataset. We will read this CSV file using \n",
      "pandas' read_csv()  function. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "tweets = pd.read_csv('data/airline-tweets.csv')\n",
      "5. Insert a new cell and add the following code to view the first 10 records of  \n",
      "the DataFrame:\n",
      "tweets.head()Chapter 8: Sentiment Analysis | 413\n",
      "The code generates the following output:\n",
      "Figure 8.8: The first few tweets\n",
      "6. If we look at the preceding figure, we can see that the tweets contain Twitter \n",
      "handles, which start with the @ symbol. It might be useful to extract those \n",
      "handles. The string  column included in the DataFrame has an extract()  \n",
      "function, which uses a regex to get parts of a string. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "tweets['At'] = tweets['tweet'].str.extract(r'^(@\\S+)')\n",
      "This code declares a new column called At and sets the value to what the \n",
      "extract  function returns. The extract  function uses a regex, ^(@\\S+) , \n",
      "to return strings that start with @. To view the initial 10 records of the tweets  \n",
      "DataFrame, we insert a new cell and write the following code:\n",
      "tweets.head(10)\n",
      "The output should look something like this (only top four tweets are  \n",
      "shown here):\n",
      "Figure 8.9: The first 10 tweets along with the Twitter handles\n",
      "414 | Appendix\n",
      "7. Now, we want to remove the Twitter handles inside the tweets since they are \n",
      "irrelevant for sentiment analysis. First, create a function named remove_\n",
      "handles() , which accepts a DataFrame as a parameter. After passing the \n",
      "DataFrame, the re.sub()  function will remove the handles in the DataFrame. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "def remove_handles(tweet):\n",
      "    return re.sub(r'@\\S+', '', tweet)\n",
      "8. To remove the handles, insert a column in the DataFrame called tweets_\n",
      "preprocessed  and add the following code:\n",
      "tweets['tweet_preprocessed'] = tweets['tweet']\\\n",
      "                               .apply(remove_handles)\n",
      "tweets.head(10)\n",
      "The expected output for the tweets after removing the Twitter handles should \n",
      "look like this (only the top four are shown in this figure):\n",
      "Figure 8.10: The first 10 tweets after removing the Twitter handles\n",
      "From the preceding figure, we can see that the Twitter handles have been \n",
      "separated from the tweets.\n",
      "Chapter 8: Sentiment Analysis | 415\n",
      "9. Now we can apply sentiment analysis on the tweets. First, create a get_\n",
      "sentiment()  function, which accepts a DataFrame and a column as \n",
      "parameters. Using this function, we create two new columns, Polarity  and \n",
      "Subjectivity , which will show the sentiment scores of each tweet. Insert a \n",
      "new cell and add the following code to implement this:\n",
      "def get_sentiment(dataframe, column):\n",
      "    text_column = dataframe[column]\n",
      "    textblob_sentiment = text_column.apply(TextBlob)\n",
      "    sentiment_values = [{'Polarity': v.sentiment.polarity, \\\n",
      "                         'Subjectivity': v.sentiment.subjectivity}\n",
      "                   for v in textblob_sentiment.values]\n",
      "    return pd.DataFrame(sentiment_values)\n",
      "This function takes a DataFrame and applies the TextBlob  constructor to each \n",
      "value of text_column . Then it extracts and creates a new DataFrame with the \n",
      "Polarity  and Objectivity  columns.\n",
      "10. Since the function has been created, we test it and pass the necessary \n",
      "parameters. The result of this will be stored in a new DataFrame, sentiment_\n",
      "frame . Insert a new cell and add the following code to implement this:\n",
      "sentiment_frame = get_sentiment(tweets, 'tweet_preprocessed')\n",
      "11. To view the initial four values of the new DataFrame, type the following code:\n",
      "sentiment_frame.head(4)\n",
      "The code generates the following output:\n",
      "Figure 8.11: Polarity and subjectivity scores\n",
      "416 | Appendix\n",
      "12. To join the original tweet  DataFrame to the sentiment_frame  DataFrame, \n",
      "use the concat()  function. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "tweets = pd.concat([tweets, sentiment_frame], axis=1)\n",
      "13. To view the initial 10 rows of the new DataFrame, we add the following code:\n",
      "tweets.head(10)\n",
      "The expected output with sentiment scores added should be as follows:\n",
      "Figure 8.12: Tweets DataFrame with sentiment scores added\n",
      "From the preceding figure, we can see that for each tweet , Polarity , and \n",
      "Subjectivity  scores have been calculated.\n",
      "14. To distinguish between the positive, negative, and neutral tweets, we need to \n",
      "add certain conditions. Consider tweets with polarity scores greater than 0.5  as \n",
      "positive, and tweets with polarity scores less than or equal to -0.5  as negative. \n",
      "For neutral tweets, consider only those tweets that fall in the range of -0.1  and \n",
      "0.1 . Insert a new cell and add the following code to implement this:\n",
      "positive_tweets = tweets[tweets.Polarity > 0.5]\n",
      "negative_tweets = tweets[tweets.Polarity <= - 0.5]\n",
      "neutral_tweets = tweets[ (tweets.Polarity > -0.1) \\\n",
      "                        & (tweets.Polarity < 0.1) ]\n",
      "Chapter 8: Sentiment Analysis | 417\n",
      "15. To view positive, negative, and neutral tweets, add the following code:\n",
      "positive_tweets.head(15)\n",
      "negative_tweets.head(15)\n",
      "neutral_tweets\n",
      "This displays the result of positive, negative, and neutral tweets. We have seen \n",
      "how to perform sentiment analysis using the textblob  library. The following \n",
      "image shows the top four neutral tweets:\n",
      "Figure 8.13: Neutral tweets\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/2XfcuIC .\n",
      "You can also run this example online at https://packt.live/2DqDSfq .\n",
      "418 | Appendix\n",
      "Activity 8.02: Training a Sentiment Model Using TFIDF and Logistic Regression\n",
      "Solution\n",
      "1. Open a Jupyter Notebook.\n",
      "2. Insert a new cell and add the following code to import the necessary libraries:\n",
      "import pandas as pd\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "3. To load all three datasets, insert a new cell and add the following code:\n",
      "DATA_DIR = 'data/sentiment_labelled_sentences/'\n",
      "IMDB_DATA_FILE = DATA_DIR + 'imdb_labelled.txt'\n",
      "YELP_DATA_FILE = DATA_DIR + 'yelp_labelled.txt'\n",
      "AMAZON_DATA_FILE = DATA_DIR + 'amazon_cells_labelled.txt'\n",
      "COLUMN_NAMES = ['Review', 'Sentiment']\n",
      "yelp_reviews = pd.read_table(YELP_DATA_FILE, names=COLUMN_NAMES)\n",
      "amazon_reviews = pd.read_table(AMAZON_DATA_FILE, \\\n",
      "                               names=COLUMN_NAMES)\n",
      "imdb_reviews = pd.read_table(YELP_DATA_FILE, names=COLUMN_NAMES)\n",
      "If we look at the code, even though the data comes from three different business \n",
      "domains, they are labeled and stored in the same format, which can help us to \n",
      "concatenate them together. This is the reason we can combine them to train our \n",
      "sentiment analysis model.\n",
      "4. Now we concatenate the different datasets into one dataset using the \n",
      "concat()  function. Insert a new cell and add the following code to  \n",
      "implement this:\n",
      "review_data = pd.concat([amazon_reviews, imdb_reviews, \\\n",
      "                         yelp_reviews])\n",
      "Since we combined the data from three separate files, let's make use of the \n",
      "sample()  function, which returns a random selection from the dataset. This \n",
      "will allow us to see the reviews from different files. Insert a new cell and add the \n",
      "following code to implement this:\n",
      "review_data.sample(10)Chapter 8: Sentiment Analysis | 419\n",
      "The code generates the following output (only the top four reviews are  \n",
      "displayed here):\n",
      "Figure 8.14: Output from calling the sample() function\n",
      "5. To view the number of counts, add the following code:\n",
      "review_data.Sentiment.value_counts()\n",
      "6. Create a function named clean()  and do some preprocessing. Basically, we \n",
      "need to remove unnecessary characters. Insert a new cell and add the following \n",
      "code to do this:\n",
      "import re\n",
      "def clean(text):\n",
      "    text = re.sub(r'[\\W]+', ' ', text.lower())\n",
      "    text = text.replace('hadn t' , 'had not')\\\n",
      "               .replace('wasn t', 'was not')\\\n",
      "               .replace('didn t', 'did not')\n",
      "    return text\n",
      "In the preceding code snippet, first, the text is converted to lowercase and \n",
      "cleaned, and then keywords with apostrophes are converted into their  \n",
      "original form.\n",
      "7. Once the function is defined, we can clean and tokenize the text. It is a good \n",
      "practice to apply transformation functions on copies of our data unless you are \n",
      "really constrained with memory. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "review_model_data = review_data.copy()\n",
      "review_model_data.Review = review_data.Review.apply(clean)\n",
      "420 | Appendix\n",
      "8. Now sample the data again to see what the processed text looks like. Add the \n",
      "following code in a new cell to implement this:\n",
      "review_model_data.sample(10)\n",
      "The code generates the following output (only the top four reviews are  \n",
      "displayed here):\n",
      "Figure 8.15: Sample of 10 after cleaning the Review column\n",
      "In the preceding figure, we can see that the text is converted to lowercase and \n",
      "only alphanumeric characters remain.\n",
      "9. Now it is time to develop our model. We will use TfidfVectorizer  \n",
      "to convert each review into a TFIDF  vector. We will then use \n",
      "LogisticRegression  to build a model. Insert a new cell and add the \n",
      "following code to import the necessary libraries:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "Next, combine TfidfVectorizer  and LogisticRegression  in a \n",
      "Pipeline  object. In order to do this, insert a new cell and add the  \n",
      "following code:\n",
      "tfidf = TfidfVectorizer()\n",
      "log_reg = LogisticRegression()\n",
      "log_tfidf = Pipeline([('vect', tfidf),\\\n",
      "                      ('clf', log_reg)])\n",
      "Chapter 8: Sentiment Analysis | 421\n",
      "10. Once the data is ready, split it into train and test sets. Split it into 70%  \n",
      "for training and 30% for testing. This can be achieved with the help of the \n",
      "train_test_split()  function. Insert a new cell and add the following code \n",
      "to implement this:\n",
      "X_train, X_test, y_train, y_test = train_test_split\\\n",
      "                                   (review_model_data.Review, \\\n",
      "                                    review_model_data.Sentiment,\\\n",
      "                                    test_size=0.3, \\\n",
      "                                    random_state=42)\n",
      "11. Fit the training data to the training pipeline with the help of the fit()  function. \n",
      "Insert a new cell and add the following code to implement this:\n",
      "log_tfidf.fit(X_train.values, y_train.values)\n",
      "The code generates the following output:\n",
      "Figure 8.16: Output from calling the fit() function on the training model\n",
      "12. In order to check our model's accuracy , use the score()  function. Insert a new \n",
      "cell and add the following code to implement this:\n",
      "test_accuracy = log_tfidf.score(X_test.values, y_test.values)\n",
      "'The model has a test accuracy of {:.0%}'.format(test_accuracy)\n",
      "You should an output as follows:\n",
      "'The model has a test accuracy of 81%'\n",
      "As you can see from the preceding figure, our model has an accuracy of 81% , \n",
      "which is pretty good for such a simple model.\n",
      "422 | Appendix\n",
      "13. The model is ready with an accuracy of 81% . Now we can use it to predict \n",
      "the sentiment of sentences. Insert a new cell and add the following code to \n",
      "implement this:\n",
      "log_tfidf.predict(['I loved this place', 'I hated this place'])\n",
      "You should see an output like the following:\n",
      "array([1, 0], dtype=int64)\n",
      "In the preceding figure, we can see how our model predicts sentiment. For a \n",
      "positive test sentence, it returns a score of 1. For a negative test sentence, it \n",
      "returns a score of 0.\n",
      "Note\n",
      "To access the source code for this specific section, please refer  \n",
      "to https://packt.live/3gto1M9 .\n",
      "You can also run this example online at https://packt.live/2PowrIN .IndexA\n",
      "airshow: 42, 47, 51\n",
      "AllenNLP: 350\n",
      "Amazon: 126, 131, \n",
      "137, 142, 149, 157, \n",
      "162, 282, 348-349, \n",
      "356-359, 361\n",
      "arrays: 280\n",
      "B\n",
      "banamali: 205-206\n",
      "bigrams: 45-46\n",
      "D\n",
      "dendrogram: \n",
      "110-111, 115-116\n",
      "doctype: 203\n",
      "Dropbox: 337\n",
      "G\n",
      "Gaussian: 134\n",
      "Gutenberg: 208-210, \n",
      "254, 259-260, \n",
      "295-296, 322, \n",
      "329, 333\n",
      "H\n",
      "Hadoop: 156\n",
      "histogram: 243J\n",
      "Jaccard: 92-94, 96\n",
      "JavaScript: 216\n",
      "joblib: 194-195\n",
      "K\n",
      "key-value: 216, \n",
      "286, 290\n",
      "kmeanModel: 122\n",
      "k-nearest: 124, \n",
      "135-137, 140, 198\n",
      "L\n",
      "lsamodel: 242-243\n",
      "M\n",
      "matplotlib: 84, 87, \n",
      "99-101, 111, 119, \n",
      "126, 132, 137, 143, \n",
      "149, 157, 162, 174, \n",
      "180, 238, 247, 254, \n",
      "260, 295, 299, 302, \n",
      "305, 310, 314\n",
      "MongoDB: 217\n",
      "N\n",
      "n-grams: 44, 47\n",
      "O\n",
      "one-hot: 32, 271, \n",
      "275-278, 280-290, \n",
      "292-293, 316P\n",
      "perplexity: 246-247, \n",
      "253, 265\n",
      "pprint: 218, 224-225\n",
      "pyplot: 84, 99, 111, \n",
      "119, 126, 132, 137, \n",
      "143, 149, 157, 162, \n",
      "174, 180, 238, \n",
      "247, 254, 260, \n",
      "295, 299-300, 302, \n",
      "305, 310, 314\n",
      "PyTorch: 351\n",
      "S\n",
      "seaborn: 119, 176\n",
      "stemmer: 20-22, 59-61\n",
      "stemming: 1, 8, 15, \n",
      "19-20, 22, 30, 32-33, \n",
      "36, 41, 58, 60-61, \n",
      "67-68, 105, 108, \n",
      "192, 275, 362\n",
      "stop-word: 36, \n",
      "65-66, 105, 108\n",
      "T\n",
      "TextBlob: 41, 44, \n",
      "46-47, 49, 63-66, \n",
      "69-71, 343, 352-354\n",
      "TextRank: 319, \n",
      "327-329, 333, \n",
      "335-340\n",
      "tf-idf: 33, 329\n",
      "toarray: 282todense: 81-82, 89, \n",
      "94, 96, 115, 120, \n",
      "128, 133, 139, 144, \n",
      "151, 159, 164, 176, \n",
      "182, 193, 195-196\n",
      "tokenizer: 50-57, 67, \n",
      "285-288, 290-292\n",
      "tomotopy: 237, 244, \n",
      "246-247, 249, \n",
      "252-254, 256, \n",
      "259, 262, 265\n",
      "Tweepy: 201, 227-228\n",
      "U\n",
      "unigrams: 8\n",
      "urllib: 210\n",
      "X\n",
      "XGBoost: 107, \n",
      "155-156, 160-162, \n",
      "165-166, 191, 198\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04\n",
      "SRH University Heidelberg\n",
      "Curriculum\n",
      "Applied Data Science and\n",
      "Analytics\n",
      "Master of Science (M.Sc.)\n",
      "Valid for Group April 23 BatchContent\n",
      "Required courses  1  ........................................................................................................................................................................................... \n",
      "Scope Big Data Science - Practice and Research  2  .............................................................................................................................. \n",
      "ADSA First Steps into Case Studies  3  ..................................................................................................................................................... \n",
      "ADSA Case Study I  5  .............................................................................................................................................................................. \n",
      "ADSA Master Thesis Project  7  ................................................................................................................................................................ \n",
      "Scope Data Engineering and Programming  9  ....................................................................................................................................... \n",
      "ADSA Data Engineering 1: Big Data Databases  10  ................................................................................................................................ \n",
      "ADSA Data Engineering 2: Big Data Architectures  12  ............................................................................................................................ \n",
      "ADSA: Big Data Programming: Python  14  .............................................................................................................................................. \n",
      "Scope Data Management  16  .................................................................................................................................................................... \n",
      "ADSA Data Management 1: Data Acquisition and Data Cleaning  17  ...................................................................................................... \n",
      "ADSA Data Management 2: Data Curation and Data Management  19  .................................................................................................. \n",
      "Scope Data Analytics  21  ........................................................................................................................................................................... \n",
      "ADSA Data Analytics 1: Statistics and Machine Learning  22  .................................................................................................................. \n",
      "ADSA Data Analytics 2: Text Mining and Natural Language Processing  24  ............................................................................................ \n",
      "ADSA Data Analytics 3: Deep Learning  26  ............................................................................................................................................. \n",
      "Scope Data Visualization and Storytelling  28  ....................................................................................................................................... \n",
      "ADSA Data Visualization and Storytelling 1+2: Design Basics and Designing Interactive Dashboards  29  ............................................ \n",
      "ADSA Data Visualization and Storytelling 3: Advanced Data Visualization  31  ....................................................................................... \n",
      "Scope Data Privacy  33  .............................................................................................................................................................................. \n",
      "ADSA Privacy, Ethics and International Law  34  ...................................................................................................................................... \n",
      "Elective Module  36  .................................................................................................................................................................................... \n",
      "ADSA Case Study 2  37  ........................................................................................................................................................................... \n",
      "ADSA Internship  39  ................................................................................................................................................................................ SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 1\n",
      "Required coursesSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 2\n",
      "Scope Big Data Science - Practice and ResearchSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 3ADSA First Steps into Case Studies\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "3627-2\n",
      "Credits\n",
      "5 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 125 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 50 hoursTeaching and learning methods\n",
      "Casework\n",
      "Group Work\n",
      "Problem-oriented learning\n",
      "ExerciseLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "Using the examination types project work, the students are allowed to reﬂect practically on their learning progress and scientiﬁc abilities.\n",
      "Through the analysis of practical examples and the performance of a complete case study using the business perspective as well as the\n",
      "multiple forms of Data handling (Choice, evaluation, cleansing, providing, analysis, and communication), the students get a ﬁrst glimpse of\n",
      "the technical, organizational and methodological principals of Big Data. They can also interconnect them directly with the diﬀerent aspects\n",
      "and phases of a big data project. Another element for the examination mentioned above types is that they enable a step-by-step\n",
      "improvement in skills and ﬁt optimally to the practice-oriented character of this module. Virtual teams are formed to allow for the studiablity\n",
      "parallel to the students’ main occupation. These teams are provided the main course material (data, software, scripts, literature) via a cloud\n",
      "platform, and they may also store and share their progress. Additional coaching of the teams during the module is provided via live chats\n",
      "and SAS e-learning.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students know the fundamental aspects of Big Data science.\n",
      "- They are able to deﬁne the “ﬁve Vs“ of Big Data (Volume, Velocity, Variety, Value and Veracity) as well as the diﬀerent phases of a Big\n",
      "Data project. They may explain them in well-deﬁned practical sessions in an application-oriented manner.\n",
      "- They know diverse practical examples of Big Data projects and are able to explain their approach in business context as well as to\n",
      "compare with each other.\n",
      "- They understand the diﬀerent phases of a Big Data project and may explain them in context with Big Data projects.\n",
      "- They are able to structure their scientiﬁc work as well as their results\n",
      "- They gain results based on appropriate scientiﬁc criteria, e.g. objectivity, validity and reliability.\n",
      "Methodological competence\n",
      "- The students are able to understand various types of project management and may apply as well as validate their knowledge in teams.\n",
      "Moreover, they understand the connection between business oﬀers and the Big Data Life Cycle.\n",
      "Self competence\n",
      "- The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks (i.e.\n",
      "occupation, private life and studies) simultaneously.\n",
      "Social competence\n",
      "The main function of this module is to lay the essential knowledge foundation for all later modules. Students analyze various practical\n",
      "examples of Big Data projects. As teams, they perform well-deﬁned Big Data case studies which involve the whole process of a Big Data\n",
      "project: Deﬁnition of a concrete problem within a business – Data acquisition and cleansing – Data saving – Data analysis and interpretation\n",
      "– Data visualization and communication of the results provided by Data analytics - Recommendation of further actions. This setup enables\n",
      "the Big Data project to be embedded into a business context. Thus, Students are encouraged to interconnect commercial necessities and\n",
      "decision-making with ethical issues during the Big Data project, which, vice versa, avoids an approach exclusively catering to technical and\n",
      "analytical requirements. The datasets used in the case studies are prepared by SRH University and distributed to the students using a cloud\n",
      "platform. The concepts, methods, and tools learned in this module will be repeated and intensiﬁed in the following modules – this especially\n",
      "aﬀects the modules Case Studies I and II. Parallel to the Case Studies the students are taught basic scientiﬁc competencies: They\n",
      "understand the essential aspects of scientiﬁc work and can plan and structure a scientiﬁc process. Furthermore, they show profound\n",
      "knowledge in applying tools and methods during the scientiﬁc writing process.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 4- The students know the fundamental tools to organize working processes in virtual teams and approaches to dealing with team-intern\n",
      "conﬂicts. They can use both tools to accomplish results mutually.\n",
      "Module content\n",
      "-Introduction to Artiﬁcial Intelligence, Big Data, Data Science, Data Analysis, and Data Engineering\n",
      "-Five Vs.: Volume, Velocity, Variety, Value, and Veracity\n",
      "-Big Data Life Cycle: Generation and collection of data, Data\n",
      "-Applications and Examples\n",
      "-Job Roles\n",
      "-KDD and CRISP-DM\n",
      "-Business Understanding\n",
      "-Data Understanding\n",
      "-Data Preparation\n",
      "-Modeling\n",
      "-Evaluation\n",
      "-Deployment\n",
      "-Data Preparation\n",
      "-Relational Databases - SQL\n",
      "-Introduction to the SAS Analytics Platform\n",
      "-Self-study: SAS Visual Analytics 1 for SAS Viya Basic\n",
      "-Self-study: SAS Visual Analytics 2 for SAS Viya Advanced\n",
      "-2 e-Learning Badges\n",
      "Literature recommendations\n",
      "- Dorschel J: Praxishandbuch Big Data. Wirtschaft-Recht-Technik, Springer Gabler, Heidelberg, 2015\n",
      "- Davenport, Thomas. Big data at work: dispelling the myths, uncovering the opportunities. Harvard Business Review Press, 2014.\n",
      "- Fan, W., & Bifet, A. (2013). Mining big data: current status, and forecast to the future. ACM SIGKDD explorations newsletter, 14(2), 1-5.\n",
      "- Aggarwal, C. C. (2015). Data mining: the textbook (Vol. 1). New York: Springer.\n",
      "- Gupta, G. K. (2014). Introduction to data mining with case studies. PHI Learning Pvt. Ltd.\n",
      "- Kantardzic, M. (2011). Data mining: concepts, models, methods, and algorithms. John Wiley & Sons.\n",
      "- Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). The KDD process for extracting useful knowledge from volumes of data.\n",
      "Communications of the ACM, 39(11), 27-34.\n",
      "- Azevedo, A., & Santos, M. F. (2008). KDD, SEMMA and CRISP-DM: a parallel overview. IADS-DM.\n",
      "- Schröer, C., Kruse, F., & Gómez, J. M. (2021). A systematic literature review on applying CRISP-DM process model. Procedia Computer\n",
      "Science, 181, 526-534.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 5ADSA Case Study I\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "3628\n",
      "Credits\n",
      "8 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 200 hours\n",
      "- presence time: 60 hours\n",
      "- self study: 140 hoursTeaching and learning methods\n",
      "Casework\n",
      "Case Work and reﬂection\n",
      "Group Work\n",
      "Project development\n",
      "ExerciseLanguage\n",
      "English\n",
      "Participation requirements\n",
      "First Steps into Case StudiesExamination\n",
      "Project workCourse work\n",
      "Case Study, Practical work, Presentation, Praxis Situation, Project work\n",
      "Constructive Alignment\n",
      "The examination forms project work and presentation evaluates the students‘ abilities to application-speciﬁcally document their learning\n",
      "progress. Through the application of the skills and competencies acquired in the previous modules during “case studies 1” the students are\n",
      "given the opportunity to intensify their knowledge in a holistic manner. Furthermore, the students learn to evaluate the applicability and\n",
      "cooperation of methods, techniques and tools in a context of a certain project. The project work is an adequate examination for project\n",
      "module because of its emphasis on the visualization and communication of the results/recommendations\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students are able to transfer a company problem into a Big Data question as well as planning and performing it afterwards.\n",
      "- They identify the data being necessary for this question and are able to estimate properly the value of the data in context of the problem.\n",
      "- They may prepare data for Data Mining.\n",
      "- They execute a data mining analysis with the help of established tools and software.\n",
      "- They are capable to adequately visualize and communicate the results in context of the developed problem\n",
      "Methodological competence\n",
      "- The students are able to speciﬁcally apply creativity techniques to develop a problem and to identify required data.\n",
      "- They evaluate properly the applicability of methods and tools for the diﬀerent phases of the Big Data project in context of a certain project\n",
      "and are able to select and execute the adequate methods.\n",
      "- They interpret and evaluate the results of the analytics process with regard to the developed Big Data problem\n",
      "Self competence\n",
      "- The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks (i.e.\n",
      "occupation, private life and studies) simultaneously.\n",
      "Social competence\n",
      "- The students know the fundamental tools to organize working processes in virtual teams and approaches to dealing with team-intern\n",
      "conﬂicts. They can use both tools to accomplish results mutually.\n",
      "Module content\n",
      "- Project management\n",
      "- Agile Data Science\n",
      "- SCRUM\n",
      "- Organization and management\n",
      "- Creativity techniques and formulations of problems\n",
      "- Big Data architectures\n",
      "- Data Mining / Text Mining\n",
      "- Storage and Retrieval Tools\n",
      "- Data Mining Tools, methods, and techniquesSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 6Literature recommendations\n",
      "- Freiknecht J: Big Data in der Praxis: Lösungen mit Hadoop, Hbase und Hive. Daten speichern, aufbereiten, visualisieren, 2014.\n",
      "- Han J et al.: Data Mining: Concepts and Techniques, Elsevier/Morgan Kaufmann, Amsterdam, 2006.\n",
      "- Hand D. et al.: Principles of Data Mining, MIT Press, Cambridge (Mass.)/London, 2001.\n",
      "- Kantardzic M: Data Mining, Wiley, 2011.\n",
      "- Schwaber, K. (1997). Scrum development process. In Business Object Design and Implementation: OOPSLA’95 Workshop Proceedings 16\n",
      "October 1995, Austin, Texas (pp. 117-134). Springer London.\n",
      "- Larson, D., & Chang, V. (2016). A review and future direction of agile, business intelligence, analytics and data science. International\n",
      "Journal of Information Management, 36(5), 700-710.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 7ADSA Master Thesis Project\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "A-1003-2\n",
      "Credits\n",
      "27 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 675 hours\n",
      "- presence time: 67 hours\n",
      "- self study: 608 hoursTeaching and learning methods\n",
      "Laboratory\n",
      "Problem-oriented learningLanguage\n",
      "English\n",
      "Participation requirements\n",
      "Scientic work and ethicsExamination\n",
      "Colloquium and\n",
      "ThesisCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The thesis project's main focus is the work's scientiﬁc content, which will be submitted in written form. This way, the student can also prove\n",
      "their ability to apply scientiﬁc methods, which are not restricted to a written text only. Initially, a research question and the structure of the\n",
      "thesis have to be done and conﬁrmed by the supervisor. The master thesis can also be carried out in a company.\n",
      "The student demonstrates the capability to apply logical thinking to gather information and draw valid results from it to earn the title of\n",
      "Master of Science. Elements such as experiments or modeling can be included. The students understand the fundamental aspects of\n",
      "scientiﬁc work. They can structure and exert the cognitive process from the original problem to systematically answering a well-deﬁned\n",
      "scientiﬁc problem individually. They know the essential methods and tools for developing scientiﬁc work and may critically reﬂect on the\n",
      "results. They are capable of a qualitative and quantitative evaluation of the method used. The students may work independently on a\n",
      "scientiﬁcally applied data science problem using common scientiﬁc methods and gain new insights. Finally, the student is required to\n",
      "present their ﬁndings to the supervisors. In the presentation, the candidate proves their ability to summarize the most important content of\n",
      "his/her thesis coherently and comprehensively. During this examination, the student needs to justify his/her choices and conclusions\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "Learning outcomes professional skills and methodology\n",
      "- The students are able to structure their scientiﬁc cognitive process. They yield results according to the criteria of good scientiﬁc work (i.e.\n",
      "objectivity, validity and reliability)\n",
      "- They are capable of giving proper qualitative as well as quantitative judgments regarding the adequate use of scientiﬁc methods\n",
      "- They may critically evaluate and reﬂect on the gained results and method.\n",
      "- They intensify functional and scientiﬁc-methodological competences learned during the master program over the deﬁned problem of the\n",
      "master thesis project.\n",
      "- They are able to transfer the knowledge of “Scientiﬁc work and ethics” on the master thesis project.\n",
      "- They are competent to lead and moderate a functional scientiﬁc discussion to analytically-critically reﬂect scientiﬁc results and use of\n",
      "methods.\n",
      "Methodological competence\n",
      "- The student are able to integrate the knowledge and abilities which they accumulated during the Master course into the thesis.\n",
      "- They are able to do independent research under the guidance of a supervisor so that the thesis extends existing knowledge with new\n",
      "professional insights.\n",
      "- They demonstrate the ability to investigate, discuss, evaluate, and verify information on the scientiﬁc level.\n",
      "- The student demonstrates the ability to apply research methods to their own project, select an appropriate research question and give a\n",
      "suitable, logical structure to the thesis project.\n",
      "Self competence\n",
      "- The students are able to perform research work systematically and independently as well as to reﬂect insights using iterative thinking\n",
      "processes.\n",
      "- They are competent to structure the scientiﬁc cognitive process of the master thesis project regarding scheduling, systematic structuring\n",
      "and gaining of insights.\n",
      "Social competence\n",
      "- The students are able to evaluate results, gain in insights on a functional basis and may verbalize constructive feedback.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 8- They are capable of leading a functional discussion to gain insights.\n",
      "Module content\n",
      "Master Thesis Guidelines:\n",
      "The master’s thesis is a carefully argued scholarly paper of approximately 20,000 words (roughly 80 pages). It should present an original\n",
      "argument that is carefully documented from primary and/or secondary sources. The thesis must have a substantial research component and\n",
      "a focus that falls within arts and science, and it must be written under the guidance of an advisor. As the ﬁnal element in the master’s\n",
      "degree, the master's thesis allows students to demonstrate expertise in the chosen research area.\n",
      "After doing the initial research on their topic, students prepare a 1-2 paragraph abstract, a preliminary bibliography (approximately ten to\n",
      "ﬁfteen books or journal articles), and a brief outline before approaching a possible advisor. These will help students to convince their future\n",
      "advisors of the value and interest of their project. Once a faculty member has agreed to be the advisor, students need to discuss the\n",
      "anticipated graduation date and agree on a timetable for meetings and submission of drafts. It is each student’s responsibility to keep\n",
      "his/her advisor apprised of the work’s progress.\n",
      "After a student has reﬁned his/her topic and his/her advisor has approved it, the student needs to complete the Application for Approval of\n",
      "Master’s Thesis Topic, have the advisor sign it, and submit it to the oﬃce.\n",
      "In most cases, students and advisors need to meet three or four times: initially to ﬁnalize a topic and to review the ﬁrst or second draft.\n",
      "Remember that the advisor must have enough time to read and evaluate the work before returning it to the student with comments and\n",
      "that the student must have time to incorporate those comments. Don’t expect the advisor to return the thesis in a day or two, whether it is\n",
      "an early draft or the ﬁnal copy. Students should also be prepared for the possibility that their advisor will request substantial changes in the\n",
      "thesis. Do not expect that the draft thesis will require only minor corrections or that the proposed ﬁnal version will necessarily be approved\n",
      "without further changes. It is each student’s responsibility to see that the ﬁnal copy is free from spelling and grammatical errors; the advisor\n",
      "is not responsible for line-by-line editing.\n",
      "Literature recommendations\n",
      "Links\n",
      "- Google Scholar\n",
      "- DBLP\n",
      "- IEEE Computer Society\n",
      "- IEEE TVCG camera ready document guidelines\n",
      "Literature management\n",
      "- CitaviSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 9\n",
      "Scope Data Engineering and ProgrammingSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 10ADSA Data Engineering 1: Big Data Databases\n",
      "Module responsibility\n",
      "Prof. Dr. Binh VuLevel\n",
      "MasterModule Nr.\n",
      "4087\n",
      "Credits\n",
      "5 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 125 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 50 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Problem-oriented learning\n",
      "Seminar\n",
      "ExerciseLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "This introductory course focuses on the fundamental concepts and technologies of data engineering. The objective is to provide students\n",
      "with a comprehensive understanding of the engineering principles behind Big Data projects, including the handling and processing of large\n",
      "amounts of data. The course covers various technological foundations of Big Data processing, including scale-out architectures, the\n",
      "MapReduce paradigm, and popular technologies like Hadoop and Spark. The students will learn about the design of Wide-Column databases\n",
      "and the advantages they oﬀer. The course also introduces the concepts of distributed systems and their impact on NoSQL databases,\n",
      "including eventual consistency. The students will also be introduced to the basics of DevOps and DataOps, demonstrated through Docker,\n",
      "Kubernetes, and Terraform. As part of the course project, students will apply the concepts they have learned by creating a data pipeline,\n",
      "covering the main steps of data collection, storage, and retrieval through a practical example. Students are tasked with building a big data\n",
      "architecture for a speciﬁc application scenario. To accomplish this goal, they work to develop the necessary knowledge and skills for\n",
      "planning and constructing eﬀective architecture. This process begins by gathering the knowledge and abilities of all team members and\n",
      "identifying any gaps in knowledge or expertise. The module also includes studying practical examples and hands-on exercises using the\n",
      "most commonly used big data technologies. Throughout the module, students document their learning progress using a practical journal\n",
      "and presentations and ultimately demonstrate their understanding and mastery of the subject through a ﬁnal project presentation.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- Upon completion of this course, the students have a comprehensive understanding of the common Big Data architectures and are able to\n",
      "distinguish between them.\n",
      "- They have the ability to plan and construct a complete Big Data pipeline for data storage and retrieval and have gained practical\n",
      "experience with a variety of data engineering tools.\n",
      "- The students are equipped with the knowledge to evaluate and make informed decisions on selecting the appropriate Big Data\n",
      "technologies to fulﬁll speciﬁc requirements.\n",
      "- The students have a solid foundation in data engineering and are able to apply their skills and knowledge to real-world challenges.\n",
      "Methodological competence\n",
      "- The students' problem-solving abilities have been built and sharpened, positioning them as valuable assets to their future employers and\n",
      "clients.\n",
      "- They are well-equipped to tackle the complex challenges faced by data engineers in the industry, increasing their chances of success in\n",
      "their careers.\n",
      "Self competence\n",
      "- The students develop a strong sense of competence and conﬁdence in their ability to detect and close gaps in their knowledge\n",
      "independently.\n",
      "Social competence\n",
      "- The students develop the necessary skills to work eﬀectively in virtual teams and have the conﬁdence to utilize the collective knowledge\n",
      "and abilities of their team members to achieve their objectives.\n",
      "Module content\n",
      "- Introduction to Data EngineeringSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 11- Data Engineering Lifecycle\n",
      "- CAP Theorem, BASE Principle\n",
      "- Relational Databases: ACID model, MySQL\n",
      "- NoSQL Databases: MongoDB\n",
      "- Big Column: Cassandra & HBase\n",
      "- Key Value Store & Graph Database: Redis, Neo4j\n",
      "- Data Acquisition, Data Crawling\n",
      "- DevOps & DataOps\n",
      "- Containerization: Docker & Kubernetes\n",
      "- Infrastructure as Code: Terraform\n",
      "- Continuous Integration & Continuous Delivery\n",
      "- Hadoop Ecosystem\n",
      "- Cloud Introduction\n",
      "- Cache and Memory-based Storage Systems\n",
      "- Indexing, Partitioning, and Clustering\n",
      "- HDFS\n",
      "Literature recommendations\n",
      "Bengtfort B & Kim J: Data Analytics with Hadoop - An Introduction for Data Scientists\n",
      "Freiknecht J: Big Data in der Praxis - Lösungen mit Hadoop, Hbase und Hive. Daten speichern, aufbereiten, visualisieren\n",
      "Grus J: Data Science from Scratch\n",
      "Redmond E & Wilson JR: Seven Databases in Seven Weeks - A Guide to Modern Databases and the NoSQL Movement\n",
      "White T: Hadoop: The Deﬁnite Guide\n",
      "Recent research literature from peer-reviewed journalsSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 12ADSA Data Engineering 2: Big Data Architectures\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "4088\n",
      "Credits\n",
      "6 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 150 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 75 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Seminar\n",
      "Tutorium\n",
      "ExerciseLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "Based on the ﬁrst module on data engineering, the students continue the exploration of data engineering tasks and gain experience with\n",
      "additional tools. Beginning with the problem to guarantee the quality of the provided raw data in an application context, the students\n",
      "develop the necessary know-how in the ﬁeld of data management. Starting with collecting the knowledge distributed amongst the team\n",
      "members, the students recognized and closed knowledge gaps by researching and exercising in their respective groups. The students prove\n",
      "their gain in competencies in project work as well as a ﬁnal presentation.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students have a comprehensive knowledge of Big Data architectures.\n",
      "- They are able to evaluate and select Big Data technologies adequately for fulﬁlling given requirements.\n",
      "- They are capable of planning and building a complete Big Data pipeline for various purposes.\n",
      "- The students have gained extended hands-on experience with various data engineering tools.\n",
      "- The students are able to evaluate and select Big Data technologies from a large range of potential options.\n",
      "Methodological competence\n",
      "- The students increase their competencies in problem solving.\n",
      "Self competence\n",
      "- The students improve their competence to detect and close gaps in knowledge independently\n",
      "Social competence\n",
      "- The students intensify their ability to work in virtual teams and are also capable to use the knowledge and abilities distributed amongst the\n",
      "team to solve a problem in a target-oriented manner.\n",
      "Module content\n",
      "Data Architecture\n",
      "• What is data architecture\n",
      "• Principles of good architecture\n",
      "• Types of Data Architecture\n",
      "• Data Warehouse\n",
      "• Data Lake\n",
      "• Lambda Architecture\n",
      "• Kappa Architecture\n",
      "• Dataﬂow model\n",
      "• Data Mesh\n",
      "Distributed Processing\n",
      "• Infrastructure: Spark Cluster (DE)\n",
      "• Streaming\n",
      "• Queue: Kafka, Pub/SubSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 13• Processing: Spark Streaming\n",
      "Data Pipeline with Google Cloud Services\n",
      "• Storage components on GCP (GCS & Dataproc HDFS)\n",
      "• Loading Data into a Data-warehousing tool on GCP (BigQuery)\n",
      "• Handling/Writing Data Orchestration and dependencies using Apache Airﬂow (Google Composer)\n",
      "• Batch Data ingestion using CloudSql or Apache Airﬂow\n",
      "• Real Time data streaming and analytics using the latest API, Spark Structured Streaming\n",
      "• Micro batching using PySpark streaming & Hive on Dataproc\n",
      "Deployment\n",
      "• Backend Engineering: Flask API\n",
      "Literature recommendations\n",
      "| Recent research literature from peer-reviewed journals\n",
      "| Cielen D & Meysman A: Introducing Data Science, Manning Verlag, 2016\n",
      "| Garofalakis M & Gehrke J: Data Stream Management: Processing High-Speed Data Streams (Data-Centric Systems and Applications),\n",
      "Springer Verlag, 2016\n",
      "| Komball R & Caserta J: The Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data,\n",
      "Kimball Group, 2004\n",
      "| Lindstrom M: Small Data: Was Kunden wirklich wollen - wie man aus Hinweisen geniale Schlüsse zieht, Plassen Verlag, 2016\n",
      "| Mitchell MN: Data Management Using Stata: A Practical Handbook, Stata Press, 2010\n",
      "| Rossak I & Hanser C: Datenintegration: Integrationsansätze, Beispielszenarien, Problemlösungen, Talend Open Studio, 2013\n",
      "| Thome G & Solbach W: Grundlagen und Modelle des Information Lifecycle Management, Xpert.press, 2007SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 14ADSA: Big Data Programming: Python\n",
      "Module responsibility\n",
      "Dr.-Ing. Kamellia ReshadiLevel\n",
      "MasterModule Nr.\n",
      "3642\n",
      "Credits\n",
      "6 (unbenotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 150 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 75 hoursTeaching and learning methods\n",
      "Laboratory\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project work\n",
      "The basis for calculation is generally 1 ECTS = 25 hrs. Deviations are covered exclusively by\n",
      "Appendix 2 (Bachelor) and 2a (Master) of the SPO.Course work\n",
      "Project work\n",
      "Constructive Alignment\n",
      "The project work is designed to provide participants with hands-on experience in project management by simulating a real-world project\n",
      "scenario. By working in teams, participants not only gain practical knowledge in project management methods but also develop their social\n",
      "and self-competencies. They learn to communicate and collaborate eﬀectively with their team members and work towards a common goal.\n",
      "Additionally, the project work allows participants to enhance their software development skills by utilizing modern software engineering\n",
      "concepts and tools. Through the examination form of project work, students are evaluated on their ability to write clean code and apply\n",
      "programming methodologies. Furthermore, the project work provides an opportunity for students to demonstrate their advanced scientiﬁc\n",
      "abilities by conducting research and presenting their ﬁndings to their peers. The presentation also helps students develop their public\n",
      "speaking skills and teaches them to defend their ideas in front of an audience. Overall, the project work serves as a comprehensive learning\n",
      "experience that prepares students for their future careers in the ﬁeld of software development and project management.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students know the fundamentals of the Python programming languages and can develop and implement more complex programs\n",
      "independently.\n",
      "- They can work with code repository management.\n",
      "- They can develop clean code in python programming.\n",
      "- They can independently develop simple machine-learning procedures.\n",
      "Methodological competence\n",
      "- The students are equipped with fundamental knowledge in programming and are provided with an understanding of the latest\n",
      "developments in cloud-based programming. By learning the basics of programming and exploring programming in cloud environments,\n",
      "students are well-prepared to tackle real-world challenges and are equipped with the skills required to build innovative solutions.\n",
      "-The program places a strong emphasis on developing the problem-solving and logic-building competencies of the students. Throughout the\n",
      "curriculum, students are presented with various problem-solving challenges that require them to think critically and logically. As a result,\n",
      "they become adept at breaking down complex problems into manageable components, identifying patterns, and developing eﬀective\n",
      "solutions. The development of these competencies not only helps students succeed academically but also prepares them for success in their\n",
      "future careers. They will be able to approach any problem systematically and develop logical and innovative solutions that drive business\n",
      "outcomes.\n",
      "- The students increase their competencies in problem-solving and logic building.\n",
      "Self competence\n",
      "- The students are provided with a comprehensive understanding of programming fundamentals, allowing them to approach coding\n",
      "systematically and with conﬁdence. They are equipped with the skills necessary to analyze problems and identify the underlying principles\n",
      "and concepts required to develop eﬀective solutions. With a strong foundation in coding, students are well-prepared to create complex\n",
      "applications and software systems that meet the demands of modern businesses and organizations.\n",
      "- The program emphasizes the development of teamwork and problem-solving skills. Through hands-on projects and collaborative\n",
      "assignments, students learn to work eﬀectively in teams and develop solutions to complex problems. They learn how to communicate\n",
      "eﬀectively, delegate tasks, and utilize each other's strengths to achieve a common goal. Additionally, students are encouraged to solve\n",
      "problems individually, developing their critical thinking skills and independent problem-solving abilities. With a focus on teamwork and\n",
      "individual skill development, students graduate with a well-rounded set of competencies that prepare them for success in any environment.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 15In summary, this program provides students with the skills and knowledge required to approach programming systematically and solve\n",
      "complex problems eﬀectively. With a focus on teamwork and individual skill development, students graduate with a comprehensive set of\n",
      "competencies that prepare them for success in the rapidly evolving ﬁeld of technology. They are well-equipped to analyze and understand\n",
      "problems, develop innovative solutions, and work eﬀectively in teams to achieve common goals.\n",
      "Social competence\n",
      "- The students improve their ability to analyse problems, to break large problems down into digestible portions.\n",
      "- By presenting their own and other’s work, they also improve their communication skills\n",
      "- The student learns to work with a team comprising of diﬀerent skilled people and are also capable of using the knowledge and abilities\n",
      "among the team to solve a problem in a target-oriented manner.\n",
      "Module content\n",
      "Introduction to the Python and R programming language\n",
      "- Source code management, revision and branch and version management, refactoring\n",
      "- Software documentation and tools\n",
      "- Test driven development and architecture\n",
      "- Coding design principles, clean coding, safe coding\n",
      "- Introduction to the Python programming language: Object oriented python programming, Module and Package management, Iterators and\n",
      "decorators, Context managers\n",
      "- Introduction to programming in Cloud: Introduction to IaaS, PaaS, and SaaS, Container management and container orchestrations\n",
      "- Fundamentals of using public python packages: NumPy, Pandas\n",
      "- Advanced Python Programming: Multi-threading, Multi-processing, Asynchronous Python Programming\n",
      "- Web development in Python using Flask\n",
      "- Python code packaging and deployment: Cloud deployment and DevOps, Understanding continuous integration, continuous delivery and\n",
      "development on a cloud platform, Setup.py management, Deployment of python code in a productive environment (Docker compose, Cloud\n",
      "Foundary, Heroku, Kubernetes cluster management)\n",
      "Literature recommendations\n",
      "- Martin RC: The Clean Coder: A Code of Conduct for Professional Programmers, 1st edition, Prentice Hall, 2011.  \n",
      "- Martin RC: Clean Architecture: A Craftsman’s Guide to Software Structure and Design, 1st edition, Prentice Hall, 2017.  \n",
      "- Ramalho, L: Fluent Python, 1st edition, O’Reilly, 2015  \n",
      "- Wickham H & Grolemund G, R for Data Science, 1st edition, O’Reilly, 2017\n",
      "- McKinney, Wes. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media, 2017.\n",
      "- Grus, Joel. Data Science from Scratch: First Principles with Python. O'Reilly Media, 2015.\n",
      "- Fandango, Armando. Big Data Analytics with Python. Packt Publishing, 2016.\n",
      "- Albon, Chris. Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning. O'Reilly Media, 2018.\n",
      "- Raschka, Sebastian. Python Machine Learning. Packt Publishing, 2015.\n",
      "- Witten, Ian H., Eibe Frank, and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, 2016.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 16\n",
      "Scope Data ManagementSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 17ADSA Data Management 1: Data Acquisition and Data\n",
      "Cleaning\n",
      "Module responsibility\n",
      "Prof. Dr. Theodoros SoldatosLevel\n",
      "MasterModule Nr.\n",
      "4089\n",
      "Credits\n",
      "4 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 100 hours\n",
      "- presence time: 50 hours\n",
      "- self study: 50 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The module aims at preparing students to address the challenge of ensuring the quality of raw data in an application context, and develop\n",
      "the necessary expertise in data management. The module also includes classes that introduce publically available data proﬁling/cleaning\n",
      "tools. The students demonstrate their competence via a project work and a ﬁnal presentation.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- Students are able to understand the importance of data quality in the context of data analysis.\n",
      "- They can apply data acquisition and cleaning techniques to improve the quality of data.\n",
      "- The students are able to identify areas for improvement.\n",
      "- The students are able to apply data proﬁling and cleansing techniques.\n",
      "- The students are able to demonstrate competence in main project work.\n",
      "- The students should be able not only to apply data quality assessment and cleaning techniques to real-world data management scenarios\n",
      "but also comprehensively communicate their ﬁndings. This will enhance the value and reliability of data-driven decisions and will contribute\n",
      "to a culture of data quality in their future workplaces.\n",
      "Methodological competence\n",
      "- The students know methods and tools for cleaning data and can use a particular spectrum of them.\n",
      "- The students can apply their theoretical competency practically, using selected software systems.\n",
      "- They know the essential methods of data cleansing and apply the introduced concepts correctly by solving speciﬁc problems and\n",
      "interpreting the results.\n",
      "Self competence\n",
      "- The students are competent to structure and recognize the ambiguity in the raw data and systematically clean them.\n",
      "- The students are capable of applying quality control techniques on raw datasets and of creating a technical reusable solution for cleaning\n",
      "the raw data.\n",
      "Social competence\n",
      "- The students are capable of analyzing and understanding the problems in the data individually and may develop user-centered solutions\n",
      "based on proﬁling results.\n",
      "- The students can compare, evaluate and discuss the methods applied for cleansing the data\n",
      "Module content\n",
      "Main content includes:\n",
      "- Five V’s of Big Data\n",
      "- Data identiﬁcation, veriﬁcation, cleansing\n",
      "- Data quality\n",
      "- Data proﬁling\n",
      "- Data formatting, cleansing\n",
      "- MetadataSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 18Literature recommendations\n",
      "| Barton RD: Talend Open Studio Cookbook, Packt Publishing, 2013\n",
      "| Blokdyk G: Data transformation: A Clear and Concise Reference, CreateSpace Independent Publishing Platform, 2018\n",
      "| Cielen D & Meysman A: Introducing Data Science, Manning Verlag, 2016\n",
      "| Halevy A et al.: Principles of Data Integration, Elsevier LTD, 2012.\n",
      "| Mitchell MN: Data Management Using Stata: A Practical Handbook, Stata Press, 2010\n",
      "| Verborgh R & De Wilde M: Using OpenReﬁne, Packt Publishing, 2013\n",
      "| Recent research literature from peer-reviewed journalsSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 19ADSA Data Management 2: Data Curation and Data\n",
      "Management\n",
      "Module responsibility\n",
      "Prof. Dr. Binh VuLevel\n",
      "MasterModule Nr.\n",
      "4090\n",
      "Credits\n",
      "4 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 100 hours\n",
      "- presence time: 50 hours\n",
      "- self study: 50 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The ﬁrst course in Data Management focuses on ensuring the quality of raw data in an application context. This is achieved by covering the\n",
      "relevant issues and challenges that arise in this area. In the second course, students delve deeper into the ﬁeld of data management,\n",
      "exploring advanced concepts and techniques. The module starts with a thorough investigation of the raw data, including proﬁling, in order\n",
      "to gain a deeper understanding of it. Each student is then tasked with designing a SMART data pipeline to transform the data and answer a\n",
      "speciﬁc business question. This module is supported by classes that impart a comprehensive understanding of various data management\n",
      "methods. To demonstrate their mastery of these concepts, students are required to present their ﬁndings in a ﬁnal presentation,\n",
      "showcasing the methods and techniques they have learned throughout the course.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- Students will have a deep understanding of the methods and technologies involved in managing and curating massive amounts of data in\n",
      "a data warehouse with an automated ETL pipeline.\n",
      "- They will be able to assess the quantity of data in terms of volume and velocity and its potential value in a well-deﬁned scenario.\n",
      "- Students will be able to integrate data from various sources and formats, consolidate their attributes, and design a solution to address a\n",
      "speciﬁc hypothesis.\n",
      "- They will be equipped to cleanse, homogenize, aggregate, transform, and prepare collected data in accordance with a deﬁned application\n",
      "context.\n",
      "- Students will be aware of the role of open-source and commercial tools and programming libraries in designing eﬀective ETL pipelines for a\n",
      "data warehouse.\n",
      "Methodological competence\n",
      "- Upon completion of this module, the students will have a comprehensive understanding of programming libraries, methods, and tools for\n",
      "automating the management and curation of data through ETL pipelines.\n",
      "- They will be familiar with the process of transforming raw data into actionable information through the use of either OLAP techniques or\n",
      "machine learning models, enabling them to make informed decisions.\n",
      "- Students will have a broad understanding of the various applications and methods utilized in industry for transforming data required for\n",
      "business decision-making.\n",
      "Self competence\n",
      "- The students will have the necessary competencies to structure and comprehend raw data and to design a robust ETL pipeline that\n",
      "addresses the speciﬁc business needs of the use case.\n",
      "- To demonstrate their understanding, they will be tasked with creating an ETL pipeline that integrates OLAP or ML-based techniques that\n",
      "they have learned in the course. This pipeline will serve as evidence of their mastery of the concepts covered in the module and will be\n",
      "showcased through a ﬁnal presentation and technical demonstration.\n",
      "Social competence\n",
      "- Students possess the skills to identify and comprehend the requirements of a business problem and apply the knowledge and techniques\n",
      "learned in class to develop eﬀective solutions to address the business use case.\n",
      "- Students have the competence to make informed decisions, choose the right technology, and use it eﬀectively to tackle the business\n",
      "challenge.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 20Module content\n",
      "- Data Extraction, Transformation, and Integration\n",
      "- Businesses & Smart Data\n",
      "- Data Modelling\n",
      "- Data Lifecycle Management\n",
      "- Star & Snowﬂake Schema\n",
      "- Big Data Processing\n",
      "- Batch Processing\n",
      "- Stream Processing\n",
      "- MapReduce\n",
      "- Spark\n",
      "- ETL & ELT\n",
      "- Data Warehouse\n",
      "- BigQuery\n",
      "- On-Premises & Cloud Data Warehouse\n",
      "- Data Cubes & OLAP Operations\n",
      "Literature recommendations\n",
      "Barton RD: Talend Open Studio Cookbook\n",
      "Blokdyk G: Data transformation: A Clear and Concise Reference\n",
      "Ramez Elmasri, Shamkant B. Navathe: Fundamentals of Database Systems, Addison-Wesley, 2015\n",
      "Chambers B & Zaharu M: The Deﬁnitive Guide: Big Data Processing Made Simple\n",
      "Huy Nguyen, Ha Pham, Cedric Chin: The Analytics Setup Guidebook, Holistics, 2020\n",
      "Garofalakis M & Gehrke J: Data Stream Management: Processing High-Speed Data Streams (DataCentric Systems and Applications)\n",
      "Bernard Marr: Big Data In Practice: How 45 successful companies used big data analytics to deliver extraordinary results, Wiley, 2016\n",
      "Halevy A et al.: Principles of Data Integration\n",
      "Komball R & Caserta J: The Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data,\n",
      "Wiley, 2004\n",
      "Bernard Marr: Big Data - Using SMART Big Data, Analytics and Metrics To Make Better Decisions and Improve Performance, Wiley, 2015\n",
      "Paulraj Ponniah: Data Warehousing Fundamentals for IT Professionals, Wiley, 2010SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 21\n",
      "Scope Data AnalyticsSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 22ADSA Data Analytics 1: Statistics and Machine\n",
      "Learning\n",
      "Module responsibility\n",
      "Prof. Dr. Theodoros SoldatosLevel\n",
      "MasterModule Nr.\n",
      "4091\n",
      "Credits\n",
      "8 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 200 hours\n",
      "- presence time: 100 hours\n",
      "- self study: 100 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Practical workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "During the module, students learn essential tools and methods of inductive and descriptive statistics as well as machine learning\n",
      "fundamentals. Students learn also how to become independent analysts, by using common public tools and software (currently, main focus\n",
      "is on R language). In this way, students develop an understanding of the applicability, prerequisites and interpretation purposes of the\n",
      "various statistical methods taught in the module.The course consists of theoretical lectures as well as of practical sessions where the\n",
      "concepts and methods presented in the theoretical lectures are implemented and practiced on smaller and larger problems using statistical\n",
      "software (currently, the R language). In this setting, students learn theory alongside with practical implementation. Similarly, the exam\n",
      "consists of a practical test which tests both theory and practical concepts.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- Students are able to demonstrate proﬁciency in applying relevant statistical concepts and methods adequately.\n",
      "- Students understand the purpose of each method in context and can utilize them eﬀectively to solve problems.\n",
      "- Students are able to perform complex analyses and to evaluate results analytically.\n",
      "- Students are able to adapt their knowledge and skills to tackle new and unfamiliar projects with a goal-oriented approach.\n",
      "Methodological competence\n",
      "Learning outcomes include:\n",
      "- The ability to analyze and understand data problems.\n",
      "- The ability to apply theoretical competency practically, by using taught software tool(s).\n",
      "- The ability to evaluate results in an analytical manner.\n",
      "Self competence\n",
      "Learning outcomes include:\n",
      "- The ability to learn new methods independently.\n",
      "- The ability to compare and discuss applied methods and own solutions.\n",
      "Social competence\n",
      "Learning outcomes include:\n",
      "- The ability to tackle problems independently, both individually and as a team member.\n",
      "- The ability to communicate implemented choices and solutions.\n",
      "Module content\n",
      "Main content includes:\n",
      "- Introduction of key concepts of logic and statistics\n",
      "- Introduction of data mining methods\n",
      "- Data mining as a process\n",
      "- Descriptive Statistics: Measure of central tendency, Dispersion parameters, Variable distribution\n",
      "- Inferential Statistics: Corelation and Co-variance, Hypothesis testing, Analysis of count dataSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 23- Analysis of Variance (ANOVA, MANOVA)\n",
      "- Time Series Analysis\n",
      "- Machine Learning\n",
      "- Supervised Machine Learning: Regression, Classsiﬁcation\n",
      "- Unsupervised Machine Learning: Clustering, Association, Survival Analysis\n",
      "For more comprehensive training, theorotical and practical sessions are co-ordinated and delivered in sync with each other, as much as\n",
      "possible.\n",
      "Literature recommendations\n",
      "| Schmueli G et al: Data Mining for Business Analytics, Concepts, Techniques and Applications in R, Wiley 2018\n",
      "| James G, Witten D, Hastie T, Tibshirani R: An Introduction to Statistical Learning with Applications in R, Springer 2017 (8th edition)\n",
      "| Marsland S: Machine Learning An Algorithmic Perspective CRC Press, 2nd Edition\n",
      "| Bruce P, Bruce A: Practical Statistics for Data Scientists, 50 Essential Concepts, O’Reilly, 2017\n",
      "| Reinhart A: Statistics done wrong, No Starch Press\n",
      "| Backhaus K, Erichson B, Plinke W, Weiber R: Multivariate Analysemethoden, Springer-Gabler 15th edition\n",
      "| Bamberg G & Baur F: Statistik, 12. Auﬂage, Oldenbourg, München/Wien, 2006.\n",
      "| Fahrmeir L et al.: Statistik: Der Weg zur Datenanalyse, 7. Auﬂage, Springer, Berlin, 2010\n",
      "| Handl A: Multivariate Analysemethoden: Theorie und Praxis multivariater Verfahren unter besonderer Berücksichtigung von S-Plus, 2.\n",
      "Auﬂage, Springer, Berlin, 2010.\n",
      "| Hartung J: Statistik - Lehr- und Handbuch der angewandten Statistik, 14. Auﬂage, Oldenbourg Verlag, München, 2005.\n",
      "| Mosler K & Schmid F: Beschreibende Statistik und Wirtschaftsstatistik, 4. Auﬂage, Springer Verlag, Heidelberg, 2009\n",
      "| Schlittgen R: Multivariate Statistik, Oldenbourg, München/Wien, 2009.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 24ADSA Data Analytics 2: Text Mining and Natural\n",
      "Language Processing\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "4092\n",
      "Credits\n",
      "7 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 174 hours\n",
      "- presence time: 87 hours\n",
      "- self study: 87 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Seminar\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Practical workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The students know the challenges posed by Big data on structured data acquisition and their processing of the information, helping make\n",
      "business-relevant decisions. They are capable of structuring complex problems and performing systematic research work. They may analyze\n",
      "and prepare reports based on huge data amounts to generate user-centered knowledge. They can select adequate text mining and natural\n",
      "language processing techniques to solve speciﬁc business-relevant problems and to visualize the gained results appropriately. Finally, the\n",
      "results are evaluated critically regarding their validity. The students prove their application-oriented knowledge and competencies by\n",
      "solving well-deﬁned problems and exercises during a practical examination. This form of examination also evaluates the student’s\n",
      "interpretation capabilities.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students know the essential methods for the procedural steps of the preparation of text mining methods: preparation of raw data,\n",
      "structuring and reﬁning.\n",
      "- They are capable to perform and apply the most important methods of context analyses\n",
      "- They are able to perform more complex analyses and to evaluate the results in a functional way.\n",
      "- They may critically reﬂect the validity of the results regarding qualitative as well as quantitative aspects.\n",
      "- They know the most important web mining tools and are capable to apply them in relevant practical exercises.\n",
      "- They identify state of the art concept to visualize data mining results.\n",
      "Methodological competence\n",
      "- The students may apply the introduced methods correctly by solving speciﬁc problems and interpreting the results adequately.\n",
      "- They know the essential methods in text analysis.\n",
      "- They can apply the methods learned by using speciﬁc software solutions and may critically reﬂect the result’s validity\n",
      "Social competence\n",
      "- The students are capable to analyze the methods used as well as the results gained in their entity and evaluate them beneﬁt-oriented\n",
      "during a business-speciﬁc decision-making process.\n",
      "Module content\n",
      "- Refresher: Machine Learning from Analytics 1\n",
      "- Python for Data Science\n",
      "- Text Mining: Text Preprocessing, Feature Creation, Feature Selection, Pattern Discovery\n",
      "- Natural Language Processing\n",
      "- Text Processing\n",
      "- Information Extraction\n",
      "- String Similarity\n",
      "- Information Retrieval\n",
      "- Ranked Retrieval\n",
      "- Model SelectionSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 25- Feature Engineering: Feature Reduction, Feature Scaling, Feature Encoding, Feature Selection\n",
      "Literature recommendations\n",
      "| Dorschel J: Praxishandbuch Big Data. Wirtschaft-Recht-Technik, Springer Gabler, Heidelberg, 2015.\n",
      "| Ester, M & Sander J: Knowledge Discovery in Databases. Techniken und Anwendungen. Springer, Berlin 2000.\n",
      "| Ferber R: Information Retrieval. Suchmodelle und Data Mining - Verfahren für Textsammlungen und das Web, dpunkt.verlag, Heidelberg,\n",
      "2003.\n",
      "| Fischer P: Algorithmisches Lernen, B.G. Teubner, Stuttgart, 1999\n",
      "| Han J et al.: Data Mining: Concepts and Techniques, Elsevier/Morgan Kaufmann, Amsterdam, 2006.\n",
      "| Hand D. et l.: Principles of Data Mining, MIT Press, Cambridge (Mass.)/London, 2001.\n",
      "| Kantardzic M: Data Mining, Wiley, 2011.\n",
      "| Liu B: Web Data Mining: Exploring Hyperlinks, Contents and Usage Data (Data-Centric Systems and Applications, 2. Auﬂage, Springer,\n",
      "2011.\n",
      "| Marsland S: Machine Learning - An Algorithmic Perspective, CRC Press, 2009.\n",
      "| Mitchell TM: Machine Learning, McGraw-Hill, 1997.\n",
      "| Sutton: Reinforcement Learning: An Introduction, second edition, 2018\n",
      "| Runkler TA: Data Mining - Methoden und Algorithmen intelligenter Datenanalyse, Springer Vieweg, 2010.\n",
      "| Schwarz T: Big Data im Marketing: Chancen und Möglichkeiten für eine eﬀektive Kundenansprache, Haufe-Lexware, 2015.\n",
      "| Witten IH et al.: Data Mining: Practical Machine Learning, Tools and Techniques, 3rd edition, Elsevier, 2011.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 26ADSA Data Analytics 3: Deep Learning\n",
      "Module responsibility\n",
      "Prof. Dr. Binh VuLevel\n",
      "MasterModule Nr.\n",
      "4093\n",
      "Credits\n",
      "8 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 200 hours\n",
      "- presence time: 100 hours\n",
      "- self study: 100 hoursTeaching and learning methods\n",
      "Problem-oriented learning\n",
      "Seminar\n",
      "ExerciseLanguage\n",
      "English\n",
      "Participation requirements\n",
      "Data Analytics 1Examination\n",
      "Practical workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The students gain a comprehensive understanding of essential predictive analytics techniques and develop proﬁciency in using common\n",
      "standard tools, such as, e.g., Python, Tensorﬂow, Keras, and Scikit-learn, to perform independent analyses. They learn to critically evaluate\n",
      "the validity and relevance of results and apply their newfound skills to tackle new and complex scenarios objectively. By the end of the\n",
      "module, students will have developed a strong foundation in predictive analytics, allowing them to make informed decisions and solve real-\n",
      "world problems using data-driven insights. The students prove their application-oriented knowledge and competences by solving well-\n",
      "deﬁned problems and exercises during a practical examination. This form of examination also evaluates the student’s interpretation\n",
      "capabilities.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students are taught to use functional terms eﬀectively and apply predictive analytics methods in a practical setting.\n",
      "- They develop the ability to perform complex analyses and functionally evaluate results, considering both qualitative and quantitative\n",
      "factors.\n",
      "- The students are encouraged to critically reﬂect on the validity of their results, fostering a deeper understanding of the concepts and\n",
      "methods learned.\n",
      "Methodological competence\n",
      "- The students become proﬁcient in applying the methods learned through the use of speciﬁc software solutions.\n",
      "- They develop the ability to use these tools to analyze data and draw meaningful insights, gaining hands-on experience in a professional\n",
      "setting.\n",
      "- The students are encouraged to critically reﬂect on the validity of their results, fostering their analytical skills and ability to make informed\n",
      "decisions based on data-driven insights.\n",
      "Social competence\n",
      "- The students possess the skills to both individually and collaboratively analyze and categorize problems, allowing them to develop\n",
      "eﬀective, user-centered solutions.\n",
      "- Through hands-on experience, they learn to apply their analytical and problem-solving skills to real-world scenarios and create innovative\n",
      "solutions that meet the needs of the intended users.\n",
      "- This development of critical thinking and collaboration skills will serve them well in their future careers.\n",
      "Module content\n",
      "- Introduction to Deep Learning\n",
      "- Linear Neural Networks\n",
      "- Multilayer Neural Networks\n",
      "- Backward Propagation\n",
      "- Improving Deep Neural Networks\n",
      "- Convolutional Neural Networks\n",
      "- ConvNets in Practice\n",
      "- Transfer Learning\n",
      "- Word Embedding\n",
      "- Recurrent Neural NetworksSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 27- Long Short-Term Memory\n",
      "- Gated Recurrent Unit\n",
      "- Generative Adversarial Networks\n",
      "- Autoencoders\n",
      "- Attention and Transformers\n",
      "- Graph Neural Networks\n",
      "- Diﬀusion Models\n",
      "- Deep Reinforcement Learning\n",
      "Literature recommendations\n",
      "Andrew Glassner: Deep Learning - A Visual Approach, No Starch Press, 2021\n",
      "Charu C. Aggarwal: Neural Networks and Deep Learning - A Textbook, Springer, 2018\n",
      "Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola: Dive into Deep Learning, arXiv preprint, 2021\n",
      "Hisham El-Amir, Mahmoud Hamdy: Deep Learning Pipeline - Building a Deep Learning Model with TensorFlow, Apress, 2020\n",
      "Ian Goodfellow, Yoshua Bengio, Aaron Courville: Deep Learning, MIT Press, 2016\n",
      "Liangqu Long, Xiangming Zeng: Beginning Deep Learning with TensorFlow - Work with Keras, MNIST Data Sets, and Advanced Neural\n",
      "Networks, Apress, 2022\n",
      "Nikhil Ketkar, Jojo Moolayil: Deep Learning with Python - Learn Best Practices of Deep Learning Models with PyTorch, Apress, 2021\n",
      "Nithin Buduma, Nikhil Buduma, and Joe Papa: Fundamentals of Deep Learning - Designing Next-Generation Machine Intelligence Algorithms,\n",
      "O’Reilly, 2022\n",
      "David Paper: State-of-the-Art Deep Learning Models in TensorFlow - Modern Machine Learning in the Google Colab Ecosystem, Apress, 2021\n",
      "Wei Di, Anurag Bhardwaj, Jianing Wei: Deep Learning Essentials, Packt Publishing, 2018SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 28\n",
      "Scope Data Visualization and StorytellingSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 29ADSA Data Visualization and Storytelling 1+2: Design\n",
      "Basics and Designing Interactive Dashboards\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "4095\n",
      "Credits\n",
      "7 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 175 hours\n",
      "- presence time: 100 hours\n",
      "- self study: 75 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Seminar\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Learning Diary and\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The module “Data Visualization and Storytelling 1” provides students with the necessary knowledge and practical skills to develop a strong\n",
      "foundation in data visualization, and to design and develop advanced applications for visual data analysis for eﬀective communication of\n",
      "insights regarding the original problem. These insights are often described and presented using interactive dashboards, infographics, etc.\n",
      "This module introduces the skills required to create professional business dashboards and infographics, and is covered during the ﬁrst and\n",
      "second semester as following:\n",
      "First Semester\n",
      "- Course → Data Visualization and Storytelling 1: Design Basics (2 CP)\n",
      "- Type of exam → Learning Diary\n",
      "Second Semester →\n",
      "- Course → Data Storytelling and Visualization 2: Creating Interactive Dashboards (5 CP)\n",
      "- Type of exam → Project Work\n",
      "This module is meant to prepare students to work on complex data science projects that require the development of interactive visual\n",
      "interfaces for data analysis. Various projects in this module allow students to decode, critique, and redesign interactive dashboards using\n",
      "real-world datasets that will help students to identify the story within data and discover how to use data story points to create a powerful\n",
      "story to leave a long and lasting impression on the target audience.\n",
      "The students analyze diﬀerent story examples and develop own stories based on role-plays. The methodological spectrum reaches from\n",
      "data selection and visualization to interpretations for diﬀerent target groups. This work is performed individually as well as in teams. Finally,\n",
      "the students or their respective groups present their developed stories to each other and subsequently critically reﬂect the results. The\n",
      "examination form consisting of a learning diary for the ﬁrst semester and project work in the second semester suit the module intention\n",
      "adequately because there is a continuous documentation and evaluation of the students’ improvements in competences as well as of the\n",
      "status of the data story.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students will gain knowledge and understanding of design principles and elements\n",
      "- The students are able to use data visualizations for interactive storytelling, enabling and supporting the exploration of analysis results as\n",
      "well as the derivation of new problems.\n",
      "- The students will learn about basic and advanced visualization techniques.\n",
      "- They may visualize and communicate analysis results in a target group-oriented way.\n",
      "- They will understand how people work in data visualization projects.\n",
      "Methodological competence\n",
      "- Students are capable to choose eﬀective visualization techniques for a particular problem.\n",
      "- After completion of this module the students know tools supporting interactive data storytelling (e.g. Tableau Public, Tableau Desktop,\n",
      "Tableau Prep) and are able to use them in a target-oriented manner.\n",
      "- They are capable to prepare insights according to their target group and help them to make better decisions regarding the original\n",
      "problemSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 30Social competence\n",
      "- They improve their communication abilities\n",
      "- They are capable of recognizing the needs of a target audience so that they prepare information and communicate insights properly.\n",
      "Module content\n",
      "Working knowledge of design principles and elements\n",
      "- Design vocabulary based on principles of design\n",
      "- Theories of data visualization and data storytelling\n",
      "- User persona, storyboarding, paper prototyping\n",
      "- Assess the quality of the data and perform explorative data analysis\n",
      "- Eﬀectively choosing visualization techniques\n",
      "- Design principles (Forms, colors, etc.)\n",
      "- Preattentive attributes\n",
      "- Storytelling with data\n",
      "- Interplay between narrative and visual communication\n",
      "- Designing Infographics, interactive dashboards for the target audience\n",
      "- Combine and the data and follow the best practices to present your data\n",
      "- Examine, navigate, learn to use various features of Tableau\n",
      "- Tools: Tableau Desktop, Tableau Public, Tableau Prep; Basics of Data preparation and Tableau; Time series, aggregation, and ﬁlters;\n",
      "Creating maps, working with hierarchies; Working with interactive action – Filter and Highlighting; Joining and Blending, Dual Axis charts;\n",
      "Table Calculations, Level of Detail (LOD) Calculations; Groups and Sets; Advanced Dashboards, Data Storytelling\n",
      "Literature recommendations\n",
      "| Alexander B: The New Digital Storytelling: Creating Narratives with New Media, ABC-Clio, 2011.\n",
      "| Berinato S: Good Charts: The HBR Guide to Making Smarter, More Persuasive Data Visualizations, Harvard Business Review Press, 2016.\n",
      "| DeBarros A: Practical SQL: A Beginner's Guide to Storytelling with Data, No Starch Press, 2018.\n",
      "| Evergreen SDH: Eﬀective Data Visualization: The Right Chart for the Right Data, Sage Pubn, 2016.\n",
      "| Foreman JW: Data Smart: Using Data Science to Transform Information into Insight, 1st edition, Wiley, 2013.\n",
      "| Marr B: Big Data: Using SMART Big Data, Analytics and Metrics to Make Better Decisions and Improve Performance, 1st edition, Wiley,\n",
      "2015.\n",
      "| Nussbaumer Knaﬂic C: Storytelling mit Daten. Die Grundlagen der eﬀektiven Komunikation und Visualisierung mit Daten, 1. Auﬂage,\n",
      "Vahlen, 2017.\n",
      "| Provost F & Fawcett T: Data Science for Business: What you need to know about data mining and data-analytic thinking, 1st edition,\n",
      "O'Reilly, 2013.\n",
      "| Wong DM: The Wall Street Journal Guide to Information Graphics: The Dos and Don'ts of Presenting Data, Facts, and Figures, Reprint-\n",
      "Auﬂage, Ww Norton & Co, 2014.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 31ADSA Data Visualization and Storytelling 3: Advanced\n",
      "Data Visualization\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "4097\n",
      "Credits\n",
      "5 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 125 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 50 hoursTeaching and learning methods\n",
      "Group Work\n",
      "Exercise\n",
      "LectureLanguage\n",
      "English\n",
      "Participation requirements\n",
      "Knowledge of design principles and the process of data storytellingExamination\n",
      "Project workCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The module “Data Visualization and Storytelling 3” provides students with the advanced knowledge and practical skills to develop\n",
      "dashboards for visual data analysis for eﬀective communication of insights regarding a speciﬁc business problem using PowerBI and D3.js.\n",
      "Also, one of the most important concepts students will learn in this module is to conduct a heuristic evaluation for usability in data\n",
      "visualization. This module introduces the skills required to create professional business dashboards using PowerBI, and will enable students\n",
      "to recruit users and conduct user interviews for evaluating the dashboards.\n",
      "This module is meant to prepare students to apply advanced data visualization concepts in data science projects that require the creation of\n",
      "interactive visual interfaces for descriptive data analysis. The students analyze diﬀerent story examples and subsequently develop their own\n",
      "stories based on role-plays. The methodological spectrum reaches from data selection and visualization to interpretations for diﬀerent target\n",
      "groups. This work is performed individually as well as in teams. Finally, the students or their respective groups present their developed\n",
      "stories to each other and subsequently critically reﬂect on the results. The examination form consisting of project work suits the module\n",
      "intention adequately because there is a continuous documentation and evaluation of the students’ improvements in competencies as well\n",
      "as of the status of the data story.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students are able to use advanced data visualizations for interactive storytelling to support exploration results\n",
      "- The students will learn other advanced visualization techniques.\n",
      "- The students will understand complete business intelligence workﬂow from end-to-end\n",
      "- The students will be to able to blend and create beautiful and advanced interactive dashboards\n",
      "- The students will learn what is user experience in data visualization.\n",
      "Methodological competence\n",
      "- After completion of this module the students know other visualization technologies such as Power BI, and D3.js and are able to use them in\n",
      "a target-oriented manner.\n",
      "- They are able to conduct cognitive walkthrough, recruit user for usability studies, and conduct user observations\n",
      "Social competence\n",
      "- They improve their communication abilities\n",
      "- They are capable of recognizing the needs of a target audience so that they prepare information and communicate insights properly.\n",
      "- They are capable of conducting user interviews\n",
      "Module content\n",
      "- Working knowledge of design principles and data storytelling process\n",
      "- Advanced data visualization techniques\n",
      "- Power BI: Connecting and Shaping Data, Creating Table Relationships and Data Models, Analyzing Data DAX Calculations, Visualizing Data\n",
      "with PowerBI Reports, Artiﬁcial Intelligence VisualsSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 32Literature recommendations\n",
      "| Few S: Information Dashboard Design, Analytics Press, 2015\n",
      "| Iliinsky N, Steele J: Beautiful Visualization, O’Reilly Media, 2010\n",
      "| Cairo A: The functional art: An introduction to information graphics and visualization, New Riders Publishing, 2012\n",
      "| Berinato S: Good Charts: The HBR Guide to Making Smarter, More Persuasive Data Visualizations, Harvard Business Review Press, 2016.\n",
      "| Evergreen SDH: Eﬀective Data Visualization: The Right Chart for the Right Data, Sage Pubn, 2016.\n",
      "| Foreman JW: Data Smart: Using Data Science to Transform Information into Insight, 1st edition, Wiley, 2013.\n",
      "| Marr B: Big Data: Using SMART Big Data, Analytics and Metrics to Make Better Decisions and Improve Performance, 1st edition, Wiley,\n",
      "2015.\n",
      "| Nussbaumer Knaﬂic C: Storytelling mit Daten. Die Grundlagen der eﬀektiven Komunikation und Visualisierung mit Daten, 1. Auﬂage,\n",
      "Vahlen, 2017.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 33\n",
      "Scope Data PrivacySRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 34ADSA Privacy, Ethics and International Law\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "3639-5\n",
      "Credits\n",
      "6 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 150 hours\n",
      "- presence time: 75 hours\n",
      "- self study: 75 hoursTeaching and learning methods\n",
      "Problem-oriented learningLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Written ExaminationCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "During this module, the students develop fundamental knowledge about privacy, ethics, and the judicial aspects in the context of data\n",
      "analysis. They generate awareness of ethically relevant problems, and they are able to evaluate individual, social, and institutional actions\n",
      "in socio-technical situations (e.g., based on privacy law). Additionally, they learn to impose privacy requirements through organizational-\n",
      "technical measures. During the module, the students learn, analyze and discuss ethical and judicial aspects in the context of big data and\n",
      "data analysis through well-deﬁned practical examples as well as presentations. The gained theoretical competencies in the existing privacy\n",
      "laws and regimentations are evaluated through a written examination. This form of examination enables the students to reproduce, apply\n",
      "and discuss judicial aspects of privacy law on well-deﬁned examples and scenarios. This module enables the students to develop technical\n",
      "and organizational measures to enforce privacy and personality laws in big data projects and data analyses. The evaluation of the student’s\n",
      "competence is performed via casework.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "MUSS NOCH VERTEILT WERDEN:\n",
      "Abstract:\n",
      "-----------------\n",
      "- The students are able to examine contexts of origin and eﬀects from an ethical perspective and may apply ethical and privacy concepts on\n",
      "deﬁned examples of socio-technical scenarios.\n",
      "- They know the prerequisites of a transparent, informed approval as well as the prerequisites of data transfer and may derive\n",
      "consequences for big data projects.\n",
      "- They are capable of reproducing and applying the principles of data curation and utilization according to national and international law\n",
      "- They know and exert the relevant privacy laws, regulations and stragies.\n",
      "Methodological competence\n",
      "- The students know and target-orientedly apply organizational as well as technical measures to impose privacy and personal rights\n",
      "Social competence\n",
      "- The students may analyze and evaluate well-deﬁned problems independently.\n",
      "- They a are able discuss in a functional and scientiﬁc way.\n",
      "Module content\n",
      "Ethics and international law\n",
      "- Terminology of ethics, business ethics\n",
      "- Ethics within the technical civilization/occupations\n",
      "- Individual and institutional ethics\n",
      "- Ethical codices for computer scientists\n",
      "- Ethics within an interconnected world\n",
      "- Lawful actions and conﬂict of interests\n",
      "- Rights of the persons aﬀected\n",
      "- International data processing and jurisdictionSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 35- Principles of appropriation and approval requirements\n",
      "- Regimentation in big data inquirys\n",
      "- Contracts regarding data and data analyses\n",
      "- German privacy, internet and communication laws (Bundesdatenschutzgesetz, Telemediengesetz, Telekommunikationsgesetz)\n",
      "- Data transfer within a business and places outside the EU\n",
      "Privacy and its enforcement\n",
      "- Principles of privacy law\n",
      "- Data separation\n",
      "- Technologies to enforce privacy requirements\n",
      "- Organizational measures\n",
      "- Anonymization and pseudonymization\n",
      "- Application scenarios\n",
      "- Risks caused by data aggregation\n",
      "- Misuse of data\n",
      "Literature recommendations\n",
      "| Bachmann R et al.: Big Data - Fluch oder Segen? Unternehmen im Spiegel gesellschaftlichen Wandels, mitp Press, 2014.\n",
      "| Dorschel J: Praxishandbuch Big Data: Wirtschaft-Recht-Technik, Springer-Gabler, Heidelberg, 2015\n",
      "| Gola P & Reif Y: Praxisfälle Datenschutzrecht, 1.A., Heidelberg, 2013\n",
      "| Grunwald A: Technikfolgenabschätzung, 2. Auﬂage, Berlin, 2010.\n",
      "| Hausmanninger T & Capurro R: Netzethik. Grundlegungsfragen der Internetethik, München, 2002.\n",
      "| Kuhlen R: Informationsethik, Konstanz, 2004.\n",
      "| Lenk H & Ropohl G: Technik und Ethik, Stuttgart, 1993.\n",
      "| Richter P: Privatheit, Öﬀentlichkeit und demokratische Willensbildung in Zeichen von Big Data, Nomos, Baden-Baden, 2015\n",
      "| Stamatellos, G: Computer Ethics. A global perpective, Sudbury, 2007.\n",
      "| Stoecker R et al.: Handbuch Angewandte Ethik, Stuttgart, 2011.\n",
      "| Taeger J: Einführung in das Datenschutzrecht, 1.A., Heidelberg, 2013.\n",
      "| Worms N: Informationsethik und Online-Netzwerke: Im Spannungsfeld zwischen struktureller Bedingtheit und Privatsphäre, 1.A., Berlin,\n",
      "2010.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 36\n",
      "Elective ModuleSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 37ADSA Case Study 2\n",
      "Module responsibility\n",
      "Prof. Dr. Swati ChandnaLevel\n",
      "MasterModule Nr.\n",
      "3629\n",
      "Credits\n",
      "14 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 350 hours\n",
      "- presence time: 105 hours\n",
      "- self study: 245 hoursTeaching and learning methods\n",
      "Casework\n",
      "Group Work\n",
      "Project workLanguage\n",
      "English\n",
      "Participation requirements\n",
      "First steps into case studies / Data Engineering / Analytics 2Examination\n",
      "Project workCourse work\n",
      "Case Study, Presentation, Project work\n",
      "Constructive Alignment\n",
      "The examination forms project work and presentation evaluates the students‘ abilities to document their learning progress application-\n",
      "speciﬁcally. Through the application of the skills and competencies acquired in the previous modules during “case studies 2” the students\n",
      "are given the opportunity to intensify their knowledge in a holistic manner. Furthermore, the students learn to evaluate the applicability and\n",
      "cooperation of methods, techniques and tools in a context of a certain project. The project work is an adequate examination for project\n",
      "module because of its emphasis on the visualization and communication of the results/recommendations. Moreover, the students proof their\n",
      "ability to communicate the recommendations based on the results of the case studies via the ﬁnal presentations. By documenting their\n",
      "progresses within the module using a learn journal the students are given the opportunity to solve problems in a self-reﬂecting manner.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "- The students are able to identify a real-world (research) problem, translate it into a complex Data Science research question, and address\n",
      "it using adequate methods from the ﬁeld of Data Science. The module results are to be communicated as business-oriented advice and, if\n",
      "possible.\n",
      "- The students are able to identify a research question and transfer it into a Big Data question\n",
      "- They are able to plan and pursue a Data Science Project according to the 6-step process of Data Science.\n",
      "- They are able to identify the data being necessary for this question and are able to properly estimate the value of the data in the context\n",
      "of the problem.\n",
      "- They are able to prepare their data as required for the analysis or machine learning algorithms.\n",
      "- They can execute a predictive analysis with the help of established tools and software.\n",
      "- They are capable to adequately visualizing and communicate the results in the context of the developed problem\n",
      "Methodological competence\n",
      "- The students are able to speciﬁcally apply creativity techniques to develop a problem and to identify required data.\n",
      "- They evaluate properly the applicability of methods and tools for the diﬀerent phases of the Big Data project in context of a certain project\n",
      "and are able to select and execute the adequate methods.\n",
      "- They interpret and evaluate the results of the analytics process with regard to the developed Big Data problem\n",
      "Self competence\n",
      "- The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks (i.e.\n",
      "occupation, private life and studies) simultaneously.\n",
      "Social competence\n",
      "The students know the fundamental tools to organize working processes in virtual teams as well approaches dealing with team-intern\n",
      "conﬂicts. They are able to use both tools to accomplish results mutually.\n",
      "Module content\n",
      "- Project management\n",
      "- Organization and management\n",
      "- Creativity techniques, formulation of questions\n",
      "- Data management\n",
      "- Big Data architecturesSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 38- Predictive analytics tools\n",
      "- Data mining tools\n",
      "- Visualization tools\n",
      "- Predictive Analytics\n",
      "- Ethics\n",
      "- Predictive Customer Insight\n",
      "Literature recommendations\n",
      "- Chamoni P & Gluchowski P: Analytische Informationssysteme: Business Intelligence-Technologien und -Anwendungen, akt. Auﬂage.\n",
      "- Dorschel J: Praxishandbuch Big Data: Wirtschaft-Recht-Technik, Springer Gabler, Heidelberg, 2015.\n",
      "- Freiknecht J: Big Data in der Praxis: Lösungen mit Hadoop, Hbase und Hive. Daten speichern, aufbereiten, visualisieren, 2014.\n",
      "- Kemper et al.: Business Intelligence - Grundlagen und praktische Anwendungen, 3. Auﬂage, Vieweg, Wiesbaden, 2010.\n",
      "- Koster K: International Project Management, Sage Publications Ltd., 2009.SRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 39ADSA Internship\n",
      "Module responsibility\n",
      "Prof. Dr. Theodoros SoldatosLevel\n",
      "MasterModule Nr.\n",
      "2532\n",
      "Credits\n",
      "14 (benotet)Duration\n",
      "1 SemesterFrequency\n",
      "2x each year\n",
      "Workload\n",
      "Total: 350 hours\n",
      "- presence time: 10 hours\n",
      "- self study: 340 hoursTeaching and learning methods\n",
      "InternshipLanguage\n",
      "English\n",
      "Participation requirements\n",
      "noneExamination\n",
      "Scientiﬁc Poster Presentation and\n",
      "Internship ReportCourse work\n",
      "N/A\n",
      "Constructive Alignment\n",
      "The Internship phase is an essential element of the Applied Data Science and Analytics (ADSA) master program, oﬀering students hands-on\n",
      "experience applying the knowledge and skills acquired in a real-world industry setting. Its primary objectives are to provide professional\n",
      "expertise, familiarize students with relevant industries and companies, and prepare them for successful careers. The internship's main focus\n",
      "is to apply the knowledge and skill gained in the study course in practical work.\n",
      "The students will be informed about the intention, content, and possibilities for an internship (compare internship regulations). They search\n",
      "and apply for internship by themselves and develop their social skills in interviews. The lecturers have a consulting role. Interim meetings\n",
      "with the supervisor and, optionally, the mentoring professor ensure that the internship has an optimum outcome.\n",
      "To integrate practical and theoretical aspects of the program, students must submit a written report on their internship experience, allowing\n",
      "them to reﬂect on it critically, apply theoretical knowledge to real-life problems, compare and contextualize theory with practice, and make\n",
      "informed decisions about future specializations. It aims to allow students to document and reﬂect upon their personal learning process and\n",
      "skill acquisition during the internship. In addition, they should present examples of how the material and methodology skills they acquired\n",
      "throughout their studies were applied in the ﬁeld. At the end of the internship, students are expected to have developed a comprehensive\n",
      "understanding of the latest technologies used in various business environments and their applications. The program also fosters critical\n",
      "thinking skills and the ability to compare and evaluate the beneﬁts of modern technologies in applied data analysis and computer science.\n",
      "ADSA constructive alignment ensures that the student cannot but achieve the desired learning goals.\n",
      "Qualiﬁcation goals (learning outcomes) and competencies:\n",
      "Professional competence\n",
      "Upon completing the working internship, a student should have acquired practical skills, knowledge, and professional competencies\n",
      "necessary for success in their future career.\n",
      "- The students are able to show deeper knowledge, understanding, and attitudes in the context of the practical work in the ﬁeld of data\n",
      "science.\n",
      "- The students are able to participate in a structured and supervised work experience, where theoretical knowledge gained in the classroom\n",
      "can be applied to real-world situations.\n",
      "- The students are able to ask questions, seek clariﬁcation, and engage in critical thinking to deepen their own understanding of the\n",
      "workplace and industry.\n",
      "- The student are able to will receive constructive feedback on their ﬁnal presentation, with suggestions for improvement and areas of\n",
      "strength.\n",
      "Methodological competence\n",
      "- The students can apply the knowledge and skills learned during the master's program in a real-world industry setting.\n",
      "- The students are able to compare and contrast the industry's work practices with the theoretical expectations learned during the program.\n",
      "-The students are able to reﬂect on personal learning processes and skill acquisition during the internship, including how the skills and\n",
      "materials learned in the program were applied in the ﬁeld.\n",
      "-The students are able to acquire state-of-the-art knowledge regarding the methods and tools used for applied data analysis and analytics\n",
      "tasks within various organizations, including industry, administration, and research organizations.\n",
      "- The students are able to present examples of how the material and methodology skills they acquired throughout their studies were applied\n",
      "in the ﬁeld.\n",
      "Self competenceSRH University\n",
      "Heidelberg\n",
      "Master of Science (M.Sc.) Applied Data Science and Analytics | Curriculum | last edit 01.04.2023 02:53:04 40- The students are able to eﬀectively communicate and present the results of their own work in a company, as well as during scientiﬁc\n",
      "poster presentations and subsequent discussions in front of an academic audience.\n",
      "- The students are able to demonstrate appropriate professional behavior in various workplace environments and adapt accordingly.\n",
      "- The students can analyze and discuss (both orally and in written form) various concepts for solving applied data science problems and\n",
      "break down large problems into manageable parts for eﬀective project planning and execution.\n",
      "Social competence\n",
      "- The students can work eﬀectively in professional teams,\n",
      "- The students are able to manage communication within teams and in meetings with colleagues and coaches and contribute to discussions\n",
      "in a valuable way.\n",
      "These experiences help students develop strong communication and collaboration skills essential in the workplace.\n",
      "Module content\n",
      "The Internship should focus on as many aspects of data science as possible (i.e., from data collection to data management, analysis, and\n",
      "storytelling).\n",
      "Literature recommendations\n",
      "Links\n",
      "- Google Scholar\n",
      "- DBLP\n",
      "- IEEE Computer Society\n",
      "- IEEE TVCG camera ready document guidelines\n",
      "Literature management\n",
      "- Citavi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Applied Data Science andAnalytics\n",
      "Applied Data Science andAnalyticsMaster ofScienceBigData -Challenges\n",
      "20October2021SRHHochschuleHeidelberg–BDBA2DiscoveryData QualityStorageSecurityAnalyticsDifficulty in finding patterns and insights\n",
      "Problem managing large amount of data setsKeeping data secure is a challengePerforming right analysisMessy, Inconsistent,Incomplete \n",
      "SRH Hochschule Heidelberg –Prof. Dr. Swati ChandnaLearningOutcomes•In-depth knowledge and understanding of Applied Data Science.•Application of analytical techniques and machine Learning/deep Learning algorithms to solve complex data problems in business.•Skills you need to leverage data to reveal valuable insights and help make customers valuable decisions.•Formulate technical problem solutions and represent them in discourse.\n",
      "1/23/22\n",
      "3SRH Hochschule Heidelberg –Prof. Dr. Swati ChandnaMasters in Applied Data Science and Analytics\n",
      "4\n",
      "Understand\n",
      "Collect\n",
      "CleanVisualize\n",
      "Resolve\n",
      "Analyze\n",
      "Case studies\n",
      "data engineering\n",
      "data management\n",
      "analytics\n",
      "visualisation\n",
      "Data storytellingund communication\n",
      "SRH Hochschule Heidelberg –Prof. Dr. Swati ChandnaCourse Curriculum\n",
      "SRH Hochschule Heidelberg –Prof. Dr. Swati Chandna5Study accordingtotheCORE principleCompetence Oriented Research and EducationOur activated teaching & learning method—Group project,Flipped classroom, case studies, project pitchesfrom local companies—Practical teaching and exams—5-week blocks –no end of the semester exam stress!—Focus only on 1-2 subjects in 5 weekblocks—Direct contact hours with professors 6SRH Hochschule Heidelberg –Prof. Dr. Swati Chandna\n",
      "Thankyou!!Prof. Dr. Swati ChandnaStudy Programme Director AppliedDataScience andAnalytics, M.Sc.Swati.chandna@srh.deAdmissionsOfficeStudyinheidelberg@srh.deConnect withyourstudentson Unibuddy7\n",
      "@hsheidelberg@SRHHochschuleHD@SRHHochschuleHD@SRH HochschuleHeidelberg\n",
      "\n",
      "\n",
      "www.heidelberg.de /erweiterung\n",
      "Special Activities  \n",
      "2024comfortable\n",
      "comfortable\n",
      "picturesque\n",
      "picturesque\n",
      "exciting\n",
      "exciting\n",
      "moving\n",
      "moving\n",
      "culinary\n",
      "culinary\n",
      "sporting\n",
      "sporting\n",
      "cultural\n",
      "cultural\n",
      "informative\n",
      "informativeHeidelberg Marketing GmbH  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg, Germany  \n",
      "Phone +49 6221 5840-223/ -225 \n",
      "Fax +49 6221 5840-222 \n",
      "guide@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.com\n",
      "The service team is \n",
      "pleased to help you: *\n",
      "Monday – Thursday  \n",
      "9:00 am – 5:00 pm \n",
      "Friday  \n",
      "9:00 am – 3:00 pm\n",
      "* subject to changeWelcome to Heidelberg  \n",
      "As early as in the 19th century, Heidelberg cast its spell on poets such as Joseph von Eichen -\n",
      "dorff and Johann Wolfgang von Goethe with its romantic charisma. The city was immortalized \n",
      "in many publications. Heidelberg has managed to preserve this irresistible magic to this day.\n",
      "Heidelberg has many facets: its romantic castle ruin towering over the roofs of the  \n",
      "picturesque Old Town, idyllic nature around the Neckar River, which is winding its way through \n",
      "the green valley between the Königstuhl (King’s Seat) and Heiligenberg (Holy Mountain), the \n",
      "inspiring university that has drawn those looking for knowledge from around the world into \n",
      "the city for more than 600 years – all of this enchants millions of guests in Heidelberg every \n",
      "year. Let the city cast its spell on you as well!\n",
      "We look forward to seeing you!\n",
      "Your Heidelberg Marketing team\n",
      "/heidelberg4youSpecial Activities 2024  \n",
      "On the following pages, we have compiled some diverse programs for you.\n",
      "Let us spoil you with culinary delights and experience Heidelberg in all its diversity: whether \n",
      "local wine, beer or other delicacies, there are no limits for your enjoyment. Choose between \n",
      "varied tours through the Old Town or through the world-famous castle and its castle garden, \n",
      "which offer a wonderful view of our picturesque city. Hike to the idyllically located Benedictine \n",
      "abbey Stift Neuburg or get to know Heidelberg from a different perspective by bike or by boat. \n",
      "Numerous guided tours in this fascinating city with its lively history will enrich your stay with \n",
      "interesting and exciting stories. Immerse yourself in the special flair of Heidelberg and  \n",
      "ecome part of our city. \n",
      "We look forward to welcoming you as a guest in Heidelberg and offer you free assistance in \n",
      "planning your stay to make your Heidelberg experience as pleasant as possible. Contact us to \n",
      "take advantage of our one-stop full service for your perfect vacation.\n",
      "Your Heidelberg Marketing team\n",
      " Note  Please note that the statutory provisions do not include any statutory revoca -\n",
      "tion right after conclusion of the contract for contracts on services for domestic tourism, \n",
      "in particular for contracts on accommodation, guest tours and cultural events. Only a \n",
      "withdrawal – usually subject to fees – according to the agreed terms and conditions or \n",
      "the statutory provisions is possible in most cases. \n",
      "Our service –  \n",
      "your benefit\n",
      "– free and professional consulting \n",
      "– mediation of qualified tour guides \n",
      "– booking of boat trips  \n",
      " and bus transfers \n",
      " and much more around  \n",
      " your stay Special Activities  | 5 Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de\n",
      "6 | Tours of the Old Town and the castle\n",
      "Tours of the Old Town and the castle\n",
      "Prices for all Old Town & Castle Tours\n",
      "Prices (in €)\n",
      "Hours German Foreign languages\n",
      "1 – 1,5 € 100 € 110\n",
      "2 € 110 € 120\n",
      "2,5 – 3 € 135 € 145\n",
      "4 € 145 € 155\n",
      "5 € 165 € 175\n",
      "6 € 185 € 195\n",
      "7 € 205 € 215\n",
      "8 € 255 € 265\n",
      "Recommended group size  max. 20 persons per tour guide\n",
      "Languages  German, English, Arabic, Chinese, Czech, Danish, Dutch, French, Italian, Japanese, Polish, Portuguese, Romanian, \n",
      "Russian, Swedish, Spanish, Turkish\n",
      " Note  For Castle Tours, the admission fee to the castle courtyard is not included in the rates. For more information on  \n",
      "castle courtyard admissions, see pages 32 / 33 (castle ticket incl. funicular railway).\n",
      " Tip If you arrive with your own bus, we recommend the Neckarmünzplatz or, if you arrive individually, the Löwenbrunnen \n",
      "(Lion’s Fountain) at the Universitätsplatz (University Square) to meet with your tour guide. For other meeting points, see the \n",
      "booking form on page 38.Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deTours of the Old Town and the castle | 7\n",
      "1 ǀ Guided Tour of the Old Town\n",
      "The oldest part of Heidelberg has a lot more to offer than \n",
      "just the Alte Brücke (Old Bridge), the picture-perfect alleys  \n",
      "and the unique view to the most famous ruin in the world. \n",
      "The Old Town is vibrant with its charming squares and a \n",
      "mixture of people of all ages and countries. They meet in \n",
      "many small cafés and pubs and not only the unique cultu -\n",
      "ral programs of our museums and theaters attract our visi -\n",
      "tors. Discover Heidelberg with your guide and really get to \n",
      "know the city.\n",
      "Recommended duration  1.5 or 2 hours\n",
      "3 ǀ Old Town and Castle Tour\n",
      "The combined Old Town and Castle Tour brings history to \n",
      "life. Use a guided tour through the winding alleys of the Old \n",
      "Town to learn about the many facets of Heidelberg – sleepy \n",
      "squares, Germany’s oldest university, the Heiliggeistkirche \n",
      "(Church of the Holy Spirit) and the Jesuit quarter. After -\n",
      "wards, the funicular railway will take you up to the castle. \n",
      "Explore the electoral ruin on a tour through the inner court-  \n",
      "yard and the castle garden and visit the famous Großes \n",
      "Fass (Great Barrel).\n",
      "Recommended duration  3 hours\n",
      "5 ǀ Old Town Tour with Boat Trip\n",
      "Discover the beauty of Heidelberg in an Old Town Tour  \n",
      "followed by experiencing the city’s special flair during a \n",
      "50-minute boat trip along the banks of the Neckar River. \n",
      "This combined guided tour on foot and by boat will intro -\n",
      "duce Heidelberg’s trio of river, city and castle to you.\n",
      "Dates April – October,\n",
      "between 11:30 am – 6:00 pm\n",
      "Recommended duration 2 hours\n",
      "Languages German, English, Danish,\n",
      "Dutch, French, Spanish\n",
      "Price plus ticket for the boat trip\n",
      "Meeting points Neckarmünzplatz, Kornmarkt, Universi -\n",
      "tätsplatz (University Square), Marktplatz (Market Square) \n",
      "or the boat landing stage2 ǀ Castle Tour\n",
      "Heidelberg Castle majestically towers above the roofs of \n",
      "the Old Town, inspiring millions of people every year. A gui -\n",
      "ded tour of the castle courtyard and the castle garden will \n",
      "allow you to immerse yourself in the eventful history of the \n",
      "world-renowned ruin. A visit to the Großes Fass (Great Bar -\n",
      "rel), the largest wine barrel in the world that has ever been \n",
      "filled, is part of the program as well.\n",
      "Recommended duration  1.5 hours\n",
      "4 ǀ City and Castle Sightseeing Tour\n",
      "Get to know the city with a City Sightseeing Tour (approx. 1.5 \n",
      "hours) in your own bus. Immerse yourself in the fascinating \n",
      "history of Heidelberg and admire the worldfamous sights \n",
      "comfortably from the bus. You will continue on to the castle. \n",
      "Explore the electoral ruin by foot in a guided tour (no inner \n",
      "rooms).\n",
      "Recommended duration  2.5 hours\n",
      " Note  For groups with more than 20 persons, a second \n",
      "tour guide will be needed for the Castle Tour (surcharge:  \n",
      "German € 100, other languages € 110).\n",
      " Note  Tour busses travelling from Heidelberg Old Town \n",
      "to the castle must take the detour via Gaiberg. The Neue \n",
      "Schlossstraße is closed to vehicles with a total weight of 3.5 \n",
      "tons or more until further notice. The detour route of  \n",
      "approx. 15 km leads via Leimen, Lingental and Gaiberg.  \n",
      "Please note that the journey to the castle and therefore the \n",
      "total duration of the City and Castle Sightseeing Tour will be \n",
      "extended by approx. 30 minutes due to this detour. If you \n",
      "have any questions, please do not hesitate to contact us at \n",
      "any time.\n",
      " Note  Please note that the Ziegelhäuser Brücke (Ziegel -\n",
      "häuser Bridge) may only be used by vehicles up to 3.5 tons.\n",
      " Tip Are you looking for a digital guided tour for your event or would you like to explore Heidelberg comfortably from home? \n",
      "Then book our „Digital guided tour: A different kind of city tour - digital and yet personal“. Duration 30 minutes, price € 35.Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de\n",
      "8 | Costume Tours\n",
      "Costume Tours\n",
      "Booking information for all Costume Tours\n",
      "Languages  German, English\n",
      "Group size max. 20 persons per tour guide\n",
      "6 ǀ Murder Mystery Tour “The death in \n",
      "sight – Murder at the university”\n",
      "A death at the university – was it murder? A terrible secret \n",
      "awaits in the middle of the cozy Heidelberg Old Town! A \n",
      "murder mystery tour will take you back to the time around \n",
      "the turn of the century. Are the professors and their wives \n",
      "truly as venerable and righteous as they seem? Are the stu -\n",
      "dents as reputable and the bourgeois daughters as deco -\n",
      "rous as the better circles of society desire? Uncover the \n",
      "truth during your walk through the alleys of the Old Town, \n",
      "where evidence and witness statements wait to be disco -\n",
      "vered! Experience this exciting, interactive murder mystery \n",
      "tour together with various historically dressed actors. Dis -\n",
      "cover the time around 1900 up close in an entertaining \n",
      "manner.\n",
      "Duration approx. 2 hours\n",
      "Price  German € 248, English € 2587 ǀ Sweet Temptations and Naked Truths \n",
      "– “Heidelberg Love Stories“\n",
      "Let us follow some lovers from Heidelberg‘s history, liste -\n",
      "ning to the city‘s love stories. This walk through the Old \n",
      "Town will tell you of sweet temptations and naked truths, of \n",
      "divorce and strife, romantic yearning and electoral be -\n",
      "droom stories. Let delicate passions, sensuous poetry and \n",
      "sinful love letters fascinate you, while recipes for love spells \n",
      "and impotence treatments offer practical advice for life. \n",
      "This amorous view of historical morals offers a frivolously \n",
      "amusing view of the customs of love and marriage through \n",
      "the centuries. Love is always a fitting subject!\n",
      "Duration approx. 1.5 hours\n",
      "Price  German € 150, English € 160Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deCostume Tours  | 9\n",
      "8 ǀ Out in old Heidelberg with the\n",
      "“Bürgersfrau” (citizen’s wife)\n",
      "Up close to the action: A citizen‘s wife in historical dress \n",
      "tells the stories behind history, presenting everything first -\n",
      "hand. Past eras come back to life as she chats about every -\n",
      "day excitement in the Old Town, bad student habits, court \n",
      "intrigues and much more. All of this is served with a good \n",
      "dash of humor and a pinch of scandal. Let us surprise you!\n",
      "Duration approx. 1.5 hours\n",
      "Price  German € 150, English € 160\n",
      "10 ǀ Through the dark alleys of\n",
      "Heidelberg with the night watchman\n",
      "Since the Middle Ages, the night watchman has ensured \n",
      "the safety of the citizens within the town walls. When the \n",
      "romantics began to celebrate Heidelberg in songs and im -\n",
      "mortalize the enormous castle ruin in copper engravings \n",
      "at the start of the 19th century, guests from all over the \n",
      "world came to the Neckar River. The night watchman pro -\n",
      "tected them as well from robbers, fraudulent antiques dea -\n",
      "lers and other immoral offers. Walk through the alleys of \n",
      "the Old Town by the side of the night watchman or his \n",
      "daughter while you are told long-forgotten stories from \n",
      "the past.\n",
      "Duration approx. 1 or 1.5 hours\n",
      "Price  German € 150, English € 160 (approx. 1 hour)\n",
      "German € 180, English € 190 (approx. 1.5 hours)9 ǀ Witches, oppressors, poor sinners:\n",
      "from Heidelberg’s legal history\n",
      "Executions used to be true spectacles for everyone. In \n",
      "1762, the university was closed to allow the students to at -\n",
      "tend an execution – a “revolting example”. In this guided \n",
      "tour, a “real” witch will take you through the centuries. On \n",
      "the way through the city, she reports of amusing regulati -\n",
      "ons and gruesome punishments. City history – not fit for \n",
      "children or weak nerves.\n",
      "Duration approx. 1.5 hours\n",
      "Price  German € 150, English € 160\n",
      "11 ǀ With washerwoman Babett\n",
      "through Heidelberg\n",
      "The big wash was a tough business back in the day: grin -\n",
      "ding, rubbing, crushing, beating, rinsing, bleaching, dry -\n",
      "ing. Best done in company. Listen to the stories of the 18th \n",
      "century washerwoman Babett. Not one to mince words, \n",
      "the good woman chats about hard work, soft water, every -\n",
      "day life, juicy details and Heidelberg‘s history.\n",
      "Duration approx. 1.5 hours\n",
      "Price  German € 150, English € 160Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de\n",
      "10 | Thematic Tours\n",
      "Thematic Tours\n",
      "Booking information for all Thematic Tours\n",
      "Duration approx. 1.5 – 2 hours\n",
      "Languages  German, English, other languages available\n",
      "Price  German € 130, other languages € 140 / student groups German € 105, other languages € 115\n",
      "Group size max. 20 persons per tour guide\n",
      " Note Possible admission fees not included, costume surcharge € 50, costumed appearances on request.\n",
      "12 ǀ Heidelberg‘s Nobel Prize\n",
      "Laureates of Medicine\n",
      "The excellent reputation of the Heidelberg University is not \n",
      "least based on its Nobel Prize laureates from the areas of \n",
      "physiology or medicine (Adolf Kossel, Otto Meyerhof, Bert \n",
      "Sakmann, Harald zur Hausen). Learn about the scientific \n",
      "achievements, the special circumstances of the discoveries \n",
      "and the scientists‘ own personalities on a tour of the laure -\n",
      "ates‘ worksites. Discover the excitement in research.\n",
      "Meeting point  In front of the main entrance of the  \n",
      "Kopfklinik (north side), Im Neuenheimer Feld 40013 ǀ Architectural exploration through \n",
      "Heidelberg’s Old Town\n",
      "The winding alleys of Heidelberg date back to the Middle \n",
      "Ages. The harmoniously looking Old Town is made up of \n",
      "buildings from many different styles and eras. This ex -\n",
      "pert walk will take you from the origins to the current \n",
      "building projects. From the icons of tourist interest such \n",
      "as the university library, you will move on to the hidden \n",
      "beauties, such as the Bluntschli house. This is an exciting \n",
      "search for tracks and traces, not only for architecture en -\n",
      "thusiasts!Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deThematic Tours  | 11\n",
      "14 ǀ Heidelberg at the time of Romanti -\n",
      "cism “Down there in verdant meadows,\n",
      "a mill wheel turns around ...”\n",
      "Famous Heidelberg Romanticism – the young poets Joseph \n",
      "von Eichendorff, Clemens Brentano, Achim von Arnim and \n",
      "their friends turned Heidelberg into a center of late Ro -\n",
      "manticism in the early 19th century. Their traces can still be \n",
      "found throughout the city and will be brought to light in \n",
      "this guided tour of the Old Town.\n",
      "16 ǀ The famous Philosophenweg\n",
      "(Philosophers’ Walk) – Heidelberg at\n",
      "its most beautiful\n",
      "Once, scholars were walking here in their stiff frock coats, \n",
      "relaxing their thoughts during strolls – today, the Philo -\n",
      "sophenweg beckons with unique views of the city, the ri -\n",
      "ver, the Alte Brücke (Old Bridge), the castle and the near -\n",
      "ly 600 meters high Königstuhl (King’s Seat). Walking up \n",
      "the Philosophenweg, you will be rewarded with a specta -\n",
      "cular view. The path will also take you to the Philosophen -\n",
      "gärtchen (Philosophers’ Garden), on the sunspoiled na -\n",
      "tural balcony on which an abundance of exotic plants is \n",
      "thriving.15 ǀ In the beginning, there was thirst –\n",
      "Pub and restaurant culture in Heidelberg\n",
      "Enjoyment is at the focus of this entertaining guided \n",
      "tour, full of anecdotes: Heidelberg is famous for its \n",
      "pub and inn culture. Rustic pubs, student drinking ha -\n",
      "bits and specialties – this tour will spirit you away into \n",
      "the enjoyable history of Heidelberg, from the Homo \n",
      "heidelbergensis to this day.\n",
      " Note No food and beverages are served during this tour.\n",
      "17 ǀ The University in the Old Town\n",
      "Founded by Prince Elector Ruprecht I. in 1386, the “Ruper -\n",
      "to Carola” is Germany’s oldest university and one of the \n",
      "most venerable education facilities in Europe. The tour \n",
      "conveys not only the university’s history but also provides \n",
      "an insight into student life. It takes you from the university \n",
      "library and the Peterskirche (St. Peter’s Church), the oldest \n",
      "church in Heidelberg, to the Alte Aula (Old Auditorium) and \n",
      "the historical Studentenkarzer (Student Prison). Visit the \n",
      "place that once made people suffer! From 1778 to 1914, \n",
      "students were punished there for “gentlemanly offences”.\n",
      "Price  The entrance fee of € 6 per person for the Alte Aula (Old \n",
      "Auditorium) and the Studentenkarzer (Student Prison) will be \n",
      "added. Interior visits are only possible on request and accor -\n",
      "ding to availability on certain days.Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de12 | Thematic Tours\n",
      "18 ǀ Bergfriedhof (Mountain Graveyard)\n",
      "– Fire, Earth and Immortality\n",
      "Concealed behind high walls on a former vineyard and deli -\n",
      "berately designed as a park, Heidelberg‘s Bergfriedhof sur -\n",
      "prises with its new Garten der Kulturen (Garden of Cultures). \n",
      "Poetry readings are held in a pavilion. Botanical treasures \n",
      "and monumental graves ornamentally line the paths. Fami -\n",
      "liar names of famous Heidelberg citizens bring to mind the \n",
      "relevance of this city in past centuries.\n",
      "Meeting point  In front of the main entrance of the Berg -\n",
      "friedhof (bus / tram stop “Bergfriedhof“)\n",
      " Note  The tour must begin before twilight. Male partici -\n",
      "pants must cover their heads while in the Jewish graveyard. \n",
      "Tours on Saturdays exclude the Jewish part of the grave-  \n",
      "yard.\n",
      "20 ǀ QueerTour Heidelberg –\n",
      "Heidelberg and homosexual history\n",
      "Join us for a walking tour in the Old Town through the \n",
      "“Rainbow City Heidelberg” (since September 2020 Heidel -\n",
      "berg is a member of the “Rainbow Cities”) with the focus \n",
      "on “Heidelberg and homosexual history”. Get to know the \n",
      "most important sights of Heidelberg from the point of \n",
      "view of queer life, but also visit lesser-known places in the \n",
      "city that have a special connection to queer culture. The \n",
      "tour meant to remind one to contemplate and to remem -\n",
      "ber those, who have been forgotten. Additionally, it shows \n",
      "you how colorful, vigorous and encouraging queer life in \n",
      "Heidelberg is.\n",
      "Duration approx. 2.5 hours\n",
      "Meeting point  Karlsplatz (Carl‘s Square), Sebastian-\n",
      "Münster-Brunnen (Sebastian Münster Fountain)\n",
      " Note  The tour is bookable Friday – Sunday,\n",
      "currently only in German.19 ǀ Christmas Market Walking Tour\n",
      "Take a walk through the Old Town and immediately \n",
      "get in the Christmas spirit with the scent of roasted \n",
      "almonds and mulled wine. Lovingly arranged booths \n",
      "spread over various historical squares. The unique \n",
      "backdrop with Heidelberg Castle towering above the \n",
      "Old Town creates an outstanding atmosphere and \n",
      "makes the Heidelberg Christmas Market one of the \n",
      "most fairy tale-like events in Germany. The Advent -\n",
      "themed guided tour provides interesting information \n",
      "on the region’s Christmas and pre-Christmas tradi -\n",
      "tions. The walk starts at the Marktplatz (Market \n",
      "Square) and passes the most beautiful corners of the \n",
      "Old Town ending at the Universitätsplatz (University \n",
      "Square).\n",
      "21 ǀ Heidelberg – History criss-cross\n",
      "Take this entertaining walk through the historic Old Town \n",
      "and find out what kind of cross the people of the Palati- \n",
      "nate had and why beer was helpful when changing religi -\n",
      "on. Learn how the medieval navigator looked like and \n",
      "why it is good for your knees to have the right elector. \n",
      "Learn about lousy times, fun student life and unexpected \n",
      "haute couture problems. The tour is a criss-crossing mix \n",
      "of city history and anecdotes of everyday life.\n",
      " \n",
      " Note  Currently the tour is only available in German.Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deThematic Tours  | 13\n",
      "22 ǀ The Bahnstadt – Research, Living, \n",
      "Work – A new district comes into being\n",
      "The extent to which the university and the science, re -\n",
      "search and education sectors have helped shape the \n",
      "city’s distinctive complexion over the centuries can be \n",
      "seen in Heidelberg‘s history, townscape, architecture and \n",
      "also in numerous new projects. Today, the structure and \n",
      "image of the city are inextricably linked with scientific de -\n",
      "velopments. These developments are particularly visible \n",
      "in Heidelberg’s newest district, the Bahnstadt. It is one of \n",
      "the world’s largest passive housing areas and relies on \n",
      "innovative solutions to combine environmental protec-  \n",
      "tion, attractive living space and optimal infrastructure.\n",
      "Meeting point  SkyLabs, Bahnstadt district\n",
      "24 ǀ BODY WORLDS –\n",
      "“The Anatomy of Happiness”\n",
      "Heidelberg is the location of Germany’s oldest university \n",
      "and has been a center of medical innovation for centuries. \n",
      "Walk through the history in the footsteps of medical action \n",
      "and learn about Heidelberg’s diverse history of anatomy: \n",
      "When and where was the first department of anatomy set \n",
      "up? What happened in the early “anatomic theater”? Hu -\n",
      "man health is directly connected to the development of \n",
      "medicine. The tour therefore ends at the entrance of the \n",
      "KÖRPERWELTEN Museum (BODY WORLDS museum) in the \n",
      "Altes Hallenbad, where you can subsequently explore the \n",
      "exhibition “The Anatomy of Happiness” on your own.\n",
      "Opening hours\n",
      "Monday – Sunday 10:00 am – 6:00 pm\n",
      "(last admission at 5:00 pm). Closed on December 24.\n",
      "Price A museum admission fee of € 19 per person\n",
      "(Monday to Friday), € 21 per person (Saturday, Sunday \n",
      "and public holidays), incl. audio guide (German/English) \n",
      "via smartphone, will be charged additionally.\n",
      "Meeting point Anatomiegarten at the Bunsendenkmal \n",
      "(Bunsen Monument)23 ǀ Coffeehouse culture in Heidelberg\n",
      "On the way through the scenic alleys of Heidelberg‘s \n",
      "Old Town, you follow the traces of Heidelberg‘s cof -\n",
      "feehouse culture once and today: Where did crowned \n",
      "heads, corporate students and coffeehouse poets \n",
      "meet? What was the favourite snack and how much \n",
      "did a cup of coffee cost? Get to know which specialties \n",
      "can be discovered in Heidelberg‘s cafés and where \n",
      "small, family-run coffee roasters and a real coffee \n",
      "sommelier are located.\n",
      "25 ǀ Let‘s get down to brass tacks! –\n",
      "The Proverbs Tour\n",
      "“My name is Hare, I don‘t know a thing!” But don‘t worry, \n",
      "you don‘t have to feel that way! Find out what you “had on \n",
      "the ball” (auf der Pfanne haben) in Heidelberg, what went \n",
      "“beyond the pale” (nicht auf eine Kuhhaut gehen) and how \n",
      "you were “fobbed off” (abgespeist). “Don‘t make tracks” \n",
      "(Kratzen Sie nicht die Kurve), because we’ll untie the ety -\n",
      "mological knots for you and certainly won’t “lead you down \n",
      "the garden path” (ein X für und U vormachen) – after all we \n",
      "majored in the subject. After that, you’re allowed to “pull \n",
      "our leg”, too (einen Bären aufbinden). For real! You may be \n",
      "lead (but not on a merry chase) by: Dr. Kauder-Welsch or \n",
      "Dr. Gobble de Gook. The special city sightseeing tour with \n",
      "aha effect! Experience Heidelberg in entertaining word \n",
      "stories, idioms and plays on words.\n",
      " Note  Also recommended as a guided tour for school \n",
      "classes (from grade 10).14 | Active Tours\n",
      "Active Tours\n",
      "26 ǀ Guided Hike to the Benedictine\n",
      "Abbey Stift Neuburg\n",
      "The hike will take you along the Philosophenweg (Philo -\n",
      "sophers’ Walk), Heidelberg’s famous natural balcony, and \n",
      "on through the forest into “Neuenheimer Schweiz” \n",
      "(Neuenheim’s Switzerland), an impressive nature preser -\n",
      "ve with beautiful views of the Neckar valley. Finally, you \n",
      "will reach the old Benedictine abbey Stift Neuburg, where \n",
      "you can visit the monastery church after your hike.\n",
      "Duration approx. 3 hours\n",
      "Languages  German, English\n",
      "Price  German € 130, English € 140\n",
      "Meeting point  Bus stop “Bergstraße” (Neuenheim)\n",
      "Group size max. 20 persons per tour guide27 ǀ Heidelberg City Rally\n",
      "Exploration tours as communicative experiences. The play -\n",
      "ing field is Heidelberg’s romantic Old Town with its winding \n",
      "alleys and beautiful squares. The participating teams must \n",
      "solve tricky riddles and tasks. All participants will receive a \n",
      "certificate and the winners will get an additional gift. \n",
      "Duration 2 hours\n",
      "Languages  German, English, French\n",
      "Price  German € 130, other languages € 140\n",
      "Group size max. 28 persons per tour guide\n",
      " Note  If preferred, the rally can end and be evaluated in \n",
      "a restaurant in the Old Town. The costs for food and beve -\n",
      "rages are not included and must be paid separately on site. \n",
      "Please provide a list of participants for the certificates.\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deActive Tours  | 15\n",
      "28 ǀ Guided Bike Tour\n",
      "through Heidelberg\n",
      "Explore Heidelberg’s Old Town on a guided bicycle tour \n",
      "and visit the Neuenheim quarter, the Neuenheimer Feld \n",
      "with its Botanical Garden and the internationally renowned \n",
      "institutes and clinics as well as the research campus of the \n",
      "Ruprecht-Karls-Universität (Ruprecht Karl University). The \n",
      "tour continues through the Handschuhsheimer Feld to \n",
      "Handschuhsheim, Heidelberg’s oldest quarter with the fa -\n",
      "mous Tiefburg and back to the Old Town, where the unique \n",
      "view of the Heidelberg Castle and the Old Town from the \n",
      "Alte Brücke (Old Bridge) offers a grand finale.\n",
      "Duration approx. 3 hours\n",
      "Languages  German, English, Dutch\n",
      "Price  German € 130, other languages € 140\n",
      "Group size max. 10 persons per tour guide29 ǀ Guided Bike Tour on the\n",
      "Lower Neckar River\n",
      "On a guided bike tour along the lower Neckar River you will \n",
      "discover both flora and fauna of the Altneckar as well as as -\n",
      "pects of life and work along the river. You will cycle along \n",
      "hidden architectural treasures and get to know Roman tra -\n",
      "ces in Ladenburg and rural Neubotzheim. After having \n",
      "passed Heidelberg‘s vegetable garden the tour continues to \n",
      "the centre of scientific research and finally reaches the favo -\n",
      "urite recreation area of Heidelberg‘s residents.\n",
      "Duration approx. 5 hours\n",
      "Languages  German, English\n",
      "Price  German € 170, English € 180\n",
      "Group size max. 10 persons per tour guide\n",
      " Note  Please carry appropriate protective clothing (e.g. \n",
      "rain jacket, rain cape) depending on the weather condi -\n",
      "tions. At the beginning of the tour, you will receive a brie -\n",
      "fing from the tour guide. The route length is about 30 km, \n",
      "profile: no incline, possibility of picnic and stop.\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Note for Guided Bike Tours\n",
      "For bike rental, please contact our partners directly.\n",
      "Ifa GmbH Radolino Fahrradverleih\n",
      "Radhof Bergheim Lahrer Straße 24\n",
      "Bergheimer Straße 101 69126 Heidelberg \n",
      "69115 Heidelberg Phone: +49 163 6213898 \n",
      "Phone: +49 6221 6599452 info@radolino.de\n",
      "martin.rachfahl@ifa-heidelberg.de www.radolino.de\n",
      "www.ifa-heidelberg.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de16 | Active Tours\n",
      "30 ǀ Segway Tour: “Highly Philosophical”\n",
      "The Segway Tour reveals an entirely new perspective. \n",
      "Glide along the Neckar River to the historical and trendy \n",
      "quarter Neuenheim accompanied by a tour guide. After -\n",
      "wards, you move on to the zoo, the Neuenheimer Feld \n",
      "with its impressive university and hospital buildings and \n",
      "up to the Philosophenweg (Philosophers’ Walk). This Hei -\n",
      "delberg sight is not easily seen at first glance. The former \n",
      "vintners’ path was once taken by scholars who desired \n",
      "the inspiration of the “Heidelberg Trio”: The harmony of \n",
      "the baroque city, the Heidelberg Castle and the Neckar \n",
      "River. It is an extraordinary view!\n",
      "Dates  February – November \n",
      "Duration  approx. 1 ¾ hours  \n",
      "Languages German, English  \n",
      "Price  € 59 per person including helmet for rent  \n",
      "and segway license\n",
      "Meeting point  Neckarmünzplatz  \n",
      "Group size 10 – 20 persons, smaller and larger  \n",
      "groups on request\n",
      "Minimum requirements  14 years old,\n",
      "size 1.40 m, weight 45 – 115 kg\n",
      "31 ǀ Segway Tour “All in 360°”\n",
      "Discover Heidelberg and the Neckar valley on a city safari \n",
      "that provides a mixture of “everything“. Your tour starts at \n",
      "the Neckarmünzplatz and continues to the Philosophen -\n",
      "weg (Philosophers’ Walk) with its fantastic view of the city. \n",
      "The second place steeped in history is the Neuburg mo -\n",
      "nastery, which was founded in 1130 and looks back on a \n",
      "fascinating past. Head towards the Köpfel and Ziegelhau -\n",
      "sen to the other side of the Neckar River and right up to \n",
      "the Wolfsbrunnen. Proceed in the direction of Heidelberg \n",
      "Castle. Enjoy the great view of the castle garden above the \n",
      "castle before continuing to the Klingenteich. Passing the \n",
      "Jewish cemetery, the tour takes you back to the Old Town \n",
      "and your starting point at the Neckarmünzplatz.\n",
      "Dates  February – November  \n",
      "Duration  approx. 2.5 hours  \n",
      "Languages  German, English \n",
      "Price  € 69 per person including helmet for rent \n",
      "and segway license \n",
      "Meeting point  Neckarmünzplatz  \n",
      "Group size 10 – 20 persons, smaller and larger \n",
      "groups on request  \n",
      "Minimum requirements  14 years old, \n",
      "size 1.40 m, weight 45 – 115 kg18 | Culinary Offers\n",
      "Culinary Offers\n",
      "32 ǀ Wine tasting in Heidelberg’s Old Town\n",
      "Instructed by a wine expert, you will be able to get to \n",
      "know six different wines in the restaurant “Sudpfanne”. \n",
      "The special tasting experience is accompanied by expla -\n",
      "nations on the growing areas and characters of the \n",
      "wines. The tour through the world of wine is completed \n",
      "by a hearty 2-course menu.\n",
      "Languages  German, English\n",
      "Price  € 57 per person\n",
      "Group size 10 – 50 persons\n",
      " Note  Further drinks are charged separately on site.\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deIncluded Services\n",
      "–  Wine tasting with six different regional red,\n",
      " rosé and white wines \n",
      "– Hearty, seasonal meal of your choice: \n",
      " April – October :\n",
      " Menu A Small mixed salad; „Kurpfälzer Saumagen“  \n",
      " (stuffed pig‘s stomach), served with winesauerkraut,  \n",
      " fried potatoes and fried eggs\n",
      " Menu B Tomato soup; Mediterranean vegetable dish  \n",
      " with gratinated sheep‘s cheese\n",
      " November – March:\n",
      " Menu A Small mixed salad; roast pork with dark beer  \n",
      " sauce, served with winesauerkraut and fried potatoes\n",
      " Menü B French onion soup with cheese crouton;  \n",
      " Homemade bread dumplings with mushroom cream\n",
      "  Note  Menu is subject to change.\n",
      "Culinary Offers  | 19\n",
      "33 ǀ Guided Tour “Delectable Heidelberg”\n",
      "Fall in love with the culinary delights of romantic Heidelberg. \n",
      "Take a tour through the picturesque Old Town with an ape -\n",
      "ritif matured under the sun of Baden and a regional 3-course \n",
      "menu in two traditional restaurants. The time and journeys \n",
      "between the courses and locations are entertainingly spiced \n",
      "up with history and stories, “magical elixirs”, and are topped \n",
      "off with a sweet Heidelberg treat.\n",
      "Duration approx. 3 hours\n",
      "Languages German, English\n",
      "Price € 98 per person\n",
      "Group size 10 – 25 persons\n",
      " Note  Drinks are charged separately in each location. \n",
      "Please note that this tour is not suitable for children.\n",
      "Included services\n",
      "– Guided tour\n",
      "– Aperitif, appetizer, main course, dessert\n",
      "– 2 “surprise treats”\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de34 ǀ Brewery Tour with beer tasting\n",
      "and Old Town Tour\n",
      "Have you always wondered how beer is brewed? After an \n",
      "one hour guided tour through the historical Old Town, a \n",
      "master brewer will teach you everything from the world of \n",
      "beers. Try three types of beer in the brewery cellar with a \n",
      "warm pretzel. After this, a table buffet will be waiting for \n",
      "you in the brewery.\n",
      "Languages German, English\n",
      "Price € 61 per person\n",
      "Group size 15 – 80 persons\n",
      " Tip Groups with tight schedule can book the brewery  \n",
      "tour without the preceding Old Town Tour (price reduction:  \n",
      "€ 3.50 per person).\n",
      " Note  Further drinks are not included and charged  \n",
      "separately on site. Menu is subject to change.\n",
      "Included services\n",
      "– 1-hour Old Town Tour\n",
      "– Brewery tour\n",
      "– Beer tasting with pretzels\n",
      "– Followed by a rich buffet in the brewery: Crusted roast  \n",
      " of Swabian pork on sauerkraut, bacon and bread  \n",
      " dumplings, braised veal boiled fillet and grilled corn  \n",
      " poularde on seasonal vegetables, homemade spaetzle  \n",
      " and buttered potatoes, Mediterranean vegetable casse-  \n",
      " role with peppers, eggplant, zucchini – baked with  \n",
      " feta cheese and crème fraiche (vegetarian) Additional tip\n",
      "Abtei Stift Neuburg (Benedictine Abbey Stift Neuburg)\n",
      "Embedded in the Neckar valley, surrounded by a lush green and with a  \n",
      "wonderful view of the meandering river and the slopes of the Odenwald, the \n",
      "monastery attracts locals and guests alike. The Benedictine abbey was found -\n",
      "ed by the monastery Lorsch in 1130. It was struggling with its bad economic \n",
      "situation until the monastery was dissolved in 1562. For a long time, the buil -\n",
      "ding was given to different uses thereafter, among others as the “Jungfern-\n",
      "Stift”, where women from noble families were to live together as a “communi -\n",
      "ty of virtuous women” (therefore, it is called the “Stift Neuburg” to this day). \n",
      "Since 1927, monks have been living here again according to the rules of Saint \n",
      "Benedict. The monastery includes a small organic brewery. Following the  \n",
      "rules of their Benedictine order, the monks focus on pastoral work, supporting \n",
      "and meeting with guests through diverse cultural events such as concerts or \n",
      "lectures on contemporary subjects. \n",
      "Travel to the monastery\n",
      "– By foot along the Neckar River or the Philosophenweg (Philosophers’ Walk) \n",
      "– By bus with line 34 and 37, towards Ziegelhausen, Stift Neuburg stop,  \n",
      " www.vrn.de \n",
      "– By ship with the Weisse Flotte Heidelberg (White Fleet), landing stage  \n",
      " Kloster Neuburg, www.weisseflottehd.de\n",
      "Information on monastery tours\n",
      "Monastery tours can be booked upon request and as available during the  \n",
      "following times: Monday – Saturday: 10:00 am – 4:00 pm, Sundays and public  \n",
      "holidays: 11:30 am – 3:00 pm. Information and booking: +49 6221 895143,  \n",
      "fuehrungen@stift-neuburg.de \n",
      "The monastery does not receive any governmental financial support.  \n",
      "The monks would be grateful for any donation on site.\n",
      "Further information  \n",
      "can be found  here .\n",
      "www.heidelberg.de/wlanFree Wi-Fi\n",
      "Heidelberg4You\n",
      "1. Choose network\n",
      "Heidelberg4you\n",
      "2. Connect\n",
      "Accept terms and conditions\n",
      "3. Let‘s go\n",
      "Join more than 200 hotspots\n",
      "22 | Guided Tours for students, young people and families\n",
      "Guided Tours for students,  \n",
      "young people and families\n",
      "35 ǀ Game of Stones –\n",
      "A different kind of history lessons\n",
      "Fantasy novels, movies and TV shows determine a great \n",
      "part of our current image of the past. Is it all made up, or \n",
      "have there been only few changes? How much is based \n",
      "on what actually happened? Magic, sword fights, elves, \n",
      "dangerous intrigues and princesses in distress are true \n",
      "parts of Palatinate history! Search for traces of J. R. R. \n",
      "Tolkien, Joanne K. Rowling, George R. R. Martin and Co in \n",
      "Heidelberg.\n",
      "Duration approx. 1.5 hours\n",
      "Languages  German, English\n",
      "Price for student groups German € 105, English € 115\n",
      "Meeting point  Universitätsplatz (University Square)\n",
      "at Löwenbrunnen (Lion’s Fountain)\n",
      "Group size max. 20 persons per tour guide,\n",
      "larger groups on request\n",
      " Note  Age-specifically adjusted guided tour for children, \n",
      "young adults and student groups in the age brackets from  \n",
      "9 – 13 years and 14 – 19 years. Can be booked as outside \n",
      "Castle or Old Town Tour.36 ǀ The secret of the lost Carbuncle\n",
      "Stone - A Guided Tour full of mysteries\n",
      "A mysterious box, which was found by a castle guide con -\n",
      "tains an unsolved riddle from bygone times. Deeper and \n",
      "deeper it goes into the history of Heidelberg Castle. Cle -\n",
      "ver minds are needed to uncover the secret of an ancient \n",
      "curse and to find the Carbuncle stone that was thought \n",
      "lost! Who says that castle tours have to be boring?\n",
      "Duration approx. 1.5 hours\n",
      "Languages  German\n",
      "Price for student groups German € 105\n",
      "Meeting point  Heidelberg Castle, visitor center\n",
      "Group size max. 20 persons per tour guide,\n",
      "larger groups on request\n",
      " Note  Suitable for children aged 8 - 14 years, well suited \n",
      "for families with children.\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de37 ǀ Exciting Exploration Tour\n",
      "through Heidelberg\n",
      "Popular for years: the children and student’s guided tour. \n",
      "An exciting journey of exploration through the Old Town \n",
      "or at the castle. Robbers, avid readers and much more – \n",
      "exciting and funny stories are waiting for you in \n",
      "Heidelberg’s Old Town. Have you ever been to a real cast -\n",
      "le – with immense walls, thick towers, drawbridges and \n",
      "exciting legends?\n",
      "Duration approx. 1.5 – 2 hours\n",
      "Sprachen  German, English, Danish, Finnish,\n",
      "French, Italian, Spanish\n",
      "Price for student groups German € 105,\n",
      "other languages € 115\n",
      "Group size max. 20 persons per tour guide,\n",
      "larger groups on request\n",
      " Note  Suitable for children and young people aged \n",
      "5 - 18 years.\n",
      "39 ǀ Animal Tracking in\n",
      "Heidelberg’s Old Town\n",
      "Why is a lion with a sword, orb and crown enthroned on \n",
      "the fountain at the Universitätsplatz (University Square)? \n",
      "Who is hiding on the wall of the Marstall? What other  \n",
      "creatures can be found in the streets and squares of \n",
      "Heidelberg‘s Old Town? While strolling through the Old \n",
      "Town, you discover a number of animal figures, some are \n",
      "clearly visible in famous squares and buildings, some are \n",
      "almost hidden so that you have to search for them very \n",
      "precisely. All of them have a story of their own, telling us \n",
      "about life in the city and exciting former times. Come \n",
      "along on the animals‘ journey through the Old Town and \n",
      "find out how they have conquered their place over the  \n",
      "years and are now waiting for visitors to tell them their \n",
      "stories.\n",
      "Duration approx. 1 1/4 – 1.5 hours\n",
      "Languages  German\n",
      "Price for student groups German € 105\n",
      "Group size max. 20 persons per tour guide, larger groups \n",
      "on request\n",
      " Note  Suitable for children aged 6 – 12 years and also for \n",
      "families with children. The tour can be booked onwards \n",
      "from 11.00 a.m. The presence of an accompanying adult is \n",
      "required.38 ǀ Heidelberg City Rally\n",
      "for students\n",
      "Exploration tours as communication experiences. With \n",
      "open eyes, imagination and team spirit, the city and its \n",
      "history are revealed to the students in an entertaining \n",
      "way. In small teams, various tasks and riddles are solved \n",
      "between Universitätsplatz (University Square) and Markt -\n",
      "platz (Market Square). Each team receives a folder with \n",
      "tasks and a city map from the tour guide. On demand, all \n",
      "participants will receive certificates and the winners will \n",
      "get an additional gift.\n",
      "Duration approx. 2 hours\n",
      "Languages  German, English, French\n",
      "Price for student groups German € 105,\n",
      "other languages € 115\n",
      "Group size max. 28 persons per tour guide\n",
      " Note  There are four different levels of difficulty. There-  \n",
      "fore, it is necessary to indicate the type of school and the \n",
      "grade level.\n",
      "40 ǀ Family Tour\n",
      "What can be discovered about the old electoral coat of \n",
      "arms? Which figures and symbols can be found on the his -\n",
      "torical facades and what did and do they mean? When dra -\n",
      "wing Neckar water from the city’s symbol, the Alte Brücke \n",
      "(Old Bridge), you can physically experience just how high \n",
      "that bridge is. Playful exploration for the little ones and \n",
      "interesting stories for the older family members are inclu -\n",
      "ded. The tour starts at the Marktplatz (Market Square) and \n",
      "leads along the most exciting and important sights such \n",
      "as the Heiliggeistkirche (Church of the Holy Spirit), the Alte \n",
      "Brücke (Old Bridge) and the Friedrich Ebert Memorial to \n",
      "the Universitätsplatz (University Square).\n",
      "Duration approx. 1.5 hours\n",
      "Languages  German\n",
      "Price € 105\n",
      "Group size max. 20 persons (max. 10 children)\n",
      "per tour guide\n",
      " Tip The tower of the Heiliggeistkirche (Church of the \n",
      "Holy Spirit) can be visited by asking admission via \n",
      "e-mail heiliggeist@ekihd.de. Further information can be  \n",
      "found on the homepage www.heiliggeist-heidelberg.de.  \n",
      "Donations for this service are welcome.\n",
      "Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deGuided Tours for students, young people and families | 23\n",
      "NEWInformation and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de\n",
      "24 | Boat Trips\n",
      "Boat Trips\n",
      "41 ǀ Boat trips with the Weisse Flotte\n",
      "(White Fleet) Heidelberg\n",
      "From Easter until the end of October, the Weisse Flotte \n",
      "Heidelberg offers daily trips from Heidelberg to Neckar-  \n",
      "steinach (approx. 1.5 hours one way) and back. Food and \n",
      "drinks are available on board. All rides are moderated in \n",
      "English and German.\n",
      "Boat landing stage Stadthalle Heidelberg, Neckarstaden 25\n",
      "Times\n",
      "March 29 – October 31, 2024:\n",
      "Monday – Sunday 10:30 am, 11:30 am, 2:30 pm and 3:30 pm\n",
      "During summer vacation (July 25 – September 7, 2024): \n",
      "Monday – Sunday \n",
      "10:00 am, 11:00 am, 12:00 am, 2:00 pm, 3:00 pm and 4:00 pm\n",
      "Price Round trip Heidelberg – Neckarsteinach –\n",
      "Heidelberg € 25 per person\n",
      "One-way trip Heidelberg – Neckarsteinach or reverse\n",
      "€ 16 per person\n",
      "Boarding at Stift Neuburg (Benedictine Abbey Stift Neuburg), \n",
      "Neckargemünd and Neckarsteinach\n",
      "Group discount  as of 15 persons -5 %,\n",
      "as of 25 persons -10 %,\n",
      "as of 100 persons -15 %, as of 250 persons -20 %  Note  The boats can be rented for special trips and  \n",
      "trips with exclusive groups. Prices on request. Boat capacity:  \n",
      "approx. 50 – 400 persons.\n",
      " Tip The Weisse Flotte Heidelberg offers diverse and  \n",
      "entertaining themed tours for any age year-round.\n",
      " Tip For groups of 15 persons or more, the Weisse Flotte \n",
      "Heidelberg offers coffee and cake during the Four-Castles-\n",
      "Tour. Advance orders required.\n",
      "Tickets and further information\n",
      "can be found here.Information and booking: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de42 ǀ Solar-powered boat “Neckarsonne”\n",
      "Enjoy a fantastic all-round view of Heidelberg‘s sights \n",
      "from the solar-powered boat “Neckarsonne”. Glazed on \n",
      "all sides, the solar-powered catamaran runs on sunlight \n",
      "only as it glides gently and silently across the water. The \n",
      "50-minute round trip on the Neckar River along the \n",
      "banks of Heidelberg comes with interesting information \n",
      "about the “Neckarsonne“, shipping in general and, of \n",
      "course, the history and peculiarities of the city and its \n",
      "surroundings. Gastronomy on board.\n",
      "Boat landing stage Karl-Theodor Bridge\n",
      "(Alte Brücke (Old Bridge)), Old Town side\n",
      "Times  March 29 – October 31, 2024\n",
      "Tuesday – Sunday 11:30 am, 1:00 pm, 3:00 pm and 4:30 pm\n",
      "booking of at least 20 persons additionally\n",
      "10:00 am and 6:00 pm\n",
      "Price adults € 13.50 per person,\n",
      "school and university students € 11 per person\n",
      "Group size as of 10 adults\n",
      " Note  Special rides and rides for exclusive groups are \n",
      "available upon request.\n",
      "Capacity: approx. 250 persons (inside: 150, outside: 100)\n",
      "44 | The sloop cruise\n",
      "with a guide on board\n",
      "Would you like to take a unique boat trip and enjoy the \n",
      "view of Heidelberg Castle and other sights in a relaxed \n",
      "way from the water and privately with your loved ones? \n",
      "Then book our 1.5-hour cruise on the extraordinary and \n",
      "traditional 100-year-old wooden sloops. Discover the \n",
      "Neckar River and Heidelberg from a new perspective \n",
      "with up to 11 people exclusively. During the trip you will \n",
      "be accompanied by a Heidelberg guide.43 ǀ Heidelberg Sightseeing Tour with the \n",
      "Weisse Flotte (White Fleet) Heidelberg\n",
      "No matter if this is your first time visiting Heidelberg or if you \n",
      "want to rediscover your home region: round trips by boat on \n",
      "the Neckar River offer many impressions that will inspire you \n",
      "anew every time, in particular with the option of experienc-  \n",
      "ing the city and its diverse river valley from a very unique \n",
      "angle. Enjoy a special view of Heidelberg and its sights, such \n",
      "as the Alte Brücke (Old Bridge) or the Heidelberg Castle, from \n",
      "the water in a 50-minute round trip.\n",
      "Boat landing stage Stadthalle Heidelberg, pier 5,\n",
      "Neckarstaden 25\n",
      "Times  March, 29 – October 31, 2024\n",
      "Monday – Sunday at 12:00 am, 1:30 pm,\n",
      "3:00 pm, 4:30 pm and 6:00 pm\n",
      "Price  adults € 17 per person including 1 soft drink or beer, \n",
      "children € 8.50 per person including 1 soft drink\n",
      "Group size as of 10 adults\n",
      "Boat landing stage Stadthalle Heidelberg,\n",
      "Neckarstaden 25\n",
      "Times  April - October\n",
      "Duration  approx. 1.5 hours\n",
      "Languages  German, English\n",
      "Price  € 495 including 2 bottles of high quality Palatinate \n",
      "rosé wine, 2 bottles of water, a guide and a captain\n",
      "Group size 1 - 11 persons, larger groups up to 22 \n",
      "persons (two sloops) on request for an additional charge.\n",
      " Tip Longer trips as well as themed tours can be \n",
      "booked on request.Boat Trips  | 25Information und Buchung: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de Information und Buchung: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de\n",
      "26 | Accessible Tours\n",
      "Accessible Tours\n",
      "45 ǀ Discover Heidelberg by “Sensing,\n",
      "Smelling, Feeling”\n",
      "A guided tour that brings the city’s history to life by sensing \n",
      "the environment, feeling monuments and smelling the \n",
      "scents of the city. A tour for the blind and people with im -\n",
      "paired vision.\n",
      "Duration approx. 2 hours\n",
      "Languages German, English\n",
      "Price German € 100, English € 110\n",
      "Group size max. 15 persons per tour guide,\n",
      "larger groups on request\n",
      "Meeting point Neckarmünzplatz\n",
      " Note  We ask the accompanying person to contact the \n",
      "guide in advance of the tour. “Heidelberg hürdenlos”\n",
      "(barrier-free)\n",
      "Guided tours for deaf and\n",
      "hearing-impaired guests.\n",
      "The company GebärdenVerstehen from Heidelberg pro -\n",
      "vides sign language interpreters to translate guided \n",
      "tours of the city into Deutsche Gebärdensprache (DGS) \n",
      "(German Sign Language) for you. They can be booked \n",
      "from GebärdenVerstehen directly upon request:\n",
      "Phone +49 6221 7287478, Fax +49 6221 3541477,  \n",
      "dolmetschen@gebaerdenverstehen.de\n",
      "Please book the interpreters several weeks in  \n",
      "advance, so that your preferred date can be guar-\n",
      "anteed. City guide, quarter maps, regional excursion \n",
      "destinations, gastronomy, culture – Heidelberg can be \n",
      "experienced accessibly as well. The Beirat von Men -\n",
      "schen mit Behinderungen der Stadt Heidelberg (coun -\n",
      "cil of people with disabilities of the city of Heidelberg) \n",
      "and the Verein zur beruflichen Integration und Qualifi -\n",
      "zierung e. V. have collected interesting and valuable \n",
      "information about an accessible stay in Heidelberg.\n",
      "Further information about the company  \n",
      "GebärdenVerstehen:\n",
      "www.gebaerdenverstehen.de\n",
      "Information about\n",
      "an accessible stay in Heidelberg:\n",
      "www.heidelberg.huerdenlos.deInformation und Buchung: +49 6221 5840-223 / -225, guide@heidelberg-marketing.deGuided Tours of the surroundings\n",
      "46 ǀ Schwetzingen – A castle garden\n",
      "based on the image of Versailles\n",
      "From Heidelberg, this trip will take you to Schwetzingen, \n",
      "twelve kilometers away. Based on the image of Versailles, \n",
      "the Schwetzingen Castle with its baroque castle garden \n",
      "was built as the Prince Electors’ summer residence, that \n",
      "made even French philosopher Voltaire eloquently en -\n",
      "thuse about it as a truly magical garden!\n",
      "Duration approx. 5 hours\n",
      "Languages German, English, French,\n",
      "other languages on request\n",
      "Price German € 165, other languages € 17547 ǀ Speyer – In the footsteps of famous \n",
      "emperors\n",
      "The cathedral and imperial city at the Rhein River has long \n",
      "been among the excursion destinations that have a very \n",
      "special attraction to those interested in art and cultural his -\n",
      "tory. You will learn a lot about the cathedral built by the  \n",
      "Salian emperors 950 years ago and its important history. A \n",
      "tour through the city will teach you more about the history \n",
      "of Speyer. On request, the famous Mikveh, the Jewish ritual \n",
      "bath, in the former Jewish quarter can also be visited (for an \n",
      "additional admission fee).\n",
      "Duration approx. 5 hours\n",
      "Languages German, English, French,\n",
      "other languages on request\n",
      "Price German € 165, other languages € 175\n",
      "Guided Tours of the surroundings  | 27Information und Buchung: +49 6221 5840-223 / -225, guide@heidelberg-marketing.de28 | Guided Tours of the surroundings\n",
      "48 ǀ Along the German Wine Road\n",
      "The all-day tour will take you on a trip through the beautiful \n",
      "wine landscape along the wine road after a side trip to the \n",
      "famous Speyer Cathedral. Get to know idyllic Neustadt on a \n",
      "tour with your guide and have a look at the Riesenfass  \n",
      "(Giant Barrel) in Bad Dürkheim.\n",
      "Duration approx. 8 hours\n",
      "Languages German, English, other languages on request\n",
      "Price German € 255, other languages € 265\n",
      "50 ǀ Odenwald trip\n",
      "Enjoy a trip through the romantic Neckar valley via Neckar-  \n",
      "steinach, the four-castle corner, all the way to Hirschhorn. \n",
      "Then you will continue on to Erbach, where you can visit the \n",
      "unique ivory museum if you like (for an admission fee). In \n",
      "Michelstadt, you can see the famous oldest city hall in Ger -\n",
      "many. The return trip will take you through the Odenwald \n",
      "via the Siegfriedstraße and along the Bergstraße.\n",
      "Duration approx. 8 hours\n",
      "Languages German, English,\n",
      "other languages on request\n",
      "Price German € 255, other languages € 26549 ǀ The Bergstraße: \n",
      "Ladenburg – Weinheim – Heppenheim\n",
      "Embedded in the picturesque landscape between the  \n",
      "gently rolling hills of the Odenwald and the sprawling  \n",
      "planes of the Rhein River, there is the Bergstraße with its \n",
      "many castles, picturesque medieval towns Weinheim,  \n",
      "Heppenheim and Ladenburg, the Roman city and the place \n",
      "where Carl Benz worked.\n",
      "Duration approx. 8 hours\n",
      "Languages German, English,\n",
      "other languages on request\n",
      "Price German € 255, other languages € 265Heidelberg Neckar meadow\n",
      "From the first warm spring days to the golden fall, the \n",
      "northern banks of the Neckar River between the Theodor-\n",
      "Heuss Bridge and the Ernst-Walz Bridge are a point of ab -\n",
      "solute attraction. This is where all generations meet – \n",
      "strollers, joggers, sun-lovers, beach-volleyballers or picnic \n",
      "enthusiasts. Particularly popular among children: the \n",
      "water playground with kiosk.\n",
      "Zoo Heidelberg\n",
      "Find fun and inspiration in the Heidelberg Zoo: Stand eye \n",
      "to eye with a lion or watch the spirited elephant bulls at \n",
      "their baths – you will love the diversity of species in the \n",
      "zoo. All children also love the gigantic playing ship “Noah’s\n",
      "Ark”, which invites them to play and climb.\n",
      "www.zoo-heidelberg.de\n",
      "Naturally Heidelberg\n",
      "Get to know Heidelberg’s fascinating nature: From an ori -\n",
      "entation course in the Odenwald to wild herb tours and \n",
      "bat exploration trips to the forest theater holiday week \n",
      "and further training offers for teachers.\n",
      "The hike “Naturally Heidelberg” comprises extraordinary \n",
      "places, views, beauties of nature and monuments strung \n",
      "together like pearls on a string. Your personal experience \n",
      "and visit in combination with exciting information on the \n",
      "natural, botanical, geological and historical special fea -\n",
      "tures will give you a particularly impressive idea of the na -\n",
      "ture and culture around Heidelberg.\n",
      "Information on the exciting group offers are available in \n",
      "the booking office “Natürlich Heidelberg”, where you can \n",
      "also book them directly.\n",
      "www.natuerlich.heidelberg.deThe Bergstraße\n",
      "The Bergstraße connects Heidelberg to Darmstadt. It is an \n",
      "ideal starting point for restorative hikes and nature-focused  \n",
      "bicycle tours. Strung in one sequence along the foot of the \n",
      "Odenwald, little towns will invite you to linger and eat. Our \n",
      "special advice: One of the first and most beautiful almond \n",
      "blossoms in Germany in spring.\n",
      "www.diebergstrasse.de\n",
      "The Burgenstraße (Castle Road)\n",
      "Take the Burgenstraße into the Neckar valley by bike, ship or \n",
      "car. Along the Neckar River, the Burgenstraße from Mann -\n",
      "heim to Bayreuth leads past old monasteries, fascinating \n",
      "castles and picturesque old towns – true to the motto of  \n",
      "“experience a travel through time“.\n",
      "www.burgenstrasse.de\n",
      "Sinsheim: Auto- & Technikmuseum\n",
      "(Car & Technology Museum) and\n",
      "Thermen- und Badewelt\n",
      "The excursion destination for your entire family. Vintage \n",
      "cars, trains and airplanes in the technology museum or ba -\n",
      "thing fun, wellness and sauna enjoyment in the bathing \n",
      "world – Sinsheim is only 30 minutes away from Heidelberg \n",
      "and offers exciting activities for young and old.Leisure tips  | 29\n",
      "Leisure tips\n",
      "www.zoo-heidelberg.dewww.diebergstrasse.de/en\n",
      "www.burgenstrasse.de/uk\n",
      "www.technik-museum.de/en/\n",
      "www.badewelt-sinsheim.de\n",
      "www.natuerlich.heidelberg.de30 | Bus parking\n",
      "KornmarktCabriob usCity tourNeckarmü nzplatz\n",
      "Molken kur\n",
      "KönigstuhlBergbah\n",
      "n(funicular railway)Karlstor/Al tstadt\n",
      "Bussem ergass e\n",
      "Kl. Mantelgasse\n",
      "Große Ma ntelgasseHaspelgasseWehrsteg\n",
      "Floring.Kr\n",
      "ämergasseMit\n",
      "tel\n",
      "bad\n",
      "gasseApotheke\n",
      "rgas\n",
      "seFisc\n",
      "he\n",
      "rg.Se\n",
      "mme\n",
      "lsg.\n",
      "SteingasseLeyerg.Obere NeckarstraßeMönchgasse\n",
      "Dreikönigsstr .Kettengasse\n",
      "Schulgasse\n",
      "Grabengasse\n",
      "Sandgasse\n",
      "Theaterstr aße\n",
      "Friedrichstr aße\n",
      "Märzgasse\n",
      "Akademiestr aße\n",
      "NeugasseRohrbacher StraßeBismar\n",
      "ckstr\n",
      "aßeNadlerstr aße\n",
      "St. Anna GasseFahrtgasse\n",
      "Thibautstr aßePfaffengasseAm BrückentorZoo\n",
      "Heiliggeiststr .\n",
      "Marstallstr aße\n",
      "Schiffgasse\n",
      "Bauamtsgasse\n",
      "Ziegelgasse\n",
      "Brunnengasse\n",
      "Bienenstr\n",
      "aßeKar\n",
      "pfen\n",
      "gas\n",
      "seUnter e Str aße\n",
      "Fischmarkt\n",
      "Marsiliusplatz\n",
      "Richar d-\n",
      "Hauser-\n",
      "PlatzStadthalle\n",
      "Friedrich-\n",
      "Ebert-PlatzBismar ck-\n",
      "PlatzRathaus\n",
      "KornmarktSchlangenweg\n",
      "Unter e Neckarstr aße\n",
      "Landfriedstr aßeNeuenheimer Landstr aße Ziegelhäuser Landstr aße\n",
      "Neckarstaden\n",
      "Friedrich-Ebert-Anlage\n",
      "GaisbergtunnelSchlossberg-tunnel\n",
      "Friedrich-Ebert-Anlage\n",
      "Kurfürsten-AnlageNeckarstaden Schurmanstr aßeUferstr aßeNeuenheimer Landstr\n",
      "aße\n",
      "PlöckPlöckHauptstr aße\n",
      "Bergheimer Straße\n",
      "BahnhofstraßeHauptstr aße\n",
      "Merianstr aße\n",
      "Ingrimstraße\n",
      "Zwingerstraße\n",
      "Unt. Fauler Pelz\n",
      "Oberer Fauler Pelz\n",
      "Neue SchlossstraßeThe\n",
      "odo\n",
      "r-H\n",
      "eu\n",
      "ss-Br\n",
      "üc\n",
      "ke\n",
      "Karl-Theodor-Brücke\n",
      "Brückenstraße\n",
      "Neue Schlossstraße\n",
      "Märchenparadies  \n",
      "(Fairy Tale Paradise)Schlierbach\n",
      "Rohrba chNeuen heim\n",
      "NeckarwieseHirschgasse\n",
      "Alte Brücke \n",
      "(Old Bridge)Fußgängerübergan g\n",
      "(pedest rian crossing)\n",
      "KarlsplatzNeckarmünz-\n",
      "platz\n",
      "Marktplatz\n",
      "Universitäts-\n",
      "platz\n",
      "SchlossSolar-po wered boat\n",
      "Heidelberger\n",
      "Schl oss (Castle)Neuenheimer Feld\n",
      "Kliniken  (Hospital )\n",
      "Hauptbahnhof (main station )Kloster  (Benedictine\n",
      "abbey) Stift Neubu rgPhilosoph enweg (Philoso phers‘ Walk)\n",
      "Kirchheim  / Weststad tZiegelhause n\n",
      "Weisse Flotte\n",
      "Footpath Bus parking Parking gar age\n",
      " Meeting point\n",
      " Funicular r ailway Pier Bus tours\n",
      " Public toilets Tourist Information\n",
      " HDCar d sale Only entry and e xit\n",
      "Bus parking\n",
      " Neckarmünzplatz\n",
      "(Boarding and unboarding point for tour buses)\n",
      "Neckarmünzplatz with its Tourist Information is the ideal  \n",
      "starting point to explore the Old Town on foot or to reach \n",
      "the funicular railway and the castle. \n",
      " Attention  This square is subject to an absolute stop -\n",
      "ping prohibition except for the purpose of passenger \n",
      "change not to exceed 10 minutes. However, the following \n",
      "bus parking places are available nearby: Karlstor / Altstadt (max. 15 parking spaces)\n",
      "For Old Town Tours, we recommend using the bus parking \n",
      "place Karlstor / Altstadt. This parking space is located at \n",
      "the end of the Old Town at the end of the famous pedes -\n",
      "trian zone right behind the Karlstor at the “S-Bahn“  \n",
      "station. If you have booked an Old Town Tour, the bus can \n",
      "ideally stop briefly at the Neckarmünzplatz, only 300 m \n",
      "away, to let passengers board or unboard, before parking \n",
      "at Karlstor / Altstadt. Bus parking | 31\n",
      "KornmarktCabriob usCity tourNeckarmü nzplatz\n",
      "Molken kur\n",
      "KönigstuhlBergbah\n",
      "n(funicular railway)Karlstor/Al tstadt\n",
      "Bussem ergass e\n",
      "Kl. Mantelgasse\n",
      "Große Ma ntelgasseHaspelgasseWehrsteg\n",
      "Floring.Kr\n",
      "ämergasseMit\n",
      "tel\n",
      "bad\n",
      "gasseApotheke\n",
      "rgas\n",
      "seFisc\n",
      "he\n",
      "rg.Se\n",
      "mme\n",
      "lsg.\n",
      "SteingasseLeyerg.Obere NeckarstraßeMönchgasse\n",
      "Dreikönigsstr .Kettengasse\n",
      "Schulgasse\n",
      "Grabengasse\n",
      "Sandgasse\n",
      "Theaterstr aße\n",
      "Friedrichstr aße\n",
      "Märzgasse\n",
      "Akademiestr aße\n",
      "NeugasseRohrbacher StraßeBismar\n",
      "ckstr\n",
      "aßeNadlerstr aße\n",
      "St. Anna GasseFahrtgasse\n",
      "Thibautstr aßePfaffengasseAm BrückentorZoo\n",
      "Heiliggeiststr .\n",
      "Marstallstr aße\n",
      "Schiffgasse\n",
      "Bauamtsgasse\n",
      "Ziegelgasse\n",
      "Brunnengasse\n",
      "Bienenstr\n",
      "aßeKar\n",
      "pfen\n",
      "gas\n",
      "seUnter e Str aße\n",
      "Fischmarkt\n",
      "Marsiliusplatz\n",
      "Richar d-\n",
      "Hauser-\n",
      "PlatzStadthalle\n",
      "Friedrich-\n",
      "Ebert-PlatzBismar ck-\n",
      "PlatzRathaus\n",
      "KornmarktSchlangenweg\n",
      "Unter e Neckarstr aße\n",
      "Landfriedstr aßeNeuenheimer Landstr aße Ziegelhäuser Landstr aße\n",
      "Neckarstaden\n",
      "Friedrich-Ebert-Anlage\n",
      "GaisbergtunnelSchlossberg-tunnel\n",
      "Friedrich-Ebert-Anlage\n",
      "Kurfürsten-AnlageNeckarstaden Schurmanstr aßeUferstr aßeNeuenheimer Landstr\n",
      "aße\n",
      "PlöckPlöckHauptstr aße\n",
      "Bergheimer Straße\n",
      "BahnhofstraßeHauptstr aße\n",
      "Merianstr aße\n",
      "Ingrimstraße\n",
      "Zwingerstraße\n",
      "Unt. Fauler Pelz\n",
      "Oberer Fauler Pelz\n",
      "Neue SchlossstraßeThe\n",
      "odo\n",
      "r-H\n",
      "eu\n",
      "ss-Br\n",
      "üc\n",
      "ke\n",
      "Karl-Theodor-Brücke\n",
      "Brückenstraße\n",
      "Neue Schlossstraße\n",
      "Märchenparadies  \n",
      "(Fairy Tale Paradise)Schlierbach\n",
      "Rohrba chNeuen heim\n",
      "NeckarwieseHirschgasse\n",
      "Alte Brücke \n",
      "(Old Bridge)Fußgängerübergan g\n",
      "(pedest rian crossing)\n",
      "KarlsplatzNeckarmünz-\n",
      "platz\n",
      "Marktplatz\n",
      "Universitäts-\n",
      "platz\n",
      "SchlossSolar-po wered boat\n",
      "Heidelberger\n",
      "Schl oss (Castle)Neuenheimer Feld\n",
      "Kliniken  (Hospital )\n",
      "Hauptbahnhof (main station )Kloster  (Benedictine\n",
      "abbey) Stift Neubu rgPhilosoph enweg (Philoso phers‘ Walk)\n",
      "Kirchheim  / Weststad tZiegelhause n\n",
      "Weisse Flotte\n",
      "Footpath Bus parking Parking gar age\n",
      " Meeting point\n",
      " Funicular r ailway Pier Bus tours\n",
      " Public toilets Tourist Information\n",
      " HDCar d sale Only entry and e xit\n",
      "Stand: December 2024. Subject to change. Castle\n",
      "Tour busses can only drive up to the castle with a parking \n",
      "reservation / special permit requiring a fee. You can obtain \n",
      "these – if parking spaces are available – at www.reiseshop-\n",
      "heidelberg.de  or by email to park@reiseshop-heidelberg.de . \n",
      "Immediate reservations can be made on the day before, or \n",
      "on the same day, by phone: +49 172 6200063.\n",
      "Between 7:00 pm and 8:00 am, driving up to the castle is \n",
      "prohibited. Individual permits can be obtained from the \n",
      "Traffic Management Office by phone: +49 6221 58-30500. \n",
      " Note  All information regarding the availability of bus \n",
      "parking and access to the castle can be found at  \n",
      "www.reiseshop-heidelberg.de or by sending an e-mail to \n",
      "park@reiseshop-heidelberg.de. Attention  The Neue Schlossstrasse is closed for vehicles  \n",
      "weighing 3.5 tons or more. A detour of approx. 15 km leads \n",
      "via Gaiberg. \n",
      "Please use  \n",
      "this detour.Castle ticket  \n",
      "including funicular  \n",
      "railway \n",
      "Two Heidelberg attractions go hand in hand: \n",
      "with the castle ticket you have the option of  \n",
      "reaching the romantic Heidelberg Castle  \n",
      "comfortably, eventful and in an environmental -\n",
      "lyfriendly way by taking the funicular railway.\n",
      "The funicular railway also combines two special \n",
      "features: the lower funicular railway to the  \n",
      "Molkenkur station is the most modern, while \n",
      "the upper one leading to the Königstuhl  \n",
      "(King‘s Seat) is the oldest cableway in Germany.€ 9 * \n",
      "per personCastle ticket including funicular railway  | 33\n",
      "Your benefit\n",
      "The castle ticket includes admission to the castle cour -\n",
      "tyard, the barrel cellar as well as the German Pharmacy \n",
      "Museum and the fare for the funicular railway trip to the \n",
      "castle with a continuation of the trip to the Molkenkur  \n",
      "station (return trip including one stop)!\n",
      "* Price per person\n",
      "(valid for using the lower railway)\n",
      "€ 9 adult, € 4.50 discount price (pupils and university \n",
      "students up to 28 years of age and severely disabled \n",
      "people with appropriate ID)\n",
      "You can get the castle ticket including the funicular \n",
      "railway at the following places in Heidelberg\n",
      "– Tourist Information at the Hauptbahnhof (main station)\n",
      "– Tourist Information at the Neckarmünzplatz (Old Town)\n",
      "– Tourist Information in the Rathaus (town hall) at the  \n",
      " Marktplatz (Market Square, Old Town)\n",
      "– Checkout counter at the Kornmarkt and castle funicular  \n",
      " railway station\n",
      "– Heidelberg Castle checkout counter\n",
      "Group order\n",
      "For groups of 15 people or more, you can order castle \n",
      "tickets via our reservation department (dispatch by post  \n",
      "within Germany). Please let us know the final number \n",
      "of participants two weeks before traveling (refund for  \n",
      "excess ordered tickets is not possible).\n",
      "Phone +49 6221 58 40-228\n",
      "gruppen@heidelberg-marketing.de\n",
      "Opening hours\n",
      "Heidelberg Castle\n",
      "Castle courtyard, Great Barrel\n",
      "All year: 9:00 am – 6:00 pm\n",
      "last admission 5:30 pm\n",
      "German Pharmacy Museum\n",
      "April – October: 10:00 am – 6:00 pm,\n",
      "last admission 5:40 pm\n",
      "November – March: 10:00 am – 5:30 pm,\n",
      "last admission 5:10 pm\n",
      " Note  Information are subject to change.HeidelbergCARD\n",
      "The HeidelbergCARD makes your journey even easier.  \n",
      "Benefit from the following included services:\n",
      "– castle ticket including funicular railway\n",
      "– free use of public transport in Heidelberg\n",
      "– combo ticket (one-time free entrance to the University \n",
      " Museum, the Student Prison and the special exhibition)\n",
      "– numerous discounts on tours, museums, leisure activities,  \n",
      " restaurants and shops. \n",
      "1 day – € 26 \n",
      "Valid from midnight to midnight on the\n",
      "day of validity.\n",
      "2 days – € 28 \n",
      "Valid all day on the first day until midnight \n",
      "the following day.\n",
      "4 days – € 30 \n",
      "Valid all day on the first day until\n",
      "midnight of the fourth day.\n",
      "Family 2 days – € 60 \n",
      "Valid all day on the first day until\n",
      "midnight the following day for a family\n",
      "(2 adults + up to 3 children or 1 adult\n",
      "+ up to 4 children under the age of 16).\n",
      "Funicular railway departure times\n",
      "Annual funicular railway maintenance\n",
      "expected March 4 – March 17, 2024\n",
      "Summer timetable\n",
      "valid from March 25 – November 1, 2024\n",
      "Kornmarkt – Castle\n",
      "9:00 am (every 10 minutes), last trip at 8:00 pm\n",
      "Castle – Kornmarkt\n",
      "9:03 am (every 10 minutes), last trip at 8:03 pm\n",
      "Winter timetable\n",
      "valid from November 2, 2024 – March 31, 2025\n",
      "Kornmarkt – Castle\n",
      "9:00 am (every 10 minutes), last trip at 5:10 pm\n",
      "Castle – Kornmarkt\n",
      "9:03 am (every 10 minutes), last trip at 5:43 pm\n",
      " Note  The lower funicular railway from the Kornmarkt station via the castle to the Molkenkur station is equipped to  \n",
      "facilitate disabled access. Strollers must be carried on the steps. The conductors are happy to assist you. We recommend  \n",
      "that families with small children use a buggy.\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Family\n",
      "Gültig ab / Valid from34 | Events 2024\n",
      "Heidelberger Frühling (Heidelberg Spring)\n",
      "March 15 - April 13, 2024\n",
      "One of the largest music festivals in Germany that belongs to \n",
      "the top league of international festivals for classical music and \n",
      "once again invites visitors to well over 100 events this year.\n",
      "www.heidelberger-fruehling.de/en\n",
      "Heidelberger Schlossfestspiele\n",
      "(Heidelberg Castle Festival)\n",
      "June 9 - July 28, 2024\n",
      "The one-of-a-kind Renaissance buildings, the sleepy nooks \n",
      "and crannies, as well as the sprawling gardens and parks \n",
      "offer fascinating opportunities for the artistic work of the \n",
      "Heidelberg Theater. Squares and surfaces become stages, \n",
      "walls and corners backdrops.\n",
      "www.schlossfestspiele-heidelberg.de\n",
      "Heidelberg Castle Illuminations\n",
      "with Fireworks\n",
      "June 1 and September 7, 2024\n",
      "Bengali lights blaze two times every summer on the walls  \n",
      "of the ruin in a picturesque and eternal manner. The illumi -\n",
      "nation of the beautiful facade of the castle is complemen -\n",
      "ted by festive fireworks which bathe the Old Town in an im -\n",
      "pressive brilliance.\n",
      "www.heidelberg-marketing.com\n",
      "Summer at the River\n",
      "August 24 and 25, 2024\n",
      "Stroll, relax, enjoy the river. The city is moving closer to the \n",
      "water and invites you to linger and stroll along the banks of \n",
      "the Neckar River, with musical entertainment and a varied \n",
      "range of information and gastronomic offerings.\n",
      "www.heidelberg-marketing.comHeidelberger Herbst (Heidelberg Autumn)\n",
      "September 28 and 29, 2024\n",
      "One of the biggest Old Town festivals in the region features \n",
      "an artisan market, a giant flea market, numerous regional \n",
      "specialties and many bands providing entertainment in  \n",
      "various different squares in the Old Town.\n",
      "www.heidelberg-marketing.com\n",
      "Heidelberg Wine Village\n",
      "October 2 - 13, 2024\n",
      "Enjoy delicious local and regional wines in the heart of  \n",
      "the Old Town.\n",
      "www.heidelberg-marketing.com\n",
      "Enjoy Jazz\n",
      "Beginning of October - Mid November 2024\n",
      "The festival offers diverse events for about seven weeks, with \n",
      "an emphasis not only on jazz, but also on other genres – in \n",
      "exclusive venues, e.g. in Heidelberg.\n",
      "www.enjoyjazz.de/en/\n",
      "Heidelberg Christmas Market\n",
      "November 25 – December 22, 2024\n",
      "Nestled in the Old Town, illuminated by the world-famous \n",
      "castle above, the Heidelberg Christmas Market invites you to \n",
      "take some time out and visit one of its six historic locations.\n",
      "“Winterwäldchen” (Winter Forest)\n",
      "and Heidelberg Ice Rink\n",
      "November 25, 2024 – Beginning of January \n",
      "2025\n",
      "At the Kornmarkt, the Winter Forest beckons visitors with its \n",
      "magical atmosphere, while one of Germany’s prettiest ice \n",
      "rinks offers ice-skating pleasure on the Karlsplatz.\n",
      "www.heidelberg-marketing.comExcerpt of the events 2024\n",
      "A great program all year round.\n",
      "Further events can be found on our website . \n",
      "Dates and information are subject to change.\n",
      "Further events can be found on our website . \n",
      "Dates and information are subject to change.36 | Good to know\n",
      "Good to know\n",
      "The central location of Heidelberg makes traveling easy with all modes of transport.\n",
      "Rail\n",
      "Heidelberg is well connected to the European long distance \n",
      "network, as well as to the German Rail Network (ICE and IC \n",
      "/ EC, www.bahn.de). Within the region and across its bor -\n",
      "ders, the Rhine-Neckar Transport Association ensures the \n",
      "best connections with the S-Bahn  (www.vrn.de).\n",
      "Car / Bus\n",
      "The A5 / A6 highways (Autobahn) access large sections of \n",
      "the entire Rhine-Neckar region with inter-connected exits \n",
      "and connecting federal roads. The A5 / A656 highways have \n",
      "direct exits to Heidelberg.\n",
      "Airports\n",
      "– City Airport Mannheim (approx. 18 km)\n",
      " www.flugplatz-mannheim.de/en\n",
      "– Frankfurt Airport (approx. 80 km)\n",
      " www.frankfurt-airport.com/en\n",
      "– Baden-Airpark Flughafen\n",
      " Karlsruhe / Baden-Baden (approx. 90 km)\n",
      " www.baden-airpark.de/en/\n",
      "– Flughafen Stuttgart (approx. 120 km)\n",
      " www.flughafen-stuttgart.de\n",
      "– Frankfurt Hahn Airport (approx. 150 km)\n",
      " www.hahn-airport.de/enTLS Transfer & Limousine Service\n",
      "Simply and conveniently book your TLS transfer that will \n",
      "pick you up directly at your terminal at Frankfurt Airport \n",
      "and take you to your hotel in Heidelberg! Prices on request.\n",
      "HLS Heidelberg Limousine Service\n",
      "Book with us an exclusive limousine service, VIP first \n",
      "class service, cab service, airport transfer or shuttle ser -\n",
      "vice. Our trained chauffeurs will bring you safely to your \n",
      "destination. Price on request.\n",
      "RV / Camping\n",
      "–  RV site\n",
      " Harbigweg 1 – 3, 69124 Heidelberg\n",
      " 48 parking spaces\n",
      " open all year round\n",
      " www.wohnmobilstellplatz-heidelberg.com\n",
      "– Camping Heidelberg\n",
      " Schlierbacher Landstraße 151, 69118 Heidelberg\n",
      " quiet location, right beside the Neckar River\n",
      " open April – October\n",
      " www.camping-heidelberg.de\n",
      "– Camping Haide\n",
      " Ziegelhäuser Landstraße 91, 69151 Neckargemünd\n",
      " 200 parking spaces, right beside the Neckar River\n",
      " open April – November\n",
      " www.camping-haide.de\n",
      " ”Nette Toilette” (Nice restrooms)\n",
      "You can use a total of 30 “nice restrooms” in the Old Town \n",
      "area without being forced to purchase or consume \n",
      "anything. Some restaurants and stores bear the “Nette  \n",
      "Toilette” symbol on the entrance door.\n",
      "www.heidelberg.deGetting here\n",
      "Good to know | 37\n",
      "Public passenger transport\n",
      "You can get information on the Heidelberg public trans -\n",
      "portation lines and fares around the clock from the service \n",
      "hotline +49 621 1077077, online at www.vrn.de or at Rhein-\n",
      "Neckar-Verkehr GmbH’s (RNV) Customer Service Center at \n",
      "the Hauptbahnhof (main station), Kurfürsten-Anlage 62.\n",
      "Bicycle\n",
      "Because Heidelberg is a very bicycle-friendly town, bike  \n",
      "rental is a good move.\n",
      "– Radolino bike rental\n",
      " www.radolino.de\n",
      "– radhof BERGHEIM\n",
      " www.fahrrad-heidelberg.de\n",
      "– DB-bike rental system in front of the main station\n",
      " www.callabike.de/en\n",
      "– Pedelec rental (E-bike), several locations\n",
      " www.rueckenwind-hd.org\n",
      "– Joyrides E-bike rental\n",
      " www.joyrides-rent.de\n",
      "VRNnextbike\n",
      "VRNnextbike has numerous bike stations in the metropo -\n",
      "litan area Rhine-Neckar and operates around the clock. \n",
      "The stations are located at busy transport hubs near bus \n",
      "and tram stops. For further information:\n",
      "Phone: +49 30 69205046, www.vrnnextbike.de/enCarsharing\n",
      "– Stadtmobil Rhein-Neckar\n",
      " Phone +49 621 12855585\n",
      " www.rhein-neckar.stadtmobil.de\n",
      "– eCarsharing Rhein-Neckar\n",
      " Phone +49 6221 3574974\n",
      " www.ecs-rn.de\n",
      "– AnyMove\n",
      " Phone + 493083795645\n",
      " www.anymove.app/heidelberg\n",
      "Taxi\n",
      "– Taxizentrale\n",
      " Phone +49 6221 302030\n",
      " www.taxizentrale-heidelberg.de\n",
      "– TaxiHDirekt\n",
      " Phone +49 6221 739090\n",
      " www.taxihdirekt.de\n",
      "e-Scooter\n",
      "At many locations throughout the city of Heidelberg, nu -\n",
      "merous e-scooters from various providers are available for \n",
      "rent. To rent an e-scooter, register via the app of the respec -\n",
      "tive rental company:\n",
      "– www.bolt.eu\n",
      "– www.li.me\n",
      "– www.tier.app/en\n",
      "– www.zeusscooters.comModes of transportation in Heidelberg\n",
      "Mobile in  \n",
      "HeidelbergBooking form Special Activities\n",
      "Please complete this form and send it to  Note  It is possible to book a tour guide for a  \n",
      "Heidelberg Marketing GmbH, Neuenheimer Landstraße 5, maximum of 20 people in a group (except for tours in  \n",
      "69120 Heidelberg (Germany),  your own coach). For larger groups it is necessary to book  \n",
      "Fax +49 6221 5840-222, guide@heidelberg-marketing.de  more guides.\n",
      "Contact for further questions +49 6221 5840-223/-225 \n",
      "(Only for urgent matters on weekends: +49 6221 58-44444)\n",
      "I would like to make a binding booking  (*mandatory fields, **You will receive the invoice a few days after the tour.)\n",
      "Date and weekday*  ....................................................................................................................................................................................................................................\n",
      "Total number of persons* .................................  Number of guides* .................................  Language* ....................................................................................\n",
      "Duration*  ....................... hours, from ...................................... to ...................................... am / pm\n",
      "Special Activities / Guided Tours*\n",
      " Old Town Tour        City and Castle Sightseeing Tour\n",
      " Castle Tour       Other tours (Please fill in the name of the tour.)\n",
      " Old Town and Castle Tour (combined)\n",
      "Name of the tour* .......................................................................................................................................................................................................................................\n",
      "Meeting point*\n",
      " University Square / Lion‘s Fountain (recommended for arrival by foot, with public transportation or car)\n",
      " Neckarmünzplatz (recommended for arrival with own bus)\n",
      " Bus parking place at Heidelberg Castle (recommended for Castle Tour and arrival with own bus)\n",
      " Castle visitor center (recommended for Castle Tour and if you are traveling individually)\n",
      " Special requests ..........................................................................................................................................................................................................................................\n",
      "Payment*\n",
      " The tour and any possible entry fees will be paid by invoice.**\n",
      " The tour will be paid by invoice**, any possible entry fees will be paid cash on site.\n",
      " Note  Cash payment for guided tours is not possible.\n",
      "Contact*\n",
      "First and last name / title ...............................................................................................................................................................................................................................\n",
      "Company / institute ........................................................................................................................................................................................................................................\n",
      "Street / zip code / city ....................................................................................................................................................................................................................................\n",
      "Phone / fax / e-mail (to receive the booking confirmation) ....................................................................................................................................................................\n",
      "Contact person on site (name and mobile number)  ...............................................................................................................................................................................\n",
      "Share some additional information to help the tour guide to get prepared for your tour:\n",
      "Type of group .................................................................................................................................................................................................................................................\n",
      "Average age (constitution, physical impairments) ...................................................................................................................................................................................\n",
      "Will the guests stay in Heidelberg for the first time?       Yes     No\n",
      "Other tour agenda items in Heidelberg .....................................................................................................................................................................................................\n",
      "Special requests / interests / requirements ..............................................................................................................................................................................................\n",
      " Please send me the Heidelberg Marketing GmbH newsletter by e-mail.\n",
      " Note  I hereby confirm that I have read, understood and accepted the General Terms and Conditions.\n",
      " Note  Please note that you will find the privacy policy in detail on our website: \n",
      "https://www.heidelberg-marketing.de/en/about-us/privacy-statement.  With your signature, you agree to our privacy policy. \n",
      "Date / signature / stamp  ..................................................................................................................................................................................................................38 | Booking form Special ActivitiesHeidelberg Marketing GmbH\n",
      "       /heidelberg4you\n",
      "www.heidelberg-marketing.com\"Heidelberg Trio\"special  \n",
      "price\n",
      "€ 22.90\n",
      "instead of  \n",
      "€ 27.90\n",
      "1 bottle of Riesling (Winery Clauer)\n",
      "1 bottle of Pinot Noir (Winery Hans Winter)\n",
      "1 bottle of Pinot Noir Rosé (Winery Bauer)\n",
      "Available at the Tourist Information\n",
      "Neckarmünzplatz.\n",
      "  \n",
      "Heidelberg Marketing GmbH\n",
      "       /heidelberg4you\n",
      "www.heidelberg-marketing.deDer Heidelberger Dreiklang  \n",
      "Heidelberg Marketing GmbH\n",
      "       /heidelberg4you\n",
      "www.heidelberg-marketing.deDer Heidelberger Dreiklang \n",
      "  \n",
      " \n",
      "General Terms and Conditions  \n",
      "Heidelberg Marketing GmbH, Special Activities  \n",
      " \n",
      "Dear guests,  \n",
      "As far as they are effectively agreed, the following provisions shall become the terms  of \n",
      "the service contract concluded between the custom er and Heidelberg Marketing GmbH  \n",
      "(hereinafter: “ HDM “) for provision of Special A ctivities . They shall supplement the \n",
      "statutory provisions of sections 611 et seq. German Civil Code and detail these . \n",
      "Therefore, please read these Terms and Conditions with car e before booking!  \n",
      " \n",
      "1. Position of HDM; Area of Application of these Terms and Conditions; Applicable \n",
      "Legal Provisions  \n",
      " \n",
      "1.1 These terms and conditions for Special A ctivities  shall apply to guided tours for \n",
      "visitors, tours and boat rides that are offered in the catalog “Special Activities” of HDM  \n",
      "and that take less than 24 hours according to section 651a para. 5 no. 2 German Civil \n",
      "Code, and do not include any overnight stay (day trips) and the travel price of which \n",
      "does  not exceed 500 Euro. These day trips are hereinafter referred to as “Special \n",
      "Activities ”. \n",
      "1.2 HDM  shall render the offered Special Activity services as a service provider and direct \n",
      "contracting partner of the customer or the client.  \n",
      "1.3 The legal relationship between HDM and the customer  or the  client shall be \n",
      "primarily subject to the agreements reached with HDM , and these terms and conditions \n",
      "as a supplement, with the statutory provisions on contracts for services section 611 et \n",
      "seqq. German Civil Code applying alternatively . \n",
      "1.4 As far as mand atory provisions under international or European law that are to apply \n",
      "to the contractual relationship with HDM do not stipulate anything else to the \n",
      "customer’s or client’s benefit, the entire legal and contractual relationship with HDM \n",
      "shall be subject to  German law exclusively . \n",
      "1.5 The following provisions shall only apply to Special Activities  of HDM . Travel \n",
      "agreements and multi -day trips that include accommodation services are subject to the \n",
      "travel conditions of HDM . \n",
      " \n",
      "2. Conclusion of the Contract; Groups; Provision of a Group Client  \n",
      " \n",
      "2.1 The following shall apply to all bookings of Special Activities : \n",
      "a) Bookings are accepted as bookings in person, by phone, by fax or by email.  \n",
      "b) The basis of the offer from HDM  and the customer’s booking shall be the d escription \n",
      "of the special activity  and the supplementary information in the booking basis, as far as \n",
      "these are available to the customer when booking.  \n",
      "c) If the content of the booking confirmation deviates from the content of the booking, \n",
      "this constitutes a new offer by HDM . The contract shall be concluded based on this new \n",
      "offer when the customer declares acceptance by express declaration, deposit  or \n",
      "payment of a remaining amount or by using the services.  \n",
      "d) The customer who places the booking shall be lia ble for the contractual obligations \n",
      "of other participants for whom he places the booking as if fo r his own, as far as he has \n",
      "assumed the corresponding obligation by express and separate declaration. The same \n",
      "shall apply respectively to group clients or per sons responsible for the group with \n",
      "regard to the participants of special activities as registered by the group client or \n",
      "person responsible for the group.  \n",
      "2.2 The following provisions shall apply as supplements for Special Activities to closed \n",
      "groups. Special Activities to closed groups within the meaning of these provisions shall \n",
      "only be group trips that are organized by HDM  as the responsible provider and booked \n",
      "and / or processed via a p erson responsible for the group or a group client who acts as \n",
      "autho rized person for a certain group of participants  and takes over the role as only  \n",
      "customer vis -à-vis HDM.  \n",
      "2.3 HDM  and the respective group client can agree in respect of such a group trip that \n",
      "the group client as authorized representative of the group trip participants is granted \n",
      "special rights.  \n",
      "2.4 HDM  shall not be liable for any services or service aspects of any kind that – with or \n",
      "without knowledge of HDM  – are offered, organized, performed and  / or provided to \n",
      "the customers by the group client or person  responsible for the group in addition to the \n",
      "services of HDM . This shall specifically include the travel to and from departure and \n",
      "return locations contractually agreed with HDM  that is organized by the group client or \n",
      "the person responsible for the group , any events not contained in the service scope of \n",
      "HDM  before and after the Special Activity and along the way (transport, excursions, \n",
      "meetings, etc.) and any tour guides deployed by the group client or person responsible \n",
      "for the group who are not contract ually owed by HDM . 2.5 HDM  shall not be liable for any measures and omissions of the group client or person \n",
      "responsible for the group or any tour guide deployed by the group client or person \n",
      "responsible for the group before, during and after the tour, in p articular not for any \n",
      "changes to contractual services that are not coordinated with HDM , instructions to local \n",
      "guides, special agreements with the different service providers, information and \n",
      "representations towards the customers.  \n",
      "2.6 As far as this is not  expressly agreed, the group client or the person responsible for \n",
      "the group or any tour guides deployed by him shall not have the right or the \n",
      "authorization to receive any reports on defects from the group tour participants. They \n",
      "also shall not have the ri ght to accept any customer complaints or payment claims in the \n",
      "name of HDM  for HDM  during or after the Special Activity . \n",
      "2.7 Bookings of Special Activities shall be directly binding upon the customer and shall \n",
      "lead to conclusion of the binding contract on the Special Activity by HDM ’s confirmation \n",
      "by phone or orally. The contract shall therefore be concluded by receipt of the booking \n",
      "confirmation (acceptance declaration) by HDM , which shall not require any specific \n",
      "form, with the consequence that oral confi rmations and confirmations by phone shall \n",
      "be legally binding upon the customer.  \n",
      "2.8 HDM  notes that there is no right to withdraw  according to the statutory provisions \n",
      "(section 312g paragraph 2 sentence 1 no. 9 German Civil Code), even if the service \n",
      "contr act was concluded by way of distance selling. The other statutory rescission and \n",
      "termination rights of the customer shall not be affected by this.  \n",
      "2.9 For bookings made through the website of HDM, the following shall apply to \n",
      "conclusion of the contract:  \n",
      "a) By clicking  the button “Book subject to payment” , the customer bindingly offers \n",
      "conclusion of the contract for the Special Activity to HDM . Receipt of the customer’s \n",
      "booking will be confirmed to him without delay electronically.  \n",
      "b) The submission of the c ontract offer by clicking  the button “Book subject to payment”  \n",
      "shall not found any claim of the customer or client to conclusion of a contract with HDM  \n",
      "according to his booking information. Instead, HDM  shall be free in its decision to accept \n",
      "or reject the  customer’s or client’s contract offer.  \n",
      "c) The contract shall be concluded by receipt of the booking confirmation of HDM  by the \n",
      "customer or client.   \n",
      " \n",
      "3. Services, Reservation of Replacement; Deviating Agreements; Changes to \n",
      "Essential Services; Duration of Services; Weather  \n",
      " \n",
      "3.1 The service owed by HDM  shall comprise rendering of the respective service \n",
      "according to the service description and the additionally concluded agreements.  \n",
      "3.2 If a certain group size must not be undercut or exceeded for a service, th is must be \n",
      "indicated in the service description.  \n",
      "3.3 Insofar as nothing else was explicitly agreed, special activities do not need to be \n",
      "offered by a specific person (f. ex. one specific tour guide). In the event that a specific \n",
      "person has been identified,  we reserve the right to replace that person with another \n",
      "person in case of compelling reasons (in particular due to illness). If HDM is not able to \n",
      "find a replacement (in particular for solo self -employed persons) in the event of \n",
      "compelling reasons not at tributable to HDM, HDM shall have the right to rescind the \n",
      "contract and/or shall have the right to an extraordinary termination of the contract \n",
      "with good cause. In that case the customer shall not be obligated to pay any \n",
      "remuneration. Any further rights of  the customer, in particular reimbursements for \n",
      "outward or inward journeys, shall be excluded.  \n",
      "3.4 Modifications of or supplements to the contractually offered services  shall require \n",
      "an express agreement with HDM , for which text form is urgently recommende d to serve \n",
      "as evidence.  \n",
      "3.5 Modifications of essential services  that deviate from the agreed terms  of the contract \n",
      "and that become necessary after conclusion of the contract (in particular also changes \n",
      "to the schedule of the respective rendering of service s) and that were not initiated by \n",
      "HDM  in bad faith shall be permitted as far as the changes are not considerable and do \n",
      "not impair the overall scope of the service. Any warranty claims of the customer or client \n",
      "in case of such modifications of essential se rvices shall not be affected.  \n",
      "3.6 Information on the duration of services shall be approximates.  \n",
      "3.7 The following shall apply to weather conditions and their effects on agreed services:  \n",
      "a) Where not expressly agreed on differently from case to case, the agreed services shall \n",
      "take place in any weather . \n",
      "b) Weather shall therefore not entitle  the customer or client to free of charge rescission  \n",
      "or termination concerning the contract with HDM . This shall not apply only if the  \n",
      "weather impairs the body, health o r property of the customer or the participants of the 40 | General Terms and Conditions \n",
      "  \n",
      "client in the service so considerably that performance is objectively unreasonable for the \n",
      "customer or client and his participants.  \n",
      "c) If such situations are present at commencement of the service or if  they are \n",
      "objectively expected for the agreed time of the service before its commencement, both \n",
      "the customer or client, and HDM  shall have the right to terminate the contract for the \n",
      "service by way of proper or extraordinary termination.  \n",
      "d) In the event of  such termination by HDM, the customer or client shall not have any \n",
      "claims for reimbursement of costs, in particular any travel and accommodation fees, \n",
      "except if contractual or statutory claims of the customer or client to damages or \n",
      "reimbursement of expen ses are justified regarding this.  \n",
      " \n",
      "4. Rendering of Services and Payment Terms  \n",
      " \n",
      "4.1 The agreed services shall include rendering of services and additionally offered and \n",
      "agreed services.  \n",
      "4.2 The agreed price shall be paid 14 days after the invoice date or at  commencement \n",
      "of the Special Activity , depending on the agreement by HDM . \n",
      "4.3 The following shall apply where the customer has no contractual or statutory \n",
      "rescission  right and HDM  is willing and able to render the contractual services:  \n",
      "a) If the  customer d oes not pay the service fee when due, or if the payment is \n",
      "incomplete, HDM  shall have the right to withdraw from the contract after sending a \n",
      "reminder stipulating an appropriate grace period, and after expiration of this period, \n",
      "and to claim damages from t he customer in accordance with sections 280 (1), 241 (2) \n",
      "German Civil Code, in accordance with the proviso of the following item 8, unless the \n",
      "customer has a right to set off or retention at the due date or is not at fault for the default \n",
      "of payment.  \n",
      "b) Without complete payment of the service price, the customer shall not have any claim \n",
      "to using the services.  \n",
      " \n",
      "5. Booking Changes  \n",
      " \n",
      "A claim of the customer or the client to modifications concerning the date of the service, \n",
      "time, departure and destination sites of the services (booking change)  after conclusion \n",
      "of the contract shall not apply . Upon the customer’s or client’s wish, it can be reviewed \n",
      "whether a booking change is possible anyway. The booking change request will only be \n",
      "accepted in text form.  \n",
      " \n",
      "6. Non -Utilization of Services  \n",
      " \n",
      "6.1 If the customer or client does not use the agreed services wholly or in part without \n",
      "any fault of HDM, in particular due to not appearing for rendering of the respective \n",
      "service without termination of the contract , although HDM  is willing and able to render \n",
      "the services, there shall be no claim to reimbursement of payments already made . \n",
      "6.2 The agreed remuneration shall be according to the statutory provisions (section 615 \n",
      "s. 1 and 2 German Civil Code) : \n",
      "a) The agreed remuneratio n shall be paid  without there being any claim to subsequent \n",
      "performance.  \n",
      "b) However, HDM  shall accept set -off of expenses saved and any remuneration that \n",
      "HDM  acquires by other use of the agreed services or neglects to acquire in bad faith \n",
      "against the remun eration.  \n",
      " \n",
      "7. Rescission by  HDM because the minimum number of participants is not \n",
      "reached  \n",
      " \n",
      "7.1 HDM may rescind  the contract if a minimum number of participants according to \n",
      "the following provisions is not reached:  \n",
      "a) The minimum number of participants and t he latest date of rescission  by HDM must \n",
      "be clearly stated in the specific service offer or, in the case of consistent provisions for \n",
      "specific types of Special Activities , in a general notice or general service description.  \n",
      "b) HDM must clearly state the mi nimum number of participants and the latest rescission  \n",
      "deadline in the booking confirmation or refer to the corresponding information in the \n",
      "service description.  \n",
      "c) HDM shall be obligated to declare cancelation of the Special Activity towards the \n",
      "customer without undue delay once it is certain that the Special Activity will not take \n",
      "place due to the minimum number of participants not being reached.  \n",
      "d) If a minimum number of participants is agreed, a deposit  specified in the offer shall \n",
      "be due upon booking, and the remaining payment shall be due  upon confirmation of \n",
      "the event.  \n",
      "7.2 If the Special Activity does not take place for this reason, the customer shall be \n",
      "reimbursed for the Special Activity without undue delay.  \n",
      " \n",
      " \n",
      " \n",
      " 8. Termination and Rescission  by the Customer or Client  \n",
      " \n",
      "8.1 The customer or client may terminate  the contract with HDM  after conclusion of \n",
      "the contract. Termination shall not require any specific form. Terminations shall be \n",
      "submitted in written form (Section 126b of the German Civil Code, f.  ex. e -mail or fax).  \n",
      "8.2. If the customer or client declares termination or does not use services without \n",
      "declaring termination – in particular by not appearing –, HDM  may demand \n",
      "reimbursement for the travel plans made and the connected expenses. Calculati on of \n",
      "the reimbursement shall generally consider  possible other uses of the service and \n",
      "usually saved  expenses. The following cancel ation fees shall apply:  \n",
      "from day 9 to day 5 before the (first) day of rendering the service 50 %,  \n",
      "from day 4 onwards and if not showing up to the event 90 %  of the total price agreed.  \n",
      "8.3 The customer shall in any case have the right to prove to HDM  that HDM  has incurred \n",
      "no damage or a much lesser damage than the flat rate claimed by it in compensation.  \n",
      "8.4 HDM  reserves the rig ht to demand a higher specific compensation instead of the \n",
      "above amounts as far as HDM  proves that HDM  has incurred considerably higher \n",
      "expenses, in particular where individual service components of the Special Activity are \n",
      "not reimbursed by the service pr ovider. If HDM  asserts such a claim, HDM  shall be \n",
      "obligated to specify the amount and document the required compensation under \n",
      "consideration of any saved expenses and any other use of the travel service.  \n",
      "8.5 The above termination rules shall not affect the  statutory or contractual termination \n",
      "rights of the customer in case of defects of the services of HDM  and any other statutory \n",
      "warranty claims.  \n",
      " \n",
      "9. Liability of HDM; Insurances  \n",
      " \n",
      "9.1 HDM shall be liable  without restriction as far as the damage results from violation \n",
      "of an essential obligation of HDM, the performance of which was required for proper \n",
      "execution of the contract or the violation of which endangers achievement of the \n",
      "purpose of the contract, or the damage results from violation of life, body or health of \n",
      "the customer. Apart from this, liability of HDM  shall be limited to damage caused by \n",
      "HDM  or its servants willfully or grossly negligently.  \n",
      "9.2 HDM  shall not be liable  for services, measures or omissi ons of accommodation and \n",
      "meal operations or any other providers that are visited in connection with the service, \n",
      "except if the damage was caused or contributed to by culpable violation of obligations \n",
      "of HDM . \n",
      "9.3 The agreed contractual services contain insu rances to the benefit of the customer or \n",
      "client only if this is expressly agreed. The customer or client is expressly  recommended \n",
      "to take out travel rescission insurance.  \n",
      " \n",
      "10. Termination due to behavior -related Reasons  \n",
      " \n",
      "10.1 HDM  may terminate the servic e contract without observing any period of notice if \n",
      "the customer causes sustained disturbance in spite of a warning by HDM  or acts in \n",
      "violation of the contract at a scope that justi fies immediate cance lation of the contract.  \n",
      "10.2  If HDM  declares terminati on, HDM  shall retain the claim to the service price; \n",
      "however, HDM  must accept set -off of the value of saved expenses and the benefit that \n",
      "HDM  acquires from other use of the service not utilized.  \n",
      " \n",
      "11. Special Obligations of Customers Concerning Special Acti vities with Physical \n",
      "Activities (e.g. by Bicycle or Segway)  \n",
      " \n",
      "11.1 The customer  shall be responsible for finding out before booking and before using \n",
      "the Special Activities whether the respective activities are suitable for them under \n",
      "consideration of their personal health.  \n",
      "11.2 HDM shall not owe any special medical information or instruction in this respect \n",
      "unless explicitly agreed, in particular concerning the respective customer’s  specific \n",
      "situation.  \n",
      "11.3 HDM or its vicarious agents (guides, etc.) may excl ude the customer  wholly or in \n",
      "part if there are any reasonable indications that the Special Activities may overtax the \n",
      "guest, provided that the customer  threatens to endanger themselves or others due to \n",
      "this. Item 6 et seq. shall apply accordingly.  \n",
      "11.4  If the customer  withdraws or discontinues due to injury or illness for which the \n",
      "provider is not at fault, or at their own request, the provisions of item 6 et seq. shall \n",
      "apply as well.  \n",
      "11.5 Although the Special Activities are accompanied by a guide, they re quire a high \n",
      "degree of personal responsibility on the side of the customer . \n",
      "11.6  Customers  are advised to wear clothing suitable for Special Activities that protects \n",
      "them from strong sunlight, rain, or wind. It is also recommended that they carry a \n",
      "change of clothes. The provider of the Special Activities reserves the right to exclude any \n",
      "customer  from the Special Activity  for safety reasons wholly or in part if they join the \n",
      "Special Activities in unsuitable clothing or footwear.  General Terms and Conditions | 41 \n",
      "  \n",
      "11.7 Instructions of the g uides before and during the Special Activities must be \n",
      "observed. Traffic rules must be observed and consideration for other road users must \n",
      "be shown at all times.  \n",
      "11.8  Non -swimmers are not permitted to participate in physically active Special \n",
      "Activities  on water.  \n",
      " \n",
      "12. Special Arrangements in Connection wit h Pandemics (in particular the \n",
      "Corona Virus)  \n",
      " \n",
      "12.1 The parties agree that the agreed services shall always be provided by HDM in \n",
      "compliance and in accordance with the official requirements and conditions applicable \n",
      "at the time at which the service is rendered.  \n",
      "12.2 The customer agrees to observe appropriate rules of use or restrictions imposed \n",
      "by HDM when using services and to inform HDM without undue delay if they experience \n",
      "any typical symptoms of illnes s. \n",
      "12.3  The provisions above shall not affect any possible warranty claims of the \n",
      "customer.  \n",
      " \n",
      "13. Choice of Law; Place of Jurisdiction; Information on  Consumer Dispute \n",
      "Resolution  \n",
      " \n",
      "13.1 The entire legal and contractual relationship between the customer and HDM shall \n",
      "be subject to German law exclusively. The customer may raise a claim against HDM  only \n",
      "at the registered office of HDM . \n",
      "13.2  For actions of HDM  against the customer, the customer’s place of residence shall \n",
      "be relevant. For claims against customers who are merchants, legal entities under public \n",
      "or private law or persons who have their residence or common abode abroad or whose \n",
      "place of residence or common abode is not known at the time at which the claim is \n",
      "raised, the place of jurisdiction shall be t he office of HDM . \n",
      "13.3  The above provisions shall not apply  \n",
      "a) if and as far as anything other results to the benefit of the customer from any terms \n",
      "that cannot be contractually waived from conventions applicable to the contract for \n",
      "services between the cu stomer and HDM or  for any contracts concluded in electronic  \n",
      "legal transactions  b) if and as far as any provisions applicable to the contract for services that cannot be \n",
      "waived in the member state of the EU to which the customer belongs are more beneficial \n",
      "for the customer than the above provisions or the correspondi ng German provisions.  \n",
      "13.4  In light of the law on consumer dispute resolution, HDM  notes that HDM  will not \n",
      "participate in any voluntary consumer dispute resolution. If consumer dispute \n",
      "resolution was to become binding upon HDM  after printing of these trave l conditions, \n",
      "HDM  shall inform the consumers about this in a suitable form. HDM  informs about the  \n",
      "European online dispute solution platform https://ec.europa.eu/consumers/odr/  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "© Copyright protection.  \n",
      "TourLaw - Noll  | Hütten | Dukic Rechtsanwälte, Stuttgart | München 2023  \n",
      " \n",
      "Tourism agency:  \n",
      "Heidelberg Marketing GmbH  \n",
      "Managing director: Mathias Schiemer  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg, Germany  \n",
      " \n",
      "Phone:      +49 6221 5840 - 200 \n",
      "Telefax:     +49 6221  5840 - 222 \n",
      "info@heidelberg -marketing.de  \n",
      " \n",
      "Commercial register number: HRB 337405  \n",
      "Register court: AG Mannheim  \n",
      "VAT ID: DE226325597  \n",
      " \n",
      " \n",
      " 42 | General Terms and ConditionsImprint\n",
      "Heidelberg Marketing GmbH  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg / Germany\n",
      "Phone +49 6221 5840-200 \n",
      "Fax +49 6221 5840-222 \n",
      "info@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.com\n",
      "The Heidelberg Marketing GmbH is a \n",
      "subsidiary of the City of Heidelberg\n",
      "Content  \n",
      "Heidelberg Marketing GmbH\n",
      "Layout  \n",
      "aB Grafik | Artem Bathauer \n",
      "www.a-b-grafik.de\n",
      "Photos\n",
      "Cover page, pages 3, 4, 6, 9, 10, 11, 17, 18, \n",
      "19, 24, 26, 32, 35, 36, 37 – Tobias Schwerdt\n",
      "Page 8 – Christian Buck\n",
      "Pages 14, 22 – TMBW / Stefan Kuhn\n",
      "Page 20 – Heidelberg Marketing GmbH\n",
      "Page 25 – Riverboat GmbH\n",
      "Page 27 – City of Schwetzingen / Tobias Schwerdt\n",
      "Page 28 – Frank Jäger\n",
      "© Copyright 2024. All contents, in particular texts, \n",
      "photographs and graphics, are protected by co -\n",
      "pyright. Unless expressly stated otherwise, Hei -\n",
      "delberg Marketing GmbH owns the copyright.Heidelberg  \n",
      "Marketing GmbH\n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg / Germany\n",
      " \n",
      "Phone +49 6221 58 - 44444 \n",
      "Fax +49 6221 58 - 40222 \n",
      "info@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.com\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SRH University of Applied  Sciences / Library | EZProxy -FAQs   1 / 2 \n",
      " What is EZProxy?  \n",
      "EZProxy gives university members the ability to access library e -books, e -journals  and \n",
      "databases outside of the campus network, on the road or from home.  \n",
      "EZProxy is a convenient alternative to VPN and CITRIX. No installation of additional \n",
      "software or plugins is required; login is simply done with the university ID.  \n",
      " \n",
      "What are the technic al requirements?  \n",
      "EZProxy works independently of the browser and operating system used.  \n",
      "To use EZProxy, Javascript, pop -ups and cookies must be allowed in the browser. The \n",
      "default settings of browsers usually meet these requirements.  \n",
      " \n",
      "Does EZProxy access wo rk for all databases?  \n",
      "EZProxy access is available for almost all databases licensed by the library.  \n",
      "However, EZProxy access does NOT work for Beck online  (remote access only possible \n",
      "after personal registration on campus with 2 -factor authentication. -> Instructions ). \n",
      " \n",
      "How can I log in?  \n",
      "University members can select the desired research source on the \" Digital Library \" page \n",
      "and acces s it by clicking on the link via EZProxy and using their university ID (username \n",
      "and password).  \n",
      " \n",
      "How can I see if the login worked?  \n",
      "You can see if you are logged in correctly in the address bar of your browser: the actual \n",
      "URL is followed by srh-hochschule -heidelberg.idm.oclc.org.  \n",
      "e.g. https://link -springer -com. srh-hochschule -heidelberg.idm.oclc.org  \n",
      "In addition, most databases display the university logo or name in the header.  \n",
      "Please note:  URLs of articles or e -books accessed with EZ -Proxy are not citable. Use the \n",
      "DOI or URN of the article or the (e)ISBN of the e -book where indicated.   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SRH University of Applied  Sciences / Library | EZProxy -FAQs   2 / 2 \n",
      "  \n",
      "Important:  If you have left the database web pages or call a database manually, e.g. via \n",
      "a bookmark, you will leave EZProxy and may have to log in again.  \n",
      "Tip: You can revise existing bookmarks on library databases by placing https:/ /login.srh -\n",
      "hochschule -heidelberg.idm.oclc.org/login?url=  in front of the original bookmark URL.  \n",
      " \n",
      "Error s when accessing with  EZProxy?  \n",
      "➢ The most common cause of errors are old browser cookies. Therefore, in case of \n",
      "EZProxy problems, first delete your browser cookies, close the browser, restart it and \n",
      "then try again. Switching browsers (e.g. Google Chrome instead of Firefox) can also \n",
      "help.  \n",
      "➢ Sometimes there are difficulties if the security options are set \"too strict\" in the \n",
      "browser settings, e.g. if cookies are rejected on principle. You must then explicitly \n",
      "allow cookies of EZProxy in the settings.  \n",
      "➢ In some networks firewalls or other proxies  prevent access to EZProxy. This is often \n",
      "the case in corporate networks. In this case please contact SRH -IT. \n",
      "➢ Sometimes images or navigation elements of web pages accessed via EZProxy are not \n",
      "displayed or are displayed incorrectly. In most cases this can b e fixed by reloading the \n",
      "page.  \n",
      "➢ Possibly EZProxy access to a newly licensed database has not yet been configured by \n",
      "us or the database provider has changed its web address. We are happy to receive \n",
      "any hints in this regard . \n",
      " \n",
      "\n",
      "Fundamentals of Data\n",
      "Engineering\n",
      "Plan and Build Robust Data Systems\n",
      "With Early Release ebooks, you get books in their earliest form—the\n",
      "authors’ raw and unedited content as they write—so you can take\n",
      "advantage of these technologies long before the of ficial release of these\n",
      "titles.\n",
      "Joe Reis and Matt HousleyFundamentals of Data Engineering\n",
      "by Joe Reis and Matt Housley\n",
      "Copyright © 2022 Joseph Reis and Matthew Housley . All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc. , 1005 Gravenstein Highway North,\n",
      "Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales\n",
      "promotional use. Online editions are also available for most titles\n",
      "(http://oreilly .com ). For more information, contact our\n",
      "corporate/institutional sales department: 800-998-9938 or\n",
      "corporate@or eilly.com .\n",
      "Acquisitions Editor:  Jessica Haberman\n",
      "Development Editor:  Nicole T ache\n",
      "Production Editor:  Gregory Hyman\n",
      "Interior Designer:  David Futato\n",
      "Cover Designer:  Karen Montgomery\n",
      "Illustrator:  Kate Dullea\n",
      "September 2022:  First Edition\n",
      "Revision History for the Early Release\n",
      "2021-10-08:  First Release\n",
      "2021-1 1-12:  Second Release\n",
      "2022-03-02:  Third ReleaseSee http://oreilly .com/catalog/errata.csp?isbn=9781098108304  for release\n",
      "details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc.\n",
      "Fundamentals of Data Engineering , the cover image, and related trade\n",
      "dress are trademarks of O’Reilly Media, Inc.\n",
      "The views expressed in this work are those of the authors, and do not\n",
      "represent the publisher ’s views. While the publisher and the authors have\n",
      "used good faith ef forts to ensure that the information and instructions\n",
      "contained in this work are accurate, the publisher and the authors disclaim\n",
      "all responsibility for errors or omissions, including without limitation\n",
      "responsibility for damages resulting from the use of or reliance on this\n",
      "work. Use of the information and instructions contained in this work is at\n",
      "your own risk. If any code samples or other technology this work contains\n",
      "or describes is subject to open source licenses or the intellectual property\n",
      "rights of others, it is your responsibility to ensure that your use thereof\n",
      "complies with such licenses and/or rights.\n",
      "978-1-098-10830-4Chapter 1. Data Engineering\n",
      "Described\n",
      "A NOTE FOR EARLY RELEASE READERS\n",
      "With Early Release ebooks, you get books in their earliest form—the\n",
      "authors’ raw and unedited content as they write—so you can take\n",
      "advantage of these technologies long before the of ficial release of these\n",
      "titles.\n",
      "This will be the first chapter of the final book.\n",
      "If you have comments about how we might improve the content and/or\n",
      "examples in this book, or if you notice missing material within this\n",
      "chapter , please reach out to the authors at\n",
      "book_feedback@ternarydata.com .\n",
      "Since this book is called Fundamentals of Data Engineering , it’s critical\n",
      "that we clearly define what we mean by data engineering and data engineer .\n",
      "Terms matter , and there’ s a lot of confusion about what data engineering\n",
      "means, and what data engineers do. Let’ s look at the landscape of how data\n",
      "engineering is described, and develop some nomenclature we can use\n",
      "throughout this book.\n",
      "What Is Data Engineering?\n",
      "So what is data engineering? This is a great question. Though data\n",
      "engineering has existed in some form since companies have done things\n",
      "with data—such as data analysis and reports—it came into sharp focus\n",
      "alongside the rise of data science in the 2010s. But what is data engineering,\n",
      "exactly? Let’ s start with some real talk—there are endless definitions ofdata engineering. A Google exact-match search for “what is data\n",
      "engineering?” returns over 91,000 unique results.\n",
      "Let’s look at a few examples to illustrate the dif ferent ways data\n",
      "engineering is defined:\n",
      "Data engineering is a set of operations aimed at cr eating interfaces and\n",
      "mechanisms for the flow and access of information. It takes dedicated\n",
      "specialists—data engineers—to maintain data so that it r emains\n",
      "available and usable by others. In short, data engineers set up and\n",
      "operate the or ganization’ s data infrastructur e preparing it for further\n",
      "analysis by data analysts and scientists.\n",
      "The first type of data engineering is SQL-focused. The work and primary\n",
      "storage of the data is in r elational databases. All of the data pr ocessing\n",
      "is done with SQL or a SQL-based language. Sometimes, this data\n",
      "processing is done with an ETL tool. The second type of data engineering\n",
      "is Big Data-focused. The work and primary storage of the data is in Big\n",
      "Data technologies like Hadoop, Cassandra, and HBase. All of the data\n",
      "processing is done in Big Data frameworks like MapReduce, Spark, and\n",
      "Flink. While SQL is used, the primary pr ocessing is done with\n",
      "programming languages like Java, Scala, and Python.\n",
      "In relation to pr eviously existing r oles, the data engineering field could\n",
      "be thought of as a superset of business intelligence and data war ehousing\n",
      "that brings mor e elements fr om softwar e engineering. This discipline also\n",
      "integrates specialization ar ound the operation of so-called “big data”\n",
      "distributed systems, along with concepts ar ound the extended Hadoop\n",
      "ecosystem, str eam pr ocessing, and in computation at scale.\n",
      "Data engineering is all about the movement, manipulation, and\n",
      "management of data.\n",
      "Wow! That’ s only a handful of definitions, and you can see there’ s a lot of\n",
      "variety . Clearly , there is not yet a consensus around what data engineering\n",
      "means. If you’ve been wondering about the term “data engineering”, your\n",
      "confusion is understandable.1\n",
      "2\n",
      "3\n",
      "4This book provides a snapshot of data engineering today . To the fullest\n",
      "extent, we’re focusing on the “immutables” of data engineering—what\n",
      "hasn’ t changed, and what won’ t likely change in the future. There are no\n",
      "guarantees, as this field is rapidly evolving. That said, we think we’ve\n",
      "captured a great framework to guide you on your data engineering journey\n",
      "for many years to come.\n",
      "Before we define data engineering, you should understand a brief history of\n",
      "data engineering, how it’ s evolved, and where it fits into the general\n",
      "backdrop of data and technology .\n",
      "Evolution of the Data Engineer\n",
      "To understand data engineering today and tomorrow , it helps to have the\n",
      "context of how the field evolved. This is not a history book, but looking to\n",
      "the past is invaluable in understanding where we are today , and where\n",
      "things are going. There’ s a common theme that recurs constantly in data\n",
      "engineering: what’ s old is new again.\n",
      "The early days: 1980 to 2000, from data warehousing to the web\n",
      "The birth of the data engineer ar guably has its roots in data warehousing,\n",
      "dating as far back as the 1970s, with the “business data warehouse” taking\n",
      "shape in the 1980s, and Bill Inmon of ficially coining the term “data\n",
      "warehouse” in 1990. After the relational database and SQL were developed\n",
      "at IBM, Oracle popularized the technology . As nascent data systems grew ,\n",
      "businesses needed dedicated tools and data pipelines for reporting and\n",
      "business intelligence (BI). T o help people properly model their business\n",
      "logic in the data warehouse, Ralph Kimball and Bill Inmon developed their\n",
      "respective data modeling techniques and approaches, which are still widely\n",
      "used today .\n",
      "Data warehousing ushered in the first age of scalable analytics, with new\n",
      "MPP systems (massively parallel processing databases) coming on the\n",
      "market and supporting unprecedented volumes of data. Roles such as BI\n",
      "engineer , ETL developer , and data warehouse engineer addressed thevarious needs of the data warehouse. Data warehouse and BI engineering\n",
      "was a precursor to today’ s data engineering, and still play a central role in\n",
      "the discipline.\n",
      "Around the mid to late 1990s, the Internet went mainstream, creating a\n",
      "whole new generation of web-first companies such as AOL, Altavista,\n",
      "Yahoo, Amazon. The dot-com boom spawned a ton of activity in web\n",
      "applications, as well as the backend systems to support them—servers,\n",
      "databases, storage. Much of the infrastructure was expensive, monolithic,\n",
      "and heavily licensed. The vendors selling these backend systems didn’ t\n",
      "foresee the sheer scale of the data that web applications would produce.\n",
      "The early 2000s: The Birth of Contemporary Data Engineering\n",
      "Fast forward to the early 2000s; the dot-com bust of the late 90s left behind\n",
      "a small cluster of survivors. Some of these companies, such as Y ahoo,\n",
      "Google, Amazon, would grow into powerhouse tech companies. Initially ,\n",
      "these companies continued to rely on the traditional monolithic, relational\n",
      "databases and data warehouses of the 1990s, pushing these systems to the\n",
      "limit. As these systems buckled, new solutions and approaches were needed\n",
      "to handle the growing volume, variety , and velocity of data in a cost-\n",
      "effective, scalable, reliable, and fault-tolerant manner .\n",
      "Coinciding with the explosion of data, commodity hardware—servers, ram,\n",
      "disks, flash drives, etc.—also became cheap and ubiquitous. T o handle the\n",
      "explosion of data, several innovations allowed distributed computation and\n",
      "storage on massive computing clusters. W ithout realizing the future\n",
      "implications, these innovations started decentralizing and breaking apart\n",
      "traditionally monolithic services. This opened an era of “big data”, which\n",
      "Oxford Dictionary defines as “extremely lar ge data sets that may be\n",
      "analyzed computationally to reveal patterns, trends, and associations,\n",
      "especially relating to human behavior and interactions.”  Another popular\n",
      "and succinct description of big data are the 3 V’ s of data—velocity , variety ,\n",
      "and volume.\n",
      "In 2003, Google published a paper on the Google File System  and shortly\n",
      "thereafter in 2004, a paper on MapReduce , an ultra-scalable data processing5paradigm. In truth, big data has earlier antecedents in MPP data warehouses\n",
      "and data management for experimental physics projects, but Google’ s\n",
      "publications constituted a “big bang” for data technologies and the cultural\n",
      "roots of data engineering as we know it today .\n",
      "The Google papers inspired engineers at Y ahoo to develop and later open\n",
      "source Hadoop in 2006.  The impact of Hadoop cannot be overstated.\n",
      "Software engineers interested in lar ge scale data problems were drawn to\n",
      "the possibilities of Hadoop and its ecosystem of technologies. As\n",
      "companies of all sizes and types saw their data grow into many terabytes\n",
      "and even petabytes, the era of the big data engineer was born.\n",
      "Around the same time, Amazon  had to keep up with their own exploding\n",
      "data needs, and created elastic computing environments (Amazon EC2),\n",
      "infinitely scalable storage systems (Amazon S3), highly scalable NoSQL\n",
      "databases (DynamoDB), streaming pipelines (Kinesis) and many other core\n",
      "data building blocks. Amazon elected to of fer these services for both\n",
      "internal and external consumption through Amazon W eb Services (A WS),\n",
      "which would go on to become the first popular public cloud. A WS created\n",
      "an ultra-flexible pay-as-you-go resource marketplace by virtualizing and\n",
      "reselling vast pools of commodity hardware. Instead of purchasing\n",
      "hardware for a data center , developers could simply rent compute and\n",
      "storage from A WS. T urns out, A WS became a very profitable driver for\n",
      "Amazon’ s growth! Other public clouds would soon follow , such as Google\n",
      "Cloud, Microsoft Azure, OVH, Digital Ocean. The public cloud is ar guably\n",
      "one of the biggest innovations of the 21st century and spawned a revolution\n",
      "in the way software and data applications are developed and deployed.\n",
      "The early big data tools and public cloud laid the foundation upon which\n",
      "today’ s data ecosystem is built. The modern data landscape—and data\n",
      "engineering as we know it today—would not exist without these\n",
      "innovations.\n",
      "The 2000s and 2010s: Big Data Engineering\n",
      "Open source big data tools in the Hadoop ecosystem quickly matured and\n",
      "spread from Silicon V alley companies to tech-savvy companies all over the6\n",
      "7world. For the first time, businesses anywhere in the world could use the\n",
      "same bleeding edge big data tools developed by the top tech companies.\n",
      "You could choose the latest and greatest—Hadoop, Pig, Hive, Dremel,\n",
      "HBase, Cassandra, Spark, Presto, and numerous other new technologies that\n",
      "came on the scene. T raditional enterprise-oriented and GUI-based data tools\n",
      "suddenly felt outmoded, and code was in vogue with the ascendance of\n",
      "MapReduce. The authors were around during this time, and it felt like old\n",
      "dogmas died a sudden death upon the altar of “big data.”\n",
      "The explosion of data tools in the late 2000s and 2010s ushered in the “big\n",
      "data engineer”. T o effectively use these tools and techniques—namely the\n",
      "Hadoop ecosystem (Hadoop, Y ARN, HDFS, Map Reduce, etc)—big data\n",
      "engineers had to be proficient in software development and low-level\n",
      "infrastructure hacking, but with a shifted focus. Big data engineers were\n",
      "interested in maintaining huge clusters of commodity hardware to deliver\n",
      "data at scale. While they might occasionally submit pull requests to Hadoop\n",
      "core code, they shifted their focus from core technology development to\n",
      "data delivery . They also had the responsibility of working with data\n",
      "scientists to write low-level map-reduce jobs in a variety of languages, but\n",
      "especially in Java.\n",
      "Big data quickly became a victim of its own success. As a buzzword, “big\n",
      "data” gained popularity during the early 2000s through the mid-2010s,\n",
      "peaking around 2014. Big data captured the imagination of companies\n",
      "trying to make sense of the ever -growing volumes of data, as well as the\n",
      "endless barrage of shameless marketing from companies selling big data\n",
      "tools and services. Due to the immense hype, it was common to see\n",
      "companies using big data tools for small data problems, sometimes standing\n",
      "up a Hadoop cluster to process just a few gigabytes. It seemed like\n",
      "everyone wanted in on the big data action. As Dan Ariely tweeted, “Big\n",
      "data is like teenage sex: everyone talks about it, nobody really knows how\n",
      "to do it, everyone thinks everyone else is doing it, so everyone claims they\n",
      "are doing it.”\n",
      "To get an idea of the rise and fall of big data, see Figure 1-1  for a snapshot\n",
      "of Google T rends for the search term ‘big data’.8Figur e 1-1. Google T rends for “big data”\n",
      "Despite the popularity of the term, big data has lost steam since the mid\n",
      "2010’ s. What happened? One word—simplification. Despite the power and\n",
      "sophistication of open source big data tools, managing them was a lot of\n",
      "work, and required a lot of attention. Oftentimes, companies employed\n",
      "entire teams of “big data engineers”—costing millions of dollars a year—to\n",
      "babysit these platforms. In addition, this new breed of data engineer had not\n",
      "been taught to work closely with business stakeholders, so it was a\n",
      "challenge to deliver insight the business needed.\n",
      "Open-source developers, clouds, and 3rd parties started looking for ways to\n",
      "abstract, simplify , and make big data available to all, without the high\n",
      "administrative overhead and cost of managing their own clusters, installing,\n",
      "configuring, and upgrading their own open-source code, etc. The term “big\n",
      "data” is essentially a relic to describe a certain time and approach to\n",
      "handling lar ge amounts of data. In truth, data is moving at a faster rate than\n",
      "ever and growing ever lar ger, but big data processing has become so\n",
      "accessible that it no longer merits a separate term; every company aims to\n",
      "solve its data problems, regardless of actual data size. Big data engineers\n",
      "are now simply data engineers .\n",
      "The 2020s: Engineering for the Data Life Cycle\n",
      "At the time of this writing, the data engineering role is evolving rapidly; we\n",
      "expect this evolution to continue at a rapid clip for the foreseeable future.\n",
      "Whereas data engineers historically spent their time tending to the low-level\n",
      "details of monolithic frameworks such as Hadoop, Spark, or Informatica,\n",
      "the trend is moving toward the use of decentralized, modularized, managed,\n",
      "and highly abstracted tools. Indeed, data tools have proliferated at an\n",
      "astonishing rate (see Figure 1-2 ). New popular trends in the early 2020’ s\n",
      "include “The Modern Data Stack”, which represents a collection of of f-the-\n",
      "shelf open source and 3rd party products assembled to make the lives of\n",
      "analysts easier . At the same time, data sources and data formats are growing\n",
      "both in variety and size. Data engineering is increasingly a discipline ofinteroperation, of interconnecting numerous technologies like lego bricks to\n",
      "serve ultimate business goals.\n",
      "Figur e 1-2. Data tools in 2012 vs 2021\n",
      "In fact, the authors believe that data engineers should focus on the\n",
      "abstraction of data processing stages in what we term the data engineering\n",
      "lifecycle  (see Figure 1-3 ).Figur e 1-3. The Data Engineering Lifecycle\n",
      "The data engineering lifecycle paradigm shifts the conversation away from\n",
      "technology , toward the data itself and the end goals that it must serve. The\n",
      "stages of the lifecycle are:Generation\n",
      "Ingestion\n",
      "Transformation\n",
      "Serving\n",
      "Storage\n",
      "We put storage last because it plays a role in all the other stages. W e also\n",
      "have a notion of under currents, i.e., ideas that are critical across the full\n",
      "lifecycle. W e will cover the lifecycle more extensively in Chapter 2, but we\n",
      "outline it here because it is critical to our definition of data engineering.\n",
      "The data engineer we discuss in this book can be described more precisely\n",
      "as a data lifecycle engineer . With greater abstraction and simplification, a\n",
      "data lifecycle engineer is no longer encumbered by the gory details of\n",
      "yesterday’ s big data frameworks. While data engineers maintain skills in\n",
      "low-level data programming and use these as required, they increasingly\n",
      "find their role focused on things higher in the value chain—data\n",
      "management, DataOps,  data architecture, orchestration, and general data\n",
      "lifecycle management.\n",
      "As tools and workflows simplify , we’ve seen a noticeable shift in the\n",
      "attitudes of data engineers. Instead of focusing on who has the “biggest\n",
      "data”, open-source projects and services are increasingly concerned with\n",
      "managing and governing data, making it easier to use and discover , and\n",
      "improving its quality . Data engineers are now conversant in acronyms such\n",
      "as CCP A and GDPR;  as they engineer pipelines, they concern themselves\n",
      "with privacy , anonymization, data garbage collection and compliance with\n",
      "regulations.\n",
      "What’ s old is new again. Now that many of the hard problems of\n",
      "yesterday’ s data systems are solved, neatly productized, and packaged,\n",
      "technologists and entrepreneurs have shifted focus back to the “enterprisey”\n",
      "stuff, but with an emphasis on decentralization and agility that contrasts\n",
      "with the traditional enterprise command-and-control approach. W e view the9\n",
      "10present as a golden age of data management. Data engineers managing the\n",
      "data engineering lifecycle have better tools and techniques at their disposal\n",
      "than have ever before. W e will discuss the data engineering lifecycle, and\n",
      "its undercurrents, in greater detail in the next chapter .\n",
      "Data Engineering Defined\n",
      "Okay , now that we’ve surveyed the recent past, we’re ready to lay out our\n",
      "definition of data engineering. Let’ s unpack the common threads. In\n",
      "general, a data engineer gets data, stores it, and prepares it for consumption\n",
      "by data scientists and analysts.\n",
      "For the purpose of this book, we define data engineering  as follows:\n",
      "Data engineering\n",
      "The development, implementation, and maintenance of systems and\n",
      "processes that take in raw data and produce high-quality , consistent\n",
      "information that supports downstream use cases, such as analysis and\n",
      "machine learning. Data engineering is the intersection of data\n",
      "management, DataOps, data architecture, orchestration, and software\n",
      "engineering.\n",
      "Data engineers\n",
      "Manage the data engineering lifecycle (see Figure 1-3 ) beginning with\n",
      "ingestion and ending with serving data for use cases, such as analysis or\n",
      "machine learning.\n",
      "Put another way , data engineers produce reliable data that serves the\n",
      "business with predictable quality and meaning. The data engineering\n",
      "workflow is built atop systems supporting the data model, underpinned by\n",
      "the management, and configuration of storage and infrastructure. Data\n",
      "engineering systems and outputs are the backbones of successful data\n",
      "analytics and data science.The data engineering workflow is roughly as follows (see Figure 1-4 ). This\n",
      "is the general  process through which all data in an or ganization flows. Raw\n",
      "data is ingested from source systems, stored and processed, and served as\n",
      "conformed data models and datasets to consumers such as data scientists\n",
      "and data analysts. W e’ll return to the data engineering workflow throughout\n",
      "the book.\n",
      "Figur e 1-4. The Data Engineering W orkflow\n",
      "Data Engineering and Data Science\n",
      "Where does data engineering fit in with data science? There’ s some debate,\n",
      "with some ar guing data engineering is a subdiscipline of data science. W e\n",
      "believe data engineering is separate from data science and analytics. They\n",
      "complement each other , but they are distinctly dif ferent. Data engineering\n",
      "sits upstream from data science. Data engineers serve data scientists and\n",
      "other data customers ( Figure 1-5 ).Figur e 1-5. Data engineering sits upstr eam fr om data science\n",
      "To justify this position, let’ s consider the Data Science Hierarchy of\n",
      "Needs  (see Figure 1-6 ). In 2017, Monica Rogati wrote an article by this\n",
      "title that showed where AI/ML sat in proximity to less “sexy” areas such as\n",
      "data movement/storage, collection, and infrastructure.1 1Figur e 1-6. The Data Science Hierar chy of Needs (Sour ce: Rogati)\n",
      "It is now widely recognized that data scientists spend an estimated 70% to\n",
      "90% of their time on the bottom 3 parts of the hierarchy—gathering data,\n",
      "cleaning data, processing data—and only a small slice of their time on\n",
      "analysis and machine learning. Rogati ar gues that companies need to build a\n",
      "solid data foundation—the bottom 3 levels of the hierarchy—before\n",
      "tackling areas such as AI and ML.\n",
      "Data scientists aren’ t typically trained to engineer production-grade data\n",
      "systems, and they end up doing this work haphazardly because they lack the\n",
      "support and resources of a data engineer . In an ideal world, data scientists\n",
      "should spend 90%+ of their time focused on the top layers of the pyramid—\n",
      "analytics, experimentation, and machine learning. When data engineers\n",
      "focus on these bottom parts of the hierarchy , they build a solid foundation\n",
      "upon which data scientists can succeed.\n",
      "With data science driving advanced analytics and machine learning, data\n",
      "engineering straddles the divide between getting data and getting value\n",
      "from data (see Figure 1-7 ). We believe data engineering will rise to be equal\n",
      "in importance and visibility to data science, with data engineers playing a\n",
      "vital role in making data science successful in production.\n",
      "Figur e 1-7. A data engineer gets data and pr ovides value fr om data\n",
      "Data Engineering Skills and Activities\n",
      "The skillset of a data engineer encompasses what we call the\n",
      "“undercurrents” of data engineering—data management, data ops, dataarchitecture, and software engineering. It requires an understanding of how\n",
      "to evaluate data tools, and how they fit together across the data engineering\n",
      "lifecycle. It’ s also necessary to know how data is produced in source\n",
      "systems, as well as how analysts and data scientists will consume and create\n",
      "value from data after it is processed and curated. Finally , a data engineer\n",
      "juggles a lot of complex moving parts, and must constantly optimize along\n",
      "the axes of cost, agility , simplicity , reuse, and interoperability ( Figure 1-8 ).\n",
      "We’ll cover these topics in more detail in upcoming chapters.\n",
      "Figur e 1-8. The balancing act of data engineering\n",
      "As we discussed, in the recent past, a data engineer was expected to know\n",
      "and understand how to use a small handful of powerful and monolithic\n",
      "technologies to create a data solution. Utilizing these technologies (Hadoop,\n",
      "Spark, T eradata, and many others) often required a sophisticated\n",
      "understanding of software engineering, networking, distributed computing,\n",
      "storage, or other low-level details. Their work would be devoted to cluster\n",
      "administration and maintenance, managing overhead, and writing pipeline\n",
      "and transformation jobs, among other tasks.\n",
      "Nowadays, the data tooling landscape is dramatically less complicated to\n",
      "manage and deploy . Modern data tools greatly abstract and simplify\n",
      "workflows. As a result, data engineers are now focused on balancing the\n",
      "simplest and most cost-ef fective, best-of-breed services that deliver value to\n",
      "the business. The data engineer is also expected to create agile data\n",
      "architectures that evolve as new trends emer ge.\n",
      "What are some things a data engineer does NOT do? A data engineer\n",
      "typically does not directly build machine learning models, create reports or\n",
      "dashboards, perform data analysis, build KPIs, or develop software\n",
      "applications. That said, a data engineer should have a good functioning\n",
      "understanding of all of these areas, in order to best serve stakeholders.Data Maturity and the Data Engineer\n",
      "The level of data engineering complexity within a company depends a great\n",
      "deal on the company’ s data maturity . This, in turn, has a significant impact\n",
      "on a data engineer ’s day-to-day job responsibilities, and potentially on their\n",
      "career progression as well. What is data maturity , exactly?\n",
      "Data maturity is the progression toward higher data utilization, capabilities,\n",
      "and integration across the or ganization, but data maturity does not simply\n",
      "depend on the age or revenue of a company . An early-stage startup can have\n",
      "greater data maturity than a 100-year -old company with annual revenues in\n",
      "the billions. What matters is how data is leveraged as a competitive\n",
      "advantage.\n",
      "There are many versions of data maturity models, such as Data\n",
      "Management Maturity (DMM)  and others, and it’ s hard to pick one that is\n",
      "both simple and useful for data engineering. So, we’ll create our own\n",
      "simplified data maturity model. In our data maturity model ( Figure 1-9 ), we\n",
      "have three stages—starting with data, scaling with data, and leading with\n",
      "data. Let’ s look at each of these stages, and what a data engineer typically\n",
      "does at each stage.\n",
      "Figur e 1-9. Our simplified data maturity model for a company12Stage 1: Starting with data\n",
      "A company getting started with data is, by definition, in the very early\n",
      "stages of its data maturity . The company may have fuzzy , loosely defined\n",
      "goals, or no goals at all. Data architecture is in the very early stages of\n",
      "planning and development. Adoption and utilization are likely low or\n",
      "nonexistent. The data team is small, often with a headcount in the single\n",
      "digits. At this stage, a data engineer is often a generalist, and will typically\n",
      "play several other roles, such as data scientist or software engineer . A data\n",
      "engineer ’s goal is to move fast, get traction, and add value.\n",
      "The practicalities of getting value from data are typically poorly\n",
      "understood, but the desire exists. Reports or analyses lack formal structure,\n",
      "and most requests for data are ad hoc. Machine learning is rarely successful\n",
      "at this stage, and so such projects are not recommended. W e’ve seen\n",
      "countless data teams get stuck and fall short when they try to jump to\n",
      "machine learning without building a solid data foundation. The authors\n",
      "half-jokingly call themselves “recovering data scientists”, lar gely from\n",
      "personal experience with being involved in premature data science projects\n",
      "without adequate data maturity or data engineering support.\n",
      "In or ganizations getting started with data, a data engineer should focus on\n",
      "the following:\n",
      "Get buy-in from key stakeholders, including executive\n",
      "management. Ideally , the data engineer should have a sponsor for\n",
      "key initiatives to design and build a data architecture to support the\n",
      "company’ s goals.\n",
      "Define the right data architecture (usually solo, since a data\n",
      "architect likely isn’ t available). This means determining business\n",
      "goals, and what competitive advantage you’re aiming to achieve\n",
      "with your data initiative. W ork toward a data architecture that\n",
      "supports these goals. See Chapter 3 for our advice on “good” data\n",
      "architecture.13Identify and audit data that will support key initiatives, and operate\n",
      "within the data architecture that you designed.\n",
      "Build a solid data foundation upon which future data analysts and\n",
      "data scientists can generate reports and models that provide\n",
      "competitive value. In the meantime, you may also have to generate\n",
      "these reports and models until this team is hired.\n",
      "Things to watch out for:\n",
      "This is a delicate stage with lots of pitfalls.\n",
      "Organizational willpower may wane.\n",
      "Getting quick wins will establish the importance of data within the\n",
      "organization. Just keep in mind that quick wins will likely create\n",
      "technical debt. Have a plan to reduce this debt, as it will otherwise\n",
      "add friction for future delivery .\n",
      "Be visible and continue getting support.\n",
      "Avoid undif ferentiated heavy lifting. Don’ t get caught in the trap of\n",
      "boxing yourself in with unnecessary technical complexity . Use of f-\n",
      "the-shelf, turnkey solutions wherever possible.\n",
      "Build custom solutions and code only where you’re creating a\n",
      "competitive advantage.\n",
      "Stage 2: Scaling with data\n",
      "At this point, a company has moved away from ad-hoc data requests and\n",
      "has formal data practices. Now the challenge is creating scalable data\n",
      "architectures and planning for a future where the company is truly data-\n",
      "driven. Data engineering roles move from generalists to specialists, with\n",
      "people focusing on particular aspects of the data engineering lifecycle.\n",
      "In or ganizations that are in Stage 2 of data maturity , a data engineer ’s goals\n",
      "are to:\n",
      "Establish formal data practicesCreate scalable and robust data architectures\n",
      "Adopt DevOps and DataOps practices\n",
      "Build systems that support machine learning\n",
      "Continue to avoid undif ferentiated heavy lifting and customizing\n",
      "only where there’ s a competitive advantage\n",
      "We will return to each of these goals later in the book.\n",
      "Things to watch out for:\n",
      "As we grow more sophisticated with data, there’ s a temptation to\n",
      "adopt bleeding-edge technologies based on social proof from\n",
      "Silicon V alley companies. This is rarely the best use of your time\n",
      "and ener gy. Any technology decisions should be driven by the\n",
      "value that they’ll deliver to your customers.\n",
      "The main bottleneck for scaling is not cluster nodes, storage, or\n",
      "technology , but the data engineering team itself. Focus on solutions\n",
      "that are simple to deploy and manage to expand your team’ s\n",
      "throughput.\n",
      "You’ll be tempted to frame yourself as a technologist, as a data\n",
      "genius who can deliver magical products. Shift your focus instead\n",
      "to pragmatic leadership—begin transitioning to the next maturity\n",
      "stage now . Communicate with other teams about the practical\n",
      "utility of data. T each the or ganization how to consume and\n",
      "leverage data.\n",
      "Stage 3: Leading with data\n",
      "At this stage, the company leads with data and is data-driven. The\n",
      "automated pipelines and systems created by data engineers allow people\n",
      "within the company to do self-service analytics and machine learning.\n",
      "Introducing new data sources is seamless and tangible value is derived.\n",
      "Data engineers implement formal controls and practices to ensure data isalways available to the people and systems that need it. Data engineering\n",
      "roles continue to specialize more deeply than in Stage 2.\n",
      "In or ganizations that are in Stage 3 of data maturity , a data engineer will\n",
      "continue building on prior stages, plus:\n",
      "Create automation for the seamless introduction and usage of new\n",
      "data.\n",
      "Focus on building custom tools and systems that leverage data as a\n",
      "competitive advantage.\n",
      "Focus on the “enterprisey” aspects of data. Data management, data\n",
      "serving, DataOps, etc. Deploy tools that expose and disseminate\n",
      "data throughout the or ganization, including data catalogs, data\n",
      "lineage tools, metadata management systems, etc.\n",
      "Collaborate ef ficiently with software engineers, machine learning\n",
      "engineers, analysts, etc. Create a community and environment\n",
      "where people can collaborate and speak openly , no matter what\n",
      "role or position you’re in.\n",
      "Things to watch out for:\n",
      "At this stage, complacency is a significant danger . Once\n",
      "organizations reach Stage 3, they must focus constantly on\n",
      "maintenance and improvement or they risk falling back to a lower\n",
      "stage.\n",
      "Technology distractions are a bigger danger here than in the other\n",
      "stages. There’ s a temptation to pursue expensive hobby projects\n",
      "that don’ t deliver value to the business. Utilize custom-built\n",
      "technology only where it provides a competitive advantage.\n",
      "The Background and Skills of a Data Engineer\n",
      "Because data engineering is a relatively new discipline, there is little\n",
      "available in the way of formal training to enter the field. Universities don’ thave a common data engineering path. Although there are a handful of data\n",
      "engineering boot camps and online tutorials covering random topics, a\n",
      "common curriculum for the subject doesn’ t yet exist. Those entering data\n",
      "engineering arrive with varying backgrounds in education, career , and\n",
      "skillset. Everyone entering the field should expect to invest a significant\n",
      "amount of time in self-study (including reading this book).Figur e 1-10. Data engineering is the fastest-gr owing tech occupation (2020)\n",
      "If you’re pivoting your career into data engineering (see Figure 1-10  for\n",
      "recent growth of the field), we’ve found that the transition is easiest when\n",
      "moving from an adjacent field, such as software engineering, ETL\n",
      "development, database administration, data science, and data analysis.\n",
      "These disciplines tend to be “data aware” and provide good context for data\n",
      "roles in an or ganization. They also tend to equip folks with the relevant\n",
      "technical skills and context to solve data engineering problems.\n",
      "Despite the lack of a formalized path, there is a requisite body of\n",
      "knowledge that we believe a data engineer should know in order to be\n",
      "successful. By definition, a data engineer must understand both data and\n",
      "technology . With respect to data, this entails knowing about various best\n",
      "practices around data management. On the technology end, a data engineer\n",
      "must be aware of various options for tools, their interplay , and their\n",
      "tradeof fs. This requires a good understanding of software engineering,\n",
      "DataOps, and data architecture.\n",
      "Zooming out, a data engineer must also understand the requirements of data\n",
      "consumers—data analysts and data scientists—as well as the broader\n",
      "implications of data across the or ganization. Data engineering is a holistic\n",
      "practice—the best data engineers view their responsibilities through both\n",
      "business and technical lenses.\n",
      "Business Responsibilities\n",
      "The macro responsibilities we list below aren’ t just important for a data\n",
      "engineer , but for anyone working in a data or technology field. Because a\n",
      "simple Google search will yield tons of resources to learn about these areas,\n",
      "we will simply list them for brevity:\n",
      "Know how to communicate with nontechnical and technical people.\n",
      "Communication is key , and you need to be able to establish rapport and\n",
      "trust with people across the or ganization. W e suggest paying close\n",
      "attention to or ganizational hierarchies, who reports to whom, how14people interact with each other , and which silos exist. These\n",
      "observations will be invaluable to your success.\n",
      "Understand how to scope, and how to gather business and pr oduct\n",
      "requir ements.\n",
      "Simply put, you need to know what to build and make sure that your\n",
      "stakeholders agree with your assessment. In addition, develop a sense of\n",
      "how data and technology decisions impact the business.\n",
      "Understand the cultural foundations of Agile, DevOps, and DataOps.\n",
      "Many technologists mistakenly believe these practices are solved\n",
      "through technology . This is dangerously wrong; in fact, they are\n",
      "fundamentally cultural, requiring buy-in across the or ganization.\n",
      "Cost contr ol.\n",
      "You’ll be successful when you can keep costs low while providing\n",
      "outsized value. Know how to optimize for time to value, the total cost of\n",
      "ownership, and opportunity cost. Learn to monitor costs to avoid\n",
      "surprises.\n",
      "Continuously learn.\n",
      "The data field feels like it’ s changing at light speed. People who succeed\n",
      "in it are great at picking up new things while sharpening their\n",
      "fundamental knowledge. They’re also good at filtering, determining\n",
      "which new developments are most relevant to their work, which are still\n",
      "immature, and which are just fads. Stay abreast of the field and learn\n",
      "how to learn.\n",
      "A successful data engineer always zooms out to understand the big picture,\n",
      "and how to achieve outsized value for the business. Communication is key ,\n",
      "both for technical and non-technical people. W e often see data teams\n",
      "succeed or fail based upon their communication with other stakeholders;\n",
      "success or failure is very rarely a technology issue. Knowing how tonavigate an or ganization, scope and gather requirements, control costs, and\n",
      "continuously learn will set you apart from the data engineers who rely\n",
      "solely on their technical abilities to carry their career .\n",
      "Technical Responsibilities\n",
      "At a high level, you must understand how to build architectures that\n",
      "optimize performance and cost, whether the components are prepackaged or\n",
      "homegrown. Ultimately , architectures and constituent technologies are\n",
      "building blocks to serve the data engineering lifecycle. Recall the stages of\n",
      "the data engineering lifecycle ( Figure 1-3 ):\n",
      "Data generation\n",
      "Data ingestion\n",
      "Data storage\n",
      "Data transformation\n",
      "Serving data\n",
      "The undercurrents of the data engineering lifecycle are:\n",
      "Data management\n",
      "DataOps\n",
      "Data architecture\n",
      "Software engineering\n",
      "Zooming in a bit, we discuss some of the tactical data and technology skills\n",
      "you’ll need as a data engineer here; we will discuss these in much more\n",
      "detail in subsequent chapters.\n",
      "People often ask—should a data engineer know how to code? Short answer\n",
      "—yes. A data engineer should have production-grade software engineering\n",
      "chops. W e note that the nature of software development projects undertaken\n",
      "by data engineers has changed fundamentally in the last few years. A greatdeal of low-level programming ef fort previously expected of engineers is\n",
      "now replaced by fully managed services, managed open-source, and simple\n",
      "plug-and-play software-as-a-service (SAAS) of ferings. For example, data\n",
      "engineers now focus on high-level abstractions, writing pipelines as code\n",
      "within an orchestration framework, or using dataframes inside Spark\n",
      "instead of having to worry about the nitty-gritty of Spark internals.\n",
      "Even in a more abstract world, software engineering best practices provide\n",
      "a competitive advantage, and data engineers who can dive into the deep\n",
      "architectural details of a codebase give their companies an edge when\n",
      "specific technical needs arise. In short, a data engineer who can’ t write\n",
      "production-grade code will be severely handicapped, and we don’ t see this\n",
      "changing anytime soon. Data engineers remain software engineers, in\n",
      "addition to their many other roles.\n",
      "What languages should a data engineer know? W e divide data engineering\n",
      "programming languages into primary and secondary categories. At the time\n",
      "of this writing, the primary languages of data engineering are SQL, Python,\n",
      "a JVM language (usually Java or Scala), and Bash:\n",
      "SQL\n",
      "The most common interface for databases and data lakes. After briefly\n",
      "being sidelined by the need to write custom map-reduce code for big\n",
      "data processing, SQL (in various forms) has reemer ged as the lingua\n",
      "franca of data.\n",
      "Python\n",
      "The bridge language between data engineering and data science.\n",
      "Further , a growing number of data engineer tools are written in Python\n",
      "or have Python APIs. It’ s known as “the second-best language at\n",
      "everything.” Python underlies popular data tools such as Pandas,\n",
      "Numpy , Airflow , SKLearn, T ensorflow , Pytorch, PySpark, and\n",
      "countless others. Python acts as the glue between underlying\n",
      "components and is frequently a first-class API language for interfacing\n",
      "with a framework.JVM languages, such as Java and Scala\n",
      "Popular for Apache open source projects such as Spark, Hive, Druid,\n",
      "and many more. The JVM is generally more performant than Python\n",
      "and may provide access to lower -level features than a Python API (For\n",
      "example, this is the case for Apache Spark and Beam). If you’re using a\n",
      "popular open-source data framework, understanding Java or Scala will\n",
      "be very helpful.\n",
      "Bash\n",
      "The command line interface for Linux operating systems. Knowing\n",
      "Bash commands and being comfortable using CLI’ s will greatly\n",
      "improve your productivity and workflow when you need to script or\n",
      "perform OS operations. Even today , data engineers frequently use\n",
      "command line tools like A wk or sed to process files in a data pipeline or\n",
      "call Bash commands from orchestration frameworks. If you’re using\n",
      "Windows, feel free to substitute Powershell for Bash.THE UNREASONABLE EFFECTIVENESS OF SQL\n",
      "The advent of map reduce and the “Big Data” era relegated SQL to\n",
      "passé status. Since then, a variety of developments have dramatically\n",
      "enhanced the utility of SQL in the data engineering life cycle. Spark\n",
      "SQL, BigQuery , Snowflake, and many other data tools can process\n",
      "massive amounts of data using declarative, set-theoretic SQL\n",
      "semantics; SQL is also supported by many streaming frameworks, such\n",
      "as Flink, Beam, and Kafka. W e believe that competent data engineers\n",
      "should be highly proficient in SQL.\n",
      "Are we saying that SQL is an end-all-be-all language? Not at all. SQL\n",
      "is a powerful tool that can quickly solve many complex analytics and\n",
      "data transformation problems. Given that time is a primary constraint\n",
      "for data engineering team throughput, a tool that combines simplicity\n",
      "and high productivity should be embraced. Data engineers also do well\n",
      "to develop expertise in composing SQL with other operations, either\n",
      "within frameworks such as Spark and Flink or by using orchestration to\n",
      "combine multiple tools. Data engineers should also learn modern SQL\n",
      "semantics for dealing with JSON parsing and nested data, and consider\n",
      "leveraging a SQL management framework such as DBT (Data Build\n",
      "Tool).\n",
      "A proficient data engineer also recognizes when SQL is not the right\n",
      "tool for the job and can choose and code in a suitable alternative; while\n",
      "a SQL expert could likely write a query to stem and tokenize raw text in\n",
      "an NLP (natural language processing) pipeline, coding in native Spark\n",
      "is a far superior alternative to this masochistic exercise.\n",
      "Data engineers may also need to develop proficiency in secondary\n",
      "programming languages including R, Javascript, Go, Rust, C/C++, C#,\n",
      "Julia, etc. Developing in these languages is often necessary when they are\n",
      "popular across the company , or to use domain-specific data tools. For\n",
      "instance, Javascript has proven popular as a language for user -defined15functions in cloud data warehouses, while C# and Powershell are important\n",
      "in companies that leverage Azure and the Microsoft ecosystem.\n",
      "KEEPING PACE IN A FAST-MOVING FIELD\n",
      "Once a new technology r olls over you, if you’r e not part of the\n",
      "steamr oller, you’r e part of the r oad.\n",
      "—Stewart Brand\n",
      "How do you keep your skills sharp in a rapidly changing field like data\n",
      "engineering? Should you focus on the latest tools or deep dive into\n",
      "fundamentals? Here’ s our advice: focus on the fundamentals to\n",
      "understand what’ s not going to change; pay attention to ongoing\n",
      "developments to understand wher e the field is going.  New paradigms\n",
      "and practices are introduced all the time, and it’ s incumbent on you to\n",
      "stay current. Strive to understand how new technologies will be useful\n",
      "in the lifecycle.\n",
      "The Continuum of Data Engineering Roles, from A to B\n",
      "Despite job descriptions that paint a data engineer as a “unicorn” who must\n",
      "possess every data skill imaginable, data engineers don’ t all do the same\n",
      "type of work or have the same skillset. Just as data maturity is a helpful\n",
      "guide to understand the types of data challenges a company will face as it\n",
      "grows its data capability , it’s helpful to look at some key distinctions in the\n",
      "types of work data engineers do. Though these distinctions are simplistic, it\n",
      "clarifies what data scientists and data engineers do, and avoids lumping\n",
      "either role into the “unicorn” bucket.\n",
      "In data science, there’ s the notion of T ype A and T ype B data scientists.\n",
      "Type A data scientists—where A stands for Analysis—focus on\n",
      "understanding and deriving insight from data. T ype B data scientists—\n",
      "where B stands for Building—share similar backgrounds as T ype A data\n",
      "scientists, and also possess strong programming skills. The T ype B Data\n",
      "Scientist builds systems that make data science work in production.16Borrowing from the T ype A and T ype B data scientist continuum, we’ll\n",
      "create a similar distinction for two types of data engineers.\n",
      "Type A data engineers—A stands for Abstraction . In this case, the data\n",
      "engineer avoids undif ferentiated heavy lifting, keeping data architecture as\n",
      "simple and abstract as possible and not re-inventing the wheel. T ype A data\n",
      "engineers manage the data engineering lifecycle using mostly or completely\n",
      "off-the-shelf products, managed services, and tools. T ype A data engineers\n",
      "work at companies across industries, and across all levels of data maturity .\n",
      "Type B data engineers—B stands for Build . Type B data engineers build\n",
      "data tools and systems that scale and leverage a company’ s core\n",
      "competency and competitive advantage. In the data maturity range, a T ype\n",
      "B data engineer is more commonly found at companies in Stage 2 and 3\n",
      "(scaling and leading with data), or when an initial data use case is so unique\n",
      "and mission-critical that custom data tools are required to get started.\n",
      "Type A and T ype B data engineers may work in the same company , and\n",
      "they may even be the same person! More commonly , a Type A data\n",
      "engineer is first hired to set the foundation, with T ype B data engineer skill\n",
      "sets either learned or hired as the need arises within a company .\n",
      "Data Engineers Inside an Organization\n",
      "Data engineers don’ t work in a vacuum. Depending on what they’re\n",
      "working on, they will interact both with technical and non-technical people,\n",
      "as well as face dif ferent directions (internal and external). Let’ s explore\n",
      "what data engineers do inside an or ganization, and who they interact with.\n",
      "Internal-Facing V ersus External-Facing Data Engineers\n",
      "A data engineer serves several end-users and faces many internal and\n",
      "external directions ( Figure 1-1 1). Since not all data engineering workloads\n",
      "and responsibilities are the same, it’ s important to understand who the data\n",
      "engineer serves. Depending on the end use cases, a data engineer ’s primary\n",
      "responsibilities are external-facing, internal-facing, or a blend of the two.Figur e 1-1 1. The dir ections a data engineer faces\n",
      "An external-facing data engineer normally aligns with the users of external-\n",
      "facing applications, such as social media apps, IoT devices, and e-\n",
      "commerce platforms. This data engineer architects, builds, and manages the\n",
      "systems that collect, store, and process transactional and event data from\n",
      "these applications. The systems built by these data engineers have a\n",
      "feedback loop from the application, to the data pipeline, then back to the\n",
      "application (see Figure 1-12 ).\n",
      "We note that external-facing data engineering comes with a special set of\n",
      "problems. External facing query engines often handle much lar ger\n",
      "concurrency loads than internal-facing systems; engineers also need to\n",
      "consider putting tight limits on queries that users can run in order to limit\n",
      "the infrastructure impact of any single user; and security is a much more\n",
      "complex and sensitive problem for external queries, especially if the data\n",
      "being queried is multi-tenant, i.e., data from many customers is housed in a\n",
      "single table.Figur e 1-12. External-facing data engineer systems\n",
      "An internal-facing data engineer typically focuses on activities crucial to\n",
      "the needs of the business and internal stakeholders ( Figure 1-13 ). Examples\n",
      "include the creation and maintenance of data pipelines and data warehouses\n",
      "for BI dashboards, reports, business processes, data science, and machine\n",
      "learning models.\n",
      "Figur e 1-13. Internal-facing data engineerExternal-facing and internal-facing responsibilities are often blended. In\n",
      "practice, internal-facing data is usually a prerequisite to external-facing\n",
      "data. The data engineer has two sets of users, with very dif ferent\n",
      "requirements for query concurrency , security , etc., as mentioned above.\n",
      "Data Engineers and Other T echnical Roles\n",
      "In practice, the data engineering lifecycle cuts across many dif ferent\n",
      "domains of responsibility . Data engineers sit at the nexus of a variety of\n",
      "roles, interacting with many or ganizational units, either directly or through\n",
      "managers. Let’ s look at who a data engineer may impact. W e’ll start with a\n",
      "discussion of technical roles connected to data engineering ( Figure 1-14 ).\n",
      "Figur e 1-14. Key technical stakeholders of data engineeringIn a nutshell, the data engineer is a hub between data producers, such as\n",
      "software engineers and data architects, and data consumers such as data\n",
      "analysts, data scientists, and machine learning engineers. In addition, data\n",
      "engineers will interact with those in operational roles, such as DevOps\n",
      "engineers\n",
      "Given the pace at which new data roles come into vogue (analytics and\n",
      "machine learning engineers come to mind), this is by no means an\n",
      "exhaustive list.\n",
      "Upstream stakeholders\n",
      "To be successful as a data engineer , you need to understand the data\n",
      "architecture you’re using or designing, as well as the source systems\n",
      "producing the data you’ll need. Below we discuss a few common upstream\n",
      "stakeholders you’ll encounter , such as data architects, software engineers,\n",
      "and devops engineers.\n",
      "Data architects\n",
      "Data ar chitects  function at a level of abstraction one step removed from\n",
      "data engineers. Data architects design the blueprint for or ganizational data\n",
      "management,  mapping out processes, and overall data architecture and\n",
      "systems. They also serve as a bridge between the technical and non-\n",
      "technical sides of an or ganization. Successful data architects generally have\n",
      "‘battle scars’ from extensive engineering experience, allowing them to\n",
      "guide and assist engineers, while successfully communicating the\n",
      "challenges of engineering to non-technical business stakeholders.\n",
      "Data architects implement policies for managing data across silos and\n",
      "business units, steer global strategies such as data management and data\n",
      "governance, and guide major initiatives. Data architects often play a central\n",
      "role in cloud migrations and greenfield cloud design, which at the time of\n",
      "this writing are either inflight initiatives or on a roadmap for many\n",
      "companies.\n",
      "The advent of the cloud has shifted the boundary between data architecture\n",
      "and data engineering. Cloud data architectures are much more fluid than on-17premises systems, so architecture decisions that traditionally involved\n",
      "extensive study , long lead times, purchase contracts, and hardware\n",
      "installation are now often made during the implementation process, just one\n",
      "step in a lar ger strategy . Nevertheless, data architects will remain important\n",
      "visionaries in enterprises, working hand in hand with data engineers to\n",
      "determine the big picture of architecture practices and data strategies.\n",
      "Depending on the data maturity and size of a company , a data engineer may\n",
      "overlap with or assume the responsibilities of a data architect. Therefore, a\n",
      "data engineer should have a good understanding of architecture best\n",
      "practices and approaches.\n",
      "Readers will note that we have placed data architects in the upstr eam\n",
      "stakeholder ’s section. This is because data architects often help to design\n",
      "application data layers that are source systems for data engineers. In\n",
      "general, architects may also interact with data engineers at various other\n",
      "stages of the data engineering lifecycle.\n",
      "We will cover “good” data architecture in Chapter 3.\n",
      "Software engineers\n",
      "Softwar e engineers  build the software and systems that run a business; they\n",
      "are lar gely responsible for generating the internal data that data engineers\n",
      "will consume and process. The systems built by software engineers\n",
      "typically generate application event data and logs, which are a major asset\n",
      "in their own right. This internal data is in contrast to external data , pulled\n",
      "from SAAS platforms or partner businesses. In well-run technical\n",
      "organizations, software engineers and data engineers coordinate from the\n",
      "inception of a new project to design application data for consumption by\n",
      "analytics and machine learning applications.\n",
      "A data engineer should work together with software engineers to\n",
      "understand the applications that generate data, the volume, frequency , and\n",
      "format of the data being generated, and anything else that will impact the\n",
      "data engineering lifecycle, such as data security and regulatory compliance.\n",
      "In addition, data engineers and architects can coordinate on designing thedata bus  that captures data from an application, whether this is a messaging\n",
      "queue, a log sink, or a batch extraction process.\n",
      "DevOps engineers\n",
      "DevOps engineers often produce data through operational monitoring; we\n",
      "classify them as upstream of data engineers for this reason, but they may\n",
      "also be downstream, consuming data through dashboards; or they may\n",
      "interact with data engineers directly in coordinating operations of data\n",
      "systems.\n",
      "Downstream stakeholders\n",
      "The modern data engineering profession exists to serve downstream data\n",
      "consumers and use cases. In this section, we’ll discuss how data engineers\n",
      "interact with a variety of downstream roles. W e’ll also introduce a few\n",
      "different service models, including centralized data engineering teams and\n",
      "cross-functional teams. Chapter 8 will provide an in-depth discussion of\n",
      "these topics.\n",
      "Data scientists\n",
      "Data scientists  build forward-looking models to make predictions and\n",
      "recommendations. These models are then evaluated on live data to provide\n",
      "value in various ways. For example, model scoring might determine\n",
      "automated actions in response to real-time conditions, recommend products\n",
      "to customers based on the browsing history in their current session, or make\n",
      "live economic predictions used by traders.\n",
      "According to common industry folklore, data scientists spend between 40\n",
      "and 80% of their time collecting, cleaning, and preparing data.  In the\n",
      "authors’ experience, these numbers are representative of poor data science\n",
      "and engineering practices. In particular , many popular and useful data\n",
      "science frameworks can become bottlenecks if they are not scaled up\n",
      "appropriately . Data scientists who work exclusively on a single workstation\n",
      "force themselves to downsample data, making data preparation significantly\n",
      "more complicated and potentially compromising the quality of the models\n",
      "that they produce. Furthermore, locally developed code and environments18are often dif ficult to deploy in production, and a lack of automation\n",
      "significantly hampers data science workflows. If data engineers are doing\n",
      "their job and collaborating successfully , data scientists shouldn’ t spend their\n",
      "time collecting, cleaning, and preparing data after initial exploratory work;\n",
      "data engineers should do this work.\n",
      "The need for production-ready data science is a major driver behind the\n",
      "emer gence of the data engineering profession. Data Engineers should help\n",
      "data scientists to enable a path to production. In fact, the authors moved\n",
      "from data science to data engineering in recognition of this fundamental\n",
      "need. Data engineers work to provide the data automation and scale that\n",
      "make data science more ef ficient.\n",
      "Data analysts\n",
      "Data analysts (or business analysts) seek to understand business\n",
      "performance and trends. Whereas data scientists are forward-looking, a data\n",
      "analyst typically focuses on the past or present. Data analysts usually run\n",
      "SQL queries in a data warehouse or a data lake. They may also utilize\n",
      "spreadsheets for computation and analysis, and various business\n",
      "intelligence tools such as PowerBI, Looker , or T ableau. Data analysts are\n",
      "domain experts in the data they work with frequently , and therefore become\n",
      "intimately familiar with data definitions, characteristics, and quality\n",
      "problems. A data analyst’ s typical downstream customers are business\n",
      "users, management, and executives.\n",
      "Data engineers work with data analysts to build pipelines for new data\n",
      "sources required by the business. Data analysts’ subject matter expertise is\n",
      "invaluable in improving data quality , and they frequently collaborate with\n",
      "data engineers in this capacity .\n",
      "Machine learning engineers and AI researchers\n",
      "Machine learning engineers  (ML Engineers) overlap with both data\n",
      "engineers and data scientists. Machine learning engineers develop advanced\n",
      "machine learning techniques, train models, and design and maintain the\n",
      "infrastructure running machine learning processes in a scaled productionenvironment. ML engineers often have advanced working knowledge of\n",
      "machine learning and deep learning techniques, as well as frameworks such\n",
      "as PyT orch or T ensorflow .\n",
      "Machining learning engineers also understand the hardware, services, and\n",
      "systems required to run these frameworks both for model training and\n",
      "model deployment at a production scale. Increasingly , machine learning\n",
      "flows run in a cloud environment where ML engineers can spin up and\n",
      "scale infrastructure resources on-demand or rely on managed services.\n",
      "As mentioned above, the boundaries between ML engineering, data\n",
      "engineering, and data science are fuzzy . Data engineers may have some\n",
      "DevOps responsibilities over ML systems, and data scientists may work\n",
      "closely with ML engineering in designing advanced machining learning\n",
      "processes.\n",
      "The world of ML engineering is growing at a rapid pace and parallels a lot\n",
      "of the same developments occurring in data engineering. Whereas several\n",
      "years ago the attention of machine learning was focused on how to build\n",
      "models, ML engineering now increasingly emphasizes incorporating best\n",
      "practices of ML Operations (MLOps) and other mature practices previously\n",
      "adopted in software engineering and DevOps.\n",
      "AI resear chers  work on new , advanced machine learning techniques. AI\n",
      "researchers may work inside lar ge technology companies, specialized\n",
      "intellectual property startups (OpenAI, DeepMind), or academic\n",
      "institutions. Some practitioners are partially dedicated to research in\n",
      "conjunction with ML engineering responsibilities inside a company . Those\n",
      "working inside specialized machine learning labs are often 100% dedicated\n",
      "to research. Research problems may tar get immediate practical applications\n",
      "or more abstract demonstrations of artificial intelligence. AlphaGo and\n",
      "GPT-3 are great examples of machine learning research projects. W e’ve\n",
      "provided some references in the further r eading  section at the end of the\n",
      "chapter .\n",
      "AI researchers in well-funded or ganizations are highly specialized and\n",
      "operate with supporting teams of engineers to facilitate their work. Forexample, see job listings for OpenAI or DeepMind. Machine learning\n",
      "engineers in academia usually have fewer resources, but rely on teams of\n",
      "graduate students, postdocs, and university staf f to provide engineering\n",
      "support. Machine learning engineers who are partially dedicated to research\n",
      "often rely on the same support teams for research and production.\n",
      "Data Engineers and Business Leadership\n",
      "So far , we’ve discussed technical roles a data engineer interacts with. But\n",
      "data engineers also operate more broadly as or ganizational connectors,\n",
      "often in a non-technical capacity . Businesses have come to rely increasingly\n",
      "on data as a core part of many products, or as a product in itself. Data\n",
      "professionals, such as data engineers, now participate in strategic planning\n",
      "and lead key initiatives that extend beyond the boundaries of IT . Data\n",
      "engineers often support data architects by acting as the glue between the\n",
      "business and data science/analytics.\n",
      "Data in the C-suite\n",
      "C-level executives are increasingly involved in data and analytics as these\n",
      "are recognized as central assets for modern businesses. CEOs now concern\n",
      "themselves with initiatives that were once the exclusive province of IT , such\n",
      "as major cloud migrations or deployment of a new customer data platform.\n",
      "This is certainly the case at tech companies such as Google, Amazon, and\n",
      "Facebook, but also in the wider business world.\n",
      "Chief executive officer\n",
      "CEOs at non-tech companies generally don’ t concern themselves with the\n",
      "nitty-gritty of data frameworks and software. Rather , they define a vision in\n",
      "collaboration with technical C-suite roles and company data leadership.\n",
      "Data engineers provide a window into what’ s possible with data. That is,\n",
      "data engineers and their managers maintain a map of what data is available\n",
      "to the or ganization -- both internally and from third parties -- in what\n",
      "timeframe. They are also tasked to study major data architectural changes in\n",
      "collaboration with other engineering roles. For example, data engineers areoften heavily involved in cloud migrations, migrations to new data systems,\n",
      "or deployment of streaming technologies.\n",
      "Chief information officer\n",
      "A chief information of ficer (CIO) is the senior C-suite executive\n",
      "responsible for information technology within an or ganization; it is an\n",
      "internal-facing role. A CIO must possess deep knowledge of both\n",
      "information technology and business processes—either alone is insuf ficient.\n",
      "CIOs direct the information technology or ganization, setting ongoing\n",
      "policies while also defining and executing major initiatives under the\n",
      "direction of the CEO.\n",
      "In or ganizations with well-developed data culture, CIOs often collaborate\n",
      "with data engineering leadership. If an or ganization is not very high in its\n",
      "data maturity , a CIO will typically help shape its data culture. CIOs will\n",
      "work with engineers and architects to map out major initiatives and make\n",
      "strategic decisions on the adoption of major architectural elements, such as\n",
      "ERP and CRM systems, cloud migrations, data systems, and internal-facing\n",
      "IT.\n",
      "Chief technology officer\n",
      "A chief technology of ficer (CT O) is similar to a CIO but faces outward. A\n",
      "CTO owns the key technological strategy and architectures for external-\n",
      "facing applications, such as mobile, web apps, and IoT , all critical data\n",
      "sources for data engineers. The CT O is likely a skilled technologist and has\n",
      "a good sense of software engineering fundamentals, system architecture,\n",
      "and much more. Data engineers often report directly or indirectly through a\n",
      "CTO.\n",
      "Chief data officer\n",
      "The chief data of ficer (CDO) was created in 2002 at Capital One in\n",
      "recognition of the growing importance of data as a business asset. The CDO\n",
      "is responsible for a company’ s data assets and strategy . CDOs are focused\n",
      "on the business utility of data but should have a strong technical grounding.\n",
      "CDOs oversee data products, strategy , and initiatives, as well as corefunctions such as master data management and data privacy . Occasionally ,\n",
      "CDOs oversee business analytics and data engineering.\n",
      "Chief analytics officer\n",
      "The chief analytics of ficer (CAO) is a variant of the CDO role. Where both\n",
      "roles exist, the CDO focuses on the technology and or ganization required to\n",
      "deliver data, where the chief analytics of ficer is responsible for analytics,\n",
      "strategy , and decision making for the business. A CAO may oversee data\n",
      "science and machine learning as well, though this lar gely depends on\n",
      "whether or not the company has a CDO or CT O role.\n",
      "Chief algorithms officer\n",
      "A chief algorithms of ficer (CAO-2) is a very recent innovation in the C-\n",
      "suite, a highly technical role focused specifically on data science and\n",
      "machine learning. The CAO-2’ s typically have experience as individual\n",
      "contributors and team leads in data science or machine learning projects.\n",
      "Frequently , they have a background in machine learning research and a\n",
      "related advanced degree.\n",
      "CAO-2’ s are expected to be conversant in current machine learning\n",
      "research and have deep technical knowledge of their company’ s machine\n",
      "learning initiatives. In addition to creating business initiatives, they provide\n",
      "technical leadership, set research and development agendas, build research\n",
      "teams, etc.\n",
      "Data engineers and project managers\n",
      "Data engineers often work on lar ge initiatives, potentially spanning many\n",
      "years. As we write this book, many data engineers are working on cloud\n",
      "migrations, migrating pipelines and warehouses to the next generation of\n",
      "data tools. Other data engineers are starting greenfield, creating new data\n",
      "architectures from scratch, able to choose from a breathtaking number of\n",
      "best-of-breed architecture and tooling options.\n",
      "These lar ge initiatives often benefit from project management  (in contrast\n",
      "to product management, which we discuss below). Where data engineersfunction in an infrastructure and service delivery capacity , project managers\n",
      "direct traf fic and serve as gatekeepers. Most project managers operate\n",
      "according to some variation of Agile and Scrum, with W aterfall still\n",
      "appearing occasionally . Business never sleeps, and business stakeholders\n",
      "often have a significant backlog of things they want to be addressed, and\n",
      "new initiatives they want to launch. Project managers must filter a long list\n",
      "of requests and prioritize key deliverables to keep projects on track and\n",
      "better serve the company as a whole.\n",
      "Data engineers interact with project managers, often in planning sprints for\n",
      "projects, as well as ensuing standups related to the sprint. Feedback goes\n",
      "both ways, with data engineers informing project managers and other\n",
      "stakeholders about progress and blockers, and project managers balancing\n",
      "the cadence of technology teams against the ever -changing needs of the\n",
      "business.\n",
      "Data engineers and product managers\n",
      "Product managers oversee product development, often owning product\n",
      "lines. In the context of data engineers, these products are called “data\n",
      "products”. Data products are either built from the ground up or are\n",
      "incremental improvements upon existing products. As the broader corporate\n",
      "world has adopted a data-centric focus, data engineers interact more\n",
      "frequently with product managers . Similar to project managers, product\n",
      "managers balance the activity of technology teams against the needs of the\n",
      "customer and business.\n",
      "Data engineers and other management roles\n",
      "Data engineers interact with a variety of managers beyond project and\n",
      "product managers. However , these interactions usually follow either the\n",
      "services model or the cross-functional model. That is, data engineers either\n",
      "serve a variety of incoming requests as a centralized team or work as a\n",
      "resource assigned to a particular manager , project, or product.\n",
      "For more information on data teams and how to structure them, we\n",
      "recommend John Thompson’ s “Building Analytics T eams” and JesseAnderson’ s “Data T eams” (see the further r eading  section at the end of the\n",
      "chapter). Both books provide strong frameworks and perspectives on the\n",
      "roles of executives with data, who to hire, and how to construct the most\n",
      "effective data team for your company .\n",
      "Conclusion\n",
      "This chapter provided you with a brief overview of the data engineering\n",
      "landscape, including:\n",
      "Defining data engineering, and describing what data engineers do\n",
      "Describing the types of data maturity a company may\n",
      "Type A and T ype B data engineers\n",
      "Who data engineers work with\n",
      "A stab at the future of data engineering\n",
      "We hope that the first chapter has whetted the reader ’s appetite, whether\n",
      "they are practitioners of software development, data science, machine\n",
      "learning engineering, or business stakeholders, entrepreneurs, venture\n",
      "capitalists, etc. Of course, there is a great deal still to elucidate in\n",
      "subsequent chapters. In Chapter 2, we will cover the data engineering\n",
      "lifecycle in detail, followed by architecture in Chapter 3. In subsequent\n",
      "chapters, we will get into the nitty-gritty of technology decisions for each\n",
      "part of the lifecycle. The entire data field is in flux, and the aim in each\n",
      "chapter is to focus on the “immutables,” or at least perspectives that will be\n",
      "valid for many years to come in the midst of relentless change.\n",
      "1 https://www .altexsoft.com/blog/datascience/what-is-data-engineering-explaining-data-\n",
      "pipeline-data-warehouse-and-data-engineer -role/\n",
      "2 Jesse Anderson, https://www .jesse-anderson.com/2018/06/the-two-types-of-data-engineering/\n",
      "3 Maxime Beauchemin, https://medium.com/free-code-camp/the-rise-of-the-data-engineer -\n",
      "91be18f1e6034 What Is Data Engineering?  (O’Reilly 2020), https://learning.oreilly .com/library/view/what-is-\n",
      "data/9781492075578/ch01.html\n",
      "5 https://www .lexico.com/en/definition/big_data\n",
      "6 https://www .wired.com/201 1/10/how-yahoo-spawned-hadoop/\n",
      "7 https://techcrunch.com/2016/07/02/andy-jassys-brief-history-of-the-genesis-of-aws/\n",
      "8 https://twitter .com/danariely/status/287952257926971392?lang=en\n",
      "9 DataOps (aka Data Operations). W e will return to this topic throughout the book, starting in\n",
      "Chapter 2. For more information, read the DataOps Manifesto.\n",
      "10 California Consumer Privacy Act and General Data Protection Regulation.\n",
      "1 1 https://hackernoon.com/the-ai-hierarchy-of-needs-18f1 11fcc007\n",
      "12 https://cmmiinstitute.com/dmm\n",
      "13 https://www .linkedin.com/pulse/what-recovering-data-scientist-joe-reis/\n",
      "14 https://towardsdatascience.com/should-you-become-a-data-engineer -in-2021-4db57b6cce35\n",
      "15 https://www .getdbt.com/\n",
      "16 https://medium.com/@rchang/my-two-year -journey-as-a-data-scientist-at-twitter -\n",
      "f0c13298aee6\n",
      "17 https://www .dataversity .net/data-architect-vs-data-engineer/\n",
      "18 There are a variety of references for this notion: https://blog.ldodds.com/2020/01/31/do-data-\n",
      "scientists-spend-80-of-their -time-cleaning-data-turns-out-no/ ;\n",
      "https://www .datanami.com/2020/07/06/data-prep-still-dominates-data-scientists-time-survey-\n",
      "finds/ . This cliche is widely known, but there’ s healthy debate around its validity in dif ferent\n",
      "practical settings.Chapter 2. The Data\n",
      "Engineering Lifecycle\n",
      "A NOTE FOR EARLY RELEASE READERS\n",
      "With Early Release ebooks, you get books in their earliest form—the\n",
      "authors’ raw and unedited content as they write—so you can take\n",
      "advantage of these technologies long before the of ficial release of these\n",
      "titles.\n",
      "This will be the second chapter of the final book.\n",
      "If you have comments about how we might improve the content and/or\n",
      "examples in this book, or if you notice missing material within this\n",
      "chapter , please reach out to the authors at\n",
      "book_feedback@ternarydata.com .\n",
      "The major goal of this book is to encourage you to move beyond viewing\n",
      "data engineering as a specific collection of data technologies. The data\n",
      "landscape is seeing an explosion of new data technologies and practices,\n",
      "with higher levels of abstraction and ease of use. Some of these\n",
      "technologies will succeed, and most will fade into obscurity at an ever -\n",
      "faster rate. Because of increased technical abstraction, data engineers will\n",
      "increasingly become data lifecycle engineers , thinking and operating in\n",
      "terms of the principles  of data lifecycle management.\n",
      "In this chapter , you’ll learn about the data engineering lifecycle , which is\n",
      "the central theme of this book . The data engineering lifecycle is our\n",
      "framework describing “cradle to grave” data engineering . You will also\n",
      "learn about the undercurrents of the data engineering lifecycle, which are\n",
      "key foundations that support all data engineering ef forts.What Is the Data Engineering Lifecycle?\n",
      "In simple terms, the data engineering lifecycle is the series of stages that\n",
      "turn raw data ingredients into a useful end product, ready for consumption\n",
      "by analysts, machine learning engineers, etc. In this chapter , we introduce\n",
      "the major stages of the data engineering lifecycle, focusing on core\n",
      "concepts and saving details of each stage to later chapters.\n",
      "We divide the data engineering lifecycle into the following five stages\n",
      "(Figure 2-1 ):\n",
      "Generation: source systems\n",
      "Ingestion\n",
      "Storage\n",
      "Transformation\n",
      "Serving data\n",
      "In general, the data engineering lifecycle starts by getting data from source\n",
      "systems, storing it, and transforming and serving data to analysts, data\n",
      "scientists, machine learning engineers, and others. The stages in the middle\n",
      "—ingestion, storage, transformation—can get a bit jumbled. And that’ s ok.\n",
      "Although we split out the distinct parts of the data engineering lifecycle, it’ s\n",
      "not always a neat, continuous flow . Various stages of the lifecycle may\n",
      "repeat themselves, occur out of step, or weave together in interesting ways.\n",
      "Acting as a bedrock are undercurrents ( Figure 2-1 ) that cut across multiple\n",
      "stages of the data engineering lifecycle—data management, DataOps, data\n",
      "architecture, orchestration, and software engineering. No part of the data\n",
      "engineering lifecycle can properly function without each of these\n",
      "undercurrents.Figur e 2-1. Components and under currents of the data engineering lifecycle\n",
      "The Data Lifecycle V ersus the Data Engineering\n",
      "LifecycleYou may be wondering what the dif ference is between the overall data\n",
      "lifecycle and the data engineering lifecycle. There’ s a subtle distinction\n",
      "between the two. The data engineering lifecycle is a subset of the full data\n",
      "lifecycle that is owned by data engineers (see Figure 2-2 ). Whereas the full\n",
      "data lifecycle encompasses data across its entire lifespan, the data\n",
      "engineering lifecycle focuses on the stages a data engineer controls, namely\n",
      "data generation in source systems to serving the data for analytics and\n",
      "machine learning use-cases.\n",
      "Figur e 2-2. The data engineering lifecycle is a subset of the full data lifecycle\n",
      "Generation: Source Systems\n",
      "The data engineer needs to understand how source systems work, how they\n",
      "generate data, the frequency and velocity of the data, and the variety of data\n",
      "they generate. A major challenge in modern data engineering is thatengineers must work with and understand a dizzying array of data source\n",
      "systems. As an illustration, let’ s look at two common source systems, one\n",
      "very traditional and the other a modern example.\n",
      "Figure 2-3  illustrates a traditional source system, with applications backed\n",
      "by a database. This source system pattern became popular in the 1980s with\n",
      "the explosive success of relational databases. The application + database\n",
      "pattern remains popular today with various modern evolutions of software\n",
      "development practices. For example, with microservices, applications often\n",
      "consist of many small service/database pairs rather than a single monolith.\n",
      "NoSQL databases like MongoDB, CosmosDB, Spanner , and DynamoDB\n",
      "are compelling alternatives to traditional RDBMS systems.Figur e 2-3. Source system example: an application database.\n",
      "Let’s look at another example of a source system. Figure 2-4  illustrates an\n",
      "IoT swarm, where a fleet of devices (circles) sends data messages\n",
      "(rectangles) to a central collection system. This type of system isincreasingly common as IoT devices—sensors, smart devices, and much\n",
      "more—proliferate in the wild.\n",
      "Figur e 2-4. Source system example: an IoT Swarm and Messaging Queue\n",
      "Evaluating source systems: key engineering considerationsBelow are some important characteristics of source systems that data\n",
      "engineers must think about. This is by no means an exhaustive list, but\n",
      "rather a starting set of evaluation questions:\n",
      "What are the basic characteristics of the data source? Is it an\n",
      "application? A swarm of IoT devices?\n",
      "How does the source handle state?\n",
      "At what rate is data generated? How many events per second? How\n",
      "many GB per hour?\n",
      "What level of consistency can data engineers expect from the\n",
      "output data? If you’re running data quality checks against the\n",
      "output data, how often do data inconsistencies occur—nulls where\n",
      "they aren’ t expected, bad formatting, etc—deviate from the norm?\n",
      "How often do errors occur?\n",
      "Will the data contain duplicates?\n",
      "Will some data values arrive late, possibly much later than other\n",
      "messages produced at the same time?\n",
      "What is the schema of the ingested data? W ill data engineers need\n",
      "to join across several tables or even several systems to get a full\n",
      "picture of the data?\n",
      "If schema changes—say , a new column is added—how is this dealt\n",
      "with and communicated to downstream stakeholders?\n",
      "How frequently should data be pulled from the source system?\n",
      "For stateful systems, e.g. a database tracking customer account\n",
      "information, is data provided as periodic snapshots or as update\n",
      "events from change data capture (CDC)? What’ s the logic for how\n",
      "changes are performed, and how are these tracked in the source\n",
      "database?Who/what is the data provider that will transmit the data for\n",
      "downstream consumption?\n",
      "Will reading from a data source impact its performance?\n",
      "Does the source system have upstream data dependencies? What\n",
      "are the characteristics of these upstream systems?\n",
      "Are data quality checks in place to check for late or missing data?\n",
      "Sources produce data that is consumed by downstream systems. This\n",
      "includes human-generated spreadsheets, IOT sensors, web and mobile\n",
      "applications, and everything in between. Each source has its unique volume\n",
      "and cadence of data generation. A data engineer should understand how the\n",
      "source generates data, including relevant quirks or nuances. Data engineers\n",
      "also need to understand the limits of the source systems they interact with.\n",
      "For example, will queries to feed analytics cause performance issues with\n",
      "an application?\n",
      "One of the most challenging nuances of source data is a schema . The\n",
      "schema defines the hierarchical or ganization of data. Logically , we can\n",
      "think of data at the level of a full source system, drilling down into\n",
      "individual tables, all the way to the structure of individual fields. The\n",
      "schema of data shipped from source systems is handled in a variety of\n",
      "ways. One popular model is schemaless . Schemaless doesn’ t mean the\n",
      "absence of schema—rather , it means that the schema is defined by the\n",
      "application as data is written, whether to a messaging queue, a flat file, a\n",
      "blob, or a document database such as MongoDB. A more traditional model\n",
      "built on relational database storage uses a fixed schema enforced in the\n",
      "database, which application writes must conform to.\n",
      "Either of these models presents challenges for data engineers. Schemas\n",
      "change over time; in fact, schema evolution is encouraged in the agile\n",
      "approach to software development. T aking raw data input in the source\n",
      "system schema and transforming this into output useful for analytics is a\n",
      "key component of the data engineer ’s job. This job becomes more\n",
      "challenging as the source schema evolves.There are numerous ways to transmit data from a source, including:\n",
      "Programmatically\n",
      "Message brokers\n",
      "APIs\n",
      "RPC\n",
      "Streams\n",
      "Output files\n",
      "We will dive into source systems in greater detail in Chapter 5.\n",
      "Ingestion\n",
      "After you understand the data source and the characteristics of the source\n",
      "system you’re using, you need to gather the data. The second stage of the\n",
      "data engineering lifecycle is data ingestion from source systems. In our\n",
      "experience, source systems and ingestion represent the biggest bottlenecks\n",
      "of the data engineering lifecycle. The source systems are normally outside\n",
      "of your direct control, and might randomly become unresponsive or provide\n",
      "data of poor quality . Or, your data ingestion service might mysteriously stop\n",
      "working for any number of reasons. As a result, data flow stops or delivers\n",
      "bad data for storage, processing, and serving. Unreliable source and\n",
      "ingestion systems have a ripple ef fect across the data engineering lifecycle.\n",
      "Assuming you’ve answered the big questions listed above about source\n",
      "systems, and you’re in good shape, you’re now ready to ingest data. Let’ s\n",
      "cover some key things to think about.\n",
      "Key engineering considerations for the ingestion phase\n",
      "When preparing to architect or build a system, here are some primary\n",
      "questions to ask yourself related to the ingestion stage:\n",
      "What’ s the use case for the data I’m ingesting?Can I re-use this data, versus having to create multiple versions of\n",
      "the same dataset?\n",
      "Where is the data going? What’ s the destination?\n",
      "How frequently will I need to access the data?\n",
      "In what volume will the data typically arrive?\n",
      "What format is the data in? Can my downstream storage and\n",
      "transformation systems handle this format?\n",
      "Is the source data in good shape for immediate downstream use? If\n",
      "so, for how long, and what may cause it to be unusable?\n",
      "If the data is from a streaming source, does the data need to be\n",
      "transformed before it reaches its destination? If so, would an in-\n",
      "flight transformation, where the data is transformed within the\n",
      "stream itself, be appropriate?\n",
      "These are just a sample of the things you’ll need to think about with\n",
      "ingestion, and we’ll cover those questions and more in Chapter 6. Before\n",
      "we leave, let’ s briefly turn our attention to two major data ingestion\n",
      "paradigms—batch versus streaming, and push versus pull.\n",
      "Batch versus streaming\n",
      "Virtually all data we deal with is inherently streaming. That is, data is\n",
      "nearly always produced and updated continually at its source. Batch\n",
      "ingestion is simply a specialized and convenient way of processing this\n",
      "stream in lar ge chunks, for example handling a full day’ s worth of data in a\n",
      "single batch.\n",
      "Streaming ingestion allows us to provide data to downstream systems—\n",
      "whether other applications, databases, or analytics systems—in a\n",
      "continuous, real-time fashion. Here, “real-time” (or “near real-time”) means\n",
      "that the data is available to a downstream system a short time after it is\n",
      "produced, for example, less than one second later . The latency required to\n",
      "qualify as real-time varies by domain and requirements.Batch data is ingested either on a predetermined time interval or as data\n",
      "reaches a preset size threshold. Batch ingestion is a one-way door—once\n",
      "data is broken into batches, the latency for downstream consumers is\n",
      "inherently constrained. Due to factors that limited the ways that data could\n",
      "be processed, batch was—and still is—a very popular way to ingest data for\n",
      "downstream consumption, particularly in the areas of analytics and machine\n",
      "learning. However , the separation of storage and compute in many systems,\n",
      "as well as the ubiquity of event streaming and processing platforms, make\n",
      "continuous processing of data streams much more accessible and\n",
      "increasingly popular . The choice lar gely depends on the use case and\n",
      "expectations for data timeliness.\n",
      "Key considerations for batch versus stream ingestion\n",
      "Should you go streaming-first? Despite the attractiveness of a streaming-\n",
      "first approach, there are many tradeof fs to understand and think about.\n",
      "Below are some questions to ask yourself when determining if streaming\n",
      "ingestion is an appropriate choice over batch ingestion:\n",
      "If I ingest the data in real-time, can downstream storage systems\n",
      "handle the rate of data flow?\n",
      "Do I need true, to the millisecond real-time data ingestion? Or\n",
      "would a micro-batch approach work where I accumulate and ingest\n",
      "data, say every minute?\n",
      "What are my use cases for streaming ingestion? What specific\n",
      "benefits do I realize by implementing streaming? If I get data in\n",
      "real-time, what actions can I take on that data that would be an\n",
      "improvement upon batch?\n",
      "Will my streaming-first approach cost more in terms of time,\n",
      "money , maintenance, downtime, and opportunity cost than simply\n",
      "doing batch?\n",
      "Is my streaming pipeline and system reliable and redundant in the\n",
      "event of infrastructure failure?What tools are most appropriate for the use case? Should I use a\n",
      "managed service (Kinesis, Pubsub, Dataflow), or stand up my own\n",
      "instances of Kafka, Flink, Spark, Pulsar , etc.? If I do the latter , who\n",
      "will manage it? What are the costs and tradeof fs?\n",
      "If I’m deploying a machine learning model, what benefits do I\n",
      "have with online predictions, and possibly continuous training?\n",
      "Am I getting data from a live production instance? If so, what’ s the\n",
      "impact of my ingestion process on this source system?\n",
      "As you can see, streaming-first might seem like a good idea, but it’ s not\n",
      "always straightforward; there are inherently extra costs and complexities.\n",
      "Many great ingestion frameworks do handle both batch and micro-batch\n",
      "ingestion styles. That said, we think batch is a perfectly fine approach for\n",
      "many data ingestion very common use-cases such as model training and\n",
      "weekly reporting, which are inherently batch-oriented. Adopt true real-time\n",
      "streaming-only after identifying a business use case that justifies the extra\n",
      "complexity .\n",
      "Push versus pull\n",
      "In the push  model of data ingestion, a source system writes data out to a\n",
      "target, whether that be a database, object storage, or a file system. In the\n",
      "pull model, data is retrieved from the source system. In practice, the line\n",
      "between the push and pull paradigms can be quite blurry; often data is both\n",
      "pushed and pulled as it works its way through the various stages of a data\n",
      "pipeline.\n",
      "Consider , for example, the ETL (extract, transform, load) process,\n",
      "commonly used in batch-oriented ingestion workflows. The extract  part of\n",
      "ETL makes it clear that we’re dealing with a pull ingestion model. In\n",
      "traditional ETL, the ingestion system queries a current table snapshot on a\n",
      "fixed schedule.\n",
      "In another example, consider continuous change data capture (CDC), which\n",
      "is achieved in a few dif ferent ways. One common method triggers amessage every time a row is changed in the source database. This message\n",
      "is pushed to a queue where it is picked up by the ingestion system. Another\n",
      "common CDC method uses binary logs, which record every commit to the\n",
      "database. The database pushes  to its logs. The ingestion system reads the\n",
      "logs but doesn’ t directly interact with the database otherwise. This adds\n",
      "little to no additional load to the source database. Some versions of batch\n",
      "CDC use the pull pattern. For example, in timestamp-based CDC an\n",
      "ingestion system queries the source database and pulls the rows that have\n",
      "changed since the previous update.\n",
      "With streaming ingestion, data bypasses a backend database and is pushed\n",
      "directly to an endpoint, typically with some kind of data buf fering such as a\n",
      "queue. This pattern is useful with fleets of IoT sensors emitting sensor data.\n",
      "Rather than relying on a database to maintain the current state, we simply\n",
      "think of each recorded reading as an event. This pattern is also growing in\n",
      "popularity in software applications as it simplifies real-time processing,\n",
      "allows app developers to tailor their messages for the needs of downstream\n",
      "analytics, and greatly simplifies the lives of data engineers.\n",
      "We’ll discuss ingestion best practices and techniques in depth in Chapter 6.\n",
      "Storage\n",
      "After ingesting data, you need a place to store it. Choosing a storage\n",
      "solution is key to success in the rest of the data lifecycle, and it’ s also one of\n",
      "the most complicated stages of the data lifecycle for a variety of reasons.\n",
      "First, modern data architectures in the cloud often leverage several  storage\n",
      "solutions. Second, few data storage solutions function purely as storage,\n",
      "with many also supporting complex transformation queries; even object\n",
      "storage solutions may support powerful query capabilities (e.g A WS S3\n",
      "Select). Third, while storage is a stage of the data engineering lifecycle, it\n",
      "frequently touches on other stages, such as ingestion, transformation, and\n",
      "serving.\n",
      "Storage frequently occurs in multiple places in a data pipeline, with storage\n",
      "systems crossing over with processing, serving, and ingestion stages. Forexample, cloud data warehouses can store data, process data in pipelines,\n",
      "and serve it to analysts; streaming frameworks such as Kafka and Pulsar\n",
      "can function simultaneously as ingestion, storage, and query systems for\n",
      "messages; and object storage is a standard layer for the transmission of data.\n",
      "In general with storage, data moves and goes somewhere, gets stored\n",
      "temporarily or permanently , then goes somewhere else, gets stored again,\n",
      "and so on.\n",
      "Evaluating storage systems: key engineering considerations\n",
      "Here are a few key engineering questions to ask when choosing a storage\n",
      "system for a data warehouse, data lakehouse, database, object storage, etc.:\n",
      "Is this storage solution compatible with the required write speeds\n",
      "for the architecture?\n",
      "Will storage create a bottleneck for downstream processes?\n",
      "Do you understand how this storage technology works? Are you\n",
      "utilizing the storage system optimally , or committing unnatural\n",
      "acts? For instance, are you applying a high rate of random access\n",
      "updates in an object storage system, an antipattern with significant\n",
      "performance overhead?\n",
      "Will this storage system handle anticipated future scale? Y ou\n",
      "should take into account all capacity limits on the storage system:\n",
      "total available storage, read operation rate, write volume, etc.\n",
      "Will downstream users and processes be able to retrieve data in the\n",
      "required SLA?\n",
      "Are you capturing metadata about schema evolution, data flows,\n",
      "data lineage, and so forth? Metadata has a significant impact on the\n",
      "utility of data. Metadata represents an investment in the future,\n",
      "dramatically enhancing discoverability and institutional knowledge\n",
      "to streamline future projects and architecture changes.Is this a pure storage solution (object storage) or does it support\n",
      "complex query patterns (i.e., a cloud data warehouse)?\n",
      "Is the storage system schema-agnostic (object storage)? Flexible\n",
      "schema? (Cassandra) Enforced schema? (A cloud data warehouse)\n",
      "For data governance, how are you tracking master data, golden\n",
      "records data quality , and data lineage? (W e’ll have more to say on\n",
      "these in the data management  subsection of this chapter .)\n",
      "How are you handling compliance and data sovereignty? For\n",
      "example, can you store your data in certain geographical locations,\n",
      "but not others?\n",
      "Data access frequency\n",
      "Not all data is accessed in the same way . Retrieval patterns will greatly\n",
      "vary, based upon the type of data being stored and queried. This brings up\n",
      "the notion of the “temperatures” of data. Data access frequency will\n",
      "determine what “temperature” your data is. Data that is most frequently\n",
      "accessed is called hot data . Hot data is commonly retrieved many times per\n",
      "day, perhaps even several times per second in systems that serve user\n",
      "requests. This is data that should be stored for fast retrieval, where “fast” is\n",
      "relative to the use case. Lukewarm data is data that might be accessed every\n",
      "so often, say every week or month. Cold data is seldom queried and is\n",
      "appropriate for storing in an archival system. Cold data is often data that is\n",
      "retained for compliance purposes, or in case of a catastrophic failure in\n",
      "another system. In the “old days,” cold data would be stored on tapes and\n",
      "shipped to remote archival facilities. In cloud environments, vendors of fer\n",
      "specialized storage tiers with extremely low monthly storage costs, but high\n",
      "prices for data retrieval.\n",
      "Selecting a storage system\n",
      "What type of storage solution should you use? This depends on your use\n",
      "cases, data volumes, frequency of ingestion, format, and size of the data\n",
      "being ingested -- essentially , the key considerations listed above. There isnot a one-size-fits-all universal storage recommendation. Every storage\n",
      "technology has its tradeof fs. Let’ s quickly list some of the common storage\n",
      "systems that a data engineer will encounter . There are countless varieties of\n",
      "storage technologies, and it’ s easy to be overwhelmed when deciding the\n",
      "best option for your data architecture. Here are several popular storage\n",
      "options:\n",
      "Relational database management systems (RDBMS)\n",
      "Data lake\n",
      "Data warehouse\n",
      "Data lakehouse\n",
      "Streaming systems with data retention capabilities.\n",
      "Graph database\n",
      "In-memory (Redis, Memcached, etc.)\n",
      "High performance NoSQL databases: RocksDB, etc.\n",
      "Feature stores\n",
      "Data catalog\n",
      "Metadata stores\n",
      "Spreadsheets\n",
      "Chapter 7 will cover storage best practices and approaches in greater detail,\n",
      "as well as the crossover between storage and other lifecycle stages.\n",
      "Transformation\n",
      "The next stage of the data engineering lifecycle is transformation . As we\n",
      "mentioned, data can be ingested in its raw form, with no transformations\n",
      "performed. In most cases, however , data needs to be changed from its\n",
      "original form into something useful for downstream use cases. W ithoutproper transformations, data will not be in an appropriate form for reports,\n",
      "analysis, or machine learning. T ypically , the transformation stage is where\n",
      "data begins to create value for downstream user consumption.\n",
      "There are a whole host of data transformation types, which we will cover\n",
      "extensively in chapter 8. Immediately after ingestion, basic transformations\n",
      "map data into correct types (changing ingested string data into numeric and\n",
      "date types for example), put records into standard formats and remove bad\n",
      "records. Later stages of transformation may transform the data schema and\n",
      "apply normalization. Downstream, we can apply lar ge scale aggregation for\n",
      "reporting or featurize data for machine learning processes.\n",
      "Key considerations for the transformation phase\n",
      "When thinking about data transformations within the context of the data\n",
      "engineering lifecycle, it helps to consider the following. W e’ll cover\n",
      "transformations in-depth in Chapter 8.\n",
      "What’ s the cost and ROI of the transformation? All transformations\n",
      "should have an associated business value attached.\n",
      "Is the transformation expensive from a time and resource\n",
      "perspective?\n",
      "What value will the transformation bring downstream? In other\n",
      "words, what is the ROI of a transformation?\n",
      "Is the transformation as simple and self-isolated as possible?\n",
      "What business rules do the transformations support?\n",
      "During transformation, am I minimizing data movement between\n",
      "the transformation and the storage system?\n",
      "Data can be transformed in batch, or while streaming in-flight. As\n",
      "mentioned in the section on ingestion, virtually all data starts life as a\n",
      "continuous stream; batch is just a specialized way of processing a data\n",
      "stream. Batch transformations are overwhelmingly popular , but given the\n",
      "growing popularity of stream processing solutions such as Flink, Spark,Beam, etc., as well as the general increase in the amount of streaming data,\n",
      "we expect the popularity of streaming transformations to continue growing,\n",
      "perhaps entirely replacing batch processing in certain domains soon.\n",
      "Logically , we treat transformation as a standalone area of the data\n",
      "engineering lifecycle, but the realities of the lifecycle can be much more\n",
      "complicated in practice. T ransformation is often entangled in other phases\n",
      "of the data engineering lifecycle. T ypically , data is transformed in source\n",
      "systems or during ingestion. For example, a source system may add an\n",
      "event timestamp to a record before forwarding it to an ingestion process. Or\n",
      "a record within a streaming pipeline may be “enriched” with additional\n",
      "fields and calculations before it’ s sent to a data warehouse. T ransformations\n",
      "are ubiquitous in various parts of the lifecycle. Data preparation, data\n",
      "wrangling, and cleaning—all of these transformative tasks add value to end-\n",
      "consumers of data.\n",
      "Business logic is a major driver of data transformation. Business logic is\n",
      "critical for obtaining a clear and current picture of business processes. A\n",
      "simple view of raw retail transactions might not be useful in itself without\n",
      "adding the logic of accounting rules so that the CFO has a clear picture of\n",
      "financial health. In general, ensure a standard approach for implementing\n",
      "business logic across your transformations.\n",
      "Data featurization for machine learning is another data transformation\n",
      "process. Featurization intends to extract and enhance features of data that\n",
      "will be useful for training machine learning models. Featurization is\n",
      "something of a dark art, combining domain expertise (to identify which\n",
      "features might be important for prediction), with extensive experience in\n",
      "data science. For this book, the main point to take away is that once data\n",
      "scientists determine how to featurize data, featurization processes can be\n",
      "automated by data engineers in the transformation stage of a data pipeline.\n",
      "Transformation is a very deep subject, and we cannot do it justice in this\n",
      "very brief introduction. Chapter 8 will delve into the various practices and\n",
      "nuances of data transformations.Serving Data for Analytics, Machine Learning, and\n",
      "Reverse ETL\n",
      "You’ve reached the last stage of the data engineering lifecycle. Now that the\n",
      "data has been ingested, stored, and transformed into coherent and useful\n",
      "structures, it’ s time to get value from your data. “Getting value” from data\n",
      "means dif ferent things to dif ferent users. Data has value when it’ s used for\n",
      "practical purposes. Data that is not consumed or queried is simply inert.\n",
      "Data vanity projects are a major risk for companies. Many companies\n",
      "pursued vanity projects in the big data era, gathering massive datasets in\n",
      "data lakes that were never consumed in any useful way . The cloud era is\n",
      "triggering a new wave of vanity projects, built on the latest data\n",
      "warehouses, object storage systems, and streaming technologies. Data\n",
      "projects must be intentional across the lifecycle. What is the ultimate\n",
      "business purpose of the data so carefully collected, cleaned, and stored?\n",
      "Data serving is perhaps the most exciting part of the data engineering\n",
      "lifecycle. This is where the magic happens. This is where machine learning\n",
      "engineers can apply the most advanced modern techniques.\n",
      "For now , let’s take a look at some of the popular uses of data—analytics,\n",
      "machine learning, and reverse ETL.\n",
      "Analytics\n",
      "Analytics is the core of most data endeavors. Once your data is stored and\n",
      "transformed, you’re ready to generate reports, dashboards, and do ad hoc\n",
      "analysis on the data. Whereas the bulk of analytics used to encompass\n",
      "business intelligence (BI), it now includes other facets such as operational\n",
      "analytics and customer -facing analytics ( Figure 2-5 ). Let’ s briefly touch on\n",
      "these variations of analytics:Figur e 2-5. Types of Analytics\n",
      "Business intelligence\n",
      "Business intelligence (BI) marshalls collected data to describe the past\n",
      "and current state of a business. BI requires the processing of raw data\n",
      "using business logic. Note that data serving for analytics is yet another\n",
      "area where the stages of the data engineering lifecycle can get tangled.\n",
      "As we mentioned earlier , business logic is often applied to data in the\n",
      "transformation stage of the data engineering lifecycle, but a logic-on-\n",
      "read approach has become increasingly popular . That is, data is stored in\n",
      "a cleansed but fairly raw form, with minimal business logic post-\n",
      "processing. A BI system maintains a repository of business logic and\n",
      "definitions. This business logic is used to query the data warehouse so\n",
      "that reports and dashboards accord with business definitions.55\n",
      "As a company grows its data maturity , it will move from ad hoc data\n",
      "analysis to self-service analytics, which allows democratized data\n",
      "access to business users, without the need for IT to intervene. The\n",
      "capability to do self-service analytics assumes that data is in a good\n",
      "enough place that people across the or ganization can simply access the\n",
      "data themselves, slice and dice it however they choose and get\n",
      "immediate insights from it. W e find that though self-service analytics is\n",
      "simple in theory , it’s incredibly dif ficult to pull of f in practice. The main\n",
      "reason is that poor data quality and or ganizational silos get in the way ofallowing widespread use of analytics, free of gatekeepers such as IT or\n",
      "reporting departments.\n",
      "Operational analytics\n",
      "Operational analytics focuses on the fine-grained details of operations,\n",
      "promoting actions that a user of the reports can act upon immediately .\n",
      "For example, operational analytics could be a live view of inventory , or\n",
      "real-time dashboarding of website health. In this case, data is consumed\n",
      "in a real-time manner , either directly from a source system, from a\n",
      "streaming data pipeline such as A WS Kinesis or Kafka, or aggregated in\n",
      "a real-time OLAP solution like Druid. The types of insights in\n",
      "operational analytics dif fer from traditional BI since operational\n",
      "analytics is focused on the present and doesn’ t necessarily concern\n",
      "historical trends.\n",
      "Customer -facing analytics\n",
      "You may wonder why we’ve broken out customer -facing analytics\n",
      "separately from BI. In practice, analytics provided to customers on a\n",
      "SAAS (Software as a Service) platform come with a whole separate set\n",
      "of requirements and complications. Internal BI faces a limited audience\n",
      "and generally presents a handful of unified views. Access controls are\n",
      "critical, but not particularly complicated. Access is managed using a\n",
      "handful of roles and access tiers.\n",
      "With customer -facing analytics, the request rate for reports, and the\n",
      "corresponding burden on analytics systems, go up dramatically; access\n",
      "control is significantly more complicated and critical. Businesses may\n",
      "be serving separate analytics and data to thousands or more customers.\n",
      "Each customer must see their data and only their data. Where an internal\n",
      "data access error at a company would likely lead to a procedural review ,\n",
      "a data leak between customers would be considered a massive breach of\n",
      "trust, likely leading to media attention and a significant loss of\n",
      "customers. Minimize your blast radius related to data leaks and securityvulnerabilities. Apply tenant or data level security within your storage,\n",
      "and anywhere else there’ s a possibility of data leakage.\n",
      "MULTITENANCY\n",
      "Many modern storage and analytics systems support multi-tenancy in a\n",
      "variety of ways. Engineers may choose to house data for many\n",
      "customers in common tables to allow a unified view for internal\n",
      "analytics and machine learning. This data is presented externally to\n",
      "individual customers through the use of logical views with\n",
      "appropriately defined controls and filters. It is incumbent on engineers\n",
      "to understand the minutiae of multitenancy in the systems they deploy\n",
      "to ensure absolute data security and isolation.\n",
      "Machine learning\n",
      "Machine learning is one of the most exciting technology revolutions of our\n",
      "time. Once or ganizations reach a high level of data maturity , they can begin\n",
      "to identify problems that are amenable to machine learning and start\n",
      "organizing a machine learning practice.\n",
      "The responsibilities of data engineers overlap significantly in analytics and\n",
      "machine learning, and the boundaries between data engineering, machine\n",
      "learning engineering, and analytics engineering can be fuzzy . For example,\n",
      "a data engineer may need to support Apache Spark clusters that facilitate\n",
      "both analytics pipelines and machine learning model training. They may\n",
      "also need to provide a Prefect or Dagster system that orchestrates tasks\n",
      "across teams, and support metadata and cataloging systems that track data\n",
      "history and lineage. Setting these domains of responsibility , and the relevant\n",
      "reporting structures is a critical or ganizational decision.\n",
      "One recently developed tool that combines data engineering and ML\n",
      "engineering is the featur e stor e (e.g. FEAST and T ecton, among others).\n",
      "Feature stores are designed to reduce the operational burden for machine\n",
      "learning engineers, by maintaining feature history and versions, supportingfeature sharing between teams, and providing basic operational and\n",
      "orchestration capabilities, such as backfilling. In practice, data engineers are\n",
      "part of the core support team for feature stores to support ML engineering.\n",
      "Should a data engineer be familiar with machine learning? It certainly\n",
      "helps. Regardless of the operational boundary between data engineering,\n",
      "ML engineering, and business analytics, etc., data engineers should\n",
      "maintain operational knowledge about the teams they work with. A good\n",
      "data engineer is conversant in the fundamental machine learning techniques\n",
      "and related data processing requirements (deep learning, featurization, etc.),\n",
      "in the use cases for models within their company , and the responsibilities of\n",
      "the or ganization’ s various analytics teams. This helps to maintain ef ficient\n",
      "communication and facilitate collaboration. Ideally , data engineers will\n",
      "build tools in collaboration with other teams that neither team is capable of\n",
      "building on its own.\n",
      "This book cannot possibly cover machine learning in-depth. There’ s a\n",
      "growing ecosystem of books, videos, articles, and communities if you’re\n",
      "interested in learning more; we include a few additional references in the\n",
      "further r eading  section at the end of this chapter .\n",
      "Below are some considerations for the Serving Data phase, specific to\n",
      "machine learning:\n",
      "Is the data of suf ficient quality to perform reliable feature\n",
      "engineering? Quality requirements and assessments are developed\n",
      "in close collaboration with teams consuming the data.\n",
      "Is the data discoverable? Can data scientists and machine learning\n",
      "engineers easily find useful data?\n",
      "Where are the technical and or ganizational boundaries between\n",
      "data engineering and machine learning engineering? This\n",
      "organizational question has major architectural implications.\n",
      "The dataset properly represents ground truth and isn’ t unfairly\n",
      "biased.While machine learning is exciting, our experience is that companies often\n",
      "prematurely dive into it. Before investing a ton of resources into machine\n",
      "learning, take the time to build a solid data foundation. This means setting\n",
      "up the best systems and architecture across the data engineering lifecycle,\n",
      "as well as the machine learning lifecycle. More often than not, get good\n",
      "with analytics before moving to machine learning. Many companies have\n",
      "seen their machine learning dreams dashed because they undertook\n",
      "initiatives without appropriate foundations in place. On the other hand, data\n",
      "engineers should keep the serving stage of the data engineering lifecycle in\n",
      "mind at all times as they’re building out the other stages, both as motivation\n",
      "and to steer and shape their architectures and initiatives.\n",
      "Reverse ETL\n",
      "Reverse ETL has long been a practical reality in data, viewed as an\n",
      "antipattern that we didn’ t like to talk about or dignify with a name. Reverse\n",
      "ETL is the process of taking processed data from the output side of the data\n",
      "engineering lifecycle and feeding it back into source systems as shown in\n",
      "Figure 2-6 . In reality , this flow is extremely useful, and often necessary;\n",
      "reverse ETL allows us to take analytics, scored models, etc., and feed these\n",
      "back into production systems or SAAS platforms. Marketing analysts might\n",
      "calculate bids in Excel using the data in their data warehouse, then upload\n",
      "these bids to Google Ads. This process was often quite manual and\n",
      "primitive.Figur e 2-6. Reverse ETL\n",
      "As we’ve written this book, several vendors have embraced the concept of\n",
      "reverse ETL and built products around it, such as Hightouch and Census.\n",
      "Reverse ETL remains nascent as a field, but we suspect that it is here to\n",
      "stay.Reverse ETL has become especially important as businesses rely\n",
      "increasingly on SAAS and external platforms. For example, businesses may\n",
      "want to push certain metrics from their data warehouse to a customer data\n",
      "platform or customer relationship management (CRM) system. Advertising\n",
      "platforms are another common use case, as in the Google Ads example.\n",
      "Expect to see more activity in the Reverse ETL, with an overlap in both\n",
      "data engineering and ML engineering.\n",
      "The jury is out whether the term reverse ETL  will stick. And the practice\n",
      "may evolve. Some engineers claim that we can eliminate reverse ETL by\n",
      "handling data transformations in an event stream, and sending those events\n",
      "back to source systems as needed. Realizing widespread adoption of this\n",
      "pattern across businesses is another matter . The gist is that transformed data\n",
      "will need to be returned to source systems in some manner , ideally with the\n",
      "correct lineage and business process associated with the source system.\n",
      "The Major Undercurrents Across the Data\n",
      "Engineering Lifecycle\n",
      "Data engineering is rapidly maturing. Whereas prior cycles of data\n",
      "engineering simply focused on the technology layer , the continued\n",
      "abstraction and simplification of tools and practices have shifted this focus.\n",
      "Data engineering now encompasses far more than tools and technology . The\n",
      "field is now moving up the value chain, incorporating traditional\n",
      "“enterprise” practices such as data management and cost optimization, and\n",
      "newer practices like DataOps. W e’ve termed these practices under currents\n",
      "—data management, DataOps, data architecture, orchestration, and software\n",
      "engineering—that support every aspect of the data engineering lifecycle\n",
      "(Figure 2-7 ). We will give a brief overview of these undercurrents and their\n",
      "major components, which you’ll see in more detail throughout the book.Figur e 2-7. The major under currents of data engineering\n",
      "Data Management\n",
      "Data management? Y ou’re probably thinking that sounds very… corporate.\n",
      "“Old school” data management practices are making their way into data and\n",
      "machine learning engineering. What’ s old is new again. Data management\n",
      "has been around for decades but didn’ t get a lot of traction in data\n",
      "engineering until recently . We sense the motivation for adopting data\n",
      "management is because data tools are becoming simpler to use, and a data\n",
      "engineer needs to manage less complexity . As a result, the data engineer is\n",
      "moving up the value chain toward the next rung of best practices. Data best\n",
      "practices once reserved for very lar ge companies—data governance, master\n",
      "data management, data quality management, metadata management, etc—\n",
      "are now filtering down into companies of all sizes and maturity levels. As\n",
      "we like to say , data engineering is becoming “enterprisey”. This is a great\n",
      "thing!According to the DAMA-DMBOK, which we consider to be the definitive\n",
      "book for enterprise data management, “Data management is the\n",
      "development, execution, and supervision of plans, policies, programs, and\n",
      "practices that deliver , control, protect, and enhance the value of data and\n",
      "information assets throughout their lifecycle”. That’ s a bit verbose, so let’ s\n",
      "look at how it ties to data engineering, specifically . Data engineers manage\n",
      "the data lifecycle, and data management encompasses the set of best\n",
      "practices that data engineers will use to accomplish this task, both\n",
      "technically and strategically . Without a framework for managing data, data\n",
      "engineers are simply technicians operating in a vacuum. Data engineers\n",
      "need the broader perspective of data’ s utility across the or ganization, from\n",
      "the source systems to the C-Suite, and everywhere in between.\n",
      "Why is data management important? Data management demonstrates that\n",
      "data is a vital asset to daily operations, just as businesses view financial\n",
      "resources, finished goods, or real estate as assets. Data management\n",
      "practices form a cohesive framework that everyone can adopt to make sure\n",
      "that the or ganization is getting value from data and handling it\n",
      "appropriately .\n",
      "There are quite a few facets to data management, including the following:\n",
      "Data governance, including discoverability , security , and\n",
      "accountability\n",
      "Data modeling and design\n",
      "Data lineage\n",
      "Storage and operations\n",
      "Data integration and interoperability\n",
      "Data lifecycle management\n",
      "Data systems for advanced analytics and ML\n",
      "Ethics and privacy1While this book is in no way an exhaustive resource on data management,\n",
      "let’s briefly cover some salient points from each area, as they relate to data\n",
      "engineering.\n",
      "Data governance\n",
      "According to Data Governance: The Definitive Guide, “Data governance is,\n",
      "first and foremost, a data management function to ensure the quality ,\n",
      "integrity , security , and usability of the data collected by an or ganization.”\n",
      "We can expand on that definition and say that data governance engages\n",
      "people, processes, and technologies to maximize data value across an\n",
      "organization, while protecting data with appropriate security controls.\n",
      "Effective data governance is developed with intention and supported by the\n",
      "organization. When data governance is accidental and haphazard, the side\n",
      "effects can range from untrusted data to security breaches, and everything in\n",
      "between. Being intentional about data governance will maximize the\n",
      "organization’ s data capabilities and value generated from data. It will also\n",
      "(hopefully) keep a company out of headlines for questionable or downright\n",
      "reckless data practices.\n",
      "Think of the typical example where data governance is done poorly . A\n",
      "business analyst gets a request for a report, but doesn’ t know what data to\n",
      "use to answer the question. He or she may spend hours digging through\n",
      "dozens of tables in a transactional database, taking wild guesses at what\n",
      "fields might be useful. The analyst compiles a report that is “directionally\n",
      "correct,” but isn’ t entirely sure that the report’ s underlying data is accurate\n",
      "or sound. The recipient of the report also questions the validity of the data.\n",
      "In the end, the integrity of the analyst—and all data in the company’ s\n",
      "systems—is called into question. The company is confused about its\n",
      "performance, making business planning next to impossible.\n",
      "Data governance is a foundation for data-driven business practices and a\n",
      "mission-critical part of the data engineering lifecycle. When data\n",
      "governance is practiced well, people, processes, and technologies are all\n",
      "aligned to treat data as a key driver of the business. And, if data issues\n",
      "occur , they are promptly handled and things carry on.2The core categories of data governance are discoverability , security , and\n",
      "accountability . Within these core categories are subcategories, such as data\n",
      "quality , metadata, privacy , and much more. Let’ s look at each core category\n",
      "in turn.\n",
      "Discoverability\n",
      "In a data-driven company , data must be available and discoverable. End\n",
      "users should have quick and reliable access to the data they need to do their\n",
      "jobs. They should know where the data comes from (“Golden source”), how\n",
      "it relates to other data, and what the data means.\n",
      "There are some key components to data discoverability , including metadata\n",
      "management and master data management. Let’ s briefly describe these\n",
      "components.\n",
      "Metadata\n",
      "Metadata is “data about data,” and it underpins every section of the data\n",
      "engineering lifecycle. Metadata is exactly the data needed to make data\n",
      "discoverable and governable.\n",
      "We divide metadata into two major categories: auto-generated and human\n",
      "generated. Modern data engineering revolves around automation, but too\n",
      "often, metadata collection is still a manual, error prone process.\n",
      "Technology can assist with this process, removing much of the error -prone\n",
      "work of manual metadata collection. W e’re seeing a proliferation of data\n",
      "catalogs, data lineage tracking systems and metadata management tools.\n",
      "Tools can crawl databases to look for relationships and monitor data\n",
      "pipelines to track where data comes from and where it goes. A low-fidelity\n",
      "manual approach is to use an internally-led ef fort where metadata collection\n",
      "is crowdsourced by various stakeholders within the or ganization. These data\n",
      "management tools will be covered in-depth throughout the book, as they\n",
      "undercut much of the data engineering lifecycle.\n",
      "Metadata becomes a byproduct of data and data processes. However , key\n",
      "challenges remain. In particular , interoperability and standards are still3lacking. Metadata tools are only as good as their connectors to data\n",
      "systems, and their ability to share metadata with each other . In addition,\n",
      "automated metadata tools should not entirely take humans out of the loop.\n",
      "Data has a social element—each or ganization accumulates social capital\n",
      "and knowledge around processes, datasets, and pipelines. Human-oriented\n",
      "metadata systems focus on the social aspect of metadata. This is something\n",
      "that Airbnb has emphasized in their various blog posts on data tools,\n",
      "particularly their original Data Portal concept.  Such tools should provide a\n",
      "place to disclose data owners, data consumers, domain experts, etc.\n",
      "Documentation and internal wiki tools provide a key foundation for\n",
      "metadata management, but these tools should also integrate with automated\n",
      "data cataloging as mentioned above. For example, data scanning tools can\n",
      "generate wiki pages with links to relevant data objects.\n",
      "Once metadata systems and processes exist, data engineers can consume\n",
      "metadata in all kinds of useful ways. Metadata becomes a foundation for\n",
      "designing pipelines and managing data throughout the lifecycle.\n",
      "DMBOK identifies several main categories of metadata that are useful to\n",
      "data engineers:\n",
      "Business metadata\n",
      "Technical metadata\n",
      "Operational metadata\n",
      "Reference metadata\n",
      "Let’s briefly describe each category of metadata.\n",
      "Business metadata\n",
      "Business metadata relates to how data is used in the business, including\n",
      "business and data definitions, data rules and logic, how and where data is\n",
      "used, the data owner(s), and so forth.\n",
      "A data engineer uses business metadata to answer non-technical questions\n",
      "about “who”, “what”, “where”, and “how”. For example, a data engineer4may be tasked with creating a data pipeline for customer sales analysis. But,\n",
      "what is a “customer”? Is it someone who’ s purchased in the last 90 days?\n",
      "Or someone who’ s purchased at any time the business has been open? T o\n",
      "use the correct data, a data engineer would refer to business metadata (data\n",
      "dictionary or data catalog) to look up how a “customer” is defined. Business\n",
      "metadata provides a data engineer with the right context and definitions to\n",
      "properly use data.\n",
      "Technical metadata\n",
      "Technical metadata describes the data created and used by systems across\n",
      "the data engineering lifecycle. It includes the data model and schema, data\n",
      "lineage, field mappings, pipeline workflows, and much more. A data\n",
      "engineer uses technical metadata to create, connect, and monitor various\n",
      "systems across the data engineering lifecycle.\n",
      "Here are some common types of technical metadata that a data engineer\n",
      "will use:\n",
      "Pipeline metadata (often produced in orchestration systems)\n",
      "Data lineage\n",
      "Schema\n",
      "Orchestration is a central hub that coordinates workflow across various\n",
      "systems. Pipeline metadata captured in orchestration systems provides the\n",
      "details of the workflow schedule, system and data dependencies,\n",
      "configurations, connection details, and much more.\n",
      "Data lineage metadata tracks the origin and changes to data, and its\n",
      "dependencies, over time. As data flows through the data engineering\n",
      "lifecycle, it evolves through transformations and combinations with other\n",
      "data. Data lineage provides an audit trail of data’ s evolution as it moves\n",
      "through various systems and workflows.\n",
      "Schema metadata describes the structure of data that is stored in a system\n",
      "such as a database, a data warehouse, a data lake, or a file system; it is one\n",
      "of the key dif ferentiators across dif ferent types of storage systems. Objectstores, for example, don’ t manage schema metadata—instead, this must be\n",
      "managed in a system like the Hive Metastore. On the other hand, cloud data\n",
      "warehouses manage schema metadata for engineers and users.\n",
      "These are just a few examples of technical metadata that a data engineer\n",
      "should know about. This is not a complete list, and we’ll cover additional\n",
      "aspects of technical metadata throughout the book.\n",
      "Operational metadata\n",
      "Operational metadata describes the operational results of various systems\n",
      "and includes statistics about processes, job ids, application runtime logs,\n",
      "data used in a process, error logs, etc. A data engineer uses operational\n",
      "metadata to determine whether a process succeeded or failed and the data\n",
      "involved in the process.\n",
      "Orchestration systems can provide a limited picture of operational\n",
      "metadata, but the latter still tends to be scattered across many systems. A\n",
      "need for better quality operational metadata, and better metadata\n",
      "management, is a major motivation for next-generation orchestration and\n",
      "metadata management systems.\n",
      "Reference metadata\n",
      "Reference metadata is data used to classify other data. This is also referred\n",
      "to as “lookup” data. Standard examples of reference data are internal codes,\n",
      "geographic codes, units of measurement, internal calendar standards. Note\n",
      "that much of reference data is fully managed internally , but items such as\n",
      "geographic codes might come from external standard references. Reference\n",
      "data is essentially a standard for interpreting other data, so if it changes at\n",
      "all, this change happens slowly over time.\n",
      "Security\n",
      "People and or ganizational structure are always the biggest security\n",
      "vulnerabilities in any company . When we hear about major security\n",
      "breaches in the media, it quite often turns out that someone in the company\n",
      "ignored basic precautions, fell victim to a phishing attack, or otherwiseacted in an irresponsible manner . As such, the first line of defense for data\n",
      "security is to create a culture of security that pervades the or ganization. All\n",
      "individuals who have access to data must understand their responsibility in\n",
      "protecting the sensitive data of the company and its customers.\n",
      "Security must be top of mind for data engineers, and those who ignore it do\n",
      "so at their peril. Data engineers must understand both data and access\n",
      "security , at all times exercising the principle of least privilege. The principle\n",
      "of least privilege  means giving a user or system access only to the data and\n",
      "resources that are essential to perform an intended function. A common\n",
      "anti-pattern we see with data engineers with little security experience is to\n",
      "give admin access to all users. Please avoid this! Give users only the access\n",
      "they need to do their jobs today , nothing more. Don’ t operate from a root\n",
      "shell when you’re just looking for files that are visible with standard user\n",
      "access. In a database, don’ t use the superuser role when you’re just\n",
      "querying tables visible with a lesser role. Imposing the principle of least\n",
      "privilege on ourselves can prevent a lot of accidental damage, and also\n",
      "keeps you into a security-first mindset.\n",
      "Data security is also about timing—providing data access to only the people\n",
      "and systems that need to access it, and only for the duration necessary to\n",
      "perform their work . Data should be protected at all times from unwanted\n",
      "visibility—both in flight and at rest—using techniques such as encryption,\n",
      "tokenization, data masking, obfuscation or simple robust access controls.\n",
      "Data engineers must be competent security administrators, as security falls\n",
      "in their domain. A data engineer should understand security best practices,\n",
      "both for cloud and on-prem. Knowledge of user and identity access\n",
      "management (IAM)—roles, policies, groups, network security , password\n",
      "policies, and encryption are good places to start.\n",
      "Throughout the book, we’ll highlight areas where security should be top of\n",
      "mind in the data engineering lifecycle.\n",
      "Data accountability5Data accountability means assigning an individual to be responsible for\n",
      "governing some portion of data. The responsible person then coordinates\n",
      "the governance activities of other stakeholders. Fundamentally , it is\n",
      "extremely dif ficult to manage data quality if no one is accountable for the\n",
      "data in question.\n",
      "Note that people accountable for data need not be data engineers. In fact,\n",
      "the accountable person might be a software engineer , product manager , or\n",
      "serve in another role. In addition, the responsible person generally doesn’ t\n",
      "have all the resources necessary to maintain data quality . Instead, they\n",
      "coordinate with all people who touch the data, including data engineers.\n",
      "Data accountability can happen at a variety of levels; accountability can\n",
      "happen at the level of a table or a log stream but could be as fine-grained as\n",
      "a single field entity that occurs across many tables. An individual may be\n",
      "accountable for managing a customer ID across many dif ferent systems. For\n",
      "the purposes of enterprise data management, a data domain is the set of all\n",
      "possible values that can occur for a given field type, such as in this ID\n",
      "example. This may seem excessively bureaucratic and fastidious, but in\n",
      "fact, can have significant implications for data quality .\n",
      "Data quality\n",
      "Can I trust this data?\n",
      "—Everyone in the business\n",
      "Data quality is the optimization of data toward the desired state, and orbits\n",
      "the question “What do you get compared with what you expect?” Data\n",
      "should conform to the expectations in the business metadata. Does the data\n",
      "match the definition agreed upon by the business?\n",
      "A data engineer is responsible for ensuring data quality across the entire\n",
      "data engineering lifecycle. This involves performing such tasks as data\n",
      "quality tests, ensuring data conformance to schema expectations, data\n",
      "completeness and precision, and much more.\n",
      "According to Data Governance: The Definitive Guide,  data quality is\n",
      "defined by three main characteristics—accuracy , completeness, timeliness:Accuracy\n",
      "Is the collected data factually correct? Are there duplicate values? Are\n",
      "the numeric values accurate?\n",
      "Completeness\n",
      "Are the records complete? Do all required fields contain valid values?\n",
      "Timeliness\n",
      "Are records available in a timely fashion?\n",
      "Each of these characteristics is quite nuanced. For example, when dealing\n",
      "with web event data, how do we think about bots and web scrapers? If we\n",
      "intend to analyze the customer journey , then we must have a process that\n",
      "lets us separate human from machine-generated traf fic. Any bot-generated\n",
      "events misclassified as human  present data accuracy issues, and vice versa.\n",
      "A variety of interesting problems arise concerning completeness and\n",
      "timeliness. In the Google paper introducing the dataflow model (see further\n",
      "reading), the authors give the example of an of fline video platform that\n",
      "displays ads. The platform downloads video and ads while a connection is\n",
      "present, allows the user to watch these while of fline, then uploads ad view\n",
      "data once a connection is present again. This data may arrive late, well after\n",
      "the ads are watched. How does the platform handle billing for the ads?\n",
      "Fundamentally , this is a problem that can’ t be solved by purely technical\n",
      "means. Rather , engineers will need to determine their standards for late-\n",
      "arriving data and enforce these uniformly , possibly with the help of various\n",
      "technology tools.\n",
      "In general, data quality sits across the boundary of human and technology\n",
      "problems. Data engineers need robust processes to collect actionable human\n",
      "feedback on data quality , while also using technology tools to preemptively\n",
      "detect quality issues before downstream users ever see them. W e’ll cover\n",
      "these collection processes in the appropriate chapters throughout this book.MASTER DATA MANAGEMENT (MDM)\n",
      "Master data  is data about business entities such as employees,\n",
      "customers, products, locations, etc. As or ganizations grow lar ger and\n",
      "more complex through or ganic growth and acquisitions, and as they\n",
      "collaborate with other businesses, maintaining a consistent picture of\n",
      "entities and identities becomes more and more challenging. MDM isn’ t\n",
      "always strictly under the purview of data engineers, but they must\n",
      "always be aware of it; if they don’ t own MDM, they will still\n",
      "collaborate on MDM initiatives.\n",
      "Master data management  (MDM) is the practice of building consistent\n",
      "entity definitions known as golden r ecords. Golden records harmonize\n",
      "entity data across an or ganization and with its partners. Master data\n",
      "management is a business operations process that is facilitated by\n",
      "building and deploying technology tools. For example, a master data\n",
      "management team might determine a standard format for addresses,\n",
      "then work with data engineers to build an API to return consistent\n",
      "addresses and a system that uses address data to match customer\n",
      "records across divisions of the company .\n",
      "Master data management reaches across the full data cycle into\n",
      "operational databases. As such, it may fall directly under the purview of\n",
      "data engineering, but is often the assigned responsibility of a dedicated\n",
      "team that works across the or ganization.\n",
      "Data modeling and design\n",
      "To derive business insights from data, through business analytics and data\n",
      "science, the data must be in a usable form. The process for converting data\n",
      "into a usable form is known as data modeling and design. Where we\n",
      "traditionally think of data modeling as a problem for DBAs and ETL\n",
      "developers, data modeling can happen almost anywhere in an or ganization.\n",
      "Firmware engineers develop the data format of a record for an IoT device,or web application developers design the JSON response to an API call or a\n",
      "MySQL table schema -- these are all instances of data modeling and design.\n",
      "Data modeling has become more challenging due to the variety of new data\n",
      "sources and use cases. For instance, strict normalization doesn’ t work well\n",
      "with event data. Fortunately , a new generation of data tools increases the\n",
      "flexibility of data models, while retaining logical separations of measures,\n",
      "dimensions, attributes, and hierarchies. Cloud data warehouses support the\n",
      "ingestion of enormous quantities of denormalized and semistructured data,\n",
      "while still supporting common data modeling patterns, such as Kimball,\n",
      "Inmon, and Data V ault. Data processing frameworks such as Spark can\n",
      "ingest a whole spectrum of data ranging from flat structured relational\n",
      "records to raw unstructured text. W e’ll discuss these transformation patterns\n",
      "in greater detail in Chapter 8.\n",
      "With the vast variety of data that engineers must cope with, there is a\n",
      "temptation to throw up our hands and give up on data modeling. This is a\n",
      "terrible idea with harrowing consequences, made evident when people\n",
      "murmur of the WORN (write once, read never) access pattern or refer to a\n",
      "‘data swamp’. Data engineers need to understand modeling best practices,\n",
      "and also develop the flexibility to apply the appropriate level and type of\n",
      "modeling to the data source and use case.\n",
      "Data lineage\n",
      "As data moves through its lifecycle, how do you know what system af fected\n",
      "the data, or what the data is composed of as it gets passed around and\n",
      "transformed? Data lineage describes the recording of an audit trail of data\n",
      "through its lifecycle, tracking both the systems that process the data, as well\n",
      "as what data it depends upon.\n",
      "Data lineage helps with error tracking, accountability , and debugging of\n",
      "both data and the systems that process it. It has the obvious benefit of\n",
      "giving an audit trail for the data lifecycle but also helps with compliance.\n",
      "For example, if a user would like their data deleted from your systems,\n",
      "having the data lineage of that user ’s data allows you to know where that\n",
      "data is stored and its dependencies.Though data lineage has been around for a long time in lar ger companies\n",
      "with strict compliance standards for tracking its data, it’ s now being more\n",
      "widely adopted in smaller companies as data management becomes\n",
      "mainstream. A data engineer should be aware of data lineage, as it’ s\n",
      "becoming a key piece both for debugging data workflows, as well as tracing\n",
      "data that may need to be removed wherever it resides.\n",
      "Data integration and interoperability\n",
      "Data integration and interoperability is the process of integrating data\n",
      "across tools and processes. As we move away from a single stack approach\n",
      "to analytics towards a heterogeneous cloud environment where a variety of\n",
      "tools process data on demand, integration, and interoperability occupy an\n",
      "ever-widening swath of the data engineer ’s job.\n",
      "Increasingly , integration happens through general-purpose APIs rather than\n",
      "custom database connections. A pipeline might pull data from the\n",
      "Salesforce API, store it to Amazon S3, call the Snowflake API to load it\n",
      "into a table, call the API again to run a query , then export the results to S3\n",
      "again where it can be consumed by Spark.\n",
      "All of this activity can be managed with relatively simple Python code that\n",
      "talks to data systems rather than handling data directly . While the\n",
      "complexity of interacting with data systems has decreased, the number of\n",
      "systems and the complexity of pipelines has increased dramatically .\n",
      "Engineers starting from scratch quickly outgrow the capabilities of bespoke\n",
      "scripting and stumble into the need for orchestration.  Orchestration is one\n",
      "of our undercurrents, and we discuss it in detail below .\n",
      "Data lifecycle management\n",
      "The advent of data lakes encouraged or ganizations to ignore this data\n",
      "archival and destruction. Why discard data when you can simply add more\n",
      "storage and archive it eternally? T wo changes have encouraged engineers to\n",
      "pay more attention to what happens at the end of the data engineering\n",
      "lifecycle.First, data is increasingly stored in the cloud. This means pay-as-you-go\n",
      "storage costs, instead of lar ge block capex expenditures for an on-premises\n",
      "data lake. When every byte shows up on a monthly A WS statement, CFOs\n",
      "see opportunities for savings. Cloud environments make data archival a\n",
      "relatively straightforward process. Major cloud vendors of fer archival-\n",
      "specific object storage classes such as Amazon Glacier and Google Cloud\n",
      "Coldline Storage that allow long-term data retention at an extremely low\n",
      "cost, assuming very infrequent access (it should be noted that data retrieval\n",
      "isn’t so cheap, but that’ s for another conversation). These storage classes\n",
      "also support extra policy controls to prevent accidental or deliberate\n",
      "deletion of critical archives.\n",
      "Second, privacy and data retention laws such as GDPR and CCP A now\n",
      "require data engineers to actively manage data destruction, with\n",
      "consideration to the “right to be for gotten”. Data engineers must be aware\n",
      "of what consumer data they retain and must have procedures in place to\n",
      "destroy data in response to requests and compliance requirements.\n",
      "Data destruction is straightforward in a cloud data warehouse—SQL\n",
      "semantics allow deletion of rows conforming to a where clause. Data\n",
      "destruction was more challenging in data lakes, where write-once, read-\n",
      "many was the default storage pattern. T ools such as Hive Acid and Delta\n",
      "Lake now allow easy management of deletion transactions at scale. New\n",
      "generations of metadata management, data lineage, and cataloging tools\n",
      "will also streamline the end of the data engineering lifecycle.\n",
      "Ethics and privacy\n",
      "Ethical behavior is doing the right thing when no one else is watching.\n",
      "—Aldo Leopold\n",
      "The last several years of data breaches, misinformation, and mishandling of\n",
      "data make one thing clear—data impacts people. Data used to live in the\n",
      "Wild W est, freely collected and traded like baseball cards. Those days are\n",
      "long gone. Whereas the ethical and privacy implications of data were once\n",
      "considered a nice-to-have, like security , they’re now central to the generaldata lifecycle. Data engineers need to do the right thing when no one else is\n",
      "watching because someday , everyone will be watching. W e hope that more\n",
      "organizations will encourage a culture of good data ethics and privacy .\n",
      "How do ethics and privacy impact the data engineering lifecycle? Data\n",
      "engineers need to ensure that datasets mask personally identifiable\n",
      "information (PII) and other sensitive information; bias can be identified and\n",
      "tracked in datasets as they are transformed. Regulatory scrutiny—and\n",
      "penalties for failing to comply—is only growing for data. Make sure your\n",
      "data assets are compliant with a growing number of data regulations, such\n",
      "as GDPR and CCP A. At the time of this writing, the EU is working on a\n",
      "GDPR for AI. W e don’ t expect any slowdown in the popularity of ethics,\n",
      "privacy , or resultant regulation, so please take this seriously .\n",
      "Because ethics and privacy are top of mind, we will have some tips to\n",
      "ensure you’re baking ethics and privacy into the data engineering lifecycle.\n",
      "Orchestration\n",
      "We think that or chestration matters because we view it as r eally the\n",
      "center of gravity of both the data platform as well as the data lifecycle,\n",
      "the softwar e development lifecycle as it comes to data.\n",
      "—Nick Schrock, founder of Elementl\n",
      "While Nick’ s definition of the data lifecycle is somewhat dif ferent from\n",
      "ours, we fundamentally agree with his idea. Not only is orchestration a\n",
      "central DataOps process, but orchestration systems are a critical part of the\n",
      "engineering and deployment flow for data jobs, just as tools like Jenkins are\n",
      "critical to the continuous delivery and deployment of software.\n",
      "So, what is orchestration? Orchestration is the process of coordinating many\n",
      "jobs to run as quickly and ef ficiently as possible on a scheduled cadence.\n",
      "People often mistakenly refer to orchestration tools, like Apache Airflow , as\n",
      "schedulers . This isn’ t quite accurate, and there are key dif ferences. A pure\n",
      "scheduler , such as cron, is aware only of time. An orchestration engine, on\n",
      "the other hand, builds in metadata on job dependencies, generally in the\n",
      "form of a DAG (directed acyclic graph). The DAG can be run once, or bescheduled to run at a fixed interval of daily , weekly , every hour , every five\n",
      "minutes, etc.\n",
      "As we discuss orchestration throughout this book, we assume that an\n",
      "orchestration system stays online with high availability . This allows the\n",
      "orchestration system to sense and monitor constantly without human\n",
      "intervention, and to run new jobs any time they are deployed. An\n",
      "orchestration system monitors jobs that it manages and kicks of f new tasks\n",
      "as internal DAG dependencies are completed. It can also monitor external\n",
      "systems and tools to watch for data to arrive and criteria to be met. When\n",
      "certain conditions go out of bounds, the system also sets error conditions\n",
      "and sends alerts through email or other channels. Commonly , you might set\n",
      "an expected completion time for overnight daily data pipelines, perhaps 10\n",
      "am. If jobs are not done by this time, alerts go out to data engineers and\n",
      "data consumers.\n",
      "Orchestration systems typically also build in job history capabilities,\n",
      "visualization, and alerting. Advanced orchestration engines can backfill\n",
      "new DAGs or individual tasks as they are added to a DAG. They also\n",
      "support dependencies over a time range. For example, a monthly reporting\n",
      "job might check that an ETL job has been completed for the full month\n",
      "before starting.\n",
      "Orchestration has long been a key capability for data processing but was not\n",
      "often top of mind nor accessible to anyone except the lar gest companies.\n",
      "Enterprises used a variety of tools to manage job flows, but these were\n",
      "expensive, out of reach of small startups, and generally not extensible.\n",
      "Apache Oozie was extremely popular in the 2010s, but it was designed to\n",
      "work within a Hadoop cluster and was dif ficult to use in a more\n",
      "heterogeneous environment. Facebook developed Data Swarm for internal\n",
      "use in the late 2000s; this inspired popular tools such as Airflow , which\n",
      "Airbnb open-sourced in 2014.\n",
      "Airflow was open source from its inception, a key strength to its strong\n",
      "adoption. The fact that it was written in Python made it highly extensible to\n",
      "almost any use case imaginable. While there are many other interestingopen-source orchestration projects such as Luigi and Conductor , Airflow is\n",
      "arguably the mindshare leader for the time being. Airflow arrived just as\n",
      "data processing was becoming more abstract and accessible, and engineers\n",
      "were increasingly interested in coordinating complex flows across multiple\n",
      "processors and storage systems, especially in cloud environments.\n",
      "At the time of this writing, several nascent open-source projects aim to\n",
      "mimic the best elements of Airflow’ s core design, while improving on it in\n",
      "key areas. Some of the most interesting examples at the moment are Prefect\n",
      "and Dagster , which aim to improve the portability and testability of DAGs\n",
      "to allow engineers to more easily move from local development to\n",
      "production. There’ s also Ar go, which is built around Kubernetes primitives,\n",
      "and Metaflow , an open-source project out of Netflix that aims to improve\n",
      "data science orchestration. Which framework ultimately wins the mindshare\n",
      "for orchestration is still to be determined.\n",
      "We also want to mention one interesting non-open source orchestration\n",
      "platform, Datacoral. Datacoral has introduced the idea of auto-orchestration\n",
      "through data definitions. That is, one can define a table in Datacoral by\n",
      "using a SQL statement; the orchestrator analyzes the source tables in the\n",
      "query to determine upstream dependencies, then triggers a table refresh as\n",
      "these dependencies are met. W e believe that auto-orchestration will become\n",
      "a standard feature of orchestration engines in the future.\n",
      "We also point out that orchestration is strictly a batch concept. The\n",
      "streaming alternative to orchestrated task DAGs is the streaming DAG.\n",
      "Streaming DAGs remain challenging to build and maintain, but next-\n",
      "generation streaming platforms such as Pulsar aim to dramatically reduce\n",
      "the engineering and operational burden. W e will talk more about these\n",
      "developments in the chapter on transformation.\n",
      "DataOps\n",
      "DataOps maps the best practices of Agile methodology , DevOps, and\n",
      "statistical process control (SPC) to data. Whereas DevOps aims to improve\n",
      "the release and quality of software products, DataOps does the same thingfor data products. Data products dif fer from software products because of\n",
      "how data is used. A software product provides specific functionality and\n",
      "technical features for end-users. By contrast, a data product is built around\n",
      "sound business logic and metrics, whose users make decisions or build\n",
      "models that perform automated actions. A data engineer must understand\n",
      "both the technical aspects of building software products to provide good\n",
      "user experiences to downstream users, as well as the business logic, quality ,\n",
      "and metrics that will create excellent data products not just for immediate\n",
      "downstream users, but for users across the business.\n",
      "Like DevOps, DataOps borrows much from lean manufacturing and supply\n",
      "chain, mixing people, processes, and technology to reduce time to value.\n",
      "As Data Kitchen (the experts in DataOps) describes it, “DataOps is a\n",
      "collection of technical practices, workflows, cultural norms, and\n",
      "architectural patterns that enable:\n",
      "Rapid innovation and experimentation delivering new insights to\n",
      "customers with increasing velocity\n",
      "Extremely high data quality and very low error rates\n",
      "Collaboration across complex arrays of people, technology , and\n",
      "environments\n",
      "Clear measurement, monitoring, and transparency of results”\n",
      "Lean practices (such as lead time reduction, minimizing defects), and the\n",
      "resulting improvements to quality , and productivity are things we are glad\n",
      "to see gaining momentum both in software and data operations.\n",
      "First and foremost, DataOps are a cultural habit, and the data engineering\n",
      "team needs to adopt a cycle of communicating and collaborating with the\n",
      "business, breaking down silos, continuously learning from successes and\n",
      "mistakes, and rapid iteration. Only when these cultural habits are set in\n",
      "place can the team get the best results from technology and tools. The\n",
      "reverse is rarely true.6Depending on a company’ s data maturity , a data engineer has some options\n",
      "to build DataOps into the fabric of the overall data engineering lifecycle. If\n",
      "the company has no pre-existing data infrastructure or practices, DataOps is\n",
      "very much a greenfield opportunity that can be baked in from day one. W ith\n",
      "an existing project or infrastructure that lacks DataOps, a data engineer can\n",
      "begin adding DataOps into workflows. W e suggest first starting with\n",
      "observability and monitoring to get a window into the performance of a\n",
      "system, then adding in automation and incident response. In a data-mature\n",
      "company , a data engineer may work alongside an existing DataOps team to\n",
      "improve the data engineering lifecycle. In all cases, a data engineer must be\n",
      "aware of the philosophy and technical aspects of DataOps.\n",
      "There are three core technical elements to DataOps—automation,\n",
      "monitoring and observability , and incident response (see Figure 2-8 ). Let’ s\n",
      "look at each of these pieces, and how they relate to the data engineering\n",
      "lifecycle.\n",
      "Figur e 2-8. The thr ee pillars of DataOps\n",
      "Automation\n",
      "Automation enables reliability and consistency in the DataOps process and\n",
      "allows data engineers to quickly deploy new product features and\n",
      "functionality , as well as changes to existing parts of the workflow . DataOps\n",
      "automation has a similar framework and workflow to DevOps, consisting of\n",
      "change management (environment, code, and data version control),\n",
      "continuous integration/continuous deployment (CI/CD), and configuration\n",
      "as code (“X as code”). Like DevOps, DataOps practices monitor and\n",
      "maintain the reliability of technology and systems (data pipelines,orchestration, etc.), with the added dimension of checking for data quality ,\n",
      "data/model drift, metadata integrity , and much more.\n",
      "Let’s briefly discuss the evolution of DataOps automation within a\n",
      "hypothetical or ganization. An or ganization with a low level of DataOps\n",
      "maturity often attempts to schedule data processes using cron jobs. This\n",
      "works well for a while. As data pipelines become more complicated, several\n",
      "things are likely to happen. If the cron jobs are hosted on a cloud instance,\n",
      "the instance may have an operational problem, causing the jobs to stop\n",
      "running unexpectedly . As the spacing between jobs becomes tighter , a job\n",
      "will eventually run long, causing a subsequent job to fail or produce out-of-\n",
      "date data. Engineers may not be aware of job failures until they hear from\n",
      "analysts that their reports are out of date.\n",
      "As the or ganization’ s data maturity grows, data engineers will typically\n",
      "adopt an orchestration framework, perhaps Airflow or Dagster . Data\n",
      "engineers are aware that Airflow itself presents an operational burden, but\n",
      "this eventually outweighs the operational dif ficulties of cron jobs. Engineers\n",
      "will gradually migrate their cron jobs to Airflow jobs. Now , dependencies\n",
      "are checked before jobs run. More jobs can be packed into a given time\n",
      "because each job can start as soon as upstream data is ready rather than at a\n",
      "fixed, predetermined time.\n",
      "The data engineering team still has room for operational improvements. For\n",
      "instance, eventually , a data scientist deploys a broken DAG, bringing down\n",
      "the Airflow webserver and leaving the data team operationally blind. After\n",
      "enough such headaches, the data engineering team realizes that they need to\n",
      "stop allowing manual DAG deployments. In their next phase of operational\n",
      "maturity , they adopt automated DAG deployment. DAGs are tested before\n",
      "deployment, and monitoring processes ensure that the new DAGs start\n",
      "running properly . In addition, data engineers block the deployment of new\n",
      "Python dependencies until installation is validated. After automation is\n",
      "adopted, the data team is much happier and experiences far fewer\n",
      "headaches.One of the tenets of the DataOps Manifesto is “Embrace Change.” This\n",
      "does not mean change for the sake of change, but goal-oriented change. At\n",
      "each stage of our automation journey , there are opportunities for operational\n",
      "improvement. Even at the high level of maturity that we’ve described here,\n",
      "there is further room for improvement. Engineers might choose to embrace\n",
      "a next-generation orchestration framework that builds in better metadata\n",
      "capabilities. Or they might try to develop a framework that builds DAGs\n",
      "automatically based on data lineage specifications. The main point is that\n",
      "engineers constantly seek to implement improvements in automation that\n",
      "will reduce their workload and increase the value that they deliver to the\n",
      "business.\n",
      "Observability and monitoring\n",
      "As we tell our clients, “data is a silent killer”. W e’ve seen countless\n",
      "examples of bad data lingering in reports for months or years. Executives\n",
      "may make key decisions from this bad data, only to discover a substantial\n",
      "time later that the data was wrong. The outcomes are usually bad, and\n",
      "sometimes catastrophic for the business. Initiatives are undermined and\n",
      "destroyed, years of work wasted. In some of the worst cases, companies\n",
      "may be led to financial or technical disaster by bad data.\n",
      "Another horror story is when the systems that create the data for reports\n",
      "randomly stop working, resulting in reports being delayed by several days.\n",
      "The data team doesn’ t know until they’re asked by stakeholders why reports\n",
      "are late or producing stale information. Eventually , different stakeholders\n",
      "lose trust in the capabilities of the data system and start their quasi-data\n",
      "team. The result is a ton of dif ferent unstable systems, reports, and silos.\n",
      "If you’re not observing and monitoring your data, and the systems that\n",
      "produce the data, you’re inevitably going to experience your own data\n",
      "horror story . Observability , monitoring, logging, alerting, tracing are all\n",
      "critical to getting ahead of any problems that will occur along the data\n",
      "engineering lifecycle. W e recommend you incorporate statistical process\n",
      "control (SPC) to understand whether events being monitored are out of line,\n",
      "and which incidents are worth responding to.We’ll cover many aspects of monitoring and observability throughout the\n",
      "data engineering lifecycle in later chapters.\n",
      "Incident response\n",
      "A high-functioning data team using DataOps will be able to quickly ship\n",
      "new data products. But mistakes will inevitably happen. A system may have\n",
      "downtime, a new data model may break downstream reports, a machine\n",
      "learning model may become stale and provide bad predictions, and\n",
      "countless other things may happen that interrupt the data engineering\n",
      "lifecycle. Incident response is about using the automation and observability\n",
      "capabilities mentioned above to rapidly identify root causes of an incident,\n",
      "and resolve the incident as reliably and quickly as possible.\n",
      "Incident response isn’ t just about technology and tools, though they are\n",
      "immensely useful. It’ s also about open and blameless communication, both\n",
      "on the data engineering team, and across the or ganization. As W erner\n",
      "Vogels is famous for saying, “Everything breaks all the time.” Data\n",
      "engineers must be prepared for disaster , and ready to respond as swiftly and\n",
      "efficiently as possible. Even better , data engineers should proactively find\n",
      "issues before the business reports them. T rust takes a long time to build and\n",
      "can be lost in minutes. In the end, incident response is as much about\n",
      "retroactively responding to incidents as proactively addressing them before\n",
      "they happen.\n",
      "DataOps summary\n",
      "At this point, DataOps is still a work in progress. Even so, practitioners\n",
      "have done a good job of adapting the principles of DevOps to the data\n",
      "domain and mapping out an initial vision through the DataOps Manifesto\n",
      "and other resources. Data engineers would do well to implement DataOps\n",
      "practices a high priority in all of their work. The upfront ef fort will see a\n",
      "significant long-term payof f through faster delivery of products, better\n",
      "reliability and accuracy of data, and greater overall value for the business.\n",
      "The state of operations in data engineering is still quite immature compared\n",
      "with software engineering. Many data engineering tools, especially legacymonoliths, are not automation-first. That said, there’ s a recent movement to\n",
      "adopt automation best practices across the data engineering lifecycle. T ools\n",
      "like Airflow have paved the way for a new generation of automation and\n",
      "data management tools. Because the landscape is changing so fast, we’ll\n",
      "take a wait-and-see approach before we suggest specific tools. The general\n",
      "practices we describe for DataOps are aspirational, and we suggest\n",
      "companies try to adopt them to the fullest extent possible, given the tools\n",
      "and knowledge available today .\n",
      "Data Architecture\n",
      "A data architecture reflects the current and future state of data systems that\n",
      "support the long-term data needs and strategy of an or ganization. Because\n",
      "the data requirements of an or ganization will likely change very rapidly , and\n",
      "new tools and practices seem to arrive on a near -daily basis, data engineers\n",
      "must understand good data architecture. W e will cover data architecture in-\n",
      "depth in Chapter 3, but we want to highlight how data architecture is an\n",
      "undercurrent of the data engineering lifecycle.\n",
      "A data engineer should first understand the needs of the business and gather\n",
      "requirements for new use cases. Next, a data engineer needs to translate\n",
      "those requirements to design new ways to capture and serve data, balanced\n",
      "for cost and operational simplicity . This means knowing the tradeof fs with\n",
      "design patterns, technologies, and tools in the areas of source systems,\n",
      "ingestion, storage, transformation, and serving data.\n",
      "This doesn’ t imply that a data engineer is a data architect, as these are\n",
      "typically two separate roles. If a data engineer works alongside a data\n",
      "architect, the data engineer should be able to deliver on the data architect’ s\n",
      "designs, as well as provide architectural feedback. W ithout a dedicated data\n",
      "architect, however , a data engineer will realistically be entrusted to be the\n",
      "default data architect. W e will deep dive into “good” data architecture in\n",
      "Chapter 3.\n",
      "Software EngineeringSoftware engineering has always been a central skill for data engineers. In\n",
      "the early days of contemporary data engineering (2000–2010), data\n",
      "engineers worked on low-level frameworks and wrote map-reduce jobs in\n",
      "C, C++, and Java. At the peak of the big data era (the mid-2010s), engineers\n",
      "started using frameworks that abstracted away these low-level details.\n",
      "This abstraction continues today . Data warehouses support powerful\n",
      "transformations using SQL semantics. T ools like Spark have become more\n",
      "user-friendly over time, transitioning away from RDD-based coding\n",
      "towards easy-to-use dataframes. Despite this abstraction, software\n",
      "engineering is still a central theme of modern data engineering. W e want to\n",
      "briefly discuss a few common areas of software engineering that apply to\n",
      "the data engineering lifecycle.\n",
      "Core data processing code\n",
      "Though it has become more abstract and easier to manage, core data\n",
      "processing code still needs to be written and it appears throughout the data\n",
      "engineering lifecycle. Whether in ingestion, transformation, or data serving,\n",
      "data engineers need to be highly proficient and productive in frameworks\n",
      "and languages such as Spark, SQL, or Beam; we reject the notion that SQL\n",
      "is not code.\n",
      "It’s also imperative that a data engineer understand proper code testing\n",
      "methodologies, such as unit, regression, integration, end-to-end, smoke\n",
      "testing, and others. Especially with the growing number of tools and\n",
      "modularization, and the automation involved with DataOps, data engineers\n",
      "need to ensure their systems work end-to-end.\n",
      "Development of open source frameworks\n",
      "Many data engineers are heavily involved in the development of open-\n",
      "source frameworks. They adopt these frameworks to solve specific\n",
      "problems in the data engineering lifecycle, then continue to develop the\n",
      "framework code to improve the tools for their use cases and to make a\n",
      "contribution back to the community .In the big data era, we saw a Cambrian explosion of data processing\n",
      "frameworks inside of the Hadoop ecosystem. These tools were primarily\n",
      "focused on the transformation and serving parts of the data engineering\n",
      "lifecycle. Data engineering tool speciation has not ceased or even slowed\n",
      "down, but the emphasis has shifted up the ladder of abstraction, away from\n",
      "direct data processing. This new generation of open source tools assists\n",
      "engineers in managing, enhancing, connecting, optimizing, and monitoring\n",
      "data.\n",
      "For example, Apache Airflow dominated the orchestration space from 2015\n",
      "on. Now , a new batch of open source competitors (Prefect, Dagster ,\n",
      "Metaflow , etc) has sprung up to fix perceived limitations of Airflow ,\n",
      "providing better metadata handling, portability , dependency management,\n",
      "etc.\n",
      "Before data engineers begin engineering new internal tools, they would do\n",
      "well to survey the landscape of projects. Keep an eye on the total cost of\n",
      "ownership (TCO) and opportunity cost associated with implementing a tool.\n",
      "There is a good chance that some open source project already exists to\n",
      "address the problem that they’re looking to solve, and they would do well to\n",
      "collaborate on an existing project rather than reinventing the wheel.\n",
      "Streaming\n",
      "Streaming data processing is inherently more complicated than batch, and\n",
      "the tools and paradigms are ar guably less mature. As streaming data\n",
      "becomes more pervasive in every stage of the data engineering lifecycle,\n",
      "data engineers face a host of interesting software engineering problems.\n",
      "For instance, data processing tasks such as joins that we take for granted in\n",
      "the batch processing world often become more complicated in a real-time\n",
      "environment, and more complex software engineering is required.\n",
      "Engineers must also write code to apply a variety of windowing  methods.\n",
      "Windowing allows real-time systems to calculate useful metrics such as\n",
      "trailing statistics; there are a variety of techniques for breaking data into\n",
      "windows, all with subtly dif ferent nuances. Engineers have many\n",
      "frameworks to choose from, including various function platforms(OpenFaaS, A WS Lambda, Google Cloud Functions) for handling\n",
      "individual events or dedicated stream processors (Apache Spark, Beam, or\n",
      "Pulsar) for analyzing streams for reporting and real-time actions.\n",
      "Infrastructure as code\n",
      "Infrastructur e as code  (IaC) applies software engineering practices to the\n",
      "configuration and management of infrastructure. The infrastructure\n",
      "management burden of the big data era has decreased precipitously as\n",
      "companies have migrated to managed big data systems (Databricks, EMR)\n",
      "and cloud data warehouses. When data engineers have to manage their\n",
      "infrastructure in a cloud environment, they increasingly do this through IaC\n",
      "frameworks rather than manually spinning up instances and installing\n",
      "software. Several general-purpose (T erraform, Ansible...) and cloud\n",
      "platform-specific (A WS Cloudformation, Google Cloud Deployment\n",
      "Manager ...) frameworks allow automated deployment of infrastructure\n",
      "based on a set of specifications. Many of these frameworks can manage\n",
      "services as well as infrastructure. For example, we can use CloudFormation\n",
      "to spin up Amazon Redshift, or Deployment Manager to start Google Cloud\n",
      "Composer (managed Airflow). There is also a notion of infrastructure as\n",
      "code with containers and Kubernetes, using tools like Helm.\n",
      "These practices are a key part of DevOps, allowing version control and\n",
      "repeatability of deployments. Naturally , these capabilities are extremely\n",
      "valuable throughout the data engineering lifecycle, especially as we adopt\n",
      "DataOps practices.\n",
      "Pipelines as code\n",
      "Pipelines as code  are the core concept of modern orchestration systems,\n",
      "which touch every stage of the data engineering lifecycle. Data engineers\n",
      "use code (typically Python) to declare data tasks and dependencies between\n",
      "them. The orchestration engine interprets these instructions to run steps\n",
      "using available resources.\n",
      "General purpose problem solvingIn practice, regardless of which high-level tools they adopt, data engineers\n",
      "will run into corner cases throughout the data engineering lifecycle that\n",
      "requires them to solve problems outside the boundaries of their chosen\n",
      "tools, and to write custom code. Even when using frameworks like Fivetran,\n",
      "Airbyte, or Singer , data engineers will encounter data sources without\n",
      "existing connectors and need to write something custom. They should be\n",
      "proficient enough in software engineering to understand APIs, pull and\n",
      "transform data, handle exceptions, etc.\n",
      "Conclusion\n",
      "Most discussions we’ve seen in the past about data engineering involve\n",
      "technologies but miss the bigger picture of the data lifecycle management.\n",
      "As technologies become more abstract and do more heavy lifting, a data\n",
      "engineer has the opportunity to think and act on a higher level. The data\n",
      "engineering lifecycle—supported by its undercurrents—is an extremely\n",
      "useful mental model for or ganizing the work of modern data engineering.\n",
      "We break the data engineering lifecycle into the following stages:\n",
      "Generation: source systems\n",
      "Ingestion\n",
      "Storage\n",
      "Transformation\n",
      "Serving\n",
      "Several themes cut across the data engineering lifecycle, as well. These are\n",
      "the under currents of the data engineering lifecycle. At a high level, the\n",
      "undercurrents are:\n",
      "Data management\n",
      "DataOpsData architecture\n",
      "Software engineering\n",
      "Orchestration\n",
      "A data engineer has several top-level goals across the data lifecycle—\n",
      "produce optimum ROI and reduce costs (financial and opportunity), reduce\n",
      "risk (security , data quality) and maximize data value and utility . In the next\n",
      "chapter , we’ll discuss how these elements impact the design of good\n",
      "architecture. Subsequently , we’ve devoted a chapter to each of the stages of\n",
      "the data engineering lifecycle.\n",
      "1 Evren Eryurek, Uri Gilad, V alliappa Lakshmanan, Anita Kibunguchy-Grant, and Jessi\n",
      "Ashdown,  Data Governance: The Definitive Guide (O’Reilly),\n",
      "https://learning.oreilly .com/library/view/data-governance-the/9781492063483/ .\n",
      "2 Eryurek et al.\n",
      "3 Eryurek et al.\n",
      "4 https://medium.com/airbnb-engineering/democratizing-data-at-airbnb-852d76c51770\n",
      "5 https://en.wikipedia.or g/wiki/Principle_of_least_privilege\n",
      "6 https://datakitchen.io/what-is-dataops/Chapter 3. Choosing\n",
      "Technologies Across the Data\n",
      "Engineering Lifecycle\n",
      "A NOTE FOR EARLY RELEASE READERS\n",
      "With Early Release ebooks, you get books in their earliest form—the\n",
      "authors’ raw and unedited content as they write—so you can take\n",
      "advantage of these technologies long before the of ficial release of these\n",
      "titles.\n",
      "This will be the fourth chapter of the final book.\n",
      "If you have comments about how we might improve the content and/or\n",
      "examples in this book, or if you notice missing material within this\n",
      "chapter , please reach out to the authors at\n",
      "book_feedback@ternarydata.com .\n",
      "Modern data engineering suf fers from an embarrassment of riches. Data\n",
      "technologies to solve nearly any problem are available as turnkey of ferings\n",
      "consumable in almost every way—open-source, managed open source,\n",
      "proprietary software, proprietary service, etc. However , it’s easy to get\n",
      "caught up in chasing bleeding edge technology while losing sight of the\n",
      "core purpose of data engineering—designing robust and reliable systems to\n",
      "carry data through the full life cycle and serve it according to the needs of\n",
      "end-users. Just as structural engineers carefully choose technologies and\n",
      "materials to realize an architect’ s vision for a building, data engineers are\n",
      "tasked with making appropriate technology choices to shepherd data\n",
      "through the lifecycle to best serve data applications and users.In the last chapter , we discussed “good” data architecture, and why it\n",
      "matters. W e now explain how to choose the right technologies to serve this\n",
      "architecture. Data engineers must choose “good” technologies to make the\n",
      "best possible data product. W e feel the criteria to choose a “good” data\n",
      "technology is simple: does it add value to a data pr oduct and the br oader\n",
      "business?\n",
      "A lot of people confuse architecture and tools. Architectur e is strategic;\n",
      "tools ar e tactical.  We sometimes hear , “our data architecture are tools X, Y ,\n",
      "and Z”. This is the wrong way to think about architecture. Architecture is\n",
      "the top-level design, roadmap, and blueprint of data systems that satisfy the\n",
      "strategic aims for the business. Architecture is the “What”, “Why” and\n",
      "“When”. T ools are used to make the architecture a reality; tools are the\n",
      "“How”.\n",
      "We often see teams going “of f the rails” and choosing technologies before\n",
      "mapping out an architecture. The reasons for this are varied—shiny object\n",
      "syndrome, resume-driven development, a lack of expertise in architecture—\n",
      "but in practice, this prioritization of technology often means that they don’ t\n",
      "choose architecture, but instead cobble together a kind of Rube Goldber g\n",
      "machine of technologies. This is exactly backward. W e strongly advise\n",
      "against choosing technology before getting your architecture right.\n",
      "Architectur e first. T echnology second.\n",
      "This chapter discusses our tactical plan for making technology choices once\n",
      "we have an architecture blueprint in hand. Below are some considerations\n",
      "for choosing data technologies across the data engineering lifecycle:\n",
      "Cost optimization and adding value to the business\n",
      "Friction to deliver\n",
      "Team ability and maturity\n",
      "Speed to market\n",
      "Today versus the future: immutable versus transitory technologies\n",
      "Location: cloud, on-premises, hybrid-cloud, multi-cloudBuild versus buy\n",
      "Monolith versus modular\n",
      "Serverless versus servers\n",
      "The undercurrents of the data engineering lifecycle, and how they\n",
      "impact choosing technologies\n",
      "Cost Optimization: Total Cost of Ownership\n",
      "and Opportunity Cost\n",
      "In a perfect world, we’d get to experiment with all of the latest, coolest\n",
      "technologies with no consideration of cost, time investment, or value added\n",
      "for the business. In reality , budgets and time are finite, and cost is a major\n",
      "constraint for choosing the right data architectures and technologies. Y our\n",
      "organization expects a positive return on investment (ROI) from your data\n",
      "projects, so it’ s critical that you understand the basic costs you can control.\n",
      "Technology is a major cost driver , so your technology choices and\n",
      "management strategies will have a significant impact on your budget. W e\n",
      "look at costs through two main lenses—total cost of ownership and\n",
      "opportunity cost.\n",
      "Total Cost of Ownership\n",
      "Total cost of ownership (TCO)  is the total estimated cost of an initiative,\n",
      "including the direct and indirect costs of products and services utilized.\n",
      "Direct costs are the costs you can directly attribute to an initiative, such as\n",
      "the salaries of a team working on the initiative, or the A WS bill for all\n",
      "services consumed. Indirect costs, also known as overhead, are independent\n",
      "of the initiative and must be paid regardless of where they’re attributed.\n",
      "Apart from direct and indirect costs, how something is purchased  has a big\n",
      "impact on a budget’ s bottom line. These expenses fall into two big groups—\n",
      "capital expense and operational expense.Capital expenses , also known as capex, require an upfront investment.\n",
      "Payment is required today . Before the cloud, companies would typically\n",
      "purchase hardware and software upfront through lar ge acquisition contracts,\n",
      "and house this infrastructure in huge server rooms, or pay to house their\n",
      "servers in a colocation facility . These upfront investments in hardware and\n",
      "software—commonly in the hundreds of thousands to millions of dollars, or\n",
      "higher—would be treated as assets, and slowly depreciate over time. From a\n",
      "budget perspective, capital would need to be available to pay for the entire\n",
      "purchase upfront or funding secured through a loan vehicle. In addition,\n",
      "there’ s the extra cost of maintaining the hardware and software over the\n",
      "lifetime of those assets. Capex is generally reserved for long-term\n",
      "investments, where there’ s a well-thought-out plan for achieving a positive\n",
      "ROI on the ef fort and expense put forth.\n",
      "Operational expenses , known as opex, are almost the opposite of capex.\n",
      "Whereas capex is long-term focused, opex is short-term. Opex can be pay-\n",
      "as-you-go, or similar , and allows a lot of flexibility . Opex has the added\n",
      "benefit of potentially being closer to a direct cost, making it easier to\n",
      "attribute to a data project.\n",
      "Until recently , opex simply wasn’ t an option for lar ge data projects—data\n",
      "warehouse systems required multimillion dollar contracts. This has all\n",
      "changed with the advent of the cloud, where data platform services allow\n",
      "engineers to pay on a consumption based model. In general, we find that\n",
      "opex allows for a far greater ability for engineering teams to choose both\n",
      "their software and hardware. Cloud based services allow data engineers to\n",
      "iterate quickly with dif ferent software and technology configurations, often\n",
      "very inexpensively .\n",
      "Data engineers need to be pragmatic about flexibility . The data landscape is\n",
      "changing too quickly to invest in long-term hardware that inevitably goes\n",
      "stale, can’ t easily scale, and potentially hampers a data engineer ’s flexibility\n",
      "to try new things. Given the upside for flexibility and low initial costs, we\n",
      "urge data engineers to take an opex-first appr oach center ed on the cloud\n",
      "and flexible technologies.Opportunity Cost\n",
      "Data engineers often fail to evaluate opportunity cost when they undertake a\n",
      "new project. In our opinion, this is a massive blind spot. Opportunity cost  is\n",
      "the cost of choosing one thing at the expense of something else. As it relates\n",
      "to data engineering, if you choose Data Stack A, that means you’ve chosen\n",
      "the benefits of Data Stack A over all other options, ef fectively excluding\n",
      "Data Stack B through Z. Y ou’re now committed to Data Stack A, and\n",
      "everything it entails—the team to support it, training, setup, maintenance,\n",
      "and so forth. But in an incredibly fast-moving field like data, what happens\n",
      "when Data Stack A becomes obsolete? Can you still move to Data Stack B?\n",
      "A big question in a world where newer and better technologies arrive on the\n",
      "scene at an ever -faster rate—how quickly and cheaply can you move to\n",
      "something newer and better? Does the expertise you’ve built up on Data\n",
      "Stack A translate to the next wave? Or are you able to swap out components\n",
      "of Data Stack A and buy yourself some time and options?\n",
      "The first step to minimizing opportunity cost is evaluating it with eyes wide\n",
      "open. W e’ve seen countless data teams get stuck with technologies that,\n",
      "while they seemed good at the time, are either not flexible for future\n",
      "growth, or are simply obsolete. Inflexible data technologies ar e a lot like\n",
      "bear traps. They’r e easy to get into and extr emely painful to escape.  Don’ t\n",
      "let this happen to you.\n",
      "Today Versus the Future: Immutable Versus\n",
      "Transitory Technologies\n",
      "Where are you today? What are your goals for the future? Y our answers to\n",
      "these questions should inform the decisions you make about your\n",
      "architecture, and thus the technologies used within that architecture. T oo\n",
      "often, we see data engineers and architects scoping and building for “the\n",
      "future” whose date and specific needs are not well defined. These intentions\n",
      "are noble but often lead to over -architecting and over -engineering. It often\n",
      "happens that tooling chosen for the futur e is stale and out of date when thisfuture arrives; the future frequently looks little like what we envisioned\n",
      "years before. As many life coaches would tell you, focus on the present;\n",
      "choose the best technology for the moment and near future, but in a way\n",
      "that supports future unknowns and evolution. This is done by understanding\n",
      "what is likely to change, and what tends to stay the same.\n",
      "There are two classes of tools to consider—immutable and transitory .\n",
      "Immutable technologies  might be components that underpin the cloud, or\n",
      "languages and paradigms that have stood the test of time. In the cloud,\n",
      "examples of immutable technologies are object storage, networking,\n",
      "servers, and security . Object storage such as A WS S3 and Azure Blob\n",
      "Storage will be around from today until the end of the decade, and probably\n",
      "much longer . Storing your data in object storage is a wise choice. Object\n",
      "storage continues to improve in various ways, and constantly of fers new\n",
      "options, but your data will be safe and usable in object storage for the\n",
      "foreseeable future.\n",
      "For languages, SQL and Bash have been around for many decades, and we\n",
      "don’t see them disappearing anytime soon. Immutable technologies benefit\n",
      "from the Lindy Ef fect, which says the longer a technology has been\n",
      "established, the longer it will continue to be used. Think, for example, of\n",
      "relational databases, C , or the x86 processor architecture. W e suggest\n",
      "applying the Lindy Ef fect as a litmus test to determine if a technology has\n",
      "immutable potential.\n",
      "Transitory technologies  are those that come and go. The typical trajectory\n",
      "begins with a lot of hype, followed by meteoric growth in popularity , then a\n",
      "slow descent into obscurity . The Javascript frontend landscape is a classic\n",
      "example of this. How many Javascript frontend frameworks have come and\n",
      "gone between 2010 and 2020? Backbone, Ember and Knockout were\n",
      "popular in the early 2010’ s, AngularJS in the mid 2010’ s, and React and\n",
      "Vue have massive mindshare today . What’ s the popular frontend framework\n",
      "3 years from now? Who knows.\n",
      "On the data front, new well-funded entrants and open source projects arrive\n",
      "on the scene every day . Every vendor will say their product is going to1change the industry and “make the world a better place.”  Sadly , the vast\n",
      "majority of these companies and projects won’ t ultimately get traction, and\n",
      "will fade into obscurity . Top VC’ s are making big-money bets, knowing that\n",
      "most of their data tooling investments will fail. How can you possibly know\n",
      "what technologies to invest in for your data architecture? It’ s hard. Just\n",
      "consider the number of technologies in Matt T urck’ s (in)famous depictions\n",
      "of the machine learning, AI, and data (MAD) landscape  (Figure 3-1 ).2\n",
      "3Figur e 3-1. Matt T urck’s MAD data landscape\n",
      "Even relatively successful technologies often fade into obscurity quickly\n",
      "after a few years of rapid adoption, a victim of their success. For instance,\n",
      "Hive was met with rapid uptake because it allowed both analysts and\n",
      "engineers to query massive data sets without coding complex MapReduce\n",
      "jobs by hand. Inspired by the success of Hive, but wishing to improve on its\n",
      "shortcomings, engineers developed SparkSQL, Presto, and other\n",
      "technologies. Hive now appears primarily in legacy deployments. This is\n",
      "the general cycle of data technologies—incumbents reign for a time, then\n",
      "get disrupted and replaced by new technologies that also reign for a time,\n",
      "and so on.\n",
      "Our Advice\n",
      "Given the rapid pace of tooling and best practice changes, we suggest\n",
      "evaluating tools on a two-year basis ( Figure 3-2 ). Whenever possible, find\n",
      "the immutable technologies along the data engineering lifecycle, and use\n",
      "those as your base. Build transitory tools around the immutables.\n",
      "Figur e 3-2. Use a two-year time horizon to r eevaluate your technology choicesLocation: On-Premises, Cloud, Hybrid, Multi-\n",
      "Cloud, and More\n",
      "Companies now have numerous options when they decide where to run\n",
      "their technology stacks. A slow shift toward the cloud in the last decade\n",
      "now culminates in a veritable stampede of companies spinning up\n",
      "workloads on A WS, Azure, and GCP . Many CT Os now view their decisions\n",
      "around technology hosting as having existential significance for their\n",
      "organizations. If they move too slowly , they risk being left behind by their\n",
      "more nimble competition; on the other hand, a poorly planned cloud\n",
      "migration could lead to poorly functioning technology and catastrophic\n",
      "costs. Let’ s look at the main places to run your technology stack—on-\n",
      "premises, the cloud, hybrid-cloud, and multi-cloud.\n",
      "On-premises\n",
      "While new startups are increasingly born in the cloud, on-premises systems\n",
      "are still very much the default for established companies. Essentially , this\n",
      "means that they own their own hardware, which may live in data centers\n",
      "that they own, or in leased colocation space. In either case, companies are\n",
      "operationally responsible for their hardware and the software that runs on it.\n",
      "As hardware fails, they have to repair or replace it. They also have to\n",
      "manage upgrade cycles every few years as new , updated hardware is\n",
      "released and older hardware ages and becomes less reliable. They also must\n",
      "ensure that they have enough hardware to handle peaks; for an online\n",
      "retailer , this means hosting enough capacity to handle the load spikes of\n",
      "Black Friday . For data engineers in char ge of on-premises systems, this\n",
      "means buying lar ge enough systems to allow reasonable performance for\n",
      "peak load and lar ge jobs without overbuying and spending excessively .\n",
      "On the one hand, established companies have established operational\n",
      "practices that have served them well for some time. If a company that relies\n",
      "on information technology has been in business for some time, this means\n",
      "that they have managed to juggle the cost and personnel requirements ofrunning their own hardware, managing software environments, deploying\n",
      "code from dev teams, running databases, and big data systems, etc.\n",
      "On the other hand, established companies see their younger , more nimble\n",
      "competition scaling rapidly and taking advantage of cloud-managed\n",
      "services. They also see established competitors making forays into the\n",
      "cloud, allowing them to temporarily scale up to huge clusters for massive\n",
      "data jobs, or turn on temporary server capacity for Black Friday .\n",
      "Companies in competitive sectors generally don’ t have the option to stand\n",
      "still. Competition is fierce, and there’ s always the threat of being\n",
      "“disrupted” by more agile competition, often backed by significant venture\n",
      "capital dollars. Plus, many lar ge incumbent companies are also modernizing\n",
      "and moving their workloads to the cloud. Every company must keep its\n",
      "existing systems running ef ficiently while deciding what moves to make\n",
      "next. This could involve adopting newer DevOps practices, such as\n",
      "containers, Kubernetes, microservices, and continuous deployment while\n",
      "keeping their own hardware running on-premises, or it could involve a full\n",
      "migration to the cloud, as discussed below .\n",
      "Cloud\n",
      "The cloud flips the on-premises model on its head. Instead of purchasing\n",
      "hardware, you simply rent hardware and managed services from a cloud\n",
      "provider (A WS, Azure, Google Cloud, etc). These resources can often be\n",
      "reserved on an extremely short term basis—virtual machines typically spin\n",
      "up in less than a minute, and subsequent usage is billed in per second\n",
      "increments. This allows cloud users to dynamically scale resources in a way\n",
      "that is inconceivable with on-premises servers.\n",
      "In a cloud environment, engineers can quickly launch projects and\n",
      "experiment without worrying about long lead time hardware planning—\n",
      "they can begin running servers as soon as their code is ready to deploy . This\n",
      "makes the cloud model extremely appealing to startups who are tight on\n",
      "budget and time.The early era of cloud was dominated by Infrastructure as a Service (IaaS)\n",
      "offerings, i.e., products such as virtual machines, virtual disks, etc. that are\n",
      "essentially rented slices of hardware. Slowly , we’ve seen a shift toward\n",
      "Platform as a Service (PaaS), while Software as a Service (SaaS) products\n",
      "continue to grow at a rapid clip.\n",
      "PaaS includes IaaS products, but adds more sophisticated managed services\n",
      "to support applications. Examples are managed databases (Amazon RDS,\n",
      "Google Cloud SQL), managed streaming platforms (Amazon Kinesis and\n",
      "SQS), managed Kubernetes (Google Kubernetes Engine, Azure AKS), etc.\n",
      "PaaS services can allow engineers to ignore the operational details of\n",
      "managing individual machines, and deploying and configuring frameworks\n",
      "across distributed systems. They often provide turnkey access to complex\n",
      "systems with simple scaling and minimal operational overhead.\n",
      "SaaS of ferings move one additional step up the ladder of abstraction.\n",
      "Typically , Software as a Service provides a fully functioning enterprise\n",
      "software platform with very little operational management. Examples of\n",
      "SaaS include Salesforce, Google W orkspace, Of fice 365, Zoom, Fivetran,\n",
      "etc. Both the major public clouds and third parties of fer SaaS platforms.\n",
      "SaaS covers a whole spectrum of enterprise domains, including video\n",
      "conferencing, data management, ad tech, of fice applications, CRM systems,\n",
      "and so on. It can be dif ficult to define the boundary between PaaS and SaaS\n",
      "offerings; a CRM platform may be an important component of backend\n",
      "application systems, and Fivetran functions as a critical piece of data\n",
      "infrastructure.\n",
      "We also mention serverless in this section, which is increasingly important\n",
      "in PaaS and SaaS of ferings. Serverless products generally of fer fully\n",
      "automated scaling from 0 up to extremely high usage rates. They are billed\n",
      "on a pay-as-you-go basis and allow engineers to operate with no operational\n",
      "awareness of underlying servers. Many people quibble with the term\n",
      "serverless—after all, the code must run somewhere. In practice, serverless\n",
      "platforms usually run on highly distributed multi-tenant infrastructure over\n",
      "many servers. A WS Lambda and Google BigQuery are best in classserverless of ferings in the cloud functions and cloud data warehouse\n",
      "domains respectively .\n",
      "Cloud services have become increasingly appealing to established\n",
      "businesses with existing data centers and IT infrastructure. Dynamic,\n",
      "seamless scaling is extremely valuable to businesses that deal with\n",
      "seasonality (i.e. retail business coping with Black Friday load) and web\n",
      "traffic load spikes. The COVID 19 Global Pandemic has also been a major\n",
      "driver of cloud adoption, as numerous businesses have had to cope with\n",
      "substantially increased web traf fic, and internal IT infrastructure has\n",
      "groaned under the load of mass remote work.\n",
      "Before we discuss the nuances of choosing technologies in the cloud, let’ s\n",
      "first discuss why migration to the cloud requires a dramatic shift in\n",
      "thinking, specifically on the pricing front. Enterprises that migrate to the\n",
      "cloud often make major deployment errors by not appropriately adapting\n",
      "their practices to the cloud pricing model. Let’ s take a bit of a detour into\n",
      "cloud economics.\n",
      "A brief detour on cloud economics\n",
      "To understand how to use cloud services ef ficiently , you need to know how\n",
      "clouds make money . This is actually an extremely complex concept and one\n",
      "on which cloud providers of fer little transparency . Our guidance in this\n",
      "section is lar gely anecdotal, and won’ t cover the full scale of what you’ll\n",
      "likely encounter in your own experience.\n",
      "Cloud services and credit default swaps\n",
      "Recall that credit default swaps rose to infamy after the 2007 global\n",
      "financial crisis. Roughly speaking, a credit default swap was a mechanism\n",
      "for selling dif ferent tiers of risk attached to an asset (i.e. a mortgage.) It is\n",
      "not our intention to understand this idea in any detail, but rather to present\n",
      "an analogy wherein many cloud services are similar to financial derivatives;\n",
      "cloud providers not only slice hardware assets into small pieces through\n",
      "virtualization, but they also sell these pieces with varying technical\n",
      "characteristics and risks attached. While providers are extremely tight-lipped about many details of their internal systems, there are massive\n",
      "opportunities for optimization and scaling by understanding cloud pricing\n",
      "and exchanging notes with other users.\n",
      "Let’s look at the example of cloud archival storage. Google Cloud Platform\n",
      "openly admits that their archival class storage runs on the same clusters as\n",
      "standard cloud storage, yet the price per GB per month of archival storage\n",
      "is roughly 1/17th that of standard storage. How is this possible?\n",
      "Here, we are giving our best-educated guess. When we purchase cloud\n",
      "storage, we think in terms of cost per GB, but each disk in a storage cluster\n",
      "has three dif ferent assets that cloud providers and consumers make use of.\n",
      "First, it has a certain storage capacity , say 10 TB. Second, it supports a\n",
      "certain number of IOPs (input/output operations per second), say 100.\n",
      "Third, disks support a certain maximum bandwidth, which is the maximum\n",
      "read speed for optimally or ganized files. A magnetic drive might be capable\n",
      "of reading at 200 MB/s.\n",
      "For a cloud provider , any of these limits (IOPs, storage capacity ,\n",
      "bandwidth) is a potential bottleneck. For instance, the cloud provider might\n",
      "have a disk storing 3 TB of data, but hitting maximum IOPS. There’ s an\n",
      "alternative to leaving the remaining 7 TB empty: sell the empty space\n",
      "without selling IOPs. Or more specifically , sell cheap storage space and\n",
      "expensive IOPs to discourage reads.\n",
      "Much like traders of financial derivatives, cloud vendors also deal in risk.\n",
      "In the case of archival storage, vendors are selling a type of insurance, but\n",
      "one that pays out for the insurer rather than the policy buyer in the event of\n",
      "a catastrophe.\n",
      "Similar considerations apply to nearly any cloud service. T o provide\n",
      "another example, compute is no longer a simple commodity in the cloud as\n",
      "it is with on-premises hardware. Rather than simply char ging for CPU\n",
      "cores, memory , and features, cloud vendors monetize characteristics such as\n",
      "durability , reliability , longevity , and predictability; a variety of compute\n",
      "platforms discount their of ferings for workloads that are ephemeral (A WSLambda) or can be arbitrarily interrupted when capacity is needed\n",
      "elsewhere (Google Compute Engine Interruptible Instances).\n",
      "Cloud ≠ on-premises\n",
      "This heading may seem like a silly tautology; in reality , the belief that cloud\n",
      "services are just like familiar on-premises servers is an extremely common\n",
      "cognitive error that plagues cloud migrations and leads to horrifying bills.\n",
      "This cognitive error demonstrates a broader issue in tech that we refer to as\n",
      "the curse of familiarity . Generally , any technology product is designed to\n",
      "look like something familiar to facilitate ease of use and adoption; but, any\n",
      "technology product has subtleties and wrinkles that users must learn to\n",
      "identify and work with/around after the initial adoption phase.\n",
      "Moving on-premises servers one by one to virtual machines in the cloud—\n",
      "known as simple lift and shift—is a perfectly reasonable strategy for the\n",
      "initial phase of cloud migration, especially where a company is facing some\n",
      "kind of financial clif f, such as the need to sign a significant new lease or\n",
      "hardware contract if existing hardware is not shut down. However ,\n",
      "companies that leave their cloud assets in this initial state are in for a rude\n",
      "shock; on a direct comparison basis, long-running servers in the cloud are\n",
      "significantly more expensive than their on-premises counterparts.\n",
      "The key to finding value in the cloud is to understand and optimize for the\n",
      "cloud pricing model.  Rather than deploying a set of long-running servers\n",
      "capable of handling full peak load, use autoscaling to allow workloads to\n",
      "scale down to minimal infrastructure when loads are light, and up to\n",
      "massive clusters during peak times. Or take advantage of A WS spot\n",
      "instances or A WS Lambda to realize discounts through more ephemeral,\n",
      "less durable workloads.\n",
      "Data engineers can also realize new value in the cloud not by simply saving\n",
      "money , but by doing things that were simply not possible in their on-\n",
      "premises environment. The cloud gives data engineers the opportunity to\n",
      "spin up massive compute clusters quickly in order to run complex\n",
      "transformations, at scales that are unaf fordable for on-premises hardware.Data gravity\n",
      "In addition to basic errors such as following on-premises operational\n",
      "practices in the cloud, data engineers need to watch out for other aspects of\n",
      "cloud pricing and incentives that frequently catch users unawares. V endors\n",
      "want to lock you into their of ferings. For cloud platforms, this means that\n",
      "getting data onto the platform is cheap or free, but getting data out can be\n",
      "extremely expensive. Be aware of data egress fees and their long-term\n",
      "impacts on your business before you get blindsided by a lar ge bill. Data\n",
      "gravity  is real—once data lands in a cloud, the cost to extract it and migrate\n",
      "processes can be very high.\n",
      "Hybrid Cloud\n",
      "As more and more established businesses migrate into the cloud, the hybrid\n",
      "cloud model is growing in importance. V irtually no business can migrate all\n",
      "of its workloads overnight. The hybrid cloud model assumes that an\n",
      "organization will maintain some workloads outside the cloud indefinitely .\n",
      "There are a variety of reasons to consider a hybrid cloud model.\n",
      "Organizations may believe that they have achieved operational excellence\n",
      "in certain areas, for instance with their application stack and associated\n",
      "hardware. Thus, they may choose to migrate only specific workloads where\n",
      "they see immediate benefits in the cloud environment. They might choose\n",
      "to migrate their Hadoop stack to ephemeral cloud clusters, reducing the\n",
      "operational burden of managing software for the data engineering team and\n",
      "allowing rapid scaling for lar ge data jobs.\n",
      "This pattern of putting analytics in the cloud is particularly attractive\n",
      "because data flows primarily in one direction, minimizing data egress costs\n",
      "(Figure 3-3 ). That is, on-premises applications generate event data that can\n",
      "be pushed to the cloud essentially for free. The bulk of data remains in the\n",
      "cloud where it is analyzed, while smaller amounts of data might be pushed\n",
      "back for deploying models to applications, reverse ETL, etc.Figur e 3-3. Hybrid cloud data flow model that minimizes egr ess costs.\n",
      "There’ s also a new generation of managed hybrid cloud service of ferings,\n",
      "including Google Anthos and A WS Outposts. These services allow\n",
      "customers to locate cloud-managed servers in their data centers. Thus, they\n",
      "can take advantage of auto-scaling with the EC2 Management Console, or\n",
      "spin up Google Cloud Composer , while still realizing low latency and\n",
      "network access control of having servers within their own walls. These\n",
      "services integrate seamlessly with the cloud of ferings of A WS and GCP\n",
      "respectively , allowing customers to burst data and application workloads to\n",
      "the cloud as the need arises.\n",
      "Multi-cloud\n",
      "Multi-cloud simply refers to the practice of deploying workloads to\n",
      "multiple clouds, as opposed to deliberately choosing and using only a single\n",
      "cloud for the sake of simplicity . Companies may have a variety of\n",
      "motivations for multi-cloud deployments. SAAS platforms often wish to\n",
      "offer their services close to existing customer cloud workloads. Snowflakeand Databricks are two or ganizations that of fer multi-cloud services for this\n",
      "reason. This is especially critical for data-intensive applications, where\n",
      "network latency and bandwidth limitations hamper performance, and data\n",
      "egress costs can be prohibitive.\n",
      "Another common motivation for employing a multi-cloud approach is to\n",
      "take advantage of the best services across several clouds. Customers might\n",
      "want to handle their Google Ads and Analytics data on GCP and deploy\n",
      "Kubernetes through Google GKE. They might adopt Azure specifically for\n",
      "Microsoft workloads. And A WS has several best-in-class services (i.e.\n",
      "AWS Lambda) and enjoys huge mindshare, making it relatively easy to hire\n",
      "AWS proficient engineers.\n",
      "There are several disadvantages to a multi-cloud methodology . As we just\n",
      "mentioned, data egress costs and networking bottlenecks are critical. And\n",
      "going multi-cloud can introduce significant complexity . Companies must\n",
      "now manage a dizzying array of services across several clouds, and cross-\n",
      "cloud integration and security present a huge challenge. Among other\n",
      "things, it may become necessary to integrate virtual networks across several\n",
      "clouds.\n",
      "A new generation of “cloud of clouds” services aims to facilitate multi-\n",
      "cloud with reduced complexity by not only of fering services across clouds,\n",
      "but seamlessly replicating data between clouds, or managing workloads on\n",
      "several clouds through a single pane of glass. A Snowflake account runs in\n",
      "a single cloud region, but customers can readily spin up accounts in GCP ,\n",
      "AWS, or Azure; Snowflake provides simple scheduled data replication\n",
      "between these accounts. In addition, the Snowflake interface is essentially\n",
      "the same in all of these accounts, removing the training burden associated\n",
      "with switching between cloud-native services such as Amazon Redshift and\n",
      "Azure Synapse.\n",
      "Google BigQuery Omni will deliver similar functionality once it goes into\n",
      "general availability , extending the GCP native experience to multiple\n",
      "clouds. In fact, we’re seeing a growing trend of “cloud of clouds” services\n",
      "offered directly by the public clouds. Google Anthos allows users to runworkloads on A WS/Azure, and Amazon EKS Anywhere of fers similar\n",
      "functionality for Azure/GCP .\n",
      "The “cloud of clouds” space is evolving rapidly; within a few years of this\n",
      "book’ s publication, there will be far more such services available, and data\n",
      "engineers and architects would do well to maintain awareness of this\n",
      "quickly changing landscape.\n",
      "Decentralized: Blockchain and the Edge\n",
      "Though not widely used now , it’s worth briefly mentioning a new trend that\n",
      "might become popular over the next decade—decentralized computing.\n",
      "Whereas today’ s applications mainly run on-premises and in the cloud, the\n",
      "rise of blockchain, W eb 3.0, and edge computing may possibly invert this\n",
      "paradigm. While there are not a lot of popular examples to point at right\n",
      "now, it’s worth keeping platform decentralization in the back of your mind\n",
      "as you consider technologies.\n",
      "Be Cautious with Repatriation Arguments\n",
      "As we were writing this book, Sarah W ang and Martin Casado published an\n",
      "article for Andreesen Horowitz  that generated significant sound and fury in\n",
      "the tech space. This article was widely interpreted as a call for repatriation\n",
      "of cloud workloads to on-premises servers. In fact, they make a somewhat\n",
      "more subtle ar gument that companies should expend significant resources\n",
      "to control cloud spend, and should consider repatriation as a possible\n",
      "option.\n",
      "We want to take a moment to dissect one part of their discussion. W ang and\n",
      "Casado cite Dropbox’ s repatriation of significant workloads from A WS to\n",
      "their servers as a case study for companies considering the same move or\n",
      "assessing retention of existing data centers.\n",
      "You are not Dropbox, nor are you Cloudflare\n",
      "We believe that this case study is frequently used without appropriate\n",
      "context, and is a compelling example of the false equivalence  logical4fallacy . Dropbox provides very specific services where ownership of\n",
      "hardware and data centers can of fer a competitive advantage. Companies\n",
      "should not rely excessively on Dropbox’ s example when assessing cloud\n",
      "and on-premises deployment options.\n",
      "First, it’ s important to understand that Dropbox stores a vast quantity of\n",
      "data. The company is tight-lipped about exactly how much data they host,\n",
      "but they do say that it is many exabytes and that it continues to grow .\n",
      "Second, Dropbox handles a vast amount of network traf fic. W e know that\n",
      "their bandwidth consumption in 2017 was significant enough for the\n",
      "company to add “hundreds of gigabits of Internet connectivity with transit\n",
      "providers (regional and global ISPs), and hundreds of new peering partners\n",
      "(where we exchange traf fic directly rather than through an ISP).”  Their\n",
      "data egress costs would be extremely high in a public cloud environment.\n",
      "Third, Dropbox is essentially a cloud storage vendor , but one with a highly\n",
      "specialized storage product that combines characteristics of object and\n",
      "block storage. Dropbox’ s core competence is a dif ferential file update\n",
      "system that can ef ficiently synchronize actively edited files between users\n",
      "while minimizing network and CPU usage. As such, their product is not a\n",
      "good fit for object storage, block storage or other standard cloud of ferings.\n",
      "Dropbox has instead benefited from building a custom, highly integrated\n",
      "software and hardware stack.\n",
      "Fourth, while Dropbox moved their core product to their own hardware,\n",
      "they continue to build out other workloads on A WS. In ef fect, this allows\n",
      "them to focus on building one highly tuned cloud service at extraordinary\n",
      "scale rather than trying to replace numerous dif ferent services; they can\n",
      "focus on their core competence in cloud storage and data synchronization,\n",
      "while of floading management of software and hardware in other areas, such\n",
      "as data analytics.\n",
      "Other frequently cited success stories where companies have built outside\n",
      "the cloud include Backblaze and Cloudflare, and these of fer similar lessons.\n",
      "Backblaze began life as a personal cloud data backup product, but has since\n",
      "begun to of fer its own object storage similar to Amazon S3. They currently5\n",
      "6\n",
      "7store over an exabyte of data.  Cloudflare claims to provide services for\n",
      "over 25 million internet properties, with points of presence in over 200\n",
      "cities and 51 Tbps (terabits per second) of total network capacity .\n",
      "Netflix of fers yet another useful example. Netflix is famous for running its\n",
      "tech stack on A WS, but this is only partially true. Netflix does run video\n",
      "transcoding on A WS, accounting for roughly 70% of its compute needs in\n",
      "2017.  They also run their application backend and data analytics on A WS.\n",
      "However , rather than using the A WS content distribution network, they\n",
      "have built out a custom CDN in collaboration with internet service\n",
      "providers, utilizing a highly specialized combination of software and\n",
      "hardware. For a company that consumes a substantial slice of all internet\n",
      "traffic,  building out this critical infrastructure allowed them to cost\n",
      "effectively deliver high quality video to a huge customer base.\n",
      "These case studies suggest that it makes sense for companies to manage\n",
      "their own hardware in very specific circumstances. The biggest modern\n",
      "success stories of companies building and maintaining hardware involve\n",
      "extraordinary scale (exabytes of data, Tbps of bandwidth, etc.), and narrow\n",
      "use cases where companies can realize a competitive advantage by\n",
      "engineering highly integrated hardware and software stacks. Thus, Apple\n",
      "might gain a competitive edge by building its own storage system for\n",
      "iCloud,  but there is less evidence to suggest that general purpose\n",
      "repatriation ef forts will be beneficial.\n",
      "Our Advice\n",
      "From our perspective, we are still at the beginning of the transition to cloud,\n",
      "and thus the evidence and ar guments around workload placement and\n",
      "migration will continue to evolve rapidly . Cloud itself is changing, with a\n",
      "shift from the IAAS (infrastructure as a service) model built around\n",
      "Amazon EC2 that drove the early growth of A WS, toward more managed\n",
      "service of ferings such as A WS Glue, Google BigQuery , Snowflake, etc.\n",
      "We’ve also seen the emer gence of new workload placement abstractions.\n",
      "On-premises services are becoming more cloud-like (Think Kubernetes,\n",
      "which abstracts away hardware details); And hybrid cloud services such as8\n",
      "9\n",
      "10\n",
      "1 1\n",
      "12Google Anthos and A WS Outposts allow customers to run fully managed\n",
      "services within their own walls, while also facilitating tight integration\n",
      "between local and remote environments. Further , the “cloud of clouds” is\n",
      "just beginning to take shape, fueled by third party services, and increasingly\n",
      "the public cloud vendors themselves.\n",
      "Choose technologies for the present, but look toward the future\n",
      "In sum, it is a very dif ficult time to plan workload placements and\n",
      "migrations. The decision space will look very dif ferent in five to ten years.\n",
      "It is tempting to take into account every possible future architecture\n",
      "permutation.\n",
      "We believe that it is critical to avoid this endless trap of analysis. Instead,\n",
      "plan for the present. Choose the best technologies for your current needs\n",
      "and concrete plans for the near future. Choose your deployment platform\n",
      "based on real business needs, while focusing on simplicity and flexibility .\n",
      "In particular , don’ t choose a complex multi-cloud or hybrid-cloud strategy\n",
      "unless there’ s a compelling reason to do so. Do you need to serve data near\n",
      "customers on multiple clouds? Do industry regulations require you to house\n",
      "certain data in your own data centers? Do you have a compelling\n",
      "technology need for specific services on two dif ferent clouds? If these\n",
      "scenarios don’ t apply to you, then choose a simple single cloud deployment\n",
      "strategy .\n",
      "On the other hand, have an escape plan. As we’ve emphasized before, every\n",
      "technology—even open-source software—comes with some degree of lock-\n",
      "in. A single-cloud strategy has significant advantages of simplicity and\n",
      "integration, but also significant lock-in. In this instance, we’re really talking\n",
      "about mental flexibility , the flexibility to evaluate the current state of the\n",
      "world and imagine alternatives. Ideally , your escape plan will remain locked\n",
      "behind glass, but preparing this plan will help you to make better decisions\n",
      "in the present, and give you a way out if things do go wrong in the future.\n",
      "Build Versus BuyBuild versus buy is a very old debate in technology . The ar gument for\n",
      "“build” is that you have end-to-end control over the solution, and are not at\n",
      "the mercy of a vendor or open source community . The ar gument supporting\n",
      "“buy” comes down to resource constraints and expertise—do you have the\n",
      "expertise to build a solution that’ s better than something already available?\n",
      "Either decision comes down to TCO, opportunity cost, and whether the\n",
      "solution provides a competitive advantage to your or ganization.\n",
      "If you’ve caught on to a theme in the book so far , it’s that we suggest\n",
      "investing in building and customizing when doing so will provide a\n",
      "competitive advantage. Otherwise, stand on the shoulders of giants and use\n",
      "what’ s already available in the market. Given the number of open source\n",
      "and paid services—both of which may have communities of volunteers or\n",
      "highly paid teams of amazing engineers—you’re foolish to try to build\n",
      "everything yourself.\n",
      "As we often ask, “When you need new tires for your car , do you get the raw\n",
      "materials, build the tires from scratch, and install them yourself? Or do you\n",
      "go to the tire shop, buy the tires, and let a team of experts install them for\n",
      "you?” If you’re like most people, you’re probably buying tires and having\n",
      "someone install them. The same ar gument applies to build vs buy . We’ve\n",
      "seen teams who’ve built their own database from scratch. Upon closer\n",
      "inspection, a simple open-source RDBMS would have served their needs\n",
      "much better . Imagine the amount of time and money invested in this\n",
      "homegrown database. T alk about low ROI for TCO and opportunity cost.\n",
      "This is where the distinction between the T ype A and T ype B data engineer\n",
      "comes in handy . As we pointed out earlier , Type A and T ype B roles are\n",
      "often embodied in the same engineer , especially in a small or ganization.\n",
      "Whenever possible, lean towar d Type A behavior—avoid undiffer entiated\n",
      "heavy lifting, and embrace abstraction. Use open-sour ce frameworks, or if\n",
      "this is too much tr ouble, look at buying a suitable managed or pr oprietary\n",
      "solution.  In either case, there are plenty of great, modular services to choose\n",
      "from.It’s worth mentioning the shifting reality of how software is adopted within\n",
      "companies. Whereas in the past, IT used to make most of the software\n",
      "purchase and adoption decisions in a very top-down manner , these days, the\n",
      "trend is for bottom-up software adoption in a company , driven by\n",
      "developers, data engineers, data scientists, and other technical roles.\n",
      "Technology adoption within companies is becoming an or ganic, continuous\n",
      "process.\n",
      "Let’s look at some options for open source and proprietary solutions.\n",
      "Open Source Software (OSS)\n",
      "Open source software (OSS) is a software distribution model where\n",
      "software—and the underlying codebase—is made available for general use,\n",
      "typically under certain licensing terms. Oftentimes, OSS is created and\n",
      "maintained by a distributed team of collaborators. Most of the time, OSS is\n",
      "free to use, change, and distribute, but with specific caveats. For example,\n",
      "many licenses require that the source code of open source derived software\n",
      "be included when the software is distributed.\n",
      "The motivations for creating and maintaining OSS vary . Sometimes OSS is\n",
      "organic, springing from the mind of an individual or a small team who\n",
      "create a novel solution and choose to release it into the wild for public use.\n",
      "Other times, a company may make a specific tool or technology available to\n",
      "the public under an OSS license.\n",
      "There are two main flavors of OSS—community managed, and commercial\n",
      "OSS.\n",
      "Community-managed OSS\n",
      "OSS projects succeed when there’ s a strong community and vibrant user\n",
      "base. Community-managed OSS is a very common path for OSS projects.\n",
      "With popular OSS projects, the community really opens up high rates of\n",
      "innovations and contributions from developers all over the world.\n",
      "Some things to consider with a community-managed OSS project:Mindshar e\n",
      "Avoid adopting OSS projects that don’ t have traction and popularity .\n",
      "Look at the number of GitHub stars, forks, and commit volume and\n",
      "recency . Another thing to pay attention to is community activity on\n",
      "related chat groups and forums. Does the project have a strong sense of\n",
      "community? A strong community creates a virtuous cycle of strong\n",
      "adoption. It also means that you’ll have an easier time getting technical\n",
      "assistance, and finding talent qualified to work with the framework.\n",
      "Project management\n",
      "How is the project managed? Look at Git issues and if/how they’re\n",
      "addressed.\n",
      "Team\n",
      "Is there a company sponsoring the OSS project? Who are the core\n",
      "contributors?\n",
      "Developer r elations and community management\n",
      "What is the project doing to encourage uptake and adoption? Is there a\n",
      "vibrant Slack community that provides encouragement and support?\n",
      "Contributing\n",
      "Does the project encourage and accept pull requests?\n",
      "Roadmap\n",
      "Is there a project roadmap? If so, is it clear and transparent?\n",
      "Hosting and maintenance\n",
      "Do you have the resources to host and maintain the OSS solution? If so,\n",
      "what’ s the TCO and opportunity cost versus buying a managed service\n",
      "from the OSS vendor?Giving back to the community\n",
      "If you like the project and are actively using it, consider investing in the\n",
      "project. Y ou can contribute to the codebase, help fix issues, give advice\n",
      "in the community forums and chats. If the project allows donations,\n",
      "consider making one. Many OSS projects are essentially community\n",
      "service projects, and the maintainers often have full-time jobs in\n",
      "addition to helping with the OSS project. It’ s a labor of love that sadly\n",
      "doesn’ t afford the maintainer a living wage. If you can af ford to donate,\n",
      "please do so.\n",
      "Commercial OSS\n",
      "Sometimes OSS has some drawbacks. Namely , you have to host and\n",
      "maintain the solution in your environment. Depending on the OSS\n",
      "application you’re using, this may either be trivial or extremely complicated\n",
      "and cumbersome. Commercial vendors try to solve this management\n",
      "headache by hosting and managing the OSS solution for you, typically as a\n",
      "cloud SAAS of fering. Some examples of such vendors include Databricks\n",
      "(Spark), Confluent (Kafka), DBT Labs (dbt), and many , many others.\n",
      "This model is called Commercial OSS (COSS). T ypically , a vendor will\n",
      "offer the “core” of the OSS for free, while char ging for enhancements,\n",
      "curated code distributions, or fully managed services.\n",
      "A vendor is often af filiated with the community OSS project. As an OSS\n",
      "project becomes more popular , the maintainers may create a separate\n",
      "business for a managed version of the OSS. This typically becomes a cloud\n",
      "SAAS platform built around a managed version of the open source code.\n",
      "This is a very common trend—an OSS project becomes very popular , an\n",
      "affiliated company raises truckloads of VC money to commercialize the\n",
      "OSS project, and the company scales as a fast-moving rocketship.\n",
      "At this point, there are two options for a data engineer . You can continue\n",
      "using the community-managed OSS version, which you need to continue\n",
      "maintaining on your own (updates, server/container maintenance, pullrequests for bug fixes, etc). Or , you can pay the vendor and let them take\n",
      "care of the administrative management of the COSS product.\n",
      "Some things to consider with a commercial OSS project:\n",
      "Value\n",
      "Is the vendor of fering a better value than if you managed the OSS\n",
      "technology yourself? Some vendors will add lots of bells and whistles to\n",
      "their managed of ferings that aren’ t available in the community OSS\n",
      "version. Are these additions compelling to you?\n",
      "Delivery model\n",
      "How do you access the service? Is the product available via download,\n",
      "API, or web/mobile UI? Be sure you’re easily able to access the initial\n",
      "version and subsequent releases.\n",
      "Support\n",
      "Support cannot be understated, and it’ s sadly often opaque to the buyer .\n",
      "What is the support model for the product, and is there an extra cost for\n",
      "support? Oftentimes, vendors will sell support for an additional fee. Be\n",
      "sure you clearly understand the costs of obtaining support.\n",
      "Releases and bug fixes\n",
      "Is the vendor transparent about their release schedule, improvements,\n",
      "and bug fixes? Are these updates easily available to you?\n",
      "Sales cycle and pricing\n",
      "Often, a vendor will of fer on-demand pricing, especially for a SAAS\n",
      "product, and of fer you a discount if you commit to an extended\n",
      "agreement. Be sure to understand the tradeof fs of paying as you go\n",
      "versus paying upfront. Is it worth it to pay a lump sum, or is your\n",
      "money better spent elsewhere?\n",
      "Company financesIs the company viable? If the company has raised VC funds, you can\n",
      "check their funding on sites like Crunchbase. How much runway does\n",
      "the company have, and will they still be in business in a couple of\n",
      "years?\n",
      "Logos versus r evenue\n",
      "Is the company focused on growing the number of customers (logos), or\n",
      "is it trying to grow revenue? Y ou may be surprised by the number of\n",
      "companies that are primarily concerned with growing their customer\n",
      "count without the revenue to establish sound finances.\n",
      "Community support\n",
      "Is the company truly supporting the community version of the OSS\n",
      "project? How much are they contributing to the community OSS\n",
      "codebase? There’ s been controversy with certain vendors co-opting\n",
      "OSS projects, and subsequently providing very little value back to the\n",
      "community .\n",
      "Proprietary W alled-Gardens\n",
      "While OSS is extremely common, there is also a giant market for non-OSS\n",
      "technologies. Some of the biggest companies in the data industry sell closed\n",
      "source products. Let’ s look at two major types of “proprietary walled-\n",
      "gardens”—independent companies and cloud-platform of ferings.\n",
      "Independent offerings\n",
      "The data tool landscape has seen exponential growth over the last several\n",
      "years. Every day , it seems like there are new independent of ferings for data\n",
      "tools, and there’ s no indication of a slowdown anytime soon. W ith the\n",
      "ability to raise funds from VCs who are flush with capital, these data\n",
      "companies can scale, and hire great teams in engineering, sales and\n",
      "marketing. This presents a situation where users have some great product\n",
      "choices in the marketplace while having to wade through endless sales and\n",
      "marketing clutter .Quite often, a company selling a data tool will not release it as OSS, instead\n",
      "offering a black box solution. Although you won’ t have the transparency of\n",
      "a pure OSS solution, a proprietary independent solution can work quite\n",
      "well, especially as a fully managed service in the cloud. For comparison,\n",
      "Databricks uses the Apache Spark codebase with a proprietary management\n",
      "layer; Google BigQuery is a proprietary solution, not of fering access to the\n",
      "underlying codebase.\n",
      "Some things to consider with an independent of fering:\n",
      "Inter operability\n",
      "Make sure it inter -operates with other tools you’ve chosen—OSS, other\n",
      "independents, cloud of ferings, etc. Interoperability is key , so make sure\n",
      "you are able to try before you buy .\n",
      "Mindshar e and market shar e\n",
      "Is the solution popular? Does it command a presence in the\n",
      "marketplace? Does it enjoy positive customer reviews?\n",
      "Documentation and support\n",
      "Problems and questions will inevitably arise. Is it clear how to solve\n",
      "your problem, either through documentation or support?\n",
      "Pricing\n",
      "Is the pricing clearly understandable? Map out low , medium, and high\n",
      "probability usage scenarios, with respective costs. Are you able to\n",
      "negotiate a contract, along with a discount? Is it worth it? If you sign a\n",
      "contract, how much flexibility do you lose, both in terms of negotiation\n",
      "and the ability to try new options? Are you able to obtain contractual\n",
      "commitments on future pricing?\n",
      "Longevity\n",
      "Will the company survive long enough for you to get value from its\n",
      "product? If the company has raised money , check sites like Crunchbaseand see what their funding situation is like. Look at user reviews. Ask\n",
      "friends and post questions on social networks about other users’\n",
      "experiences with the product. Make sure you know what you’re getting\n",
      "into.\n",
      "Cloud platform service offerings\n",
      "Cloud vendors develop and sell their own proprietary services for storage,\n",
      "databases, etc. Many of these solutions are internal tools used by respective\n",
      "sibling companies. For example, Amazon created the database DynamoDB\n",
      "to overcome the limitations of traditional relational databases and handle\n",
      "the lar ge amounts of user and order data as Amazon.com grew into a\n",
      "behemoth. They later of fered the DynamoDB service solely on A WS; it’ s\n",
      "now an extremely popular product used by companies of all sizes and\n",
      "maturity levels. Clouds will often bundle their products to work well\n",
      "together . By creating a strong integrated ecosystem, each cloud is able to\n",
      "create “stickiness” with its user base.\n",
      "In addition to internal innovations that make their way to end-users, as we\n",
      "discussed earlier , clouds are also keen followers of strong OSS and\n",
      "independent of ferings. If a cloud vendor sees traction with a particular\n",
      "product or project, expect that they will of fer their own version. The reason\n",
      "is simple. Clouds make their money through consumption. More of ferings\n",
      "in a cloud ecosystem mean a greater chance of stickiness and increased\n",
      "spending by customers.\n",
      "Some things to consider with a proprietary cloud of fering:\n",
      "Performance versus price comparisons\n",
      "Is the cloud of fering substantially better than an independent or OSS\n",
      "version? What’ s the TCO of choosing a cloud’ s offering?\n",
      "Purchase considerations\n",
      "On-demand pricing can be expensive. Are you able to lower your cost\n",
      "by purchasing reserved capacity , or entering into a long-term\n",
      "commitment agreement?Our Advice\n",
      "Build versus buy comes back to knowing your competitive advantage, and\n",
      "where it makes sense to invest resources toward customization. In general,\n",
      "we favor OSS and COSS by default, which frees you up to focus on\n",
      "improving those areas where these options are insuf ficient. Focus on a few\n",
      "areas where building will add significant value or reduce friction\n",
      "substantially .\n",
      "It’s worth mentioning—don’ t treat internal operational overhead as a sunk\n",
      "cost. There’ s great value in upskilling your existing data team to build\n",
      "sophisticated systems on managed platforms rather than babysitting on-\n",
      "premises servers.\n",
      "Always think about how a company makes money , especially the sales and\n",
      "customer experience people. This will generally indicate how you’re treated\n",
      "during the sales cycle, as well as when you’re a paying customer .\n",
      "Monolith Versus Modular\n",
      "Monoliths versus modular systems is another longtime debate in the\n",
      "software architecture space. Monolithic systems are self-contained, often\n",
      "performing multiple functions under a single system. The monolith camp\n",
      "favors the simplicity of having everything in one place. It’ s easier to reason\n",
      "about a single entity , and you can move faster because there are fewer\n",
      "moving parts. The modular camp leans toward decoupled, best-of-breed\n",
      "technologies performing tasks at which they are uniquely great. Especially\n",
      "given the rate of change in products in the data world, the ar gument is you\n",
      "should aim for interoperability among an ever -changing array of solutions.\n",
      "What approach should you take in your data engineering stack? Let’ s\n",
      "explore the tradeof fs of a monolithic vs a modular approach.\n",
      "MonolithThe monolith ( Figure 3-4 ) has been a technology mainstay for decades. The\n",
      "old days of waterfall meant that software releases were huge, tightly\n",
      "coupled, and moved at a slow cadence. Lar ge teams worked together to\n",
      "deliver a single working codebase. Monolithic data systems continue to this\n",
      "day, with older software vendors such as Informatica, and open source\n",
      "frameworks such as Spark.\n",
      "Figur e 3-4. The monolith tightly couples its services\n",
      "The pros of the monolith are it’ s easy to reason about, and there’ s lower\n",
      "cognitive burden and context switching since everything is self-contained.\n",
      "Instead of dealing with dozens of technologies, you deal with “one”\n",
      "technology , and typically one principle programming language. Monoliths\n",
      "are a good option if you want simplicity in reasoning about your\n",
      "architecture and processes.\n",
      "Of course, there are cons with the monolith. For one, it’ s brittle. Due to the\n",
      "vast number of moving parts, updates and releases take longer , and tend to\n",
      "bake in “the kitchen sink”. If there’ s a bug in the system (and hopefully the\n",
      "software’ s been thoroughly tested before release!), it can have a deleterious\n",
      "effect on the entire system.\n",
      "User-induced problems also happen with monoliths. For example, we saw a\n",
      "monolithic ETL pipeline that took 48 hours to run. If anything brokeanywhere in the pipeline, the entire process had to restart. Meanwhile,\n",
      "anxious business users were waiting for their reports, which were already 2\n",
      "days late by default. Breakages were common enough that the monolithic\n",
      "system was eventually thrown out.\n",
      "Multitenancy in a monolithic system can also be a significant problem. It\n",
      "can be dif ficult to isolate the workloads of multiple users in a monolithic\n",
      "system. In an on-prem data warehouse, one user -defined function might\n",
      "consume enough CPU to slow down the system for other users. In Hadoop,\n",
      "Spark, and Airflow environments, conflicts between dependencies required\n",
      "by dif ferent users are a frequent source of headaches. (See the distributed\n",
      "monolith  discussion that follows.)\n",
      "Another con about monoliths—if the vendor or open source project dies,\n",
      "switching to a new system will be very painful. Because all of your\n",
      "processes are contained in the monolith, extracting yourself out of that\n",
      "system, and onto a new platform, will be costly in both time and money .\n",
      "Modularity\n",
      "Modularity ( Figure 3-5 ) is an old concept in software engineering, but\n",
      "modular distributed systems truly came into vogue with the rise of\n",
      "microservices in the 2010s. Instead of relying on a massive monolith to\n",
      "handle your needs, why not break apart systems and processes into their\n",
      "own self-contained areas of concern? Microservices can communicate via\n",
      "APIs, allowing developers to focus on their domains while making their\n",
      "applications accessible to other microservices. This is the trend in software\n",
      "engineering and is increasingly seen in modern data systems as well.\n",
      "Figur e 3-5. Modularity: each service is decoupledMajor tech companies have been key drivers in the microservices\n",
      "movement. Google engineers invented Linux containers, inspired by other\n",
      "operating systems such as Solaris, to allow the decomposition of\n",
      "applications into smaller pieces. The famous Bezos API mandate decreases\n",
      "coupling between applications, allowing refactoring and decomposition.\n",
      "Bezos also imposed the two-pizza rule. (No team should be so lar ge that\n",
      "two pizzas can’ t feed the whole group.) Ef fectively , this means that a team\n",
      "will have at most 5 members. This cap also limits the complexity of a\n",
      "team’ s domain of responsibility , in particular the codebase that it can\n",
      "manage. Where a lar ge, monolithic application might entail a team of 100\n",
      "people, dividing developers into small teams of 5 requires that this lar ge\n",
      "application be broken into small, manageable, loosely coupled pieces.\n",
      "In a modular microservice environment, components are swappable; a\n",
      "service written in Python can be replaced with a Java service. Service\n",
      "customers need only worry about the technical specifications of the service\n",
      "API, not behind-the-scenes details of implementation.\n",
      "Data processing technologies have shifted toward a modular model by\n",
      "providing strong support for interoperability . In the world of data lakes and\n",
      "lakehouses, data is stored in object storage in a standard format such as\n",
      "Parquet. Any processing tool that supports the format can read the data and\n",
      "write processed results back into the lake for processing by another tool.\n",
      "Tools such as BigQuery , Snowflake, and Redshift don’ t use a data lake\n",
      "architecture and don’ t make data directly accessible to external tools.\n",
      "However , all of these support interoperation with object storage, through\n",
      "import/export using standard formats, and external tables, i.e., queries run\n",
      "directly on data in a data lake.\n",
      "In today’ s data ecosystem, new technologies arrive on the scene at a\n",
      "dizzying rate, and most get stale and outmoded quickly . Rinse and repeat.\n",
      "The ability to swap out tools as technology changes are invaluable. In\n",
      "addition, we view data modularity as a more powerful paradigm than\n",
      "monolithic data engineering. It allows engineers to choose the best\n",
      "technology for each job, or even for each step of a pipeline.The cons of modularity are that there’ s more to reason about. Instead of\n",
      "handling a single system of concern, now you potentially have countless\n",
      "systems to understand and operate. Interoperability is a potential headache;\n",
      "hopefully , these systems all play nicely together .\n",
      "In fact, it is this very problem that led us to break orchestration out as a\n",
      "separate undercurrent, instead of placing it under “data management.”\n",
      "Orchestration is also important for monolithic data architectures—witness\n",
      "the success of Control-M in the traditional data warehousing space. But\n",
      "orchestrating five or ten tools is dramatically more complex than\n",
      "orchestrating one. Orchestration becomes the glue that binds these tools\n",
      "together .\n",
      "The Distributed Monolith Pattern\n",
      "The distributed monolith pattern is a distributed architecture that still suf fers\n",
      "from many of the limitations of monolithic architecture. The basic idea is\n",
      "that one runs a distributed system with separate services to perform\n",
      "different tasks, but services and nodes share a common set of dependencies\n",
      "and/or a common codebase.\n",
      "One standard example is a traditional Hadoop cluster . A standard Hadoop\n",
      "cluster can host a number of dif ferent frameworks simultaneously , such as\n",
      "Hive, Pig, Spark, etc. In addition, the cluster runs core Hadoop components:\n",
      "Hadoop common libraries, HDFS, Y arn, Java. In practice, a cluster\n",
      "generally has one version of each component installed.\n",
      "Essentially , a standard on-prem Hadoop system entails managing a common\n",
      "environment that works for all users and all jobs. Managing upgrades and\n",
      "installations is a significant challenge. Forcing jobs to upgrade\n",
      "dependencies risks breaking them; maintaining two versions of a\n",
      "framework entails extra complexity .\n",
      "Our second example of a distributed monolith pattern is Apache Airflow .\n",
      "The Airflow architecture is highly decoupled and asynchronous.\n",
      "Interprocess communications are passed through a backend database, with afew dif ferent services (web server , scheduler , executor) interacting. The\n",
      "web server and executor can be scaled horizontally to multiple copies.\n",
      "The problem with this architecture is that every service needs to run the\n",
      "same Airflow codebase with the same dependencies. Any executor can\n",
      "execute any task, so a client library for a single task run in one DAG must\n",
      "be installed on the whole cluster . As a consequence, Google Cloud\n",
      "Composer (managed Airflow on Google Cloud) has a huge number of\n",
      "installed dependencies to handle numerous dif ferent databases, frameworks,\n",
      "and APIs out of the box. The pip package manager shows many\n",
      "dependency conflicts even in the base environment. The installed packages\n",
      "are essentially a house of cards. Installing additional libraries to support\n",
      "specific tasks sometimes works, and sometimes breaks the whole\n",
      "installation.\n",
      "One solution to the problems of the distributed monolith is ephemeral\n",
      "infrastructure in a cloud setting. That is, each job gets its own temporary\n",
      "server or cluster , with its own dependencies installed. Each cluster remains\n",
      "highly monolithic, but conflicts are dramatically reduced by separating jobs.\n",
      "This pattern is now quite common for Spark with services like Amazon\n",
      "EMR and Google Cloud Dataproc.\n",
      "A second solution is to properly decompose the distributed monolith into\n",
      "separate software environments using containers. W e have more to say on\n",
      "containers below in our section on Serverless vs Servers.\n",
      "A third solution is to retain the distributed monolith, but pull functionality\n",
      "out into microservices. This approach has become common with Airflow;\n",
      "code with complex dependency requirements is moved into external\n",
      "containers or cloud functions to keep the core environment simple.\n",
      "Our Advice\n",
      "While monoliths are certainly attractive due to ease of understanding and\n",
      "reduced complexity , this comes at a big cost. The cost is the potential loss\n",
      "of flexibility , opportunity cost, and high friction development cycles.Here are some things to consider when evaluating monoliths versus\n",
      "modular:\n",
      "Inter operability .\n",
      "Architect for sharing and interoperability .\n",
      "Avoid the “bear trap.”\n",
      "Something that is very easy to get into might be very painful or\n",
      "impossible to extricate yourself from.\n",
      "Flexibility .\n",
      "Things are moving so fast in the data space right now . Committing to a\n",
      "monolith reduces flexibility and reversible decisions.\n",
      "Serverless Versus Servers\n",
      "A big trend for cloud providers is serverless, which allows developers and\n",
      "data engineers to run applications without managing servers behind the\n",
      "scenes. For the right use cases, serverless provides a very quick time to\n",
      "value. In other cases, it might not be a good fit. Let’ s look at how to\n",
      "evaluate whether serverless is right for you.\n",
      "Serverless\n",
      "Though serverless has been around for quite some time, the serverless trend\n",
      "kicked of f in full force with A WS Lambda in 2014. W ith the promise of\n",
      "executing small chunks of code on an as-needed basis, without having to\n",
      "run a server , serverless exploded in popularity . The main reasons for the\n",
      "popularity of serverless are cost and convenience. Instead of paying the cost\n",
      "of a server , why not just pay when your code is evoked?\n",
      "There are many flavors of serverless. Though function as a service (F AAS)\n",
      "is wildly popular , serverless systems actually predate the advent of A WS\n",
      "Lambda. As an example, Google Cloud’ s BigQuery is a serverless datawarehouse, in that there’ s no backend infrastructure for a data engineer to\n",
      "manage. Just load data into the system and start querying. Y ou pay for the\n",
      "amount of data your query consumes, as well as a small cost to store your\n",
      "data. This payment model—paying for consumption and storage—is\n",
      "becoming more and more prevalent, particularly with serverless workloads.\n",
      "Another example of serverless is Google App Engine, originally released in\n",
      "2008.\n",
      "When does serverless make sense? As with many other cloud services, it\n",
      "depends; and data engineers would do well to understand the details of\n",
      "cloud pricing to predict when serverless deployments are going to become\n",
      "expensive. Looking specifically at the case of A WS Lambda, various\n",
      "engineers have found hacks to run batch workloads at incredibly low\n",
      "costs.  On the other hand, serverless functions suf fer from an inherent\n",
      "overhead inef ficiency . Every invocation involves spinning up some new\n",
      "resources and running some initialization code, overhead that does not\n",
      "apply to a long-running service. Handling one event per function call at a\n",
      "high event rate can be catastrophically expensive. As with other areas of\n",
      "ops, it’ s critical to monitor  and model.  That is, monitor  to determine cost\n",
      "per event in a real world environment, and model  by using this cost per\n",
      "event to determine overall costs as event rates grow in the future. Similar\n",
      "considerations apply to understanding costs for other serverless models.\n",
      "Containers\n",
      "In conjunction with serverless and microservices, containers are one of the\n",
      "most significant trending operational technologies as of this writing. In fact,\n",
      "containers play a role in both.\n",
      "Containers are often referred to as lightweight virtual machines. Where a\n",
      "traditional virtual machine wraps up an entire operating system, a container\n",
      "packages an isolated user space, i.e. a filesystem and one or a few\n",
      "processes; many such containers can coexist on a single host operating\n",
      "system. This provides some of the principal benefits of virtualization (i.e.\n",
      "dependency and code isolation) without the overhead of carrying around an\n",
      "entire operating system kernel.13A single hardware node can host numerous containers with fine grained\n",
      "resource allocations. At the time of this writing, containers continue to grow\n",
      "in popularity , along with Kubernetes, a container management system.\n",
      "Serverless environments typically run on containers behind the scenes;\n",
      "indeed, Kubernetes is a kind of serverless environment because it allows\n",
      "developers and ops teams to deploy microservices without worrying about\n",
      "the details of the machines where they are deployed.\n",
      "Containers provide a partial solution to problems of the distributed\n",
      "monolith, mentioned earlier in this chapter . Hadoop now supports\n",
      "containers, allowing each job to have its own isolated dependencies. And as\n",
      "we mentioned, many Airflow users extract jobs into containers.\n",
      "WARNING\n",
      "Container clusters do not provide the same level of security and isolation of fered by full\n",
      "virtual machines. Container escape—broadly , a class of exploits where code in a\n",
      "container gains privileges outside the container at the OS level—remains an unsolved\n",
      "problem. While Amazon EC2 is a truly multi-tenant environment with VMs from many\n",
      "customers hosted on the same hardware, a Kubernetes cluster should only host code\n",
      "within an environment of mutual trust, e.g. inside the walls of a single company .\n",
      "Various flavors of container platforms add additional serverless features.\n",
      "Containerized function platforms (OpenFaas, Google Cloud Run) run\n",
      "containers as ephemeral units triggered by events, rather than persistent\n",
      "services. This gives users the simplicity of A WS Lambda with the full\n",
      "flexibility of a container environment in lieu of the highly restrictive\n",
      "Lambda runtime. And services such as A WS Far gate and Google App\n",
      "Engine run containers without the need to manage a compute cluster as is\n",
      "required for Kubernetes. These services also fully isolate containers,\n",
      "preventing the security issues associated with multi-tenancy .\n",
      "Abstraction will continue working its way across the data stack. Consider\n",
      "the impact of Kubernetes on cluster management. While you can manage\n",
      "your Kubernetes cluster—and many engineering teams do so—even\n",
      "Kubernetes is a managed service. Quite a few commercial SAAS productsuse Kubernetes behind the scenes. What you end up with is essentially a\n",
      "serverless service.\n",
      "When Servers Make Sense\n",
      "Why would you want to run a server instead of using serverless? There are\n",
      "a few reasons. Cost is a big factor . Serverless makes less sense when the\n",
      "usage and cost exceed the constant cost of running and maintaining a server\n",
      "(Figure 3-6 ). However , at a certain scale, the economic benefits of\n",
      "serverless diminish, and running servers becomes more attractive.\n",
      "Figur e 3-6. Cost of serverless vs using a server\n",
      "Customization, power , and control are other major reasons to favor servers\n",
      "over serverless. Some serverless frameworks can be underpowered or\n",
      "limited for certain use cases. Here are some things to consider when using\n",
      "servers, particularly when you’re in the cloud, where server resources are\n",
      "ephemeral:\n",
      "Expect servers to fail.\n",
      "Server failure will happen. A void using a “special snowflake” server\n",
      "that is overly customized and brittle, as this introduces a glaring\n",
      "vulnerability in your architecture. Instead, treat servers as transient\n",
      "resources that you can create as needed and then delete. If yourapplication requires specific code to be installed on the server , use a\n",
      "boot script, or build an image. Deploy code to the server through a\n",
      "CI/CD pipeline.\n",
      "Use clusters and auto-scaling.\n",
      "Take advantage of the cloud’ s ability to grow and shrink compute\n",
      "resources on demand. As your application grows its usage, cluster your\n",
      "application servers, and use auto-scaling capabilities to automatically\n",
      "expand your application as demand grows.\n",
      "Treat your infrastructur e as code.\n",
      "Automation doesn’ t just apply to servers and should extend to your\n",
      "infrastructure whenever possible. Deploy your infrastructure (servers or\n",
      "otherwise) using deployment managers such as T erraform, A WS Cloud\n",
      "Formation, Google Cloud Deployment Manager , and others.\n",
      "Consider using containers.\n",
      "For more sophisticated or heavy-duty workloads where simple startup\n",
      "scripts won’ t cut it, consider using containers and something like\n",
      "Kubernetes.\n",
      "Our Advice\n",
      "Here are some key considerations to help you determine whether serverless\n",
      "is right for you:\n",
      "Workload size and complexity\n",
      "Serverless works best for simple, discrete tasks and workloads. It’ s not\n",
      "as suitable if you have a lot of moving parts, or if you require a lot of\n",
      "compute or memory horsepower . In that case, consider using containers\n",
      "and a container workflow orchestration framework like Kubernetes.\n",
      "Execution fr equency and durationHow many requests per second will your serverless application process?\n",
      "How long will each request take to process? Cloud serverless platforms\n",
      "have limits on execution frequency , concurrency , and duration. If your\n",
      "application can’ t function neatly within these limits, it is time to\n",
      "consider a container -oriented approach.\n",
      "Requests and networking\n",
      "Serverless platforms often utilize some form of simplified networking\n",
      "and don’ t support all cloud virtual networking features, such as VPCs\n",
      "and firewalls.\n",
      "Language\n",
      "What language do you typically use? If it’ s not one of the of ficially\n",
      "supported languages supported by the serverless platform, then you\n",
      "should consider containers instead.\n",
      "Runtime limitations\n",
      "Serverless platforms don’ t give you full operating system abstractions.\n",
      "Instead, you’re limited to a specific runtime image.\n",
      "Cost\n",
      "Serverless functions are extremely convenient, but also inef ficient when\n",
      "handling one event per call. This works fine for low event rates, but\n",
      "costs rise rapidly as the event count increases. This scenario is a\n",
      "frequent source of surprise cloud bills.\n",
      "In the end, abstraction tends to win. W e suggest looking at using serverless\n",
      "first, and servers—with containers and orchestration if possible—only after\n",
      "serverless makes no sense for your use case.\n",
      "Undercurrents and How They Impact\n",
      "Choosing TechnologiesAs we’ve seen in this chapter , there’ s a lot for a data engineer to consider\n",
      "when evaluating technologies. Whatever technology you choose, be sure to\n",
      "understand how it supports the undercurrents of the data engineering\n",
      "lifecycle. Let’ s briefly review them again.\n",
      "Data Management\n",
      "Data management is a broad area, and concerning technologies, it isn’ t\n",
      "always obvious whether a technology adopts data management as a\n",
      "principal concern. For example, a third-party vendor may use data\n",
      "management best practices—such as regulatory compliance, security ,\n",
      "privacy , data quality , and governance—behind the scenes, but expose only a\n",
      "limited UI layer to the customer . In this case, while evaluating the product,\n",
      "it helps to ask the company about their data management practices. Here are\n",
      "some sample questions you should ask:\n",
      "How are you protecting data against breaches, both from the\n",
      "outside and from within?\n",
      "What is your product’ s compliance with GDPR, CCP A, and other\n",
      "data privacy regulations?\n",
      "Do you allow me to host my data to be compliant with these\n",
      "regulations?\n",
      "How do you ensure data quality , and that I’m viewing the right\n",
      "data in your solution?\n",
      "There are many other questions to ask, and these are just a few of the ways\n",
      "to think about data management as it relates to choosing the right\n",
      "technologies. These same questions should also apply to the OSS solutions\n",
      "you’re considering.\n",
      "DataOps\n",
      "Problems will happen. They just will. A server or database may die, a\n",
      "cloud’ s region may have an outage, you might deploy buggy code, bad datamight be introduced into your data warehouse, and any number of\n",
      "unforeseen problems.\n",
      "When evaluating a new technology , how much control do you have over the\n",
      "deployment of new code, how will you be alerted if there’ s a problem, and\n",
      "how are you going to respond when there’ s a problem?\n",
      "The answer to this lar gely depends on the type of technology you’re\n",
      "considering. If the technology is OSS, then you’re likely responsible for\n",
      "setting up monitoring, hosting, and code deployment. How will you handle\n",
      "issues? What’ s your incident response?\n",
      "If you’re using a managed of fering, much of the operations are out of your\n",
      "control. Consider the vendor ’s SLA, how they alert you to issues, and\n",
      "whether they’re transparent about how they’re addressing the issue, an ET A\n",
      "to a fix, etc.\n",
      "Data Architecture\n",
      "As we discussed in Chapter 3, good data architecture means assessing\n",
      "tradeof fs and choosing the best tools for the job, while keeping your\n",
      "decisions reversible. W ith the data landscape morphing at warp speed, the\n",
      "best tool  for the job is a moving tar get. The main goals are to avoid\n",
      "unnecessary lock-in, ensure interoperability across the data stack, and\n",
      "produce high ROI. Choose your technologies accordingly .\n",
      "Orchestration\n",
      "Through most of this chapter , we have actively avoided discussing any\n",
      "particular technology too extensively . We will make an exception for\n",
      "orchestration because the space is currently dominated by one open source\n",
      "technology , Apache Airflow .\n",
      "Maxime Beauchemin kicked of f the Airflow project at Airbnb in 2014.\n",
      "Airflow was developed from the beginning as a non-commercial open-\n",
      "source project. The framework quickly grew significant mindshare outsideAirbnb, becoming an Apache Incubator project in 2016, and a full Apache\n",
      "sponsored project in 2019.\n",
      "At present, Airflow enjoys many advantages, lar gely due to its dominant\n",
      "position in the open-source marketplace. First, the Airflow open source\n",
      "project is extremely active, with a high rate of commits, and a quick\n",
      "response time for bugs and security issues; and the project recently released\n",
      "Airflow 2, a major refactor of the codebase. Second, Airflow enjoys\n",
      "massive mindshare. Airflow has a vibrant, active community on many\n",
      "communications platforms, including Slack, Stack Overflow , and GitHub.\n",
      "Users can easily find answers to questions and problems. Third, Airflow is\n",
      "available commercially as a managed service or software distribution\n",
      "through many vendors, including GCP , AWS, and Astronomer .io.\n",
      "Airflow also has some downsides. Airflow relies on a few core non-scalable\n",
      "components (the scheduler and backend database) that can become\n",
      "bottlenecks for performance, scale, and reliability; the scalable parts of\n",
      "Airflow still follow a distributed monolith pattern. (See Monolith vs.\n",
      "Modular  above.) Finally , Airflow lacks support for many data native\n",
      "constructs, such as schema management, lineage, and cataloging; and it is\n",
      "challenging to develop and test Airflow workflows.\n",
      "We will not attempt an exhaustive discussion of Airflow alternatives here,\n",
      "but just mention a couple of the key orchestration contenders at the time of\n",
      "writing. Prefect and Dagster each aim to solve some of the problems\n",
      "discussed above by rethinking components of the Airflow architecture. W e\n",
      "also mention Datacoral, which intrigues us with its concept of metadata first\n",
      "architecture; Datacoral allows for automated workflow construction through\n",
      "analysis of data flows between queries.\n",
      "We highly recommend that anyone choosing an orchestration technology\n",
      "study the options discussed here. They should also acquaint themselves\n",
      "with activity in the space, as there will almost certainly be new\n",
      "developments by the time you read this.\n",
      "Software EngineeringAs a data engineer , you should strive for simplification and abstraction\n",
      "across the data stack. Buy or use pre-built open source whenever possible.\n",
      "Eliminating undif ferentiated heavy lifting should be your big goal. Focus\n",
      "your resources—custom coding and tooling—on areas that give you a\n",
      "strong competitive advantage. For example, is hand-coding a database\n",
      "connection between MySQL and your cloud data warehouse a competitive\n",
      "advantage to you? Probably not. This is very much a solved problem. Pick\n",
      "an of f-the-shelf solution (open source or managed SAAS), versus writing\n",
      "your own database connector . The world doesn’ t need the millionth +1\n",
      "MySQL to cloud data warehouse connector .\n",
      "On the other hand, why do customers buy from you? Y our business very\n",
      "likely has something special about the way it does things. Maybe it’ s a\n",
      "particular algorithm that powers your fintech platform, or similar . By\n",
      "abstracting away a lot of the redundant workflows and processes, you can\n",
      "continue chipping away , refining, and customizing the things that really\n",
      "move the needle for the business.\n",
      "Conclusion\n",
      "Choosing the right technologies is no easy task, and especially at a time\n",
      "when new technologies and patterns emer ge on a seemingly daily basis,\n",
      "today is possibly the most confusing time in history for evaluating and\n",
      "selecting technologies. Choosing technologies is a balance of use case, cost,\n",
      "build versus buy , and modularization. Always approach technology the\n",
      "same way as architecture—assess trade-of fs and aim for reversible\n",
      "decisions.\n",
      "Now that we’re armed with knowledge, let’ s dive into the first layer of the\n",
      "data engineering lifecycle—source systems, and the data they generate.\n",
      "1 As the authors were working on this chapter in September 2021, C was in position 1 on the\n",
      "TIOBE index. https://www .tiobe.com/tiobe-index/\n",
      "2 Silicon V alley - Making the W orld a Better Place3 https://mattturck.com/data2020/\n",
      "4 https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-\n",
      "repatriation-optimization/\n",
      "5 https://dropbox.tech/infrastructure/evolution-of-dropboxs-edge-network\n",
      "6 https://dropbox.tech/infrastructure/magic-pocket-infrastructure\n",
      "7 https://aws.amazon.com/solutions/case-studies/dropbox-s3/\n",
      "8 https://www .backblaze.com/company/about.html\n",
      "9 https://www .cloudflare.com/what-is-cloudflare/\n",
      "10 http://highscalability .com/blog/2017/12/4/the-eternal-cost-savings-of-netflixs-internal-spot-\n",
      "market.html\n",
      "1 1 https://variety .com/2019/digital/news/netflix-loses-title-top-downstream-bandwidth-\n",
      "application-1203330313/\n",
      "12 https://www .theinformation.com/articles/apples-spending-on-google-cloud-storage-on-track-\n",
      "to-soar -50-this-year\n",
      "13 See https://intoli.com/blog/transcoding-on-aws-lambda/Chapter 4. Ingestion\n",
      "A NOTE FOR EARLY RELEASE READERS\n",
      "With Early Release ebooks, you get books in their earliest form—the\n",
      "authors’ raw and unedited content as they write—so you can take\n",
      "advantage of these technologies long before the of ficial release of these\n",
      "titles.\n",
      "This will be the seventh chapter of the final book.\n",
      "If you have comments about how we might improve the content and/or\n",
      "examples in this book, or if you notice missing material within this\n",
      "chapter , please reach out to the authors at\n",
      "book_feedback@ternarydata.com .\n",
      "You just learned the various source systems you’ll likely encounter as a data\n",
      "engineer , as well as ways to store data. Let’ s now turn our attention to the\n",
      "patterns and choices that apply to ingesting data from a variety of source\n",
      "systems. W e will discuss what data ingestion is, the key engineering\n",
      "considerations for the ingestion phase, the major patterns for both batch and\n",
      "streaming ingestion, technologies you’ll encounter , who you’ll work with as\n",
      "you develop your data ingestion pipeline, and how the undercurrents feature\n",
      "in the ingestion phase (see Figure 4-1 ).Figur e 4-1. Data engineering life cycle\n",
      "What Is Data Ingestion?\n",
      "Data moves from source systems into storage, with ingestion as an\n",
      "intermediate step ( Figure 4-2 ). Data ingestion implies data movement and is\n",
      "a key component of the data engineering lifecycle.\n",
      "Figur e 4-2. Data fr om System 1 is ingested into System 2\n",
      "It’s worth quickly contrasting data ingestion with data integration . Whereas\n",
      "data ingestion is the movement of data from point A to B, data integration\n",
      "combines data from disparate sources into a new dataset. For example, you\n",
      "can use data integration to combine data from a CRM system, advertising\n",
      "analytics data, and web analytics to create a user profile, which is saved to\n",
      "your data warehouse. Furthermore, using reverse ETL, you can send this\n",
      "newly created user profile back  to your CRM so salespeople can use the\n",
      "data for prioritizing leads. Data integration will be described more fully in\n",
      "Chapter 8, where we discuss data transformations; reverse ETL is covered\n",
      "in Chapter 9. W e also point out that data ingestion is dif ferent from internal\n",
      "ingestion  within a system, where data stored in a database is copied from\n",
      "one table to another; we consider this to be another part of the general\n",
      "process of data transformation covered in Chapter 8.DATA PIPELINES DEFINED\n",
      "Data pipelines begin in source systems, but ingestion is the stage where\n",
      "data engineers begin actively designing data pipeline activities. In the\n",
      "data engineering space, there is a good deal of ceremony around\n",
      "different data movement and processing patterns, with older patterns\n",
      "such as ETL (extract, transform, load), newer patterns such EL T\n",
      "(extract, load, transform), and new names for long-established practices\n",
      "(reverse ETL).\n",
      "All of these concepts are encompassed in the idea of a data pipeline.  It\n",
      "is important to understand the details of these various patterns, but also\n",
      "know that a modern data pipeline could encompass all of these. As the\n",
      "world moves away from a traditional monolithic approach with very\n",
      "rigid constraints on data movement, towards an ecosystem of cloud\n",
      "services that are assembled like lego bricks to realize that products, data\n",
      "engineers prioritize using the right tools to accomplish the desired\n",
      "outcome rather than adhering to a narrow philosophy of data\n",
      "movement.\n",
      "In general, here’ s our definition of a data pipeline:\n",
      "A data pipeline is the combination of ar chitectur e, systems, and\n",
      "processes that move data thr ough the stages of the data engineering\n",
      "lifecycle.\n",
      "Our definition is deliberately fluid—and intentionally vague—to allow\n",
      "data engineers to plug in whatever they need to accomplish the task at\n",
      "hand. A data pipeline could be a traditional ETL system, where data is\n",
      "ingested from an on-premises transactional system, passed through a\n",
      "monolithic processor , and written into a data warehouse. Or it could be\n",
      "a cloud-based data pipeline that pulls data from 100 dif ferent sources,\n",
      "combines data into 20 wide tables, trains five dif ferent machine\n",
      "learning models, deploys them into production, and monitors ongoing\n",
      "performance. A data pipeline should be flexible enough to fit any needs\n",
      "along the data engineering lifecycle.Let’s keep this notion of data pipelines in mind as we proceed through\n",
      "the ingestion chapter .\n",
      "Key Engineering Considerations for the\n",
      "Ingestion Phase\n",
      "When preparing to architect or build an ingestion system, here are some\n",
      "primary considerations and questions to ask yourself related to data\n",
      "ingestion:\n",
      "What’ s the use case for the data I’m ingesting?\n",
      "Can I reuse this data and avoid ingesting multiple versions of the\n",
      "same dataset?\n",
      "Where is the data going? What’ s the destination?\n",
      "Frequency: How often should the data be updated from the source?\n",
      "What is the expected data volume?\n",
      "What format is the data in? Can downstream storage and\n",
      "transformation accept this format?\n",
      "Is the source data in good shape for immediate downstream use?\n",
      "That is, is the data of good quality? What post-processing is\n",
      "required to serve it? What are data quality risks? (e.g. could bot\n",
      "traffic to a website contaminate the data?)\n",
      "If the data is from a streaming source, does it require in-flight\n",
      "processing for downstream ingestion?\n",
      "These questions undercut both batch and streaming ingestion and apply to\n",
      "the underlying architecture you’ll create, build, and maintain. Regardless of\n",
      "how often the data is ingested, you’ll want to consider these factors when\n",
      "designing your ingestion architecture:Bounded versus unbounded\n",
      "Frequency\n",
      "Synchronous versus asynchronous\n",
      "Serialization and deserialization\n",
      "Throughput and elastic scalability\n",
      "Reliability and durability\n",
      "Payload\n",
      "Push versus pull patterns\n",
      "Let’s look at each of these.\n",
      "Bounded V ersus Unbounded\n",
      "As you might recall from Chapter 3, bounded and unbounded data\n",
      "(Figure 4-3 ), data comes in two forms—bounded and unbounded.\n",
      "Unbounded data is data as it exists in reality , where events happen when\n",
      "they happen, either sporadically or continuous, ongoing and flowing.\n",
      "Bounded data is a convenient way of bucketing data across some sort of\n",
      "boundary , such as time.Figur e 4-3. Bounded vs unbounded data\n",
      "Let us adopt this mantra:\n",
      "All data is unbounded until we bound it.\n",
      "Like many mantras, this one is not precisely true 100% of the time; the\n",
      "grocery list that I scribbled this afternoon is truly bounded data. However ,\n",
      "the idea is correct for practical purposes for the vast majority of data that\n",
      "we handle in a business context. That is, an online retailer will process\n",
      "customer transactions 24 hours a day until the business fails, the economy\n",
      "grinds to a halt, or the sun explodes.\n",
      "Business processes have long imposed artificial bounds on data by cutting\n",
      "discrete batches, but always keep in mind the true unboundedness of your\n",
      "data; streaming ingestion systems are simply a tool for preserving the\n",
      "unbounded nature of data so that subsequent steps in the lifecycle can also\n",
      "process it continuously .\n",
      "Frequency\n",
      "One of the key decisions the data engineers must make in designing data\n",
      "ingestion processes is the data ingestion frequency . Ingestion processes can\n",
      "be a batch, micro-batch, or real-time.\n",
      "Ingestion frequencies vary dramatically , on the spectrum from slow to fast\n",
      "(Figure 4-4 ). On the slow end, a business might ship its tax data to an\n",
      "accounting firm once a year . On the faster side, a change data capture\n",
      "system could retrieve new log updates from a source database once a\n",
      "minute. Even faster , a system might ingest events from IoT sensors\n",
      "continuously , and process these within seconds. In a company , data\n",
      "ingestion frequencies are often mixed, depending on the use case and\n",
      "technologies involved.Figur e 4-4. The spectrum of slow to fast ingestion, fr om batch to r eal-time\n",
      "We note that “real-time” ingestion patterns are becoming increasingly\n",
      "common. W e initially put “real-time” in quotes because no ingestion system\n",
      "is truly real-time. Any database, queue, pipeline, etc. has some inherent\n",
      "latency in delivering data to a tar get system. It is more accurate to speak of\n",
      "near r eal-time , but we often use the term real-time for the sake of brevity .\n",
      "The near real-time pattern generally does away with an explicit update\n",
      "frequency; events are processed in the pipeline either one by one as they\n",
      "arrive or in micro-batches (i.e. batches over very short time intervals). For\n",
      "this book, we will use real-time and streaming interchangeably .\n",
      "Even with a streaming data ingestion process in place, batch processing\n",
      "downstream is quite common. At the time of this writing, machine learning\n",
      "models are typically trained on a batch basis, although continuous online\n",
      "training is becoming more prevalent. Rarely do data engineers have the\n",
      "option to build a purely near real-time pipeline with no batch components.\n",
      "Instead, they choose where batch boundaries will occur , i.e., wherein the\n",
      "data engineering lifecycle data will be broken into batches. Once data\n",
      "reaches a batch process, the batch frequency becomes a bottleneck for all\n",
      "downstream processing; you move at the cadence of how the batch moves.\n",
      "We also note that streaming systems are an or ganic best fit for many\n",
      "modern data source types. In IoT applications, the common pattern is for\n",
      "each sensor to write events or measurements as they happen. While this\n",
      "data can be written into a database, a streaming ingestion platform such asAmazon Kinesis or Apache Kafka is a better fit for the application.\n",
      "Software applications can adopt similar patterns, by writing events to a\n",
      "message queue as they happen rather than waiting for an extraction process\n",
      "to pull events and state information from a backend database. This pattern\n",
      "works extremely well for event-driven architectures that are already\n",
      "exchanging messages through queues. And again, streaming architectures\n",
      "generally coexist with batch processing.\n",
      "Synchronous V ersus Asynchronous Ingestion\n",
      "Ingestion systems can have a series of dependent steps (synchronous\n",
      "systems), or operate without any dependencies (asynchronous systems).\n",
      "When we discuss dependencies in this context, we’re describing whether\n",
      "the completion of one step prevents downstream steps from starting.\n",
      "With synchronous ingestion, the source, ingestion, and destination have\n",
      "hard dependencies and are tightly coupled. As you can see in Figure 4-5 ,\n",
      "each stage of the data engineering lifecycle has processes A, B, and C that\n",
      "are directly dependent upon each other . If Process A fails, Processes B and\n",
      "C cannot start. This type of synchronous workflow is common in older ETL\n",
      "systems where data extracted from a source system must then be\n",
      "transformed before being loaded into a data warehouse. If the ingestion or\n",
      "transformation process fails for any reason, the entire process must be\n",
      "replayed until it’ s successful. W e’ve seen instances where the\n",
      "transformation process itself is a series of dozens of synchronous\n",
      "workflows, sometimes taking over 24 hours to finish. If any step of that\n",
      "transformation workflow failed (and it occasionally would fail), the entire\n",
      "transformation process needed to be restarted from the beginning!Figur e 4-5. Synchr onous data ingestion\n",
      "With asynchronous ingestion, dependencies can now operate at the level of\n",
      "individual events, much as they would in a software backend built from\n",
      "microservices ( Figure 4-6 ). For example, take the example of a web\n",
      "application that emits events into an Amazon Kinesis Data Stream (here\n",
      "acting as a buf fer); the stream is read by Apache Beam, which parses and\n",
      "enriches events, then forwards them to a second Kinesis Stream; Kinesis\n",
      "Firehose rolls up events and writes objects to Amazon S3.\n",
      "The big idea is that rather than relying on asynchronous processing, where a\n",
      "batch process runs for each stage as the input batch closes and certain time\n",
      "conditions are met, each stage of the asynchronous pipeline can process\n",
      "data items as they become available in parallel across the Beam cluster . The\n",
      "processing rate depends on available resources. The Kinesis Data Stream\n",
      "acts as the shock absorber , moderating the load so that event rate spikes will\n",
      "not overwhelm downstream processing. When the event rate is low and any\n",
      "backlog has cleared, events will move through the pipeline very quickly .\n",
      "Note that we could modify the scenario and use a Kinesis Data Stream for\n",
      "storage, eventually extracting events to S3 before they expire out of the\n",
      "stream.Figur e 4-6. Asynchr onous data ingestion\n",
      "Serialization and Deserialization\n",
      "Moving data from source to sink involves serialization and deserialization.\n",
      "Serialization means encoding the data from a source and preparing datastructures for transmission and intermediate storage stages. (See the more\n",
      "extensive discussion of serialization in the appendix on Serialization and\n",
      "Compr ession. )\n",
      "Throughput and Scalability\n",
      "In theory , your ingestion should never be a bottleneck. In practice, this is\n",
      "easier said than done. As your data volumes grow and requirements change,\n",
      "data throughput and system scalability become extremely critical. Design\n",
      "your systems to flexibly scale and shrink to match the desired data\n",
      "throughput.\n",
      "Monitoring is key , as well as knowledge of the behavior of the upstream\n",
      "systems you depend upon and how they generate data. Y ou should be aware\n",
      "of the number of events generated per time interval you’re concerned with\n",
      "(events/minute, events/second, and so on), as well as the average size of\n",
      "each event. Y our data pipeline should be able to handle both the frequency\n",
      "and size of the events you’re ingesting.\n",
      "Where you’re ingesting data from matters a lot. If you’re receiving data as\n",
      "it’s generated, will the upstream system have any issues that might impact\n",
      "your downstream ingestion pipelines? For example, suppose a source\n",
      "database goes down. When it comes back online and attempts to backfill the\n",
      "lapsed data loads, will your ingestion be able to keep up with this sudden\n",
      "influx of backlogged data?\n",
      "Another thing to consider is your ability to handle bursty data ingestion.\n",
      "Data generation is very rarely done in a constant fashion, and often ebbs\n",
      "and flows. Built-in buf fering is required to collect events during rate spikes\n",
      "to prevent data from getting lost. Even in a dynamically scalable system,\n",
      "buffering bridges the time while the system scales. Buf fering also allows\n",
      "storage systems to accommodate bursts.\n",
      "These days, it’ s worth using managed services that handle the throughput\n",
      "scaling for you. While you can manually accomplish these tasks by adding\n",
      "more servers, shards, or workers, this isn’ t necessarily value add work.Much of this heavy lifting is now automated. Don’ t reinvent the data\n",
      "ingestion wheel if you don’ t have to.\n",
      "Reliability and Durability\n",
      "Reliability and durability are especially important in the ingestion stages of\n",
      "data pipelines. Reliability  entails high uptime and appropriate failover for\n",
      "ingestion systems. Durability entails making sure that data isn’ t lost or\n",
      "corrupted.\n",
      "Some data sources (e.g., IoT devices) may not retain data if it is not\n",
      "correctly ingested. Once lost, it is gone for good. In this sense, the\n",
      "reliability  of ingestion systems leads directly to the durability  of generated\n",
      "data. If data is ingested, downstream processes can theoretically run late if\n",
      "they break temporarily .\n",
      "Our advice is to evaluate the risks and build an appropriate level of\n",
      "redundancy and self-healing based on the impact and cost of losing data.\n",
      "Will your ingestion process continue if an A WS zone goes down? How\n",
      "about a whole region? How about the power grid or the internet? Reliability\n",
      "and durability have both direct and indirect costs: building a highly\n",
      "redundant system can entail big cloud bills while keeping a team on call 24\n",
      "hours a day takes a toll on your team and resources.\n",
      "Don’ t assume that you can build a system that will reliably and durably\n",
      "ingest data in every possible scenario. Even the massive budget of the US\n",
      "federal government can’ t guarantee this. In many extreme scenarios,\n",
      "ingesting data actually won’ t matter . For example, if the internet goes down,\n",
      "there will be little to ingest even if you build multiple data centers in\n",
      "under ground bunkers with independent power . Always evaluate the\n",
      "tradeof fs and costs of reliability and durability .\n",
      "Payload\n",
      "The payload  is the dataset you’re ingesting and has characteristics such as\n",
      "kind, shape, size, schema and data types, and metadata. Let’ s look at some\n",
      "of these characteristics to get an idea of why this matters.Kind\n",
      "The kind of data you handle directly impacts how it’ s handled downstream\n",
      "in the data engineering lifecycle. Kind consists of type and format. Data has\n",
      "a type—tabular , image, video, text, etc. The type directly influences the\n",
      "format of the data, or how it is expressed in bytes, name, and file extension.\n",
      "For example, a tabular kind of data may be in formats such as CSV or\n",
      "Parquet, with each of these formats having dif ferent byte patterns for\n",
      "serialization and deserialization. Another kind of data is an image, which\n",
      "has a format of JPG or PNG and is inherently binary and unstructured.\n",
      "Shape\n",
      "Every payload has a shape  that describes its dimensions. Data shape is\n",
      "critical across the data engineering lifecycle. For instance, an image’ s pixel\n",
      "and RGB dimensions are necessary for deep learning applications. As\n",
      "another example, if you’re trying to import a CSV file into a database table,\n",
      "and your CSV has more columns than the database table, you’ll likely get\n",
      "an error during the import process. Here are some examples of the shapes of\n",
      "different kinds of data:\n",
      "Tabular\n",
      "The number of rows and columns in the dataset, commonly expressed\n",
      "as M rows and N columns\n",
      "Semi-structur ed JSON\n",
      "The key-value pairs, and depth of nesting that occurs with sub-elements\n",
      "Unstructur ed text\n",
      "Number of words, characters, or bytes in the text body\n",
      "Images\n",
      "The width, height, and RGB color depth (e.g., 8 bits per pixel)\n",
      "Uncompr essed AudioNumber of channels (e.g., two for stereo), sample depth (e.g., 16 bits\n",
      "per sample), sample rate (e.g., 48 kHz), and length (e.g., 10003 seconds)\n",
      "Size\n",
      "The size of the data describes the number of bytes of a payload. A payload\n",
      "may range in size from single bytes to terabytes, and lar ger. To reduce the\n",
      "size of a payload, it may be compressed into a variety of formats such as\n",
      "ZIP, TAR, and so on (See the discussion of compression in the appendix on\n",
      "Serialization and Compr ession) .\n",
      "Schema and data types\n",
      "Many data payloads have a schema, such as tabular and semi-structured\n",
      "data. As we’ve mentioned earlier in this book, a schema describes the fields\n",
      "and types of data that reside within those fields. Other types of data such as\n",
      "unstructured text, images, and audio will not have an explicit schema or\n",
      "data types, though they might come with technical file descriptions on\n",
      "shape, data and file format, encoding, size, etc.\n",
      "You can connect to databases in a variety of ways, i.e. file export, change\n",
      "data capture, JDBC/ODBC, etc. The connection is the easy part of the\n",
      "process. The great engineering challenge is understanding the underlying\n",
      "schema. Applications or ganize data in a variety of ways, and engineers need\n",
      "to be intimately familiar with the or ganization of the data and relevant\n",
      "update patterns to make sense of it. The problem has been somewhat\n",
      "exacerbated by the popularity of ORM (object-relational mapping), which\n",
      "automatically generates schemas based on object structure in languages\n",
      "such as Java or Python. Structures that are natural in an object-oriented\n",
      "language often map to something messy in an operational database. Data\n",
      "engineers may also need to familiarize themselves with the class structure\n",
      "of application code for this reason.\n",
      "Schema is not only for databases. As we’ve discussed, APIs present their\n",
      "schema complications. Many vendor APIs have nice reporting methods that\n",
      "prepare data for analytics. In other cases, engineers are not so lucky and the\n",
      "API is a thin wrapper around underlying systems, requiring engineers togain a deep understanding of application internals to use the data. This\n",
      "situation is more challenging than dealing with complex data internal to an\n",
      "organization because communication is more dif ficult.\n",
      "Much of the work associated with ingesting from source schemas happens\n",
      "in the transformation stage of the data engineering lifecycle, which we\n",
      "discuss in Chapter 8. W e’ve placed this discussion here because data\n",
      "engineers need to begin studying source schemas as soon they plan to ingest\n",
      "data from a new source.\n",
      "Communication is critical for understanding source data, and engineers also\n",
      "have the opportunity to reverse the flow of communication and help\n",
      "software engineers improve data where it is produced. W e’ll return to this\n",
      "topic later in this chapter , in the section on Who you’ll work with .\n",
      "Detecting and handling schema changes in upstream and\n",
      "downstream systems\n",
      "Schema changes occur frequently in source systems and often are well out\n",
      "of the control of data engineers.\n",
      "Examples of schema changes include the following:\n",
      "Adding a new column\n",
      "Changing a column type\n",
      "Creating a new table\n",
      "Renaming a column\n",
      "It’s becoming increasingly common for ingestion tools to automate the\n",
      "detection of schema changes, and even auto-update tar get tables.\n",
      "Ultimately , this is something of a mixed blessing. Schema changes can still\n",
      "break pipelines downstream of staging and ingestion.\n",
      "Engineers must still implement strategies to automatically respond to\n",
      "changes, and alert on changes that cannot be accommodated automatically .\n",
      "Automation is great, but the analysts and data scientists who rely on this\n",
      "data should be informed of the schema changes that violate existingassumptions. Even if automation can accommodate a change, the new\n",
      "schema may adversely af fect the performance of reports and models.\n",
      "Communication between those making schema changes and those impacted\n",
      "by these changes is as important as reliable automation that checks for\n",
      "schema changes.\n",
      "Schema registries\n",
      "In streaming data, every message has a schema, and these schemas may\n",
      "evolve between producers and consumers. A schema registry is a metadata\n",
      "repository used to maintain schema and data type integrity in the face of\n",
      "constantly evolving schemas. It essentially describes the data model for\n",
      "messages, allowing consistent serialization and deserialization between\n",
      "producers and consumers. Schema registries are used in Kafka, A WS Glue,\n",
      "and others.\n",
      "Metadata\n",
      "In addition to the obvious characteristics we’ve just covered, a payload\n",
      "often contains metadata, which we first discussed in Chapter 2. Metadata is\n",
      "data about data . Metadata can be as critical as the data itself. One of the\n",
      "major limitations of the early approach to the data lake—or data swamp,\n",
      "which could turn it into a data superfund site—was a complete lack of\n",
      "attention to metadata. W ithout a detailed description of the data, the data\n",
      "itself may be of little value. W e’ve already discussed some types of\n",
      "metadata (e.g. schema) and will address them many times throughout this\n",
      "chapter .\n",
      "Push V ersus Pull Patterns\n",
      "We introduced the concept of push vs pull when we introduced the data\n",
      "engineering lifecycle in Chapter 2. Roughly speaking, a push  strategy\n",
      "(Figure 4-7 ) involves a source system sending data to a tar get, while a pull\n",
      "strategy ( Figure 4-8 ) entails a tar get reading data directly from a source. As\n",
      "we mentioned in that discussion, the lines between these strategies are\n",
      "blurry .Figur e 4-7. Pushing data fr om sour ce to destination\n",
      "Figur e 4-8. Pushing data fr om sour ce to destination\n",
      "We will discuss push  versus pull in each subsection on ingestion patterns,\n",
      "and give our opinionated reasoning on when a pattern is push or pull. Let’s\n",
      "dive in!\n",
      "Batch Ingestion Patterns\n",
      "It is often convenient to ingest data in batches. This means that data is\n",
      "ingested by either taking a subset of data from a source system, based either\n",
      "on a time interval or size of accumulated data ( Figure 4-9 ).Figur e 4-9. Time interval batch ingestion\n",
      "Time interval batch ingestion is extremely common in traditional business\n",
      "ETL for data warehousing. This pattern is often used to process data once aday overnight during of f-hours to provide daily reporting, but other\n",
      "frequencies can also be used.\n",
      "Size-based batch ingestion ( Figure 4-10 ) is quite common when data is\n",
      "moved from a streaming-based system into object storage—ultimately , the\n",
      "data must be cut into discrete blocks for future processing in a data lake.\n",
      "Size-based ingestion systems such as Kinesis Firehose can break data into\n",
      "objects based on a variety of criteria, such as size bytes of the total number\n",
      "of events.Figur e 4-10. Size-based batch ingestion\n",
      "Some commonly used batch ingestion patterns, which we’ll discuss in this\n",
      "section, include:Snapshot or dif ferential extraction\n",
      "File-based export and ingestion\n",
      "ETL versus EL T\n",
      "Data migration\n",
      "Snapshot or Differential Extraction\n",
      "Data engineers must choose whether to capture full snapshots of a source\n",
      "system or dif ferential updates. W ith full snapshots, engineers grab the full\n",
      "current state of the source system on each update read. W ith the dif ferential\n",
      "update pattern, engineers can pull only the updates and changes since the\n",
      "last read from the source system.\n",
      "While dif ferential updates are ideal for minimizing network traf fic, tar get\n",
      "storage usage, etc., full snapshot reads remain extremely common due to\n",
      "their simplicity .\n",
      "File-Based Export and Ingestion\n",
      "Data is quite often moved between databases and systems using files. That\n",
      "is, data is serialized into files in an exchangeable format, and these files are\n",
      "provided to an ingestion system. W e consider file-based export to be a\n",
      "push-based  ingest pattern. This is because the work of data export and\n",
      "preparation is done on the source system side.\n",
      "File-based ingestion has several potential advantages over a direct database\n",
      "connection approach. For security reasons, it is often undesirable to allow\n",
      "any direct access to backend systems. W ith file-based ingestion, export\n",
      "processes are run on the data source side. This gives source system\n",
      "engineers full control over what data gets exported and how the data is pre-\n",
      "processed. Once files are complete, they can be provided to the tar get\n",
      "system in a variety of ways. Common file exchange methods are object\n",
      "storage (i.e. Amazon S3, Azure Blob Storage), SFTP , EDI, or SCP .ETL V ersus EL T\n",
      "In Chapter 3, we introduced ETL and EL T, which are both extremely\n",
      "common ingest, storage, and transformation patterns you’ll encounter in\n",
      "batch workloads. For this chapter , we’ll cover the Extract (E) and the Load\n",
      "(LO) parts of ETL and EL T; transformations will be covered in Chapter 8:\n",
      "Extract\n",
      "Extract means getting data from a source system. While extract  seems\n",
      "to imply pulling  data, it can also be push-based. Extraction may also\n",
      "entail reading metadata and schema changes.\n",
      "Load\n",
      "Once data is extracted, it can either be transformed (ETL) before\n",
      "loading it into a storage destination or simply loaded into storage for\n",
      "future transformation. When loading data, you should be mindful of the\n",
      "type of system into which you’re loading, the schema of the data, and\n",
      "the performance impact of loading.\n",
      "Transform\n",
      "We’ll cover transformations in much more detail in Chapter 8. Know\n",
      "that data can be transformed after it’ s been ingested, but before it’ s\n",
      "loaded into storage. This is common in classic ETL systems that do in-\n",
      "memory transformations as part of a workflow . It’s also common with\n",
      "event streaming frameworks such as Kafka, which uses the Kafka\n",
      "Stream API to transform and join data before persisting the output to\n",
      "storage.\n",
      "Inserts, Updates, and Batch Size\n",
      "Batch-oriented systems often perform poorly when users attempt to perform\n",
      "a large number of small-batch operations rather than a smaller number of\n",
      "large operations. For example, while it is a common pattern to insert one\n",
      "row at a time in a transactional database, this is a bad pattern for many\n",
      "columnar databases as it forces the creation of many small, suboptimal files,and forces the system to run a high number of create object operations.\n",
      "Running a lar ge number of small in-place update operations is an even\n",
      "bigger problem because it forces the database to scan each existing column\n",
      "file to run the update.\n",
      "Understanding the appropriate update patterns for the database you’re\n",
      "working with. Also, understand that certain technologies are purpose-built\n",
      "for high insert rates. Systems such as Apache Druid and Pinot can handle\n",
      "high insert rates. SingleStore can manage hybrid workloads that combine\n",
      "OLAP and OL TP characteristics. BigQuery performs poorly on a high rate\n",
      "of standard inserts, but extremely well if data is fed in through its stream\n",
      "buffer. Know the limits and characteristics of your tools.\n",
      "Data Migration\n",
      "Migrating data to a new database or a new environment is not usually\n",
      "trivial, and data needs to be moved in bulk. Sometimes this means moving\n",
      "data sizes that are 100s of TBs or much lar ger, often involving not just the\n",
      "migration of specific tables, but moving entire databases and systems.\n",
      "Data migrations probably aren’ t a regular occurrence in your role as a data\n",
      "engineer , but it’ s something you should be familiar with. As is so often the\n",
      "case for data ingestion, schema management is a key consideration. If\n",
      "you’re migrating data from one database system to another , say T eradata to\n",
      "BigQuery , no matter how closely the two databases resemble each other ,\n",
      "there are nearly always subtle dif ferences in how they handle schema.\n",
      "Fortunately , it is generally easy to test ingestion of a sample of data and find\n",
      "schema issues before undertaking a full table migration.\n",
      "Most modern data systems perform best when data is moved in bulk rather\n",
      "than as individual rows or events. File or object storage is often a good\n",
      "intermediate stage for moving data. Also, one of the biggest challenges of\n",
      "database migration is not the movement of the data itself, but the movement\n",
      "of data pipeline connections from the old system to the new one.Streaming Ingestion Patterns\n",
      "Another way to ingest data is from a stream. This means that data is\n",
      "ingested continuously . Let’ s look at some patterns for ingesting streaming\n",
      "data.\n",
      "Types of T ime\n",
      "While time is an important consideration for all data ingestion, it becomes\n",
      "that much more critical and subtle in the context of streaming, where we\n",
      "view data as continuous and expect to consume it shortly after it is\n",
      "produced. Let’ s look at the key types of time you’ll run into when ingesting\n",
      "data—the time the event is generated, and when it’ s ingested and processed\n",
      "(Figure 4-1 1):\n",
      "Figur e 4-1 1. Event, ingestion, and pr ocess time\n",
      "Event time\n",
      "Event time is the time at which an event is generated in a source system,\n",
      "including the timestamp of the original event itself. Upon event\n",
      "creation, there will be an undetermined time lag before the event is\n",
      "ingested and processed downstream. Always include timestamps for\n",
      "each phase through which an event travels. Log events as they occur ,\n",
      "and at each stage of time—when they’re created, ingested, and\n",
      "processed. Use these timestamp logs to accurately track the movement\n",
      "of your data through your data pipelines.Ingestion time\n",
      "After data is created, it is ingested somewhere. Ingestion time is the\n",
      "time at which an event is ingested from source systems, into a message\n",
      "queue, cache, memory , object storage, a database, or any place else that\n",
      "data is stored (see Chapter 6). After ingestion, data may be processed\n",
      "immediately , within minutes, hours, or days, or simply persist in storage\n",
      "indefinitely . We will cover the details of storage in Chapter 8.\n",
      "Processing time\n",
      "Processing time is the time at which data is processed, which is\n",
      "typically some sort of transformation. Y ou’ll learn more about various\n",
      "kinds of processing and transformations in Chapter 8.\n",
      "Late-arriving data\n",
      "A group of events might occur around the same time frame, but because\n",
      "of various circumstances, might be late in arriving for ingestion. This is\n",
      "called late-arriving data and is common when ingesting data. Y ou\n",
      "should be aware of late-arriving data, and the impact on downstream\n",
      "systems and uses. For example, if you assume that ingestion or\n",
      "processing time is the same as the event time, you may get some very\n",
      "strange results if your reports or analysis depend upon an accurate\n",
      "portrayal of when events occur .\n",
      "Key Ideas\n",
      "We spend a good deal of time on stream processing in Chapter 8, where we\n",
      "discuss data transformation. W e’ll quickly introduce key streaming\n",
      "ingestion ideas here and present a more extensive discussion there:\n",
      "Streaming ingestion and storage systems\n",
      "Streaming storage collects messages, log entries, or events and makes\n",
      "them available for downstream processing. T ypical examples are\n",
      "Apache Kafka, Amazon Kinesis Data Streams, or Google CloudPub/Sub. Modern streaming storage systems support basic\n",
      "producer/consumer patterns (publisher/subscriber), but also support\n",
      "replay , i.e., the ability to playback a time range of historic data.\n",
      "Producers and consumers\n",
      "Producers write data into streaming storage, while consumers read from\n",
      "the stream. In practice, a streaming pipeline may have many consumers\n",
      "and producers at various stages. The initial producer is usually the data\n",
      "source itself, whether a web application that writes events into the\n",
      "stream or a swarm of IoT devices.\n",
      "Clusters and partition\n",
      "In general, modern steaming systems distribute data across clusters. In\n",
      "many cases, data engineers have some control of how the data is\n",
      "partitioned across the cluster through the use of a partition key .\n",
      "Choice of ingestion partition key can be critical in preparing data for\n",
      "downstream consumption, whether for preprocessing that is a key part\n",
      "of the ingestion stage or more complex downstream processing. Often,\n",
      "we set upstream processing nodes to align with partitions in the stream\n",
      "messaging system.\n",
      "Topics\n",
      "A topic is simply a data stream; a streaming storage system can support\n",
      "a large number of separate topics (streams) simultaneously , just as a\n",
      "relational database supports many tables.\n",
      "Streaming Change Data Capture\n",
      "While there are batch versions of change data capture (CDC), we are\n",
      "primarily interested in streaming change data capture. The primary\n",
      "approaches are CDC by logging, wherein each writes to the database is\n",
      "recorded in logs and read for data extraction, and the database trigger\n",
      "pattern, where the database sends some kind of signal to another systemeach time it makes a change to a table. For streaming ingestion, we assume\n",
      "that the CDC process writes into some kind of streaming storage system for\n",
      "downstream processing.\n",
      "Real-time and Micro-batch: Considerations for\n",
      "Downstream Destinations\n",
      "While streaming ingestion generally happens in a stream storage system,\n",
      "the preprocessing  part of ingestion (e.g. data parsing) can happen in either a\n",
      "true stream processor or a micro-batch system. Micro-batch processing is\n",
      "essentially high-speed batch processing, where batches might happen every\n",
      "few seconds.\n",
      "Apache Spark often processes data in micro-batches, though there are\n",
      "newer more continuous modes as well. The micro-batch approach is\n",
      "perfectly suitable for many applications. On the other hand, if you want to\n",
      "achieve operational monitoring that is much closer to real-time, a\n",
      "continuous approach might be more appropriate.\n",
      "Ingestion Technologies\n",
      "Now that we’ve described some of the major patterns that underlie\n",
      "ingestion in general, we turn our attention to the technologies you’ll use for\n",
      "data ingestion. W e’ll give you a sample of the types of ingestion\n",
      "technologies you’ll encounter as a data engineer . Keep in mind the universe\n",
      "of data ingestion technologies is vast and growing daily . Although we will\n",
      "cite common and popular examples, it is not our intention to provide an\n",
      "exhaustive list of technologies and vendors, especially given how fast the\n",
      "discipline is changing.\n",
      "Batch Ingestion T echnologies\n",
      "To choose appropriate technologies, you must understand your data sources\n",
      "(see Chapter 5). In addition, you need to choose between full replication\n",
      "and change tracking. W ith full replication, we simply drop the old data inthe tar get and fully reload from the source. Change tracking takes many\n",
      "forms, but often it entails pulling rows based on update timestamp and\n",
      "merging these changes into the tar get.\n",
      "In addition, preprocessing should be considered a part of ingestion.\n",
      "Preprocessing is data processing that happens before the data is truly\n",
      "considered to be ingested. In streaming pipelines, raw events often arrive in\n",
      "a rather rough form directly from a source application and must be parsed\n",
      "and enriched before they can be considered fully ingested. In batch\n",
      "processing scenarios, similar considerations apply , with data often arriving\n",
      "as raw string data, then getting parsed into correct types.\n",
      "Also, think about FinOps and cost management early in the design of your\n",
      "ingestion architecture. FinOps entails designing systems and human\n",
      "processes to make costs manageable. Think about the implications of\n",
      "scaling up your solution as data scales, and design for cost ef ficiency . For\n",
      "example, instead of spinning up full-priced on-demand EC2 instances in\n",
      "AWS, instead, build in support for EC2 spot instances where this applies.\n",
      "Ensure that costs are visible and monitored.\n",
      "Direct Database Connection\n",
      "Data can be pulled from databases for ingestion by querying and reading\n",
      "over a network connection. Most commonly , this connection is made using\n",
      "ODBC or JDBC.\n",
      "ODBC 1.0 was released in 1992. ODBC (Open Database Connectivity)\n",
      "uses a driver hosted by a client accessing the database to translate\n",
      "commands issued to the standard ODBC API into commands issued to the\n",
      "database. The database returns query results over the wire, where they are\n",
      "received by the driver and translated back into a standard form, and read by\n",
      "the client.\n",
      "For purposes of ingestion, the application utilizing the ODBC driver is an\n",
      "ingestion tool. The ingestion tool may pull data through many small queries\n",
      "or a single lar ge query . The ingestion tool might pull data once a day or\n",
      "once every five minutes.JDBC (Java Database Connectivity) was released as a standard by Sun\n",
      "Microsystems in 1997. JDBC is conceptually extremely similar to ODBC; a\n",
      "Java driver connects to a remote database and serves as a translation layer\n",
      "between the standard JDBC API and the native network interface of the\n",
      "target database. It might seem a bit strange to have a database API\n",
      "dedicated to a single programming language, but there are strong\n",
      "motivations for this. The JVM (Java V irtual Machine) is standard, portable\n",
      "across hardware architectures and operating systems, and provides the\n",
      "performance of compiled code through a JIT (just in time compiler). The\n",
      "JVM is far and away from the most popular compiling virtual machine for\n",
      "running code in a portable manner .\n",
      "JDBC provides extraordinary database driver portability . ODBC drivers are\n",
      "shipped as OS and architecture native binaries; database vendors must\n",
      "maintain versions for each architecture/OS version that they wish to\n",
      "support. On the other hand, vendors can ship a single JDBC driver that is\n",
      "compatible with any JVM language (Java, Scala, Clojure, Kotlin, etc.) and\n",
      "JVM data framework (i.e Spark.) JDBC has become so popular that it is\n",
      "also used as an interface for non-JVM languages such as Python. The\n",
      "Python ecosystem provides translation tools that allow Python code to talk\n",
      "to a JDBC driver running on a local JVM.\n",
      "Returning to the general concept of direct database connections, both JDBC\n",
      "and ODBC are used extensively for data ingestion from relational\n",
      "databases. V arious enhancements are used to accelerate data ingestion.\n",
      "Many data frameworks can parallelize several simultaneous connections\n",
      "and partition queries to pull data in parallel. On the other hand, nothing is\n",
      "free—using parallel connections also increases the load on the source\n",
      "database.\n",
      "JDBC and ODBC were long the gold standards for data ingestion from\n",
      "databases. However , these connection standards are beginning to show their\n",
      "age for many data engineering applications. These connection standards\n",
      "struggle with nested data, and they send data as rows. This means that\n",
      "native nested data has to be re-encoded as string data to be sent over thewire, and columns from columnar databases must be re-serialized as rows.\n",
      "Many alternatives have emer ged for lower friction data export.\n",
      "As discussed in the section on file-based export, many databases now\n",
      "support native file export that bypasses JDBC/ODBC and exports data\n",
      "directly in modern formats. Alternatively , many databases—Snowflake,\n",
      "BigQuery , ArangoDB, etc.—provide direct REST APIs.\n",
      "JDBC connections should generally be integrated with other ingestion\n",
      "technologies. For example, we commonly use a reader process to connect to\n",
      "a database with JDBC, write the extracted data into multiple objects, then\n",
      "orchestrate ingestion into a downstream system (see Figure 4-12 ). The\n",
      "reader process can run in a fully ephemeral cloud instance or directly in an\n",
      "orchestration system.Figur e 4-12. An ingestion pr ocess r eads fr om a sour ce database using JDBC, then writes objects into\n",
      "object storage. A tar get database (not shown) can be trigger ed to ingest the data with an API call\n",
      "from an or chestration system.\n",
      "SFTP\n",
      "Engineers rightfully cringe at the mention of SFTP (occasionally , we even\n",
      "hear instances of FTP being used in production.) Regardless, SFTP is still a\n",
      "practical reality for many businesses. That is, they work with partner\n",
      "businesses that either consume or provide data using SFTP , and are not\n",
      "willing to rely on other standards. T o avoid data leaks, security analysis is\n",
      "critical in these situations. If SFTP is required, it can be combined with\n",
      "extra network security , such as only allowing authorized IP addresses to\n",
      "access the network, or even passing all traf fic over a VPN.\n",
      "Object storageIn our view , object storage is the most optimal and secure way to handle file\n",
      "exchange. Public cloud storage implements the latest security standards, has\n",
      "an extremely robust track record, and provides high-performance movement\n",
      "of data.\n",
      "We’ll discuss object storage much more extensively in Chapter 6. At a basic\n",
      "level, object storage is a multi-tenant system in public clouds, and it\n",
      "supports storing massive amounts of data. This makes object storage ideal\n",
      "for moving data in and out of data lakes, moving data between teams, and\n",
      "transferring data between or ganizations. Y ou can even provide short-term\n",
      "access to an object with a signed URL, giving a user short-term permission.\n",
      "SCP\n",
      "SCP (secure copy) is a file exchange protocol that runs over an SSH\n",
      "connection. SCP can be a secure file transfer option if it is configured\n",
      "correctly . Again, adding additional network access control (defense in\n",
      "depth) to enhance SCP security is highly recommended.\n",
      "EDI\n",
      "Another practical reality for data engineers is EDI (Electronic Data\n",
      "Interchange). The term is vague enough that it could refer to any method of\n",
      "data movement. In practice, it is used in modern parlance to refer to rather\n",
      "archaic means of file exchange, such as by email, or on a flash drive. Data\n",
      "engineers will find that some of their data sources do not support more\n",
      "modern means of data transport, often due to archaic IT systems, or human\n",
      "process limitations. They can at least enhance EDI through automation. For\n",
      "example, they can set up a cloud-based email server that saves files onto\n",
      "company object storage as soon as they are received. This can trigger\n",
      "orchestration processes to ingest and process data. This is much more\n",
      "robust than an employee downloading the attached file and manually\n",
      "uploading it to an internal system, something that we still frequently see.\n",
      "Databases and file exportEngineers should be aware of how the source database systems handle file\n",
      "export. For many transactional systems, export involves lar ge data scans\n",
      "that put a significant load on the database. Source system engineers must\n",
      "assess when these scans can be run without af fecting application\n",
      "performance and might opt for a strategy to mitigate the load. Export\n",
      "queries can be broken down into smaller exports by querying over key\n",
      "ranges or one partition at a time. Alternatively , a read-replica can reduce\n",
      "load. Read replicas are especially appropriate if exports happen many times\n",
      "a day , and exports coincide with high source system load.\n",
      "Some modern cloud databases are highly optimized for file export.\n",
      "Snowflake allows engineers to set up a warehouse (compute cluster) just for\n",
      "export, with no impact on other warehouses running analytics. BigQuery\n",
      "handles file exports using a backend service, completely independent of its\n",
      "query engine. A WS Redshift gives you the ability to issue a SQL UNLOAD\n",
      "command to dump tables into S3 object storage. In all cases, file export is\n",
      "the “best practice” approach to data movement, recommended by the\n",
      "vendor over JDBC/ODBC connections for cost and performance reasons.\n",
      "Practical issues with common file formats\n",
      "Engineers should also be aware of the file formats that they’re using to\n",
      "export. At the time of this writing, CSV is nearly universal, but also\n",
      "extremely error -prone. Namely , CSV’ s default delimiter is also one of the\n",
      "most common characters in the English language—the comma! But it gets\n",
      "worse. CSV is by no means a uniform format. Engineers must stipulate\n",
      "delimiter , quote characters, escaping, etc. to appropriately handle the export\n",
      "of string data. CSV also doesn’ t natively encode schema information, nor\n",
      "does it directly support modern nested structures. CSV file encoding and\n",
      "schema information must be configured in the tar get system to ensure\n",
      "appropriate ingestion. Auto-detection is a convenience feature provided in\n",
      "many cloud environments but is not appropriate for production ingestion.\n",
      "As a best practice, engineers should record CSV encoding and schema\n",
      "details in file metadata.More modern export formats include Parquet, A vro, Arrow , and ORC or\n",
      "JSON. These formats natively encode schema information and handle\n",
      "arbitrary string data with no special intervention. Many of them also handle\n",
      "nested data structures natively , so that JSON fields are stored using internal\n",
      "nested structures rather than simple strings. For columnar databases,\n",
      "columnar formats (Parquet, Arrow , ORC) allow more ef ficient data export\n",
      "because columns can be directly transcoded between formats, a much\n",
      "lighter operation than pivoting data into rows (CSV , Avro). Modern formats\n",
      "are also generally more optimized for query engines. The Arrow file format\n",
      "is designed to map data directly into processing engine memory , providing\n",
      "high performance in data lake environments.\n",
      "The disadvantage of these newer formats is that many of them are not\n",
      "natively supported by source systems. Data engineers are often forced to\n",
      "work with CSV data, and then build robust exception handling and error\n",
      "detection to ensure data quality on ingestion.\n",
      "See the appendix on serialization and compr ession  for a more extensive\n",
      "discussion of file formats.\n",
      "SSH\n",
      "Strictly speaking, SSH is not an ingestion strategy , but a protocol that is\n",
      "used in conjunction with other ingestion strategies. W e use SSH in a few\n",
      "different ways. First, SSH can be used for file transfer with SCP , as\n",
      "mentioned earlier . Second, SSH tunnels are used to allow secure, isolated\n",
      "connections to databases. Application databases should never be directly\n",
      "exposed on the internet. Instead, engineers can set up a bastion host, i.e., an\n",
      "intermediate host instance that can connect to the database in question. This\n",
      "host machine is exposed on the internet, although locked down for\n",
      "extremely limited access from only specified IP addresses to specified\n",
      "ports. T o connect to the database, a remote machine first opens an SSH\n",
      "tunnel connection to the bastion host, then connects from the host machine\n",
      "to the database.\n",
      "ShellThe shell is the interface by which you may execute commands to ingest\n",
      "data. In practice, the shell can be used to script workflows for virtually any\n",
      "software tool, and shell scripting is still used extensively in ingestion\n",
      "processes. For example, a shell script might read data from a database,\n",
      "reserialize the data into a dif ferent file format, upload it to object storage,\n",
      "and trigger an ingestion process in a tar get database. While storing data on a\n",
      "single instance or server is not highly scalable, many of our data sources are\n",
      "not particularly lar ge, and such approaches work just fine.\n",
      "In addition, cloud vendors generally provide robust CLI-based tools. It is\n",
      "possible to run complex ingestion processes simply by issuing commands to\n",
      "the A WS CLI. As ingestion processes grow more complicated, and the\n",
      "service level agreement grows more stringent, engineers should consider\n",
      "moving to a true orchestration system instead.\n",
      "APIs\n",
      "The bulk of softwar e engineering is just plumbing.\n",
      "—Karl Hughes\n",
      "As we mentioned in Chapter 5, APIs are a data source that continues to\n",
      "grow in importance and popularity . A typical or ganization may have\n",
      "hundreds of external data sources—SAAS platforms, partner companies,\n",
      "etc. The hard reality is that there is no true standard for data exchange over\n",
      "APIs. Data engineers can expect to spend a significant amount of time\n",
      "reading documentation, communicating with external data owners, and\n",
      "writing and maintaining API connection code.\n",
      "Three trends are slowly changing this situation. First, many vendors provide\n",
      "API client libraries for various programming languages that remove much\n",
      "of the complexity of API access. Google was ar guably a leader in this\n",
      "space, with AdW ords client libraries available in various flavors. Many\n",
      "other vendors have since followed suit.\n",
      "Second, there are numerous data connector platforms available now as\n",
      "proprietary software, open-source, or managed open source. These\n",
      "platforms provide turnkey data connectivity to many data sources; forunsupported data sources, they of fer frameworks for writing custom\n",
      "connectors. See the section below on managed data connectors.\n",
      "The third trend is the emer gence of data sharing (discussed in Chapter 5),\n",
      "i.e., the ability to exchange data through a standard platform such as Google\n",
      "BigQuery , Snowflake, Redshift, or Amazon S3. Once data lands on one of\n",
      "these platforms, it is straightforward to store it, process it, or move it\n",
      "somewhere else. Data sharing has had a significant and rapid impact in the\n",
      "data engineering space. For example; Google now supports direct sharing of\n",
      "data from a variety of its advertising and analytics products (GoogleAds,\n",
      "Google Analytics, etc.) to BigQuery . Any business that advertises online\n",
      "can now reallocate software development resources away from Google\n",
      "product data ingestion and focus instead on everything downstream.\n",
      "When data sharing is not an option and direct API access is necessary , don’ t\n",
      "reinvent the wheel. While a managed service might look like an expensive\n",
      "option, consider the value of your time and the opportunity cost of building\n",
      "API connectors when you could be spending your time on higher -value\n",
      "work.\n",
      "In addition, many managed services now support building custom API\n",
      "connectors. This may take the form of providing API technical\n",
      "specifications in a standard format, or of writing connector code that runs in\n",
      "a serverless function framework (e.g. A WS Lambda) while letting the\n",
      "managed service handle the details of scheduling and synchronization.\n",
      "Again, these services can be a huge time saver for engineers, both for\n",
      "development and ongoing maintenance.\n",
      "Reserve your custom connection work for APIs that aren’ t well supported\n",
      "by existing frameworks—you will find that there are still plenty of these to\n",
      "work on. There are two main aspects of handling custom API connections:\n",
      "software development and ops. Follow software development best\n",
      "practices: you should use version control, continuous delivery , automated\n",
      "testing, etc. In addition to following DevOps best practices, consider an\n",
      "orchestration framework, which can dramatically streamline the operational\n",
      "burden of data ingestion.Webhooks\n",
      "Webhooks, as we discussed in Ch. 5, are often referred to as reverse APIs.\n",
      "For a typical REST data API, the data provider gives engineers API\n",
      "specifications that they use to write their data ingest code. The code makes\n",
      "requests and receives data in responses.\n",
      "With a webhook ( Figure 4-13 ), the data provider documents an API request\n",
      "specification, but this specification is implemented by the data consumer .\n",
      "The consumer sets up an endpoint, and the data source calls the endpoint,\n",
      "delivering data in requests. T ypically , webhooks deliver one event at a time;\n",
      "the consumer is responsible for ingesting each request and handling data\n",
      "aggregation, storage, processing, etc.\n",
      "Webhook-based data ingestion architectures can be brittle, dif ficult to\n",
      "maintain, and inef ficient. Data engineers can build more robust webhook\n",
      "architectures—with lower maintenance and infrastructure costs—by using\n",
      "appropriate of f-the-shelf tools. A common pattern uses a serverless function\n",
      "framework (i.e. A WS Lambda) to receive incoming events, a streaming\n",
      "message bus to store and buf fer messages (i.e. A WS Kinesis), a stream\n",
      "processing framework to handle real-time analytics (i.e. Apache Flink), and\n",
      "an object store for long term storage (i.e. Amazon S3).\n",
      "Figur e 4-13. A basic webhook ingestion ar chitectur e built fr om cloud services. Using “serverless”\n",
      "services r educes operational over head.You’ll notice that this architecture does much more than simply ingesting\n",
      "the data. This underscores the fact that ingestion is highly entangled with\n",
      "the other stages of the data engineering lifecycle—it is often impossible to\n",
      "define your ingestion architecture without also making decisions about\n",
      "storage and processing.\n",
      "We note that this architecture can be simplified if one does not need real-\n",
      "time analytics. However , trying to simplify too much creates new problems.\n",
      "For instance, writing directly from a Lambda serverless function to S3\n",
      "might produce a massive number of objects, with an extremely high request\n",
      "rate to S3, and potentially high costs. This is where data engineers need to\n",
      "understand the critical role played by each component in the pipeline. T o\n",
      "simplify the architecture in a more sane way , we could remove the\n",
      "streaming analytics system, accumulate events into the messaging bus, and\n",
      "periodically query a lar ge chunk of data over a time range to write it into\n",
      "S3. Better yet, we could use a serverless tool like Kinesis Firehose, which\n",
      "automates the process of event rollup to create S3 objects of a reasonable\n",
      "size.\n",
      "Legacy data flow management tools\n",
      "We would be remiss not to mention data flow management tools such as\n",
      "Informatica and T alend, to name a few . This category of tool is designed to\n",
      "serve several stages of the data engineering lifecycle, including ingestion,\n",
      "and processing.\n",
      "In practice, these systems also integrate some orchestration capabilities.\n",
      "(See the undercurrents section in this chapter , and corresponding\n",
      "discussions throughout the book.) Such systems include connectors to a\n",
      "variety of sources, and allow users to define data pipelines, often using a\n",
      "visual interface that represents processing stages with icons. Processing can\n",
      "be set on a schedule. Each processing stage runs once its upstream\n",
      "dependencies are met. Of course, there are generally ingestion steps at the\n",
      "beginning of each pipeline.\n",
      "These systems typically employ two main processing models. W e have\n",
      "already discussed both ETL and EL T. Systems based on the ETL modelingest data internally , transform it and write it back to external storage, such\n",
      "as a cloud data warehouse. T ools that employ the EL T model handle most\n",
      "processing in an external system such as a cloud data warehouse.\n",
      "Web-interface\n",
      "Web interfaces for data access remain a practical reality for data engineers.\n",
      "We frequently run into situations where not all data and functionality in a\n",
      "SAAS (software as a service) platform is exposed through automated\n",
      "interfaces such as APIs and file drops. Instead, someone must manually\n",
      "access a web interface, generate a report and download a file to a local\n",
      "machine.\n",
      "Potentially , some automation can be applied by using web interface\n",
      "simulation frameworks such as simulation, but web interfaces remain an\n",
      "area of high friction for data ingestion. For example, you might be able to\n",
      "automate the clicking of a web interface with Selenium. But whenever\n",
      "possible, find another approach.\n",
      "Managed Data Connectors\n",
      "In the section on ingesting data from APIs, we mention the emer gence of\n",
      "managed data connector platforms and frameworks. The goal of these tools\n",
      "is to provide a standard set of connectors that are available out of the box to\n",
      "spare data engineers much detailed plumbing to connect to a particular\n",
      "source.\n",
      "For now , Fivetran has emer ged as a category leader in this space, but there\n",
      "are many up-and-coming competitors, both proprietary and open source.\n",
      "Generally , proprietary options in the space allow users to set a tar get and\n",
      "source, set permissions and credentials, configure an update frequency , and\n",
      "begin syncing data. Data syncs are fully managed and monitored—if data\n",
      "synchronization fails, users will receive an alert.\n",
      "The open-source options in the space are usually part of a product, and\n",
      "available in supported and unsupported flavors. Stitch started this trend by\n",
      "introducing the Singer Python framework. Singer provided a standard set of\n",
      "abstractions for creating data connectors. Engineers could then use Singerto manage and run their data synchronization or send their connectors to\n",
      "Stitch to run within their fully managed platform. V arious competitors to\n",
      "Stitch have emer ged. Some utilize the Singer framework; others, such as\n",
      "Airbyte and Meltano, have introduced new open-source data connector\n",
      "frameworks.\n",
      "We note also that even proprietary data connector engines allow the\n",
      "creation of custom connections with some coding ef fort. Fivetran allows\n",
      "this through various serverless function frameworks, including A WS\n",
      "Lambda and its similar competitors. Engineers write function code that\n",
      "receives a request, pulls data, and returns it to Fivetran. Fivetran takes care\n",
      "of orchestration and synchronization to the tar get database.\n",
      "These are just a few of many  options for managed connectors, and we\n",
      "expect that this space of SAAS managed services and OSS that support the\n",
      "development of custom connectors will continue to grow .\n",
      "Web scraping\n",
      "Web scraping is another common data source for engineers. Any search\n",
      "engine must scrape the web to analyze links, build an index, etc. However ,\n",
      "many other businesses also rely on web scraping to survey web content that\n",
      "may be relevant to their business activities.\n",
      "A full discussion of web scraping is well beyond the scope of this book.\n",
      "There are many books specific to the subject. W e recommend that readers\n",
      "look at web scraping books in O’Reilly’ s catalog and beyond. There are\n",
      "also numerous online resources, in video tutorials, blog posts, etc.\n",
      "Here is some top-level advice to be aware of before undertaking any web\n",
      "scraping project. First, learn to be a good citizen. Don’ t inadvertently create\n",
      "a denial of service attack, and don’ t get your IP address blocked.\n",
      "Understand how much traf fic you’re generating and pace your web\n",
      "crawling activities appropriately . Just because you can spin up thousands of\n",
      "simultaneous Lambda functions to scrape doesn’ t mean you should; in fact,\n",
      "excessive web scraping could lead to the disabling of your A WS account.Second, be aware of the legal implications of your activities. Again,\n",
      "generating denial of service attacks has legal implications. Activities that\n",
      "violate terms of service may cause legal headaches for your employer .\n",
      "Web scraping has interesting implications for the processing stage of the\n",
      "data engineering lifecycle; there are various things that engineers should\n",
      "think about at the beginning of a web scraping project. What do you intend\n",
      "to do with the data? Are you just pulling key fields from the scraped HTML\n",
      "using Python code, then writing these values to a database? Do you intend\n",
      "to maintain the full HTML code of the scraped websites and process this\n",
      "data using a framework like Spark? These decisions may lead to very\n",
      "different architectures downstream of ingestion.\n",
      "Transfer appliances for data migration\n",
      "For truly big data (100 TB or more), transferring data directly over the\n",
      "internet may be a slow and extremely expensive process—at this scale, the\n",
      "fastest, most ef ficient way to move data is not over the wire, but by truck.\n",
      "Cloud vendors of fer the ability to send your data via a physical “box of hard\n",
      "drives.” Simply order a storage device—called a transfer appliance—load\n",
      "your data from your servers, then send it back to the cloud vendor who will\n",
      "upload your data into their cloud. The suggestion is to consider using a\n",
      "transfer appliance if your data size hovers around 100 TB. On the extreme\n",
      "end, A WS even of fers Snowmobile, a transfer appliance in a semi-trailer .\n",
      "Snowmobile is intended to lift and shift an entire data center , where data\n",
      "sizes are in the petabytes or greater .\n",
      "Transfer appliances are particularly useful for creating hybrid-cloud or\n",
      "multi-cloud setups. For example, Amazon’ s data transfer appliance (A WS\n",
      "Snowball) supports both import and export. T o migrate into a second cloud,\n",
      "users can export their data into a Snowball device, then import it into a\n",
      "second transfer appliance to move data into GCP or Azure. This might\n",
      "sound like an awkward process, but even when it’ s feasible to push data\n",
      "over the internet between clouds, data egress fees make this an extremely\n",
      "expensive proposition, and physical transfer appliances are a much cheaper\n",
      "alternative when the data volumes are significant.Keep in mind that transfer appliances and data migration services are one-\n",
      "time data ingestion events, and not suggested for ongoing workloads. If\n",
      "you’ve got workloads that require constant movement of data in either a\n",
      "hybrid or multi-cloud scenario, your data sizes are presumably batching or\n",
      "streaming much smaller data sizes, on an ongoing basis.\n",
      "Streaming Ingestion T echnologies\n",
      "There are a few main technologies for ingestion and temporary storage of\n",
      "streaming data. As we discuss streaming technologies, it is more useful to\n",
      "look at the class of all frameworks that collect and buf fer messages and\n",
      "consider them in terms of various features they of fer. We’ll describe major\n",
      "characteristics that data engineers should consider for these systems, and\n",
      "cite a few examples. This discussion is by no means exhaustive—we\n",
      "discuss only a few examples and a handful of standard technologies. But\n",
      "these characteristics are a good framework for research as you choose a\n",
      "streaming ingestion technology . Decide what characteristics you need for\n",
      "your data applications and evaluate technologies according to your needs.\n",
      "We’d like to note that the terminology in the streaming ingestion space can\n",
      "be confusing. Apache Kafka has variously described itself as a distributed\n",
      "event streaming platform or a distributed commit log. Per Amazon, Kinesis\n",
      "Data Streams “is a serverless streaming data service.” RabbitMQ calls itself\n",
      "a message broker . Apache Kafka may or may not be a message queue\n",
      "depending on which author you refer to. And so on. Focus on the bigger\n",
      "picture and context of where various technologies fit in terms of\n",
      "functionality and utility and take vendor descriptions with a grain of salt.\n",
      "Horizontal scaling\n",
      "For our purposes, we’re mostly interested in streaming ingest frameworks\n",
      "that support horizontal scaling, which grows and shrinks the number of\n",
      "nodes based on the demand placed upon your system. Horizontal scaling\n",
      "enhances data scale, reliability , and durability . In some cases, a single node\n",
      "solution may work just fine for small streams but consider the reliability\n",
      "and durability implications.Stream partitions\n",
      "Kafka and Kinesis partition (shard) streams to support horizontal scaling,\n",
      "and they allow users to specify an explicit partition key that uniquely\n",
      "determines which shard a message belongs to. Each consuming server can\n",
      "consume from a specific shard. Google Cloud Pub/Sub hides all details of\n",
      "partitioning; subscribers simply read messages at the level of a topic.\n",
      "Explicit partitioning is a blessing and a curse, entailing engineering\n",
      "advantages and challenges—see the discussion of stream partitions  in\n",
      "Chapter 8.\n",
      "Subscriber pull and push\n",
      "Kafka and Kinesis only support pull subscriptions. That is, subscribers read\n",
      "messages from a topic and confirm when they have been processed. In\n",
      "addition, to pull subscriptions, Pub/Sub and RabbitMQ support push\n",
      "subscriptions, allowing these services to write messages to a listener .\n",
      "Pull subscriptions are the default choice for most data engineering\n",
      "applications, but you may want to consider push capabilities for specialized\n",
      "applications. Note that pull-only message ingestion systems can still push if\n",
      "you add an extra layer to handle this.\n",
      "Operational overhead\n",
      "As with any data engineering technology , operational overhead is a key\n",
      "consideration. W ould you rather manage your streaming data pipelines, or\n",
      "outsource this work to a team of dedicated engineers? On one end of the\n",
      "spectrum, Google Cloud Pub/Sub is a fully managed service with no knobs\n",
      "to turn; just create a topic and a consumer , give a payload, and you’re good\n",
      "to go. On the other end, Apache Kafka is packaged in a variety of ways, all\n",
      "the way from raw , hot of f the presses open source, to a fully managed\n",
      "serverless of fering from Confluent.\n",
      "AutoscalingAutoscaling features vary from platform to platform. Confluent Cloud will\n",
      "autoscale up to 100MB/s, after which some intervention is required to set\n",
      "the scale. More ef fort is required if you run your clusters. Amazon Kinesis\n",
      "Data Streams recently added a feature that automatically scales the number\n",
      "of shards. Pub/Sub is fully autoscaling.\n",
      "Message size\n",
      "This is an easily overlooked issue: one must ensure that the streaming\n",
      "framework in question can handle the maximum expected message size. For\n",
      "example, Amazon Kinesis supports a maximum message size of 1 MB.\n",
      "Kafka defaults to this maximum size but can be configured for a maximum\n",
      "of 20 MB or more. (Configurability may vary on managed service\n",
      "platforms.)\n",
      "Replay\n",
      "Replay is a key capability in many streaming ingest platforms. Replay\n",
      "allows readers to request a range of messages from the history . A system\n",
      "must support replay for us to truly consider it a streaming storage system.\n",
      "RabbitMQ deletes messages once they are consumed by all subscribers—\n",
      "storage capabilities are for temporary buf fering only; for long-term storage,\n",
      "a separate tool is required to consume messages and durably write them.\n",
      "Replay also essentially lets us hybridize batch and stream processing in one\n",
      "system.\n",
      "A key parameter for engineering decisions is maximum message retention\n",
      "time. Google Cloud Pub/Sub supports retention periods of up to 7 days,\n",
      "Amazon Kinesis Data Streams retention can be turned up to 1 year , and\n",
      "Kafka can be configured for indefinite retention, limited by available disk\n",
      "space. (Kafka also supports the option to write older messages to cloud\n",
      "object storage, unlocking virtually unlimited storage space and retention.)\n",
      "Fanout\n",
      "Fanout entails having more than one consumer per data stream. This is very\n",
      "useful in the context of complex streaming applications, where we mightwant to feed the same data to multiple tar gets. Note that this is dif ferent\n",
      "from having multiple shards and consuming these on dif ferent downstream\n",
      "servers. Fanout entails multiple consumers per shar d, where each consumer\n",
      "gets its bookmark determining the current position in the stream.\n",
      "Each streaming ingestion platform has its limits for fanout. For example, a\n",
      "Kinesis Data Stream supports up to 20 consumers.\n",
      "Exactly-once delivery\n",
      "In some cases, streaming ingestion systems such as Pub/Sub may send\n",
      "events to consumers more than once. This is known as at least once\n",
      "delivery  and is a consequence of consistency challenges in a distributed\n",
      "system.\n",
      "Kafka  recently added support for exactly-once delivery . However , before\n",
      "relying on this feature, make sure that you read the documentation carefully\n",
      "to understand the exact configuration requirements and performance\n",
      "implications. Also, it’ s important to realize that there are various ways for\n",
      "records to be processed multiple times in a streaming system even if your\n",
      "streaming storage system support exactly-once delivery . For example, if a\n",
      "publisher crashes after writing a record into the stream before receiving\n",
      "confirmation that the record was consumed, it may write a second copy of\n",
      "the record when it comes back online. And if a subscriber crashes after\n",
      "processing a record but before confirming to the storage system that it has\n",
      "completed processing, the record may get read a second time.\n",
      "In general, think through the implications of duplicate records in your\n",
      "stream processing applications. Designing for idempotency  will allow your\n",
      "system to properly deduplicate records. On the other hand, an occasional\n",
      "duplicate record may not be an issue in your system.\n",
      "Record delivery order\n",
      "The notion of record delivery order is challenging in a distributed system.\n",
      "Amazon Kinesis orders records in single shards, but this does not provide\n",
      "any guarantees for behavior across a whole topic, especially as the number\n",
      "of shards increases. Pub/Sub provides no ordering guarantees; instead,1\n",
      "2engineers are advised to use a tool like Google Cloud Dataflow (Apache\n",
      "Beam) if they want to order records.\n",
      "We give essentially the same advice that we gave in the context of at least\n",
      "once delivery: Understand the ordering characteristics of your streaming\n",
      "ingest system and think through the implications of data arriving out of\n",
      "order .\n",
      "Stream processing\n",
      "Some streaming storage systems support direct processing without using an\n",
      "external processing tool. (See Chapter 8.) For example, Kafka supports a\n",
      "variety of operations with the KSQL query language. Google Cloud\n",
      "Pub/Sub and Kinesis Data Streams rely on external tools for processing.\n",
      "Streaming DAGs\n",
      "Apache Pulsar has introduced an enhanced notion of stream processing that\n",
      "can be extremely useful for data engineers. W ith Kafka, engineers can\n",
      "potentially stitch topics together into a DAG (directed acyclic graph) to\n",
      "realize complex data processing, but this would also require custom\n",
      "services and code.\n",
      "Pulsar builds in streaming DAGs as a core abstraction, supporting complex\n",
      "processing and transformations without ever exiting the Pulsar system.\n",
      "Multisite and multiregional\n",
      "It is often desirable to integrate streaming across several locations for\n",
      "enhanced redundancy and to consume data close to where it is generated.\n",
      "For example, it might be desirable to have streaming storage running across\n",
      "several regions to improve latency and throughput for messages sent by an\n",
      "IoT swarm with millions of devices.\n",
      "An Amazon Kinesis Data Stream runs in a single region. Google Cloud\n",
      "Pub/Sub Global Endpoints allow engineers to create a single endpoint, with\n",
      "Google automatically storing data in the nearest region. While Kafka\n",
      "supports a notion of site-to-site replication through Mirror Maker , the corearchitecture of Pulsar is designed to specifically support multi-cluster use\n",
      "cases.\n",
      "Who You’ll Work With\n",
      "Data ingestion sits at several or ganizational boundaries. In the development\n",
      "and management of data ingestion pipelines, data engineers will work with\n",
      "both data producers and data consumers.\n",
      "Upstream Data Producers\n",
      "In practice, there is often a significant disconnect between those responsible\n",
      "for generating data —typically software engineers—and the data engineers\n",
      "who will prepare this data for analytics and data science. Software\n",
      "engineers and data engineers usually sit in separate or ganizational silos; if\n",
      "they think about data engineers at all, they usually see them simply as\n",
      "downstream consumers of the data exhaust from their application, not as\n",
      "stakeholders.\n",
      "We see this current state of af fairs as a problem, but also a significant\n",
      "opportunity . Data engineers can improve the quality of the data that they\n",
      "ingest by inviting software engineers to be stakeholders in data engineering\n",
      "outcomes. The vast majority of software engineers are well aware of the\n",
      "value of analytics and data science but don’ t necessarily have aligned\n",
      "incentives to directly contribute to data engineering ef forts.\n",
      "However , simply improving communication is a great first step. Often,\n",
      "software engineers have already identified potentially valuable data for\n",
      "downstream consumption. Opening a channel of communication\n",
      "encourages software engineers to get data into shape for consumers, and to\n",
      "communicate about data changes to prevent pipeline regressions.\n",
      "Beyond communication, data engineers can highlight the contributions of\n",
      "software engineers to team members, executives, and especially product\n",
      "managers. Involving product managers in the outcome and treating\n",
      "downstream data processed as part of a product encourages them to allocatescarce software development to collaboration with data engineers. Ideally ,\n",
      "software engineers can work partially as extensions of the data engineering\n",
      "team; this allows them to collaborate on a variety of projects, such as\n",
      "creating an event-driven architecture to enable real-time analytics.\n",
      "Downstream Data Consumers\n",
      "Who is the ultimate customer for data ingestion? Data engineers tend to\n",
      "focus on data practitioners and technology leaders such as data scientists,\n",
      "analysts, and chief technical of ficers. They would do well to also remember\n",
      "their broader circle of business stakeholders such as marketing directors,\n",
      "vice presidents over the supply chain, chief executive of ficers, etc.\n",
      "Too often, we see data engineers pursuing sophisticated projects (real-time\n",
      "streaming buses, big data systems) while digital marketing managers next\n",
      "door are left downloading GoogleAds reports manually . View data\n",
      "engineering as a business, and recognize who your customers are. Often,\n",
      "there is significant value in basic automation of ingestion processes,\n",
      "especially for or ganizations like marketing that control massive budgets and\n",
      "sit at the heart of revenue for the business. Basic ingestion work may seem\n",
      "boring, but delivering value to these core parts of the business will open up\n",
      "more budget and more exciting opportunities for data engineering in the\n",
      "long term.\n",
      "Data engineers can also invite more executive participation in this\n",
      "collaborative process. For good reason, the notion of data-driven culture is\n",
      "quite fashionable in business leadership circles, but it is up to data engineers\n",
      "and other data practitioners to provide executives with guidance on the best\n",
      "structure for a data-driven business. This means communicating the value\n",
      "of lowering barriers between data producers and data engineers while\n",
      "supporting executives in breaking down silos and setting up incentives that\n",
      "will lead to a more unified data-driven culture.\n",
      "Once again, communication  is the watchword. Honest communication early\n",
      "and often with stakeholders will go a long way to making sure your data\n",
      "ingestion adds value.Undercurrents\n",
      "Virtually all the undercurrents touch the ingestion phase, but we’ll\n",
      "emphasize the most salient ones here.\n",
      "Security\n",
      "Moving data introduces security vulnerabilities because you have to move\n",
      "data between locations. The last thing you want is for the data to be\n",
      "captured or compromised while it is being moved.\n",
      "Consider where the data lives and where it is going. Data that needs to\n",
      "move within your VPC should use secure endpoints, and never leave the\n",
      "confines of the VPC. If you need to send data between the cloud and an on-\n",
      "premises network, use a VPN or a dedicated private connection. This might\n",
      "cost money , but the security is a good investment. If your data traverses the\n",
      "public internet, make sure the transmission is encrypted. Don’ t ever send\n",
      "data unencrypted over the public internet.\n",
      "Data Management\n",
      "Naturally , data management begins at data ingestion. This is the starting\n",
      "point for lineage and data cataloging; from this point on, data engineers\n",
      "need to think about master data management, ethics, privacy , compliance,\n",
      "etc.\n",
      "Schema changes\n",
      "Schema changes remain, from our perspective, an unsettled issue in data\n",
      "management. The traditional approach is a careful command and control\n",
      "review process. W orking with clients at lar ge enterprises, we have been\n",
      "quoted lead times of six months for the addition of a single field. This is an\n",
      "unacceptable impediment to agility .\n",
      "On the opposite end of the spectrum is Fivetran, where schema changes are\n",
      "completely automatic. Any schema change in the source triggers tar gettables to be recreated with the new schema. This solves schema problems at\n",
      "the ingestion stage, but can still break downstream pipelines.\n",
      "One possible solution, which the authors have ruminated on for a while, is\n",
      "an approach pioneered by Git version control. When Linus T orvalds was\n",
      "developing Git, many of his choices were inspired by the limitations of\n",
      "CVS (Concurrent V ersions System). CVS is completely centralized—it\n",
      "supports only one current of ficial version of the code, stored on a central\n",
      "project server . To make Git a truly distributed system, T orvalds used the\n",
      "notion of a tree, where each developer could maintain their processed\n",
      "branch of the code and then mer ge to or from other branches.\n",
      "A few years ago, such an approach to data was unthinkable. Data\n",
      "warehouse systems are typically operated at close to maximum storage\n",
      "capacity . However , in big data and cloud data warehouse environments,\n",
      "storage is cheap. One may quite easily maintain multiple versions of a table\n",
      "with dif ferent schemas and even dif ferent upstream transformations. T eams\n",
      "can maintain multiple “development” versions of a table using orchestration\n",
      "tools such as Airflow; schema changes, upstream transformation and code\n",
      "changes, etc. can appear in development tables before of ficial changes to\n",
      "the main  table.\n",
      "Data ethics, privacy , and compliance\n",
      "Clients often ask for our advice on encrypting sensitive data in databases.\n",
      "This generally leads us to ask a very basic question: do you need the\n",
      "sensitive data that you’re trying to encrypt? As it turns out, in the process of\n",
      "creating requirements and solving problems, this question often gets\n",
      "overlooked.\n",
      "Data engineers should train themselves to always ask this question when\n",
      "setting up ingestion pipelines. They will inevitably encounter sensitive data;\n",
      "the natural tendency is to ingest it and forward it to the next step in the\n",
      "pipeline. But if this data is not needed, why collect it at all? Why not simply\n",
      "drop sensitive fields before data is stored? Data cannot leak if it is never\n",
      "collected.Where it is truly necessary to keep track of sensitive identities, it is\n",
      "common practice to apply tokenization to anonymize identities in model\n",
      "training and analytics. But engineers should look at where this tokenization\n",
      "is applied. If possible, hash data at ingestion time.\n",
      "In some cases, data engineers cannot avoid working with highly sensitive\n",
      "data. Some analytics systems must present identifiable sensitive\n",
      "information. Whenever they handle sensitive data, engineers must act under\n",
      "the highest ethical standards. In addition, they can put in place a variety of\n",
      "practices to reduce the direct handling of sensitive data. Aim as much as\n",
      "possible for touchless pr oduction  where sensitive data is involved. This\n",
      "means that engineers develop and test code on simulated or cleansed data in\n",
      "development and staging environments, but code deployments to\n",
      "production are automated.\n",
      "Touchless production is an ideal that engineers should strive for , but\n",
      "situations inevitably arise that cannot be fully solved in development and\n",
      "staging environments. Some bugs may not be reproducible without looking\n",
      "at the live data that is triggering a regression. For these cases, put a broken\n",
      "glass process in place, i.e., require at least two people to approve access to\n",
      "sensitive data in the production environment. This access should be tightly\n",
      "scoped to a particular issue, and come with an expiration date.\n",
      "Our last bit of advice on sensitive data: be wary of naive technological\n",
      "solutions to human problems. Both encryption and tokenization are often\n",
      "treated like privacy magic bullets. Most modern storage systems and\n",
      "databases encrypt data at rest and in motion by default. Generally , we don’ t\n",
      "see encryption problems, but data access problems. Is the solution to apply\n",
      "an extra layer of encryption to a single field, or to control access to that\n",
      "field? After all, one must still tightly manage access to the encryption key .\n",
      "There are legitimate use cases for single field encryption, but watch out for\n",
      "ritualistic applications of encryption.\n",
      "On the tokenization front, use common sense and assess data access\n",
      "scenarios. If someone had the email of one of your customers, could they\n",
      "easily hash the email and find the customer in your data? Thoughtlesslyhashing data without salting and other strategies may not protect privacy as\n",
      "well as you think.\n",
      "DataOps\n",
      "Reliable data pipelines are the cornerstone of the data engineering lifecycle.\n",
      "When they fail, all downstream dependencies come to a screeching halt.\n",
      "Data warehouses and data lakes aren’ t replenished. Data scientists and\n",
      "analysts can’ t effectively do their jobs; the business is forced to fly blind.\n",
      "Ensuring your data pipelines are properly monitored is a key step toward\n",
      "reliability and ef fective incident response. If there’ s one stage in the data\n",
      "engineering lifecycle where monitoring is critical, it’ s in the ingestion stage.\n",
      "Weak or nonexistent monitoring means the pipelines may or may not be\n",
      "working. In our work, we’ve seen countless examples of reports and\n",
      "machine learning models being generated from stale data. In one extreme\n",
      "case, an ingestion pipeline failure wasn’ t detected for six months. (One\n",
      "might question the concrete utility of the data in this instance.) This was\n",
      "very much avoidable through proper monitoring.\n",
      "What should you monitor? Uptime, latency , data volumes processed are a\n",
      "good place to start. If an ingestion job fails, how will you respond? This\n",
      "also applies to third-party services. In the case of these services, what\n",
      "you’ve gained in terms of lean operational ef ficiencies (reduced headcount)\n",
      "is replaced by systems you depend upon being outside of your control. If\n",
      "you’re using a third-party service (cloud, data integration service, etc), how\n",
      "will you be alerted if there’ s an outage? What’ s your response plan in the\n",
      "event a service you depend upon suddenly goes of fline?\n",
      "Sadly , there’ s not a universal response plan for third-party failures. If you\n",
      "can failover to other servers (discussed in the data architecture section as\n",
      "well), preferably in another zone or region, definitely set this up.\n",
      "If your data ingestion processes are built internally , do you have the correct\n",
      "testing and deployment automation to ensure the code will function in\n",
      "production? And if the code is buggy or fails, can you roll back to a\n",
      "working version?Data quality tests\n",
      "We often refer to data as a silent killer . If quality , valid data is the\n",
      "foundation of success in modern business, using bad data to make decisions\n",
      "is much worse than having no data at all; bad data has caused untold\n",
      "damage to businesses.\n",
      "Data is entropic; it often changes in unexpected ways without warning. One\n",
      "of the inherent dif ferences between DevOps and DataOps is that we only\n",
      "expect software regressions when we deploy changes, while data often\n",
      "presents regressions on its own.\n",
      "DevOps engineers are typically able to detect problems by using binary\n",
      "conditions. Has the request failure rate breached a certain threshold? How\n",
      "about response latency? In the data space, regressions often manifest as\n",
      "subtle statistical distortions. Is a change in search term statistics a result of\n",
      "customer behavior? Of a spike in bot traf fic that has escaped the net? Of a\n",
      "site test tool deployed in some other part of the company?\n",
      "Like systems failures in the world of DevOps, some data regressions are\n",
      "immediately visible. For example, in the early 2000s, Google provided\n",
      "search terms to websites when users arrived from search. In 201 1, “not\n",
      "provided” started appearing as a search term in Google Analytics . Analysts\n",
      "quickly saw “not provided” bubbling to the tops of their reports.\n",
      "The truly dangerous data regressions are silent and can come from inside or\n",
      "outside a business. Application developers may change the meaning of\n",
      "database fields without adequately communicating with data teams.\n",
      "Changes to data from third-party sources may go unnoticed. In the best-case\n",
      "scenario, reports break in obvious ways. Often, business metrics are\n",
      "distorted unbeknownst to decision-makers.\n",
      "Traditional data testing tools are generally built on simple binary logic. Are\n",
      "nulls appearing in a non-nullable field? Are new , unexpected items showing\n",
      "up in a categorical column? Statistical data testing is a new realm, but one is\n",
      "likely to grow dramatically in the next five years.3Orchestration\n",
      "Ingestion generally sits at the beginning of a lar ge and complex data graph;\n",
      "given that ingestion is the first stage of the data engineering lifecycle,\n",
      "ingested data will flow into many more data processing steps, and data from\n",
      "many sources will flow and mingle in complex ways. As we’ve emphasized\n",
      "throughout this book, orchestration is a key process for coordinating these\n",
      "steps.\n",
      "Organizations in an early stage of data maturity may choose to deploy\n",
      "ingestion processes as simple scheduled cron jobs. However , it is important\n",
      "to recognize that this approach is brittle and that it can slow the velocity of\n",
      "data engineering deployment and development.\n",
      "As data pipeline complexity grows, true orchestration is necessary . By true\n",
      "orchestration, we mean a system capable of scheduling full task graphs\n",
      "rather than individual tasks. An orchestration can start each ingestion task at\n",
      "the appropriate scheduled time. Downstream processing and transform steps\n",
      "begin as ingestion tasks are completed. Further downstream, processing\n",
      "steps lead to further processing steps.\n",
      "Software Engineering\n",
      "The ingestion stage of the data engineering lifecycle is engineering\n",
      "intensive. This stage sits at the edge of the data engineering domain and\n",
      "often interfaces with external systems, where software and data engineers\n",
      "have to build a variety of custom plumbing.\n",
      "Behind the scenes, ingestion is incredibly complicated, often with teams\n",
      "operating open-source frameworks like Kafka or Pulsar , or in the case of\n",
      "some of the biggest tech companies, running their own forked or\n",
      "homegrown ingestion solutions. As we’ve discussed in this chapter , various\n",
      "developments have simplified the ingestion process, such as managed data\n",
      "ingest platforms like Fivetran and Airbyte, and managed cloud services like\n",
      "AWS Lambda and Kinesis. Data engineers should take advantage of the\n",
      "best available tools—especially managed ones that do a lot of the heavy\n",
      "lifting for you—but also develop high competency in softwaredevelopment. Even for simple serverless functions, it pays to use proper\n",
      "version control and code review processes and to implement appropriate\n",
      "tests.\n",
      "When writing software, your code needs to be decoupled. A void writing\n",
      "monolithic systems that have tight dependencies on the source or\n",
      "destination systems. For example, avoid writing GET queries to an RDBMS\n",
      "with an ORM. This pattern tightly couples your ingestion pull to the ORM,\n",
      "which is tightly coupled to the RDBMS.\n",
      "Conclusion\n",
      "Okay , we’ve made it to the end of ingestion. In your work as a data\n",
      "engineer , ingestion will likely consume a significant part of your ener gy and\n",
      "effort. At heart, ingestion is plumbing, connecting pipes to other pipes,\n",
      "ensuring that data flows consistently and securely to its destination. At\n",
      "times, the minutiae of ingestion may feel tedious, but without ingestion, the\n",
      "interesting applications of data (analytics, machine learning, etc.) cannot\n",
      "happen.\n",
      "As we’ve emphasized, we’re also in the midst of a sea change toward\n",
      "streaming data pipelines. This is an opportunity for data engineers to\n",
      "discover interesting applications for streaming data, communicate these to\n",
      "the business and deploy exciting new technologies.\n",
      "1 https://www .confluent.io/blog/simplified-robust-exactly-one-semantics-in-kafka-2-5/\n",
      "2 https://discourse.getdbt.com/t/understanding-idempotent-data-transformations/518\n",
      "3 https://martech.or g/dark-google-search-terms-not-provided-one-year -later/About the Authors\n",
      "Joe Reis  is a business-minded data nerd who’ s worked in the data industry\n",
      "for 20 years, with responsibilities ranging from statistical modeling,\n",
      "forecasting, machine learning, data engineering, data architecture, and\n",
      "almost everything else in between. Joe is the CEO and Co-Founder of\n",
      "Ternary Data, a data engineering and architecture consulting firm based in\n",
      "Salt Lake City , Utah. In addition, he volunteers with several technology\n",
      "groups and teaches at the University of Utah. In his spare time, Joe likes to\n",
      "rock climb, produce electronic music, and take his kids on crazy\n",
      "adventures.\n",
      "Matt Housley  is a data engineering consultant and cloud specialist. After\n",
      "some early programming experience with Logo, Basic and 6502 assembly ,\n",
      "he completed a PhD in mathematics at the University of Utah. Matt then\n",
      "began working in data science, eventually specializing in cloud based data\n",
      "engineering. He co-founded T ernary Data with Joe Reis, where he leverages\n",
      "his teaching experience to train future data engineers and advise teams on\n",
      "robust data architecture. Matt and Joe also pontificate on all things data on\n",
      "The Monday Morning Data Chat.\n",
      "\n",
      "\n",
      "\n",
      "SRH University Heidelberg – Electrical Engineering (B.Eng.) – Status 11/20231\n",
      "Vanessa Lehr\n",
      "Your contact person\n",
      "+49 6221 6799-799\n",
      "studyinheidelberg@srh.de\n",
      "Prof. Dr. Felix Möller \n",
      "Study Programme Director \n",
      "felix.moeller@srh.deYour motivation\n",
      "You are ready for progress.\n",
      "As an electrical engineer, you will be directly involved in the devel -\n",
      "opment of innovative products, such as the development of hard -\n",
      "ware and software for electric vehicles, the programming of mobile \n",
      "devices, and the control of high-speed trains and medical devices. \n",
      "Your prospects\n",
      "You drive innovation in the modern information society.\n",
      "Electrical engineers are being sought and recruited for innovative \n",
      "tasks in all sectors. Electromobility is one of the fields in which you \n",
      "will be in great demand in the years ahead. In your future profession, \n",
      "you will typically focus on one major project or several small \n",
      "sub-projects. You will often be involved in the entire process –  \n",
      "from planning and development to the construction design and \n",
      "manufacture of new devices, equipment and systems in the field  \n",
      "of electronics and electrical engineering.\n",
      "Your potential fields of work include: \n",
      " —  Research and development\n",
      "—  Planning and project planning\n",
      "—  Testing and quality control (inspection bodies such as the  \n",
      "technical inspection agency TÜV)\n",
      "—  Technical sales (sale of technical products)\n",
      "Electrical Engineering  \n",
      "Bachelor of EngineeringSRH University Heidelberg\n",
      "SRH University Heidelberg – Electrical Engineering (B.Eng.) – Status 11/20232At a glance \n",
      "Degree\n",
      "Bachelor of Engineering (B.Eng.) \n",
      "Credit points\n",
      "210 ECTS\n",
      "Start of academic programme\n",
      "Winter semester\n",
      "Duration of study\n",
      "7 semesters\n",
      "Tuition fees\n",
      "—  € 690 per month \n",
      "—  One-time enrolment fee of € 750 \n",
      "—  One-time enrolment fee of € 1,000 \n",
      "for applicants from Non-EEA countries  \n",
      "without permanent residence permit \n",
      "State recognition \n",
      "Accredited and state-recognised\n",
      "Admission requirements\n",
      "—  A general higher education  \n",
      "entrance qualification (Abitur),  \n",
      "a subject-restricted higher  \n",
      "education entrance qualification  \n",
      "(fachgebundene Hochschulreife) or \n",
      "an entrance qualification for studies \n",
      "at universities of applied sciences \n",
      "(Fachhochschulreife)\n",
      "—  Alternatively: at least two years  \n",
      "of relevant vocational training and \n",
      "a minimum of three years’  \n",
      "professional experience, plus the \n",
      "aptitude test\n",
      "—  Or: a master craftsman’s diploma \n",
      "(Meisterbrief) or a technician  \n",
      "qualification (Technikerabschluss)\n",
      "—  Successful participation in the  \n",
      "selection processCourse content and skills\n",
      "You acquire the skills required to successfully  \n",
      "manage projects.\n",
      "Once you have graduated with a degree in Electrical Engineering \n",
      "from SRH University Heidelberg, you will have all the skills you \n",
      "need to operate successfully in the project planning, development \n",
      "and maintenance of complex systems. You will also have the \n",
      "opportunity to specialise in areas of high social relevance:  \n",
      "electromobility, energy technologies and mobile robotics.\n",
      "You engage in practice-based learning.\n",
      "Besides enabling you to acquire the expertise you need, we  \n",
      "specifically prepare you for entering the workforce from the \n",
      "very beginning: working alone or as part of a team, you solve \n",
      "practical problems in a variety of exercises and interdisciplinary \n",
      "projects. As a result, you will be prepared for the challenges of \n",
      "the future. The entire range of engineering positions in industry \n",
      "and public institutions will be open to you.\n",
      "We continually adapt your teaching content to current  \n",
      "developments. By studying at our university, you can therefore \n",
      "be sure of having excellent opportunities in a future-oriented  \n",
      "job market.\n",
      "Apply now!\n",
      "Scan the QR codeSRH University Heidelberg – Electrical Engineering (B.Eng.) – Status 11/20233Your study programme.\n",
      "Instead of getting bogged down with lots of subjects, you concentrate fully on a five-week block \n",
      "(module) in each case. Each block concludes with an examination. This sustainable process helps \n",
      "you to achieve optimal learning outcomes.\n",
      "Semester\n",
      "01Mathematics and \n",
      "Natural Sciences IMathematics and \n",
      "Natural Sciences IIFoundations of \n",
      "Electrical  \n",
      "Engineering IFoundations of  \n",
      "Electrical Engineering II\n",
      "Examination & Credits Kls & Präs I 8 ECTS Kls & Kls I 8 ECTS Kls & Kls I 8 ECTS Kls I 8 ECTS\n",
      "02Foundations of \n",
      "Computer ScienceInnovation and \n",
      "EconomicsAnalogue  \n",
      "ElectronicsElectronics  \n",
      "Development\n",
      "Examination & Credits TPL I 8 ECTS StA I 8 ECTS Kls & Lab I 8 ECTS PA I 6 ECTS\n",
      "03Production and \n",
      "Project  \n",
      "ManagementSensors and  \n",
      "ActuatorsInterconnection Engineering Design\n",
      "Project\n",
      "Examination & Credits PA I 8 ECTS DIV I 8 ECTS Kls I 8 ECTS PA & Präs I 8 ECTS\n",
      "04Business  \n",
      "AdministrationInternship\n",
      "Examination & Credits Kls & Te I 4 ECTS PB I 27 ECTS\n",
      "05Software  \n",
      "EngineeringEmbedded  \n",
      "SystemsSystems Theory Control Systems \n",
      "Engineering\n",
      "Examination & Credits TPL I 8 ECTS TPL I 8 ECTS Kls & Lab I 8 ECTS TPL I 8 ECTS\n",
      "06Information  \n",
      "Processing and \n",
      "Transmission IInformation  \n",
      "Processing and \n",
      "Transmission II♥ Elective I ♥ Elective II \n",
      "Examination & Credits TPL I 8 ECTS Lab I 8 ECTS DIV I 8 ECTS DIV I 8 ECTS\n",
      "07Digital  \n",
      "EngineeringBachelor’s Thesis \n",
      "and Colloquium*In addition, English lectures from Semester 1 to Semester 4\n",
      "Examination & Credits Kls I 8 ECTS Th & Ko I 15 ECTS\n",
      "Kls: Written Exam\n",
      "Präs: Presentation\n",
      "TPL:  Technical Problem Solving\n",
      "StA: Student Research Project \n",
      "Lab:  LaboratoryPA: Project Work\n",
      "DIV:  Various Types of Exams\n",
      "Te: Test\n",
      "PB: Internship ReportTh: Thesis\n",
      "Ko: Colloquium\n",
      "♥ Electives: Please refer to the next page for module \n",
      "content within your elective.\n",
      " Explanation  The university reserves the right to make changes.SRH University Heidelberg – Electrical Engineering (B.Eng.) – Status 11/20234Electives  \n",
      "Sharpen  \n",
      "your profile.\n",
      "Your choice of elective enables you to focus on your \n",
      "personal interests. After completing your basic  \n",
      "studies, you can choose an elective in Semester 6  \n",
      "to explore your interests in greater depth.Automotive Engineering\n",
      "Life Science EngineeringElective I ECTS\n",
      "Electrical Components 8\n",
      "Elective I ECTS\n",
      "Anatomy and Device Technology 8Elective II ECTS\n",
      "Powertrains 8\n",
      "Elective II ECTS\n",
      "Vital Sign Acquisition 8\n",
      "www.heidelberg.de /erweiterung\n",
      "Health City\n",
      "2023/24“The living cell almost always contains,  \n",
      "locked in its interior, the visible or  \n",
      "invisible products of its physiological  \n",
      "activity or its nourishment.”\n",
      "Albrecht Kossel  \n",
      "(1910 Nobel Prize winner for \n",
      "medicine from Heidelberg)Heidelberg  \n",
      "Not only has the famous Heidelberg Castle cast its spell on guests from around the \n",
      "globe: the entire city is nestled in an enchanting environment and is clearly in the pulse \n",
      "of its time. With a historical Old Town that has so much to offer: an abundance of small \n",
      "alleys, each of which tells its own story, and a location that Goethe called “ideal“, Heidel -\n",
      "berg has its place in the hearts of innumerable visitors.\n",
      "However, Heidelberg, at the same time, is a modern city of science and is deemed a light -\n",
      "house of Life Sciences: the University, the European Molecular Biology Laboratory, the \n",
      "German Cancer Research Center and the Heidelberg Max-Planck-Institutes are some of \n",
      "the leading research institutes in the world, making Heidelberg an international \n",
      "renowned science location.\n",
      "The cultural wealth of Heidelberg is just as colorful and diverse as its visitors from \n",
      "around the world offering dance, theater, music, and many events in a diverse cultural \n",
      "landscape that leaves hardly anything to be desired. This, not least, also applies to \n",
      "gastronomy and hotels: From vibrant student pubs to elegant gourmet restaurants, \n",
      "from trendy hotels to five-star accommodation excellence. It is not surprising that \n",
      "barely anyone who has ever been here will keep Heidelberg close to his or her heart.\n",
      "#heidelberg4you\n",
      "instagram.com/heidelberg4youHEIDELBERG4you\n",
      "youtube.com/heidelberg4youHeidelberg4you\n",
      "facebook.com/HeidelbergDear Guests,\n",
      "Dear Family Members,\n",
      "Young, versatile, open-minded, and international – all of these are attributes that fully de-  \n",
      "scribe Heidelberg and its 160,000 citizens. First, however, Heidelberg also is a city of international  \n",
      "renown thanks to its excellent standing in science and research. Germany’s oldest university, \n",
      "the Ruperto Carola is a beacon of knowledge and education. It received the status of “Univer -\n",
      "sity of Excellence” in 2007. The notable institutes, with subjects spanning humanities, social, \n",
      "and legal sciences, as well as natural and life sciences, including medicine, are where today’s \n",
      "and tomorrow’s leaders are teaching and studying. Heidelberg University Hospital promises \n",
      "focused expertise as well. It enjoys global recognition for its outstanding research, teaching, \n",
      "and medical care. It is hardly surprising that researchers and patients alike, find it one of the \n",
      "most important points of contact in terms of health.\n",
      "About 13,800 employees from 130 nations are currently working in over 100 different profes -\n",
      "sions at the Heidelberg University Hospital. On a daily basis, they work together to help  \n",
      "severely ill patients from around the world, who profit not only from a comprehensive range \n",
      "of cutting-edge, successful treatments, but also from ever-new ideas and impulses in patient \n",
      "care, research, and teaching.\n",
      "Top facilities, such as the National Centrum for Tumor Diseases (NCT), the German Cancer \n",
      "Research Center (Deutsches Krebsforschungszentrum, DKFZ), the Zentrum für Molekulare \n",
      "Biologie Heidelberg (ZMBH), four Max-Planck Institutes, the European Molecular  \n",
      "Biology Laboratory (EMBL), or the Center for Integrative Infectious Disease Research (CIID) \n",
      "make Heidelberg a globally leading site for international research at the highest level. These \n",
      "and other institutes research new and improved treatment methods all year round to pre-  \n",
      "serve or restore your health to the best degree.\n",
      "Nonetheless, not only is medical care outstanding internationally in Heidelberg, but our \n",
      "beautiful city in itself will contribute to your recovery on top. With 70 % of forest and green \n",
      "spaces, a moderate climate, and an urban flair, Heidelberg offers one of the highest lifestyle \n",
      "values in Germany. Embedded in the Heidelberg city forest, certified as a “recreational fo -\n",
      "rest”, the world-renowned Heidelberg Schloss (Castle), the Philosophenweg (Philosopher’s \n",
      "Walk), the historical Altstadt (Old Town), the Bahnstadt as the latest and most innovative  \n",
      "addition to the city’s quarters, and many culinary offers are waiting to be explored. These are \n",
      "just a few examples of what makes Heidelberg so attractive. Millions of visitors flock to our \n",
      "beautiful city every year, strolling on one of the longest pedestrian zones in Europe. At a \n",
      "length of 1.6 kilometers, the Hauptstraße (Main Street) with its enchanting small side alleys \n",
      "will invite you to a unique selection of local and international products alike that will leave \n",
      "hardly any shopping wish unfulfilled.\n",
      "This brochure will give you an overview of the medical services of the Heidelberg University \n",
      "Hospital, while at the same time presenting a selection of hotels that specialize in hosting  \n",
      "patients’ accompanying families. On top, we are delighted to give you some inspiration to help \n",
      "you make your time in Heidelberg interesting and diverse based on your own preferences. The \n",
      "Heidelberg Marketing GmbH service team will gladly assist you in planning your individual stay. \n",
      "Feel free to contact us directly and tell us about your personal travel requirements.\n",
      "Welcome to our beautiful city!\n",
      "Yours,\n",
      "Mathias Schiemer\n",
      "CEO\n",
      "Heidelberg Marketing GmbH / Heidelberg Congress\n",
      "Health City  | 5 6 | Health City\n",
      "Outstanding areas\n",
      "of expertise \n",
      "– High-end oncology (all disciplines), for example:  \n",
      " – surgical treatment  \n",
      " – chemotherapy  \n",
      " – innovative radiation therapy including  \n",
      "  proton and carbon ion therapy  \n",
      " – stem cell transplantation  \n",
      " – comprehensive care in the National Center for  \n",
      "  Tumor Diseases Heidelberg \n",
      "– Heart and vascular diseases\n",
      "– Neurology and neurosurgery\n",
      "– Metabolic and endocrine diseases\n",
      "– Orthopedics and Traumatology\n",
      "– Gynecology\n",
      "– PediatricsServices for international \n",
      "patients\n",
      "– Coordination of treatment requests  \n",
      " to all medical departments\n",
      "– Multilingual team at the International Office \n",
      "– Certified interpreters \n",
      "– Support in obtaining a medical visa  \n",
      " to travel to Germany \n",
      "– Friendly, English-speaking nursing personal\n",
      "– All doctors speak fluent English;  \n",
      " medical reports are provided in English \n",
      "– Special requests regarding meals are  \n",
      " taken into consideration \n",
      "– Prayer room for Muslim patients\n",
      "At a glance*\n",
      "– 1,163,400 outpatient treatment cases/ year \n",
      "– 85,500 inpatient treatment cases/ year \n",
      "– 2,500 beds \n",
      "– 50 medical specialist departments \n",
      "– 13,800 employees, including 1,900 professors  \n",
      " and doctors\n",
      " * data source: annual report 2022, roundedHeidelberg University HospitalHealth City  | 7\n",
      "Heidelberg University Hospital is one of the leading medical centers in \n",
      "Europe and offers inpatients and outpatients an innovative and effective \n",
      "diagnosis and therapy for all complex diseases. Every year, many thousands \n",
      "of patients from all over Germany and various other countries worldwide  \n",
      "travel to Heidelberg for medical treatment. Renowned professors, distin -\n",
      "guished physicians and nursing staff, state-of-the-art equipment, as well \n",
      "as the proximity and interdisciplinary cooperation of the specialist depart -\n",
      "ments guarantee the highest standards of medical care. \n",
      "Progress and innovation are essential for promising medical treatment. \n",
      "Heidelberg University Hospital and its partner research institutes, such as \n",
      "the world-renowned German Cancer Research Center, pursue a common \n",
      "aim: the development of new forms of therapy and their quick implemen -\n",
      "tation for the benefit of the patient.\n",
      "Contact and Information\n",
      "Heidelberg University Hospital\n",
      "– International Office\n",
      "Im Neuenheimer Feld 400\n",
      "69120 Heidelberg, Germany\n",
      "Phone +49 6221 56 6243\n",
      "Fax +49 6221 56 33955\n",
      "international.office@med.uni-heidelberg.de\n",
      "www.heidelberg-university-hospital.comAltstadt \n",
      "(Old Town)\n",
      "Main StationHeidelberg CastleKarlstorPhilosophenweg(Philosophers'Walk)\n",
      "Uferstraße\n",
      "Hauptstraße\n",
      "PlöckPlöckZiegelhäuserLandstraße\n",
      "Theodor-Heuss-BrückeErnst-W\n",
      "alz-Brücke\n",
      "BerlinerStraße\n",
      "NeuenheimerLandstraße\n",
      "Neckarstaden\n",
      "BergheimerStraßeIqbal-Ufer\n",
      "EppelheimerStraße\n",
      "ChaisenwegBismarck-\n",
      "platz\n",
      "Klingenteichstraße\n",
      "RohrbacherStraße Alte Brücke\n",
      "(Old Bridge)\n",
      "Neckar\n",
      "Neckarfor pedestrians \n",
      "only\n",
      "Neckar\n",
      "Kurfürsten-AnlageNeuenheim\n",
      "Bergh eim\n",
      "Weststad t\n",
      "SüdstadtPfaffeng rundOld T own\n",
      "CChhaaiisseennweegg\n",
      "8 | Recommended Hotels and Apartments\n",
      "Recommended Hotels\n",
      "and Apartments  \n",
      "Enjoy our hospitality. \n",
      "In an unfamiliar city, it is particularly important for those accompanying a hospital patient to be \n",
      "able to stay in a friendly and comfortable accommodation setup.\n",
      " \n",
      "Therefore, on the following pages, we provide a list of selected Heidelberg hotels and apartments \n",
      "specialized in looking after international guests and providing personalized service, a high level \n",
      "of quality and genuine hospitality excellence – from 3 to 5 star hotels to cosy apartments in leafy \n",
      "surroundings to luxury boutique apartments.\n",
      "Our hosts are looking forward to welcoming you warmly!\n",
      "Map is not true to scale.\n",
      " Note  For your information  \n",
      " 1 km = approx. 0.62 miles and 1 sqm = 10.76 ft²1\n",
      "21\n",
      "28\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "133\n",
      "4\n",
      "5\n",
      "6\n",
      "773\n",
      "11\n",
      "9\n",
      "10 64\n",
      "12\n",
      "Der Europäische Hof \n",
      "Heidelberg\n",
      "Boutique Suites \n",
      "Heidelberg\n",
      "Rafaela Hotel Heidelberg\n",
      "Heidelberg Marriott Hotel\n",
      "Atlantic Hotel Heidelberg\n",
      "Premier Inn Heidelberg  \n",
      "City (Zentrum)\n",
      "Premier Inn Heidelberg  \n",
      "City BahnstadtHilton Hotel Heidelberg\n",
      "Hotel Bergheim 41\n",
      "Apartments – Zweite \n",
      "Heimat Heidelberg\n",
      "Boarding House Luise5\n",
      "Heidelberg Hospitality  \n",
      "Boarding House\n",
      "Midori Guesthouse\n",
      "Heidelberg  \n",
      "University Hospital8\n",
      "513Recommended Hotels and Apartments | 9\n",
      "Der Europäische Hof Heidelberg\n",
      "“Comments made by our guests that the Europäischer Hof Heidelberg, a privately managed  \n",
      "hotel, felt like home to them are the nicest compliments for us and are a testament to our claim \n",
      "of being the friendliest luxury hotel in Germany. The entire team strives to look after each and \n",
      "every guest in a unique atmosphere and with passionate, individualized and enthusiastic service. \n",
      "Sincere excellence in service is a matter of attitude. Moreover: We love what we do!“\n",
      "Dr. Caroline von Kretschmann, Managing Director\n",
      "Your home in Heidelberg. People undergoing medi -\n",
      "cal treatment need an environment in which they \n",
      "feel comfortable and in which they receive special \n",
      "care. Who could possibly do this better than a  \n",
      "private hotel that has been family-managed for  \n",
      "generations? \n",
      "While in Heidelberg, we can offer you a place to stay \n",
      "safe where you can truly feel at home. Our individu -\n",
      "ally furnished rooms and suites provide you and \n",
      "your family with very pleasant surroundings. The  \n",
      "Fine-Dining-Restaurant “Kurfürstenstube“ offers \n",
      "culinary delights tailored to your desires and needs. \n",
      "In addition, you can relax in our PANORAMA SPA \n",
      "with roof terrace. Whenever you visit Heidelberg:  \n",
      "we look forward to welcoming you! \n",
      "Der Europäische Hof Heidelberg\n",
      "Friedrich-Ebert-Anlage 1, 69117 Heidelberg\n",
      "Phone +49 6221 5150\n",
      "reservations@europaeischerhof.com\n",
      "www.europaeischerhof.com\n",
      "– Hygiene & Safety Concept \n",
      "– 119 individual appointed \n",
      " Rooms and Suites \n",
      "– 4 luxuriously equipped \n",
      " Serviced Apartments \n",
      "– 1 Fine-Dining-Restaurant \n",
      " “Die Kurfürstenstube“\n",
      "– 1 Bar & Smokers Lounge\n",
      "– Terrace & Courtyard \n",
      "– 10 Conference Venues\n",
      "– PANORAMA SPA with \n",
      " Roof Terrace \n",
      "– 200 Parking Spaces in \n",
      " the basement \n",
      "– Personalized Service \n",
      "–  Multilingual Staff\n",
      "– Central Location \n",
      "– University Hospital: 4 km \n",
      "– Frankfurt Airport: 85 km 10 | Recommended Hotels and Apartments\n",
      "Boutique Suites Heidelberg\n",
      "“HOME MEETS HOTEL. The Boutique Suites Heidelberg expresses a luxurious and urban lifestyle with  \n",
      "a professional focus on long-term stays for medical reasons. You are safe with us! The hotel`s concept \n",
      "adapts perfectly to the new situation on safe travel. Each single suite features an independent resi-  \n",
      "dential ventilation unit and autonomous A/C to protect you and help to focus on the essential. We look \n",
      "forward to welcome you and guarantee you an atmosphere of privacy and comfort, where your well -\n",
      "being is most important.”\n",
      "Tamimu Gibreel, Chef Concierge\n",
      "YOUR WELLBEING IS OUR PRIORITY. As one of \n",
      "the most exclusive Boutique Suite Hotels in the \n",
      "whole region, we offer extra spacious serviced \n",
      "apartments for international guests with high \n",
      "demands. It is our priority to make you feel as \n",
      "comfortable as possible. Settle down and gain \n",
      "strength in one of our exclusively designed sui -\n",
      "tes with state-of-the-art technology. Experi -\n",
      "ence a dedicated and personalized service, \n",
      "enjoy your privacy and independence and  \n",
      "discover highest living comfort with premium \n",
      "equipment at the Boutique Suites Heidelberg \n",
      "– Alte Zigarrenmanufaktur. \n",
      "Boutique Suites Heidelberg \n",
      "Römerstrasse 80, 69115 Heidelberg \n",
      "Phone +49 6221 659369 \n",
      "welcome@boutiquesuitesheidelberg.com \n",
      "www.boutiquesuitesheidelberg.com\n",
      "– 3 residences 100–125 sqm \n",
      "– 20 suites sized  \n",
      " 40–100 sqm, with kitchen,  \n",
      " living and dining area \n",
      "– Accessible apartments  \n",
      " for long-term guests \n",
      "– Superior suites with  \n",
      " 2 bathrooms \n",
      "– Private parking places  \n",
      " right in front of the  \n",
      " building \n",
      "– Personal concierge  \n",
      " service \n",
      "– Private panorama sauna  \n",
      " and private gym \n",
      "– University hospital: 3 km \n",
      "– Ethianum: 1.5 km \n",
      "“Your well-being is important to us. As a result, we align our actions with it. We want you to feel right at \n",
      "home here to ensure that you will enjoy coming back! Thanks to your guest ratings, we have received \n",
      "numerous awards in the last two years. We look forward to your visit and will gladly adjust to your indi -\n",
      "vidual needs.” \n",
      "Enza Occhipinti-Apfel, Managing Director\n",
      "In its central, idyllic location with a modern air, \n",
      "Rafaela Hotel is located near the hospitals and \n",
      "research facilities in the Neuenheimer Feld, and \n",
      "very close to the popular Neckarwiese (banks of \n",
      "the Neckar) and the Altstadt (Old Town) with its \n",
      "tourist sights. Gastronomy beckons from a small \n",
      "market square right outside the hotel door, com -\n",
      "plete with outdoor seating, French, Italian, and \n",
      "sustainable specialties stores even though this is \n",
      "a calm side street. Rafaela Hotel is a new building \n",
      "with elevator, air conditioning, and level-access \n",
      "showers. \n",
      "The University Hospital is seven minutes away by \n",
      "taxi/car, twelve by bus, and can be reached on \n",
      "foot in 25 minutes.  \n",
      "Rafaela Hotel Heidelberg \n",
      "Lutherstrasse 17, 69120 Heidelberg \n",
      "Phone +49 6221 6743300 \n",
      "info@rafaela-hotel.com \n",
      "www.rafaela-hotel.com\n",
      "– Air conditioning \n",
      "– Free high-speed Wi-Fi \n",
      "– 50” flat-screen TV \n",
      "– Refrigerator \n",
      "– Safe \n",
      "– Central location \n",
      "– Calm side street \n",
      "– Idyllic environment \n",
      "– Modern new building  \n",
      " (2019) \n",
      "– University Hospital:  \n",
      " 1.75 km \n",
      "Recommended Hotels and Apartments | 11\n",
      "Rafaela Hotel Heidelberg\n",
      "12 | Recommended Hotels and Apartments\n",
      "Heidelberg Marriott Hotel\n",
      "„At the Heidelberg Marriott Hotel we understand individual needs. Our associates are just as diverse, \n",
      "with different cultural backgrounds as our guests are, and therefore ready to cater for all your needs \n",
      "with that typical Heidelberg spirit to serve. Being part of the biggest travel company in the world with \n",
      "more than 8000 hotels, you can rest assured that the Heidelberg Marriott Hotel observes the highest \n",
      "health & safety standards and with views overlooking the Neckar River, both body and mind, can take a \n",
      "well-deserved break from your daily hassle.”\n",
      "Lukas Bechtel, Director of Sales & Marketing\n",
      "The Heidelberg Marriott Hotel is located on the banks \n",
      "of the river Neckar within walking distance of the old \n",
      "town and the picturesque Heidelberg shopping mall. \n",
      "Some of our quiet rooms overlook the river facing the \n",
      "well-known University Hospitals. The Restaurant Grill \n",
      "16, with its fantastic sun deck, offers regional as well as \n",
      "international specialties but is mainly famous for its \n",
      "fabulous steaks. Besides our generous American \n",
      "breakfast buffet, we also offer the most common Ara -\n",
      "bian breakfast items. Our Chefs are happy to provide \n",
      "advice in dietary cooking and will not hesitate to fulfil \n",
      "any special requests you may have. To make your time \n",
      "in Heidelberg as pleasant as possible we offer you free \n",
      "access to our indoor pool, health club and sauna. If \n",
      "wanted you can also gain access to our unique execu -\n",
      "tive lounge, featuring a free bar with snacks during \n",
      "lunch and dinner, as well as a rooftop terrace.\n",
      "Heidelberg Marriott Hotel\n",
      "Vangerowstrasse 16, 69115 Heidelberg\n",
      "Phone +49 6221 9080\n",
      "salesoffice.heidelberg@marriott.com\n",
      "www.Heidelberg-Marriott.com\n",
      "– Right next to the  \n",
      " Neckar River\n",
      "– 234 quiet rooms and  \n",
      " 14 suites, some with a  \n",
      " water view\n",
      "– Arabian TV channels &  \n",
      " Free Wi-Fi\n",
      "– Restaurant Grill 16  \n",
      " with sun deck\n",
      "– Room service\n",
      "– Water taxi during  \n",
      " summer months\n",
      "– Indoor swimming pool,  \n",
      " fitness center with sauna\n",
      "– University Hospital 2.0 km\n",
      "“Opened in 2023, the ATLANTIC Hotel Heidelberg impresses with its central location, its state of the art \n",
      "facilities, comfort, and an unobtrusive service that always focuses on the guest. In our brand new Grand \n",
      "Suite and in our suites, Heidelberg is at your feet. Enjoy the magnificent, unique view over the city or the \n",
      "Rhine-Neckar metropolitan region. We look forward to welcoming the world to the ATLANTIC Hotel \n",
      "Heidelberg.”\n",
      "Stephan Sporer, Hotel Director\n",
      "At the northern German ATLANTIC Hotels \n",
      "Group, you can expect exceptional 4 to 5-star \n",
      "comfort and individual full service! In direct \n",
      "proximity to the new Heidelberg Congress \n",
      "Center and the railway station, the 4-star-supe -\n",
      "rior ATLANTIC Hotel Heidelberg offers 310 sty -\n",
      "lish rooms and suites, a spa and wellness area, \n",
      "a restaurant, and a rooftop bar with a terrace \n",
      "and views over Heidelberg and the surround -\n",
      "ing area. State-of-the-art conference rooms \n",
      "and a 350 sqm flexible ballroom on the 13th \n",
      "floor round off the facilities.  \n",
      "ATLANTIC Hotel Heidelberg \n",
      "Europaplatz 1\n",
      "69115 Heidelberg\n",
      "E-Mail: heidelberg@atlantic-hotels.de\n",
      "For reservations\n",
      "Phone +49 (0) 421 944888-535\n",
      "E-Mail: heidelberg@atlantic-hotels.de\n",
      "– 1 Grand Suite with 105 sqm  \n",
      " with possibility to extend  \n",
      " by an additional suite to  \n",
      " 153 sqm \n",
      "– 19 suites with 48 sqm \n",
      "– 3 wheelchair accessible  \n",
      " rooms\n",
      "– 310 rooms and suites,  \n",
      " with air conditioning, coffee  \n",
      " machine, minibar and safe \n",
      "– 350 sqm ballroom on the  \n",
      " 13th floor and additional 7  \n",
      " event facilities\n",
      "– Free WIFI\n",
      "– Restaurant and bar on  \n",
      " the 15th floor\n",
      "– Room service\n",
      "– Laundry service \n",
      "– Wellness and fitness area\n",
      "– Parking spaces \n",
      "– University Hospital 2.5 km \n",
      "– Ethianum 1.5 km\n",
      "Recommended Hotels and Apartments | 13\n",
      "Atlantic Hotel Heidelberg\n",
      "14 | Recommended Hotels and Apartments\n",
      "Premier Inn Heidelberg City Bahnstadt & \n",
      "Premier Inn Hotel Heidelberg City (Zentrum)\n",
      "„A good night‘s sleep is our top priority, which is why we do everything for your well-being and promise \n",
      "you a restful night. We offer premium quality at economy prices because Premier Inn rooms are  \n",
      "not only comfortable and functional by our international standards, they are also very affordable:  \n",
      "„Everything Premium. Except for the price.“ Try it out yourself at our two hotels in Heidelberg - our warm, \n",
      "intercultural teams look forward to meeting you!”\n",
      "Miriam Saalmann, Cluster Hotel Manager\n",
      "Our two Premier Inn hotels in Heidelberg are ide -\n",
      "ally located nearby the university hospitals as well \n",
      "as the famous sights in the old town. The proximi -\n",
      "ty to the main railway station and the connection \n",
      "to the motorways also ensure excellent connec -\n",
      "tions to the airports in Frankfurt and Stuttgart or \n",
      "to exciting excursion destinations in the surroun -\n",
      "ding area. After an eventful day, you will be welco -\n",
      "med in a spacious and air-conditioned room whe -\n",
      "re you can relax and submerge in a sound sleep in \n",
      "your comfortable Hypnos bed.  W e offer special \n",
      "conditions for your stay with a medical back -\n",
      "ground - reach out to us when booking.\n",
      "Premier Inn Heidelberg Bahnstadt\n",
      "Czernyring 26-28, 69115 Heidelberg\n",
      "Phone +49 6221 6484896\n",
      "E-Mail: Heidelberg.CityBahnstadt@premierinn.com\n",
      "www.premierinn.de\n",
      "Premier Inn Heidelberg City (Zentrum)\n",
      "Kurfürsten-Anlage 23, 69115 Heidelberg\n",
      "Phone +49 6221 6484899\n",
      "E-Mail: Heidelberg.City@premierinn.com\n",
      "www.premierinn.de\n",
      "– two premium \n",
      " economy hotels\n",
      "– 259 rooms in total \n",
      "– bathrooms with \n",
      " walk-in shower\n",
      "– three wheelchair \n",
      " accessible rooms \n",
      "– kettle with tea & coffee\n",
      "– Lounge with bar & menu\n",
      "– Breakfast buffet \n",
      " (full English)\n",
      "– Relax and work zones\n",
      "– Free WLAN\n",
      "– Parking spaces for a fee\n",
      "Premier Inn Heidelberg City\n",
      "Premier Inn Heidelberg BahnstadtRecommended Hotels and Apartments  | 15\n",
      "Warm hospitality in the centre of Heidelberg! Newly \n",
      "opened in 2023, this hotel with 244 elegant and \n",
      "comfortable rooms and suites enjoys a prime loca -\n",
      "tion. It is only a few minutes away from the old town \n",
      "as well as from the university hospital and medical \n",
      "facilities. Enjoy your meal in the hotel‘s restaurant or \n",
      "the exclusive atmosphere in the hotel bar or execu -\n",
      "tive lounge.\n",
      "Hilton Heidelberg  \n",
      "Kurfürstenanlage 1, 69115 Heidelberg \n",
      "Phone +49 (0)6221 3521 320 \n",
      "info@hiltonheidelberg.com  \n",
      "www.heidelberg.hilton.comHilton Hotel Heidelberg\n",
      "– In the city centre of Heidelberg\n",
      "– 244 elegant rooms and suites\n",
      "– Restaurant and bar\n",
      "– Executive lounge\n",
      "– Fitness centre\n",
      "– Free Wi-Fi\n",
      "– 130 underground parking spaces  \n",
      " in the building\n",
      "– Direct connection to public transport\n",
      "– Frankfurt Airport 85 km, shuttle available\n",
      "– University hospital 4 km\n",
      "Contemporary living in a protected monument. \n",
      "Our 16 individual and very well equipped apart -\n",
      "ments are furnished in high quality to make \n",
      "you comfortable. The area offers many restau -\n",
      "rants, cafés and stores to cover your daily \n",
      "needs. The good infrastructure connection will \n",
      "comfortably take you to any sights, event \n",
      "venues and hospitals.\n",
      "Zweite Heimat Heidelberg GmbH  \n",
      "Werderstrasse 43, 69120 Heidelberg \n",
      "Phone +49 6221 6561943 \n",
      "wohnen@zweiteheimatheidelberg.de  \n",
      "www.zweiteheimatheidelberg.deApartments – Zweite Heimat Heidelberg\n",
      "– 4 locations in direct proximity  \n",
      " of the city center \n",
      "– 40 – 90 sqm (1 – 3 rooms) \n",
      "– Winter garden / balcony / terrace /  \n",
      " garden use \n",
      "– Fully furnished kitchen \n",
      "– Towels and bed linen free of charge \n",
      "– Washing machine and tumble-dryer  \n",
      " free of charge \n",
      "– Parking spaces available (partly for a fee) \n",
      "– Wi-Fi included \n",
      "– University Hospital: 1.5 km \n",
      "The Bergheim 41 is easily accessible by both car and \n",
      "public transportation and is in the immediate vicinity \n",
      "of Bismarckplatz, the beginning of the pedestrian \n",
      "shopping area. The hotel has 32 rooms and studios \n",
      "of contemporary design. The university hospitals \n",
      "are just minutes away and can easily be reached by \n",
      "car or public transportation located directly in front \n",
      "of our hotel.\n",
      "Hotel Bergheim 41 \n",
      "Bergheimer Strasse 41, 69115 Heidelberg  \n",
      "Phone +49 6221 750040 \n",
      "info@bergheim41.de  \n",
      "www.bergheim41.deHotel Bergheim 41\n",
      "– 32 rooms and studios \n",
      "– Junior suite with in room sauna and  \n",
      " roof terrace \n",
      "– Located just 300 meters from  \n",
      " the pedestrian mall \n",
      "– Kaffeekultur B41 (on-site coffee shop) \n",
      "– High-speed Wi-Fi (up to 100 Mbit / s) \n",
      "– Roof garden with view of the Castle \n",
      "– Individually controlled air-conditioning \n",
      "– University Hospital: 3.2 km \n",
      "The Boardinghouse Luise5 is a unique jewel! Enjoy \n",
      "modern and spacious accommodation (each unit > \n",
      "50 sqm) at the very heart of Heidelberg and refurbis -\n",
      "hed historical building, just behind the ATOS-Clinic. \n",
      "Calmly located in short distance from the main \n",
      "street is ideal for short trips, long stays and business \n",
      "travelers. Meanwhile main sights, the Neckar River \n",
      "promenade, restaurants and public transport stops \n",
      "are easily reached.\n",
      "Boardinghouse Luise5  \n",
      "Luisenstrasse 5, 69115 Heidelberg \n",
      "Phone +49 6221 655 6114 \n",
      "welcome@luise5-boardinghouse.de  \n",
      "www.luise5-boardinghouse.deBoardinghouse Luise5\n",
      "– 6 spacious two room apartments spread  \n",
      " over 50 sqm for up to 4 persons \n",
      "– Light living room with extensible sofa bed,  \n",
      " dining table and working area \n",
      "– Fully furnished kitchen \n",
      "– Free Wi-Fi and LED TV in each room \n",
      "– Close to Bismarckplatz, main shopping area  \n",
      " and Atos Clinic \n",
      "– Onsite parking spaces are available per  \n",
      " request with applicable daily fee \n",
      "– Long stay guests can benefit from our  \n",
      " washing machine and dryer \n",
      "– Pets are welcome with additional daily costs\n",
      "High quality and sustainable living, relaxing \n",
      "and meeting in an energy-efficient passive \n",
      "house standard at the gates of Heidelberg, and \n",
      "close to Heidelberg University Hospital.\n",
      "Midori - the green guesthouse \n",
      "Friedrich-Ebert-Strasse 4  \n",
      "69221 Dossenheim / Heidelberg \n",
      "Phone +49 6221 - 87 29 80 \n",
      "info@midori-guesthouse.com  \n",
      "www.midori-guesthouse.comMidori Guesthouse\n",
      "– 66 feel-good rooms \n",
      "– 4 spacious suites\n",
      "– free W-Lan\n",
      "– free garage\n",
      "– Public transport only 2 minutes \n",
      "– Panorama lounge / roof terrace\n",
      "– Short or long term stay \n",
      "– Breakfast & Bistro\n",
      "Our beautiful serviced apartments are centrally \n",
      "located between the University Hospital and the \n",
      "Old Town. The 1-room apartments (approx. 25 \n",
      "m²) are equipped with a double bed, 49-inch \n",
      "smart TV, hotel safe, kitchenette, and a bathroom \n",
      "with ground-level shower. The 2-room suites  \n",
      "(approx. 40 m²) also have a living room with a \n",
      "sofa bed. Book your temporary home!\n",
      "Heidelberg Hospitality Boarding House  \n",
      "Kurfürsten-Anlage 47-51, 69115 Heidelberg \n",
      "Phone +49 (0)6221 – 710 318 \n",
      "info@heidelberg-hospitality.de  \n",
      "www.heidelberg-hospitality.deHeidelberg Hospitality Boarding House\n",
      "– Contact-free check-in available \n",
      "– Free Wi-Fi use \n",
      "– Non-smoking hotel \n",
      "– Coffee machine & refrigerator \n",
      "– Rooms with connecting door \n",
      "– Elevator \n",
      "– University Hospital: 1.5 km \n",
      "– Main train station: 0.5 km \n",
      "– Old town: 1.7 km\n",
      "16 | Recommended Hotels and ApartmentsMUSEUM  HEIDELBERG\n",
      "DAILY  \n",
      "10 AM – 6 PMWhat the BODY &\n",
      " \n",
      "are all about\n",
      " Airport / Health / Wedding / Limousine & Sprinter\n",
      "Be it in an elegant limousine, a comfortable Sprinter or a classic  \n",
      "Mercedes-Benz vintage car: TLS is your trusted automotive partner for all \n",
      "travel connections to Heidelberg and in the Rhine-Neckar region. Our health \n",
      "shuttles are happy to drive patients with and without medical equipment to \n",
      "clinical treatments and check-up appointments. Barrier-free and e-wheelchair \n",
      "accessible rides can be provided on request. \n",
      "More information about TLS Heidelberg is available \n",
      "online at: www.tls-heidelberg.de  \n",
      "or by phone: +49 6221 770077\n",
      "We look forward to meeting you!TLS – Travel well with usSpecial Activity \n",
      "Programs\n",
      "Experience Heidelberg with its famous trio of \n",
      "the castle, the Neckar River, and the Old Town \n",
      "in a unique way. Wander through the castle \n",
      "gardens; take a boat ride, a guided tour of the \n",
      "town, a shopping tour, or a trip on a segway. \n",
      "We have compiled a number of exciting and in -\n",
      "teresting experiences for you to make your \n",
      "stay as pleasant as possible. Of course, you can \n",
      "combine our special activity programs with \n",
      "overnight hotel stays in all-inclusive packages. \n",
      "We are here to help make your wishes come \n",
      "true! Please contact us – we would be deligh -\n",
      "ted to talk with you in person.\n",
      "16 | Special Activity ProgramsExperiences\n",
      " \n",
      "Zoo Heidelberg\n",
      "Find the red panda hidden in the branches or watch our \n",
      "elephant males taking a dip – there is a lot to see in our \n",
      "zoo: Majestic lions, elegant tigers or our “cuddly“ brown \n",
      "bears, comical meerkats, ancient giant tortoises (a centu -\n",
      "ry of lifetime experiences in every shell!), our gorilla fami -\n",
      "ly or the cool chimpanzee gang – just a sample of our \n",
      "wide variety of fascinating animals.\n",
      "Botanical Garden Heidelberg  \n",
      "The Botanical Gardens of the University of Heidelberg, \n",
      "one of the oldest in Germany, was originally established \n",
      "in 1593 as a “Hortus Medicus“ for the cultivation of medi -\n",
      "cinal herbs. Nowadays, this institution is important for its \n",
      "contribution to botanical studies. The greenhouses con -\n",
      "tain unique plant collections (e.g. Old and New World \n",
      "succulents and orchids). Walking along the outdoor sec -\n",
      "tions, visitors can enjoy the fascinating vegetation of the \n",
      "raised bog, heath bog, alpine garden, and weedy vi -\n",
      "neyard, as well as a “systematic section“.\n",
      "Shopping\n",
      "Heidelberg’s famous shopping area takes you through \n",
      "the middle of the Old Town: Over one kilometer in length, \n",
      "the main street is one of the longest pedestrian malls in \n",
      "Europe and full of temptations at every turn – from indi -\n",
      "vidual owner-operator shops and department stores \n",
      "with a wide product range to branch stores in internatio -\n",
      "nal labels. Also, in the little side streets peeling off the \n",
      "pedestrian mall and in the suburbs of Heidelberg there \n",
      "are surprises waiting at every corner: here you will find \n",
      "antiquarian bookshops, galleries, young boutiques and \n",
      "quaint little shops.\n",
      " Service and more information\n",
      "Heidelberg Marketing GmbH, Phone +49 6221 58-40 223/225,  \n",
      "health-city@heidelberg-marketing.de, www.heidelberg-marketing.comSpecial Activity Programs | 17Guided Tours\n",
      " \n",
      "Old Town Tour\n",
      "Winding alleys and historic squares, fascina -\n",
      "ting museums and galleries: Heidelberg’s Old \n",
      "Town offers a wide range of things to do and \n",
      "see. A guided tour around the Old Town brings \n",
      "the past to life. Whether the Heiliggeistkirche \n",
      "(Church of the Holy Spirit), the Jewish Quarter, \n",
      "Germany’s oldest university with the Studen -\n",
      "tenkarzer (student’s prison), or any of the many \n",
      "sights, every corner of the Old Town has an in -\n",
      "teresting story to tell. The route also takes \n",
      "you along the main street, one of the longest \n",
      "pedestrian zones in Europe. Recommended  \n",
      "duration: 1.5 to 2 hours.\n",
      "Bus Tours \n",
      "Get to know the city with a guided tour along \n",
      "the Neckar River to the Heidelberg Castle. \n",
      "During the tour, you can immerse yourself in \n",
      "the history of the world-famous building and \n",
      "its breathtaking story. Alternatively, take a ride \n",
      "on the convertible sightseeing bus with its in -\n",
      "formative commentary and numerous tips for \n",
      "additional activities in the city.\n",
      "Boat Tours\n",
      "Taking a boat tour through the narrow Neckar \n",
      "valley, with its hillsides full of vineyards and de -\n",
      "ciduous forests, is quite an experience. It is \n",
      "also possible to book tours in combination with \n",
      "guided tours through the Old Town or the \n",
      "monastery.Castle Tour\n",
      "It towers majestically above the roofs of the \n",
      "Old Town and impresses millions of people \n",
      "year after year – the Heidelberg Castle. With a \n",
      "tour through its courtyard and garden, you can \n",
      "dive in the colorful history of the world-famous \n",
      "structure. A visit to the Great Barrel, the largest \n",
      "wine barrel in the world ever to be filled, is also \n",
      "featured on the program (no inside areas). \n",
      "Recommended duration of 1.5 or 2 hours.\n",
      "Segway Tour\n",
      "A Segway tour through the city provides entire -\n",
      "ly new perspectives. A guide will accompany \n",
      "you on the route along the Neckar River to the \n",
      "historic Neuenheim district. From here, you will \n",
      "continue in the direction of the zoo and \n",
      "through the Neuenheimer Feld campus with \n",
      "its impressive university and hospital buil -\n",
      "dings. The highlight is the journey along the \n",
      "Philosophenweg (Philosophers’ Walk).\n",
      "Themed Tours \n",
      "Varieties of theme tours are available, inclu -\n",
      "ding a guided tour of the famous University or \n",
      "the traditional Christmas market, or the Philo -\n",
      "sophers’ Walk – to mention but a few. Duration \n",
      "of 2 hours.\n",
      " Service and more information\n",
      "Heidelberg Marketing GmbH, Phone +49 6221 58-40 223/225, \n",
      "health-city@heidelberg-marketing.de, www.heidelberg-marketing.com\n",
      "Gültig ab / Valid from\n",
      "FamilyHeidelbergCARD  \n",
      "Heidelberg in your pocket!\n",
      "• Discounts with attractive partners from gastronomy, \n",
      "retail, culture and leisure\n",
      "• With castle ticket including funicular railway\n",
      "• Free use of public transport in the city area\n",
      "• Combo ticket: one-time free admission to the University Museum, \n",
      "the Student Prison and the special exhibitionstarting from \n",
      "€ 26\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from18 | Special Activity Programs\n",
      "Culture and Festival Town\n",
      "Dance, theater, music – our cultural scene presents itself center stage! Heidelberg has much \n",
      "to offer as a festival town and invites cultural enthusiasts to get to know its many facets.\n",
      " \n",
      "March 15 – April 13, 2024\n",
      "International Music Festival: Heidelberger Frühling (Heidelberg Spring)\n",
      "The “Heidelberger Frühling” (Heidelberg Spring) is an “idiosyncratic hotspot for music” (F.A.Z.) and hosts \n",
      "over 100 events including star-spangled classical concerts, innovative productions which experiment \n",
      "with dance or multi-media, as well as unconventional concert formats and genre cross-overs. Of parti -\n",
      "cular interest is the song category. https://www.heidelberger-fruehling.de/en\n",
      "April 16 – May 5, 2024\n",
      "Heidelberger Stückemarkt (Play Market)\n",
      "The avant-garde of theater presents outstanding premieres from the German-speaking area. New \n",
      "plays are shown and social discourse is initiated. The international highlight is the annually changing \n",
      "guest country. Sweden will be our guest country in 2024. Though still barely known as a theatre country \n",
      "in Germany, it stands out with its vibrant and innovative scene. https://www.theaterheidelberg.de/en\n",
      "June 1 and September 7, 2024\n",
      "Heidelberg Castle Illuminations with Fireworks\n",
      "Every year the legendary Castle Illuminations capture the imagination of thousands of people – there is \n",
      "hardly another city that can boast such magical nights. Bengali flares slowly bathe Heidelberg Castle in \n",
      "a mysterious red firelight, as if the ruins were on fire once again in their long history. When the glow of \n",
      "the castle slowly dies down, the second part of the spectacle begins – the brilliant fireworks over the \n",
      "Neckar. The banks of the Neckar and the Philosophers’ Walk around 10 pm are the best locations from \n",
      "which to watch the Castle Illuminations. www.heidelberg-marketing.com\n",
      "June 9 – July 28, 2024\n",
      "Heidelberger Schlossfestspiele (Heidelberg Castle Festival)\n",
      "Heidelberg’s theater and orchestra make the famous castle ruins reverberate with the sounds of music, \n",
      "fun, and dramatic plays alike. The successful musical comedy “Im Weissen Rössl” (The White Horse Inn) \n",
      "and the famous vampire “Dracula” will be played in the Schlosshof (Castle Courtyard). A diverse concert \n",
      "program with tomorrow’s masters and the best international soloists ensures a choice of entertainment \n",
      "under the open sky that has yet to find its equal. https://www.theaterheidelberg.de/en\n",
      "End of July – beginning of August, 2024\n",
      "Metropolink – Festival of Urban Art\n",
      "The Metropolink Festival is displaying expansive wall paintings by internationally acclaimed practi -\n",
      "tioners of urban art in the whole city area. Convertible bus tours take visitors to the various decora -\n",
      "ted houses and facades. Public tours during the festival and tours for groups are available all year. \n",
      "The city turns into a public art space and Heidelberg shows that it has truly earned the title of \n",
      "“UNESCO Creative City“. www.metropolink.art\n",
      " Please note that events may be cancelled or change due to unforeseen circumstances.20 | Culture and Festival Town \n",
      "August 24 – 25, 2024\n",
      "Summer at the river\n",
      "For two days, a promenade will be created on between the Alte Brücke and Marstall, where people can \n",
      "dance, sing, play or just chill. Two stages invite you to linger and offer a various programme - from brass \n",
      "music to jazz and Dixieland to party hits. Stroll, relax, and enjoy the river. Boat trips  offer magical per -\n",
      "spectives of the city, and the Children‘s Area on Krahnenplatz offers fun, games and excitement with a \n",
      "hands-on programme. www.heidelberg-marketing.com\n",
      "September 28 – 29, 2024\n",
      "Heidelberger Herbst (Heidelberg Autumn Festival)\n",
      "Live Music, the Craft Market, Flea Market, culture, and stalls with regional specialties – Heidelberg`s  \n",
      "greatest city festival on the last weekend in September always enjoys the dependable cooperation  \n",
      "of many associations, gastronomes, ,and cultural organizers who all contribute to making it such a  \n",
      "success. By the evening, the Old Town becomes a huge open-air concert, offering music for every taste \n",
      "and venues all over the Old Town. www.heidelberg-marketing.com\n",
      "Beginning of October – beginning of November, 2024\n",
      "International Enjoy Jazz Festival\n",
      "Enjoy Jazz means high-quality festival weeks with an extraordinary program hosting many different  \n",
      "artists and music genres. The festival welcomes artists from all nations on the stages of the festival cities \n",
      "of Heidelberg, Mannheim and Ludwigshafen. Focusing on high-quality jazz, the festival in the Rhine-\n",
      "Neckar metropolitan region has found international renown as a high class musical event. It takes over \n",
      "the stages to present outstanding musicians for a period of nearly seven weeks. www.enjoyjazz.de\n",
      "October 2 – 13, 2024\n",
      "Heidelberg Wine Village\n",
      "Viticulture in Heidelberg is characterised by a centuries-old tradition. Heidelberg benefits from a mild \n",
      "climate and offers vines on sun-drenched slopes everything they need to thrive and produce high-\n",
      "quality wines. Enjoy local wines and delicious drops from the region in the heart of the Old Town. This \n",
      "event is accompanied by regional cuisine and atmospheric live music. www.heidelberg-marketing.com\n",
      "November 25 – December 22, 2024\n",
      "Heidelberg Christmas Market\n",
      "The aroma of roasted almonds, cinnamon and hot chestnuts wafts through the winding alleyways of \n",
      "the Old Town. It is Christmas time – the Heidelberg Old Town revels in Christmas magic. The around 140 \n",
      "decorated booths in the Old Town and “Germany’s most beautiful ice-rink“ open against the impressive \n",
      "setting of the Castle, and in the blink of an eye the market fills with the people of Heidelberg as well as \n",
      "many regional and international guests. The atmosphere is captivating – the stalls nestled idyllically \n",
      "amongst the historic squares of the Old Town shed a warm glow along one of the longest pedestrian \n",
      "zones in Europe. www.heidelberg-marketing.com\n",
      "Culture and Festival Town | 21\n",
      "22 | Your way to Health City\n",
      "Your journey to Health City  \n",
      "Located in the south-west of Germany – the heart of Europe – Heidelberg is also a convenient 80 km \n",
      "distance from Frankfurt International Airport (www.frankfurt-airport.de), 120 km from Stuttgart Airport \n",
      "(www.flughafen-stuttgart.de) and around 90 km from the Karlsruhe / Baden-Baden Airport\n",
      "(www.baden-airpark.de). It is so easy to get here. We hope to see you soon.\n",
      "Europ e\n",
      "Berlin\n",
      "100 kmGerman y\n",
      "Baden-Württember g\n",
      " \n",
      "FSC ® paper was produced with \n",
      "green electricity.   Imprint\n",
      "Heidelberg Marketing GmbH  \n",
      "Neuenheimer Landstrasse 5  \n",
      "69120 Heidelberg  \n",
      "Germany\n",
      "The Heidelberg Marketing GmbH is a \n",
      "subsidiary of the City of Heidelberg.\n",
      "Layout  \n",
      "aB Grafik | Artem Bathauer \n",
      "www.a-b-grafik.de\n",
      "Photos\n",
      "Cover page: Tobias Schwerdt, \n",
      "Heidelberg University Hospital /  \n",
      "Hendrick Schröder  \n",
      "Page 3, 4, 5, 15, 16, 17, 19, 20, 21: \n",
      "Tobias Schwerdt  \n",
      "Page 6, 7: Heidelberg University Hospital / \n",
      "Hendrick Schröder  \n",
      "Page 9 – 14: Hotel photos of the respective \n",
      "accommodation  \n",
      "Page 20: Susanne Reichardt, Steffen Schmid \n",
      "Page 21: Jürgen Spachmann\n",
      "© Copyright 2024  \n",
      "All contents, in particular texts,  \n",
      "photographs and graphics, are  \n",
      "protected by copyright. Unless  \n",
      "expressly stated otherwise,\n",
      "Heidelberg Marketing GmbH owns  \n",
      "the copyright.Heidelberg  \n",
      "Marketing GmbH\n",
      "Neuenheimer Landstrasse 5  \n",
      "69120 Heidelberg\n",
      "Germany  \n",
      "Phone +49 6221 58-40 223/225\n",
      "health-city@heidelberg-marketing.de\n",
      "www.heidelberg-marketing.com\n",
      "Heidelberg  \n",
      "University Hospital –\n",
      "International Office\n",
      "Im Neuenheimer Feld 400\n",
      "69120 Heidelberg\n",
      "Germany\n",
      "Phone +49 6221 56-6243\n",
      "international.office@med.uni-heidelberg.de\n",
      "www.heidelberg-university-hospital.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) Data Visualization and Storytelling 1+2: Design  Basics and Designing Interactive \n",
      "Dashboards  – Module responsible by Prof. Dr. Swati Chandna , Room: LGS 6, 2nd \n",
      "floor, Arc 215 , E-mail: swati.chandna@srh.de  Phone: +49 6221 6799 -223  \n",
      "2) Data Visualization and Storytelling 3: Advanced  Data Visualization  - Module \n",
      "responsible by Prof. Dr. Swati Chandna, Room: LGS 6, 2nd floor, Arc 215, E -mail: \n",
      "swati.chandna@srh.de Phone: +49 6221 6799 -223  \n",
      "3) First Steps into Case Studies  – Module responsible by Prof. Dr. Swati Chandna, \n",
      "Room: LGS 6, 2nd floor, Arc 215, E -mail: swati.chandna@srh.de Phone: +49 6221 \n",
      "6799-223  \n",
      "4) Case Study 1 - Module responsible by Prof. Dr. Swati Chandna, Room: LGS 6, 2nd \n",
      "floor, Arc 215, E -mail: swati.chandna@srh.de Phone: +49 6221 6799 -223  \n",
      "5) Master Thesis Project - Module responsible by Prof. Dr. Swati Chandna, Room: LGS \n",
      "6, 2nd floor, Arc 215, E -mail: swati.chandna@srh.de Phone: +49 6221 6799 -223  \n",
      "6) Data Engineering 1: Big Data Databases - Module responsible by Prof. Dr. Binh Vu , \n",
      "Room: LGS 6, 2nd floor, Arc 215, E-Mail: binh.vu@srh.de , Phone: +49 6221 6799 -208  \n",
      "7) Data Engineering 2: Big Data Architectures : Module responsible Prof. Dr. Swati \n",
      "Chandna, Room: LGS 6, 2nd floor, Arc 215, E -mail: swati.chandna@srh.de Phone: \n",
      "+49 6221 6799 -223  \n",
      "8) Big Data Programming: Python : Module responsible Dr.-Ing Kamellia Reshadi , \n",
      "Room: LGS 6, AND 2, Arc 213 , E-mail: kamellia.reshadi@srh.de  Phone: +49 6221 \n",
      "6799-216  \n",
      "9) Data Management 1: Data Acquisition and Data  Cleaning  - Module responsible by \n",
      "Prof. Dr. Theodoros Soldiers , Room: LGS 6, 2nd floor, Arc 215, E -mail: \n",
      "theodoros.soldatos@srh.de  Phone: +49 6221 6799 -213  \n",
      "10) Data Management 2: Data Curation and Data  Management  - Module responsible by \n",
      "Prof. Dr. Binh Vu, Room: LGS 6, 2nd floor, Arc 215, E -Mail: binh.vu@srh.de, Phone: \n",
      "+49 6221 6799 -208  \n",
      "11) Data Analytics 1: Statistics and Machine  Learning - Module responsible by Prof. Dr. \n",
      "Theodoros Soldiers, Room: LGS 6, 2nd floor, Arc 215, E -mail: \n",
      "theodoros.soldatos@srh.de Phone: +49 6221 6799 -213  \n",
      "12) Data Analytics 2: Text Mining and Natural  Language Processing  - Module responsible \n",
      "by Prof. Dr. Swati Chandna, Room: LGS 6, 2nd floor, Arc 215, E -mail: \n",
      "swati.chandna@srh.de Phone: +49 6221 6799 -223  \n",
      "13) Data Analytics 3: Deep Learning - Module responsible by Prof. Dr. Binh Vu, Room: \n",
      "LGS 6, 2nd floor, Arc 215, E -Mail: binh.vu@srh.de, Phone: +49 6221 6799 -208  \n",
      "14) Privacy, Ethics and International Law  - Module responsible by Prof. Dr. Swati \n",
      "Chandna, Room: LGS 6, 2nd floor, Arc 215, E -mail: swati.chandna@srh.de Phone: \n",
      "+49 6221 6799 -223  15) Case Study 2  - Module responsible by Prof. Dr. Swati Chandna, Room: LGS 6, 2nd \n",
      "floor, Arc 215, E -mail: swati.chandna@srh.de Phone: +49 6221 6799 -223  \n",
      "16) ADSA Internship  - Module responsible by Prof. Dr. Binh Vu, Room: LGS 6, 2nd floor, \n",
      "Arc 215, E -Mail: binh.vu@srh.de, Phone: +49 6221 6799 -208  \n",
      "17)  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRH University \n",
      "Heidelberg   \n",
      " \n",
      " \n",
      " \n",
      "Tuition fees  \n",
      " (effective from April 1,  2023 – EEA students ) \n",
      " \n",
      "Programme  Monthly fees  \n",
      "(EEA students)   \n",
      "Master    \n",
      "Applied Computer Science (Master o f Science)  750 €  \n",
      "Applied Data Science and Analytics  \n",
      "(Master of Science)  790 €  \n",
      "Architecture – Design for the Built Environment (Master of Arts)  750 €  \n",
      "Dance Movement Therapy (Master of Arts)  690 €   \n",
      "Global Business and Leadership (Master of Arts)  770 €   \n",
      "Information Technology (Master of Engineering)  770 €   \n",
      "International Business and Engineering  \n",
      "(Master of Engineering)  770 €   \n",
      "International Business and Engineering with pre- course  \n",
      "(Master of Engineering)  770 €   \n",
      "International Management and Leadership  \n",
      "(Master of Arts)  770 €   \n",
      "Music Therapy (Master of Arts)  690 €  \n",
      "Water Technology (Master of Engineering)  770 €   \n",
      "Water Technology with pre- course (Master of Engineering)  770 €   \n",
      "Bachelor   \n",
      "International Business (Bachelor of Arts)  750 €   \n",
      " \n",
      " \n",
      "One -time registration fee: € 750  \n",
      " \n",
      "Tuition fees in this pricelist are valid from 1 April 2023  for EEA students, starting with the Winter Semester 2023 .  \n",
      "Payment is required in advance,  as specified in the table – per semester, per year, or full programme. SRH reserves the right \n",
      "to modify the fees at any  time.  \n",
      " Tuition fee discounts for up -front payment:  \n",
      "— One year in advance: 3%  \n",
      "— Full programme in advance: 5%  \n",
      " \n",
      "Academic leave: For academic leaves there is an administration charge of € 50 (exceptions: parental leave/illness/care). EEA = European Economic Area  \n",
      " \n",
      "  SRH University \n",
      "Heidelberg   \n",
      " \n",
      " \n",
      "General Fees    \n",
      "Students from EEA countries are required to pay a one-\n",
      "time registration fee of € 750 at the beginning of the \n",
      "degree programme. This applies to all degree \n",
      "programmes . Students who completed a Bachelor’s  \n",
      "degree at SRH University Heidelberg and then \n",
      "immediately begin a Master’s degree programme are \n",
      "exempted from this fee. It also does not apply for \n",
      "participants in a module programme or auditing students.  \n",
      " \n",
      "Tuition fees and advance payments  \n",
      "The monthly tuition fees cover all regular courses and \n",
      "events associated with the degree programme in which \n",
      "the student is enrolled  as well as the language courses \n",
      "offered by the university. Fees must be paid for the full \n",
      "duration of the degree programme, and at a minimum \n",
      "for the standard study period  (“Regelstudienzeit”).  \n",
      " \n",
      "— For residency reasons, leave cannot be granted for a \n",
      "voluntary internship to students required to have a \n",
      "visa. Any other planned leave must be agreed in \n",
      "advance with the responsible auth ority for the \n",
      "residency  permit.  \n",
      "— Leave cannot be granted during mandatory internship \n",
      "periods because mandatory internships are a required component of the study programm and form part of \n",
      "the standard study period. If a student completes a \n",
      "voluntary internship without having been granted   \n",
      "leave during this period, normal study fees  apply.  \n",
      "— Tuition fee discounts for up -front payment:  \n",
      "- One year in advance: 3%  \n",
      "- Full programme in advance: 5%  \n",
      " \n",
      "Reduction of the tuition fees  \n",
      "The tuition fees for the standard  period of study may be \n",
      "reduced if \n",
      " \n",
      "— the student has a close family member ( sibling, \n",
      "parents, children, spouse ) who is attending or attended \n",
      "SRH Universit ies, \n",
      "— the student has completed a Bachelor’s  degree \n",
      "at one of the SRH Universities and has now \n",
      "been accepted to a Master’s degree \n",
      "programme.  \n",
      " \n",
      "In such cases, a discount of 10 % will be applied to the \n",
      "tuition fees for the standard period of study.  \n",
      " \n",
      "  All employees of SRH Universities are eligible for a 10 % \n",
      "employee discount to the tuition fees for the standard period of study in cases in which the student is in \n",
      "employment subject to  full social security contributions \n",
      "at the SRH universities.  The reduc tions of the tuition \n",
      "fees  mentioned above  may not be combined, nor \n",
      "applied in combination with in -house scholarships.  \n",
      "Scholarship rules  \n",
      "The current rules governing scholarships at SRH \n",
      "University Heidelberg are either publicly available on the university website during the scholarship application period or can be requested from the \n",
      "Admission Office at SRH University Heidelberg.  \n",
      "Module programm e / Auditing students  \n",
      "— Participants in a module programme pay a \n",
      "fee of € 150/credit point (ECTS) \n",
      "corresponding to the credit points of the module in which they are enroled.  \n",
      "— Auditing students pay a fee of € 120/credit \n",
      "point (ECTS) corresponding to the credit points \n",
      "of the class/module they are auditing.  This is \n",
      "only possible if the capacity of the class/module \n",
      "(maximum number of participants) is not exceeded due to the auditing stu dent’s \n",
      "participation and the applicable fees are paid in advance upon conclusion of the contract. Auditing students are not eligible to participate in examinations.  \n",
      " \n",
      "Otherwise, the provisions of the enrolment agreement and the programme -related statutes \n",
      "of SRH University Heidelberg apply at all times.  \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      "    \n",
      "    \n",
      "    \n",
      "   Adrian Thöny,                             Carsten Diener           \n",
      "Managing Director                     President  \n",
      " \n",
      "Important information (disclaimer): This translation is intended to \n",
      "provide information on fee -related matters to non- German speakers. \n",
      "Only the German version of the scale of fees is legally binding.  \n",
      " \n",
      " \n",
      "Semester fees (non-EEA \n",
      "students)\n",
      "6,450,00\n",
      "6,450,00\n",
      "5,100,00\n",
      "5,100,00\n",
      "5,500,00\n",
      "6,450,00\n",
      "6,450,00\n",
      "6,450,00\n",
      "5,500,00\n",
      "5,100,00\n",
      "6,450,00\n",
      "6,450,00\n",
      "5,100,00\n",
      "Business Engineering 5,500,00\n",
      "Electrical Enginnering 5,500,00\n",
      "Mechanical Engineering 5,500,00\n",
      "General FeesAcademic leave: For academic leaves there is an administration charge of € 50 (exceptions: parental leave/illness/care).\n",
      "EEA = European Economic Area\n",
      "For students from non-EEA countries without a\n",
      "permanent residence permit, the registration fee is € 1,000. This applies to all \n",
      "degree programmes . Students who completed a Bachelor’s degree at SRH \n",
      "University Heidelberg and then immediately begin a Master’s degree programme \n",
      "are exempted from this fee. It also does not apply for participants in a module \n",
      "programme or auditing students.\n",
      "Tuition fees and advance payments\n",
      "The monthly tuition fees cover all regular courses and events associated with the \n",
      "degree programme in which the student is enrolled as well as the language \n",
      "courses offered by the university. Fees must be paid for the full duration of the \n",
      "degree programme, and at a minimum for the standard study period \n",
      "(“Regelstudienzeit”).\n",
      "—  Students from Non-EEA countries without permanent residence permit \n",
      "applying for a degree programme are required to pay the tuition fees for one \n",
      "semester in advance. From the second semester onwards the payments for the \n",
      "remaining semesters of study are due six weeks before the end of the period of \n",
      "study already paid for. The payments have to be done semesterwise.\n",
      "—  For residency reasons, leave cannot be granted for a voluntary internship to \n",
      "students required to have a visa. Any other planned leave must be agreed in \n",
      "advance with the responsible authority for the residency permit.\n",
      "—  Leave cannot be granted during mandatory internship periods because \n",
      "mandatory internships are a required component of the study programm and \n",
      "form part of the standard study period. If a student completes a voluntary \n",
      "internship without having been granted leave during this period, normal study \n",
      "fees apply.\n",
      "—  Tuition fee discounts for up-front payment:\n",
      "-        One year in advance: 3%\n",
      "-        Full programme in advance: 5%\n",
      "Reduction of the tuition fees\n",
      "The tuition fees for the standard period of study may be reduced if\n",
      "—  the student has a close family member (sibling, parents, children, spouse) who \n",
      "is attending or attended SRH Universities,\n",
      "—  the student has completed a Bachelor’s degree at one of the SRH Universities \n",
      "and has now been accepted to a Master’s degree programme.\n",
      "In  such  cases,  a  discount  of  10%  will  be  applied  to  the tuition fees for the \n",
      "standard period of study.All employees of SRH Universities are eligible for a 10%\n",
      "employee discount to the tuition fees for the standard period of study in cases in \n",
      "cases in which the student is in employment subject to full social security \n",
      "contributions at the SRH universities. The reductions of the tuition fees \n",
      "mentioned above may not be combined, nor applied in combination with in-\n",
      "house scholarships.\n",
      "Scholarship rules\n",
      "The   current   rules   governing   scholarships   at   SRH University Heidelberg are \n",
      "either publicly available on the    university    website    during    the    scholarship \n",
      "application  period  or  can  be  requested  from   the Admission Office at SRH \n",
      "University Heidelberg.\n",
      "Module programme / Auditing students\n",
      "—  Participants in a module programme pay a fee of € 150/credit point (ECTS) \n",
      "corresponding to the credit points of the module in which they are enroled.\n",
      "—  Auditing students pay a fee of € 120/credit point (ECTS) corresponding to \n",
      "the credit points of the class/module they are auditing.  This is only possible if the \n",
      "capacity of the class/module (maximum number of participants) is not exceeded \n",
      "due to the auditing student’s participation and the applicable fees are paid in \n",
      "advance upon conclusion of the contract.\n",
      "Auditing students are not eligible to participate in examinations.\n",
      "Otherwise,    the    provisions    of    the    enrolment agreement  and  the  \n",
      "programme-related  statutes of SRH University Heidelberg apply at all times.\n",
      "Adrian Thöny,                             Carsten Diener\n",
      "Managing Director                    President\n",
      "Important information (disclaimer): This translation is intended to provide information on fee-related matters \n",
      "to non-German speakers. Only the German version of the scale of fees is legally binding.Bachelor\n",
      "International Business (Bachelor of Arts)\n",
      "One-time registration fee: €1,000.\n",
      "Tuition fees in this pricelist are valid from 1 April 2023 for applicants from non-EEA countries, starting with the Winter\n",
      "Semester 2023 .\n",
      "Payment is required in advance, as specified in the table – per semester, per year, or full programme. SRH reserves the right to modify the fees at any time.\n",
      "Tuition fee discounts for up-front payment:\n",
      "— One year in advance: 3%\n",
      "— Full programme in advance: 5%International Business and Engineering with pre-course (Master of Engineering)\n",
      "International Management and Leadership (Master of Arts)\n",
      "Music Therapy (Master of Arts)\n",
      "Water Technology (Master of Engineering)\n",
      "Water Technology with pre-course (Master of Engineering)Architecture – Design for the Built Environment (Master of Arts)\n",
      "Dance Movement Therapy (Master of Arts)\n",
      "Global Business and Leadership (Master of Arts)\n",
      "Information Technology (Master of Engineering)\n",
      "International Business and Engineering (Master of Engineering)Tuition fees\n",
      "(effective from April 1, 2023 – non-EEA students)\n",
      "Programme\n",
      "Master\n",
      "Applied Computer Science (Master of Science)\n",
      "Applied Data Science and Analytics (Master of Science)\n",
      "\n",
      "\n",
      "City Tours\n",
      "2024\n",
      "+49 62215840\n",
      "-223/-225\n",
      "INFORMATION \n",
      "& TICKETS guide@heidelberg-marketing.dewww.heidelberg-marketing.comTickets & Booking\n",
      "Heidelberg has many stories to tell. Our guides will give \n",
      "you a special insight into the colourful variety of the city. \n",
      "They lead you through the idyllic alleys of the Old Town \n",
      "or visit the world-famous castle with you. If you prefer to \n",
      "get to know the city from another perspective, you \n",
      "may experience Heidelberg within a Segway Tour or a \n",
      "City Sightseeing Tour in the Cabriobus. The Heidelberg \n",
      "Marketing team will be happy to assist you in finding the \n",
      "perfect tour for you. Tickets are available at the Tourist \n",
      "Information in the Rathaus (town hall), at the Neckar -\n",
      "münzplatz and at the Hauptbahnhof (main station).\n",
      " Notes  Since the number of participants is limited, it is suggested to \n",
      "make a reservation by phone or via the Heidelberg Marketing website. \n",
      "For reservations, please pick up your tickets 30 minutes before the tour \n",
      "starts.\n",
      " Tip For group bookings, date, topic and language of the tour can of \n",
      "course also be arranged individually.\n",
      "Information and  \n",
      "service hotline  \n",
      "Monday – Thursday 9:00 am – 5:00 pm\n",
      "Friday 9:00 am – 3:00 pm*\n",
      "* subject to change  \n",
      "Please find the opening hours of our  \n",
      "Tourist Information on our website .+49 62215840\n",
      "-223/-225\n",
      "INFORMATION& TICKETS guide@heidelberg-marketing.dewww.heidelberg-marketing.com* Eligibility criteria for the reduced price: school and university students (up to \n",
      "28 years old), people with disabilities and a disabled person’s pass; and own -\n",
      "ers of the   HDCARD . One accompanying person of a severely disabled \n",
      "person with the characteristic “B” in the severely disabled person’s identity \n",
      "card as well as of children and young people up to 18 years of age with a \n",
      "disability is free of charge. Children up to and including 5 years are free of \n",
      "charge. Children from 6 to 14 years pay a reduced price. Tickets are available \n",
      "at our Tourist Information.Walking Tour of the Old Town\n",
      "The oldest part of Heidelberg has a lot more to offer than \n",
      "just the Alte Brücke (Old Bridge), the picture-perfect alleys \n",
      "and the unique view to the most famous ruin in the world. \n",
      "The Old Town is vibrant with its charming squares and the \n",
      "mixture of people of all ages and countries. They meet in \n",
      "many small cafés and pubs. The unique cultural programs of \n",
      "our museums and theaters also attract our visitors. Discover \n",
      "Heidelberg with your guide and really get to know the city.\n",
      "Price € 12, discount price € 10  *\n",
      "Duration 1.5 hours\n",
      "Times\n",
      "April – October German: daily 10:30 am, \n",
      "additionally Friday 6:00 pm and Saturday 2:30 pm, \n",
      "English: Thursday – Saturday 10:30 am\n",
      "November – March German: Friday 2:30 pm \n",
      "and Saturday 10:30 am\n",
      "Meeting point  \n",
      "Neckarmünzplatz, in front of the Tourist Information\n",
      " Note There is no Walking Tour of the Old Town at 2:30 pm \n",
      "during the Old Town Festival Heidelberger Herbst (Heidel -\n",
      "berg Autumn) on September 28, 2024.City Tour with Guided Castle Tour \n",
      "Open view on the “Heidelberg scenery“ – Act one: The focus \n",
      "shifts to the romantic castle, which is majestically enthroned \n",
      "above the Karlsplatz. Enjoy a comfortable city tour by bus, \n",
      "accompanied by a tour guide. Act two takes you up to \n",
      "Heidelberg Castle. In addition to an exterior tour, you will \n",
      "visit the inner courtyard and the famous Great Barrel. Take \n",
      "advantage of your castle ticket with a visit of the German \n",
      "Pharmacy Museum and the funicular ride back to the Old \n",
      "Town (Kornmarkt station).\n",
      "Price  € 30, discount price € 27  *, € 28 with  HDCARD ,\n",
      " including castle ticket (funicular railway + castle courtyard \n",
      "admission)\n",
      "Duration  2 hours\n",
      "Times\n",
      "April – October German: Friday, Saturday  \n",
      "and Whitsunday 1:30 pm  \n",
      "November – March German: Saturday 1:30 pm\n",
      "Meeting point  \n",
      "Neckarmünzplatz, information board (bus stop)\n",
      " Note There is no City and Castle Sightseeing Tour during \n",
      "the Old Town Festival Heidelberger Herbst (Heidelberg \n",
      "Autumn) on September 28, 2024.\n",
      "* Eligibility criteria for the reduced price: school and university students (up to \n",
      "28 years old) and people with disabilities and a disabled person’s pass. One \n",
      "accompanying person of a severely disabled person with the characteristic \n",
      "“B” in the severely disabled person’s identity card as well as of children and \n",
      "young people up to 18 years of age with a disability is free of charge.Chil -\n",
      "dren up to and including 5 years are free of charge. Children from 6 to 14 \n",
      "years pay a reduced price. Tickets are available at our Tourist Information.including  \n",
      "castle \n",
      "ticketThe University in the Old Town\n",
      "Founded by Prince Elector Ruprecht I. in 1386, the “Ruper -\n",
      "to Carola” is Germany’s oldest university and one of the \n",
      "most venerable education facilities in Europe. The tour \n",
      "conveys not only the university’s history but also provides \n",
      "an insight into student life. It takes you from the university \n",
      "library via the Peterskirche (St. Peter’s Church), the oldest \n",
      "church in Heidelberg, to the Alte Aula (Old Auditorium) and \n",
      "the histor  ical Studentenkarzer (Student Prison). Visit the \n",
      "place that once made people suffer! From 1778 to 1914, \n",
      "students were punished here “for trivial offenses”.\n",
      "Price € 17, discount price € 15 *\n",
      "including admission to the Alte Aula (Old Auditorium) \n",
      "and the Studentenkarzer (Student Prison)\n",
      "Duration  1.5 – 2 hours\n",
      "Times \n",
      "April – October German: Saturday 11:00 am\n",
      "Meeting point  Neckarmünzplatz, in front of the Tourist \n",
      "Information\n",
      "Minimum number of participants 3 people\n",
      "* Eligibility criteria for the reduced price: school and university students (up to \n",
      "28 years old), people with disabilities and a disabled person’s pass; and own -\n",
      "ers of the   HDCARD . One accompanying person of a severely disabled \n",
      "person with the characteristic “B” in the severely disabled person’s identity \n",
      "card as well as of children and young people up to 18 years of age with a \n",
      "disability is free of charge. Children up to and including 5 years are free of \n",
      "charge. Children from 6 to 14 years pay a reduced price. Tickets are available \n",
      "at our Tourist Information.including\n",
      "1 mulled \n",
      "wine/  \n",
      "punch\n",
      "Christmas Market Tour  \n",
      "Take a walk through the Old Town and immediately perceive \n",
      "the Christmas spirit with the scent of roasted almonds and \n",
      "mulled wine. Lovingly arranged booths spread over various \n",
      "historical squares. The unique backdrop with Heidelberg Cas -\n",
      "tle towering above the Old Town creates an outstanding atmo -\n",
      "sphere and makes the Heidelberg Christmas Market one of the \n",
      "most fairy tale-like events in Germany. The Advent-themed \n",
      "guided tour provides interesting information on the region’s \n",
      "Christmas and pre-Christmas traditions. The walk starts at the \n",
      "Neckar  münzplatz and passes the most beautiful corners of the \n",
      "Old Town ending at the Universitäts  platz (University Square).\n",
      "Price € 17, discount price € 15 *\n",
      "Duration  1.5 hours\n",
      "Times – Advent Saturdays November 30, December 7, \n",
      "14 and 21, 2024; German: 4:30 pm\n",
      "Meeting point  Neckarmünzplatz, in front of the Tourist \n",
      "Information\n",
      "Minimum number of participants 3 people\n",
      " Notes The price includes one cup of mulled wine or fruit \n",
      "punch per person at our booth on Universitätsplatz (Uni -\n",
      "versity Square). You only pay the deposit on site and get it \n",
      "back when you return the mug. Of course, you can also \n",
      "keep the limited Heidelberg mug as a souvenir of your visit.\n",
      "* Eligibility criteria for the reduced price: school and university students (up to \n",
      "28 years old), people with disabilities and a disabled person’s pass; and owners \n",
      "of the   HDCARD . One accompanying person of a severely disabled person \n",
      "with the characteristic “B” in the severely disabled person’s identity card as well \n",
      "as of children and young people up to 18 years of age with a disability is free of \n",
      "charge. Children up to and including 5 years are free of charge. Children from 6 \n",
      "to 14 years pay a reduced price. Tickets are available at our Tourist Information.Cabriobus Sightseeing Tour\n",
      "The open roof of the convertible bus offers new perspec -\n",
      "tives and is a boon for all photographers. Enjoy an unre -\n",
      "stricted view for your pictures without window reflections. \n",
      "Take a relaxed ride along the city’s sights while an audio \n",
      "guide in multiple languages keeps you informed. The tour \n",
      "starts and ends at the Karlsplatz (Carl’s Square) and shows \n",
      "you several highlights before heading to Weststadt and \n",
      "onwards to Neuenheim with its magnificent villas.\n",
      "Price  € 13, discount price € 8 / 12, € 12 * with  HDCARD \n",
      "Duration  approx. 40 minutes\n",
      "Languages German, English, Chinese, Dutch, French, Italian, \n",
      "Japanese, Korean, Russian, Spanish\n",
      "Times  \n",
      "March  daily 10:00 am – 4:00 pm (departure every full hour) \n",
      "April – October daily 10:00 am – 5:00 pm (departure every \n",
      "full and half hour)\n",
      "November and December Thursday – Sunday \n",
      "11:00 am – 4:00 pm (departure every full hour)\n",
      "Meeting point  Karlsplatz (Carl’s Square), information board \n",
      "(bus stop)\n",
      " Notes Tickets are available at our Tourist Information \n",
      " at the Hauptbahnhof (main station), in the Rathaus (town hall) \n",
      "and at the Neckarmünzplatz, from the driver (cash, credit \n",
      "card, debit card) or online at www.cabrio-sightseeing.de. \n",
      "There is no tour during the Old Town Festival Heidelberger \n",
      "Herbst (Heidelberg Autumn) on September 28, 2024.\n",
      "* Children younger than 6 are free of charge, children aged 6 to 12 pay € 8. \n",
      "People with disabilities and a disabled person’s pass and owners of the\n",
      " HDCARD  pay € 12.“Delectable Heidelberg” Tour\n",
      "Fall in love with the culinary delights of romantic Heidelberg. \n",
      "Take a tour through the picturesque Old Town with an \n",
      "aperitif matured under the sun of Baden and a regional \n",
      "3-course menu in two traditional restaurants. The time and \n",
      "journeys between the courses and places are entertain -\n",
      "ingly spiced up with history and stories, “magical elixirs”, \n",
      "and are topped off with a sweet Heidelberg treat.\n",
      "Price  € 98 per person, no reductions\n",
      "including aperitif, appetizer, main course, dessert, two “sur -\n",
      "prises” and a guided tour. Drinks are charged separately in \n",
      "the respective locations/restaurants.\n",
      "Duration  approx. 3 hours\n",
      "Times\n",
      "March 16, April 6, April 20, May 4, May 18, June 8, June 15, \n",
      "July 13, July 27, August 3, August 31, September 14, Sep -\n",
      "tember 21, October 12, October 26, November 9, Novem -\n",
      "ber 16, 2024\n",
      "German: 5:30 pm\n",
      "Meeting point Marktplatz (Market Square), \n",
      "Herkulesbrunnen (Hercules Fountain)\n",
      "Minimum number of participants 10 people\n",
      " Notes  Booking required. Please note, that the tour is not \n",
      "suitable for children.Segway Tour “Highly Philosophical”\n",
      "The segway tour reveals an entirely new perspective: Glide \n",
      "along the Neckar River to the historical and trendy quarter \n",
      "Neuenheim accompanied by a tour guide. Afterwards, you \n",
      "move on to the zoo, the Neuenheimer Feld with its impres -\n",
      "sive university and hospital buildings and up to the Philo-  \n",
      "sophenweg (Philosophers’ Walk). This Heidelberg sight is \n",
      "not easily seen at first glance. The former vintners’ path \n",
      "was once taken by scholars who desired the inspiration of \n",
      "the “Heidelberg Trio”: The harmony of the baroque city, \n",
      "the Heidelberg Castle and the Neckar River. Enjoy this ex -\n",
      "traordinary view!\n",
      "Price € 59, € 54 with  HDCARD \n",
      "including helmet fee and segway license\n",
      "Duration  1 ¾ hours\n",
      "Times\n",
      "February and November\n",
      "German / English: daily 1:00 pm \n",
      "(depending on weather conditions)\n",
      "March – October\n",
      "German / English: daily 9:30 am, 1:00 pm and 4:00 pm\n",
      "Meeting point Neckarmünzplatz\n",
      "Requirements minimum age 14 years old, \n",
      "size at least 1.40 m, weight 45 – 115 kg\n",
      "Minimum number of participants 2 people\n",
      " Notes Booking required. For group inquiries of twenty \n",
      "guests and more, the price may change due to increased \n",
      "effort and logistics.Segway Tour “All in 360°“ \n",
      "Discover Heidelberg and the Neckar valley on a city safari \n",
      "that provides a mixture of “everything“. Your tour starts at \n",
      "the Neckarmünzplatz and continues to the Philosophenweg \n",
      "(Philosophers’ Walk) with its fantastic view of the city. The \n",
      "second place steeped in history is the Neuburg monastery, \n",
      "which was founded in 1130 and looks back on a fascinating \n",
      "past. Head towards the Köpfel and Ziegelhausen to the other \n",
      "side of the Neckar River and right up to the Wolfsbrunnen. \n",
      "Proceed in the direction of Heidelberg Castle. Enjoy the great \n",
      "view of the castle garden above the castle before continuing \n",
      "to the Klingenteich. Passing the Jewish cemetery, the tour \n",
      "takes you back to the Old Town and your starting point at \n",
      "the Neckarmünzplatz.\n",
      "Price  € 69, € 64 with  HDCARD \n",
      "including helmet fee and segway license\n",
      "Duration  2.5 hours\n",
      "Times\n",
      "February and November\n",
      "German / English: daily 12:30 pm\n",
      "(depending on weather conditions)\n",
      "March – October\n",
      "German / English: daily 9:30 am, 12:30 pm and 3:30 pm\n",
      "Meeting point Neckarmünzplatz\n",
      "Requirements minimum age 14 years old,\n",
      "size at least 1.40 m, weight 45 – 115 kg\n",
      "Minimum number of participants 2 people  \n",
      " Notes Booking required. For group inquiries of twenty \n",
      "guests and more, the price may change due to increased \n",
      "effort and logistics.With castle ticket including Bergbahn (funicular railway)\n",
      "Including combo ticket (one-time free admission to the \n",
      "University Museum, the Student Prison and the special \n",
      "exhibition)\n",
      "Free use of public transport in the urban area\n",
      "Discounts on all public tours  \n",
      "(except “Delectable Heidelberg”)\n",
      "Discounts with attractive partners  \n",
      "from gastronomy, retail, culture  \n",
      "and leisure\n",
      "  HeidelbergCARD sale  \n",
      "Tourist Information in the town hall (Market Square),\n",
      "Tourist Information at the Neckarmünzplatz, Tourist \n",
      "Information at the main station, Käthe Wohlfahrt \n",
      "(Hauptstraße 124) and in many Heidelberg hotelsHeidelberg CARD\n",
      "Put Heidelberg in your pocket! starting from\n",
      "€ 26 \n",
      "Gültig ab / Valid from\n",
      "4\n",
      "Gültig ab / Valid from\n",
      "2Family\n",
      "Gültig ab / Valid from\n",
      "2\n",
      "Gültig ab / Valid from\n",
      "1\n",
      "Imprint\n",
      "Heidelberg Marketing GmbH  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg\n",
      "Telefon +49 6221 58-40200  \n",
      "Telefax +49 6221 58-40209  \n",
      "info@heidelberg-marketing.de\n",
      "www.heidelberg-marketing.com\n",
      "The Heidelberg Marketing GmbH is a\n",
      "subsidiary of the City of HeidelbergPhotos \n",
      "Tobias Schwerdt \n",
      "© Copyright 2024\n",
      "All contents, in particular texts, photo -\n",
      "graphs and graphics, are protected \n",
      "by copyright. Unless expressly stated \n",
      "otherwise, Heidelberg Marketing \n",
      "GmbH owns the copyright.KornmarktCabriobusCity tourNeckarmünzplatz\n",
      "Molkenkur\n",
      "KönigstuhlBergbah n\n",
      "(funicular  \n",
      "railway)Karlstor/Altstadt\n",
      "Bussemergasse\n",
      "Kl. Mantelgasse\n",
      "Große MantelgasseHaspelgasseWehrsteg\n",
      "Floring.KrämergasseMittelbadgasse\n",
      "ApothekergasseFischerg .Semmelsg.\n",
      "SteingasseLeyerg.\n",
      "Obere NeckarstraßeMönchg asse\n",
      "Dreikönigsstr.Kettengasse\n",
      "Schulgasse\n",
      "Grabengasse\n",
      "Sandgasse\n",
      "Theaterstra ße\n",
      "Friedrichstr aße\n",
      "Märzgasse\n",
      "Akademiestra ße\n",
      "Neugasse\n",
      "Rohrbacher StraßeBismarckstraßeNadlerstr aße\n",
      "St. An na GasseFahrtgasse\n",
      "Thibautstra ßePfaffengasse Am BrückentorZoo\n",
      "Heiliggeiststr.\n",
      "Marstallstr aße\n",
      "Schiﬀgasse\n",
      "Bauamtsgasse\n",
      "Ziegelgasse\n",
      "BrunnengasseBienenstraße\n",
      "Karpfengasse\n",
      "Unter e Stra ße\n",
      "Fischmarkt\n",
      "Marsiliusplat z\n",
      "Richard -\n",
      "Hauser-\n",
      "PlatzStadthalle\n",
      "Friedrich-\n",
      "Ebert-PlatzBismar ck-\n",
      "PlatzRathaus\n",
      "KornmarktSchlangenweg\n",
      "Unter e Neckarstra ße\n",
      "Landfriedstr aßeNeuenheimer Landstr aße Ziegelhäuser Landstr aße\n",
      "NeckarstadenFriedrich-Ebert-AnlageGaisbergtunne\n",
      "l\n",
      "Schlossberg-\n",
      "tunnelFriedrich-Ebert-AnlageKurfürsten-AnlageNeckarstaden Schurmanstr aßeUferstr aße\n",
      "Neuenheimer LandstraßePlöck\n",
      "PlöckHauptstr aße\n",
      "Bergheimer St raßeBahnhofst\n",
      "raßeHauptstr aße\n",
      "Merianstr aße\n",
      "Ingrimstr aßeZwingerst\n",
      "raß\n",
      "eUn\n",
      "t. Fauler\n",
      " Pel\n",
      "zOber\n",
      "er Fauler Pelz\n",
      "Neue Schlossstr\n",
      "aße\n",
      "Theodor-Heuss-BrückeKarl-Theodor-B\n",
      "rücke\n",
      "BrückenstraßeNeue Schlossstraße\n",
      "Märc henpar adies  \n",
      "(Fairy Tale Par adise)Schlierbach\n",
      "RohrbachNeuenheim\n",
      "NeckarwieseHirschgasse\n",
      "Alte Brücke \n",
      "(Old Bridge)Fußgängerübergan g\n",
      "(pedestrian cr ossing)\n",
      "KarlsplatzNeckarmünz-\n",
      "platz\n",
      "Marktplatz\n",
      "Universitäts-\n",
      "platz\n",
      "SchlossSolar-po wered boat\n",
      "Heidelberger\n",
      "Schloss (Castle)Neuenheimer Feld\n",
      "Kliniken (Hospital)\n",
      "Hauptbahnhof (main station)Kloster (Benedictine\n",
      "abbe y) Stift Neubur gPhilosophenweg (Philosophers‘ W alk)\n",
      "Kirchheim / WeststadtZiegelhausen\n",
      "Weisse FlotteFootpath\n",
      "Pier Bus toursPublic \n",
      "toiletsTourist  \n",
      "InformationHDCard saleonly entry  \n",
      "and exitParking\n",
      "garageMeeting pointFunicular  \n",
      "railwayBus\n",
      "parking\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "www.heidelberg.de /erweiterung\n",
      "Accommodations \n",
      "& Packages  \n",
      "2023modern\n",
      "modern\n",
      "elegant\n",
      "elegant\n",
      "delicious\n",
      "delicious\n",
      "central\n",
      "central\n",
      "traditional\n",
      "traditional\n",
      "cultural\n",
      "cultural\n",
      "classical\n",
      "classicalfamous\n",
      "famous\n",
      "diverse\n",
      "diverseHeidelberg Marketing GmbH  \n",
      "Neuenheimer Landstrasse 5  \n",
      "69120 Heidelberg / Germany \n",
      "Phone +49 6221 5840-226 \n",
      "Fax +49 6221 5840-222 \n",
      "reservation@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.com\n",
      "The service team is pleased \n",
      "to help you. *\n",
      "Monday - Thursday  \n",
      "9:00 am - 6:00 pm \n",
      "Friday  \n",
      "9:00 am - 3:00 pm\n",
      "* subject to changeWelcome!  \n",
      "Let your journey through Heidelberg begin – there is a lot to discover! In this brochure, we \n",
      "have put together short trips that you should not miss. We provide complete packages on \n",
      "various subjects that include matching hotels – to guarantee your perfect stay.\n",
      "Experience our beautiful city on the Neckar River with its world-famous castle ruin, \n",
      "the romantic Old Town and the legendary atmosphere of a young university city. Enjoy \n",
      "the magic of special places and a plethora of events during any season.\n",
      "No matter if you want to stay close to the city center, have a unique, romantic or family-\n",
      "like accommodation, you will certainly find something that suits your taste. Our team \n",
      "will be happy to assist you in finding your ideal accommodation.\n",
      "Our city is delicious, romantic, scientific, captivating, cultural and much more. Choose \n",
      "what you want to explore first. Our package offers will let you experience Heidelberg \n",
      "just as you like it. Look forward, for example, to special events such as our Heidelberg \n",
      "Castle Illuminations or discover Heidelberg‘s unique natural scenery. \n",
      "Contact us. We will turn your stay into a unique experience – visit Heidelberg, the city, \n",
      "where so many have left their hearts before.\n",
      "Have fun exploring!\n",
      "Your Heidelberg Marketing team\n",
      "#heidelberg4you\n",
      "instagram.com/heidelberg4youHEIDELBERG4you\n",
      "youtube.com/heidelberg4youHeidelberg4you\n",
      "facebook.com/HeidelbergDiscover  \n",
      "Heidelberg \n",
      "– how it’s done:\n",
      "1 Choosing your trip: tell us when you would like to ar-\n",
      "rive, the number of rooms needed, the number of people \n",
      "traveling, and the name of the package from this brochure.\n",
      "2 Finding your accommodation: we will make you an \n",
      "offer with matching hotels for your stay in Heidelberg.\n",
      "3 Choosing your hotel: pick a hotel from our sugges-\n",
      "tions. We will book your accommodation and send you a \n",
      "booking confirmation.\n",
      "4 Welcome to Heidelberg! Before arriving you will re- \n",
      "ceive all documents for your package at the Tourist Infor-\n",
      "mation at the Heidelberg main station.\n",
      "We look forward to meeting you!\n",
      "Our partner hotels are listed on page 22. Bookings are \n",
      "possible on request, and based on availability, for up to \n",
      "14 persons. If your chosen hotel category / preferred ho -\n",
      "tel is no longer available, the prices may change. Please \n",
      "note that you have to request your package at the latest \n",
      "3 work  ing days before your planned arrival. Tip Save at the weekend with a stay at \n",
      "a first class hotel – thanks to our first class \n",
      "weekend rate (see note “first class week -\n",
      "end” in the price tables). This rate covers \n",
      "arrival on Friday at the earliest and depar -\n",
      "ture on Monday at the latest; public holi -\n",
      "days are treated as weekends. \n",
      " Note  Please note that, according to \n",
      "legal regulations, after the conclusion of \n",
      "contracts regarding domestic tourism \n",
      "services, particularly in the case of con -\n",
      "tracts for accommodations, guided tours \n",
      "and cultural events. There is no statutory \n",
      "right of withdrawal, except that there \n",
      "could be a cancellation – which generally \n",
      "involves a fee – in line with the agreed \n",
      "terms and conditions or of the statutory \n",
      "provisions. \n",
      "4 | Packages with overnight stayThe HeidelbergCARD is already included in all packages and  \n",
      "offers you numerous advantages and discounts within  \n",
      "the entire city area.\n",
      "Benefit from the following included services:\n",
      "– castle ticket including admission  \n",
      " to the castle courtyard, the bar-  \n",
      " rel cellar as well as the German  \n",
      " Pharmacy Museum and the fare  \n",
      " for the funicular railway trip to  \n",
      " the castle with continuation of  \n",
      " the trip to the Molkenkur station  \n",
      " (return trip including one stop)\n",
      "– combination ticket (one-time  \n",
      " free entrance to the University  \n",
      " Museum, the Student Prison  \n",
      " and the special exhibition)– free use of public transport  \n",
      " in Heidelberg\n",
      "– numerous discounts on tours,  \n",
      " museums, leisure activities,  \n",
      " restaurants and shops\n",
      "Further information can be  \n",
      "found here:Heidelberg CARD\n",
      "Heidelberg in your pocket! \n",
      " Note  The HeidelbergCard can also be booked without a package \n",
      "with best price guarantee.\n",
      "Main area\n",
      "HeidelbergHeidelbergCARD\n",
      "Packages with overnight stay | 5 \n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Family\n",
      "Gültig ab / Valid fromInformation and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "This journey will let you experience why so many have lost \n",
      "their hearts to our city. Explore the picturesque and charis -\n",
      "matic alleys during a walking tour of the Old Town and see \n",
      "the many small places that make Heidelberg one of the most \n",
      "beautiful cities. The busy activity in the his  torical city core will \n",
      "draw you in and make you forget your everyday life in its \n",
      "unique atmosphere. A trip aboard a solar-powered boat on \n",
      "the Neckar River offers a very special view of the scenery. \n",
      "Surely, you too will love our city, which is inseparably connec  - \n",
      "ted to German Romanticism!Included services\n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours)\n",
      "– Ticket for a trip on the solar boat “Neckarsonne“\n",
      "6 | Packages with overnight stay\n",
      "For an additional charge, the boat trip can also be booked as a 3-hour castle tour (round trip). We will be happy to assist you in planning.Bookable from April to October 2023  \n",
      "Lose your heart in Heidelberg\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room 125 € 185 € 240 € \n",
      "Single room 165 € 260 € 350 € \n",
      "First class  \n",
      "weekendDouble room 135 € 210 € 280 € \n",
      "Single room 190 € 310 € 420 € \n",
      "First classDouble room 145 € 230 € 310 € \n",
      "Single room 210 € 350 € 480 €Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Explore Heidelberg in all directions by bus and with a very \n",
      "special view of the backdrop of Heidelberg‘s Old Town.  \n",
      "Accompanied by a tour guide, this City and Castle Sight -\n",
      "seeing Tour along the Neckar River will take you up to Heidel -\n",
      "berg Castle. During the subsequent outdoor tour, your guide \n",
      "will introduce you to the history of the fasci nating building. \n",
      "You will also visit the inner courtyard and the famous Great \n",
      "Barrel. After the tour, you can visit the German Pharmacy \n",
      "Museum and then enjoy the ride on the funicular railway \n",
      "back to the Old Town which is included in your castle ticket.Included services\n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay \n",
      "– City and Castle Sightseeing Tour including castle \n",
      " ticket (2 hours) *\n",
      "Packages with overnight stay | 7\n",
      "Bookable all year round for 2 people and more  \n",
      "Crisscross through Heidelberg\n",
      "* Schedule for the City and Castle Sightseeing Tour, meeting point Neckarmünzplatz (information panel at the bus stop):  \n",
      " April – October (German): Friday, Saturday and Whitsunday 1:30 pm (except September 30, 2023)  \n",
      " November – March (German): Saturday 1:30 pmPrice per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 130 € 190 € 245 \n",
      "Single room € 180 € 280 € 370 \n",
      "First class  \n",
      "weekendDouble room € 140 € 215 € 285\n",
      "Single room € 205 € 325 € 435 Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "8 | Packages with overnight stay\n",
      "Bookable at the weekends of June 3, July 8 and September 2, 2023  \n",
      "Heidelberg Castle Illumination\n",
      "The Heidelberg Castle Illuminations in June, July and Sep -\n",
      "tember cast their spell on thousands every year. Bengali fires \n",
      "illuminate the world-famous ruin in a mysterious red light, in \n",
      "memory of the destruction of the castle in the Palatinate War \n",
      "of Succession. The origin of the subsequent fireworks from \n",
      "the Alte Brücke (Old Bridge), however, is a romantic one. \n",
      "Prince Elector Frederic V had fireworks held in 1613 for his \n",
      "new wife Elisabeth Stuart, thereby founding this tradition \n",
      "that is now more than four hundred years old. Experience \n",
      "the Heidelberg Castle Illumination from the front seats as \n",
      "the highlight of a 1.5 hour boat trip into the Neckar valley. \n",
      "When the anchor is lowered at the Alte Brücke, you will have \n",
      "a gorgeous view of the spectacle.Included services\n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Fireworks boat trip on Saturday evening with the \n",
      " “Weisse Flotte Heidelberg” *\n",
      "– 2-course menu on board \n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours)\n",
      "* Schedule for the boat trip to the Heidelberg Castle Illumination:  \n",
      " Admission as of 7:00 pm, departure at 8:00 pm, end approx. 11:00 pm. The seats on the ship are reserved for you.  \n",
      " Drinks are charged separately and paid on-site.Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 195 € 255 € 310 \n",
      "Single room € 240 € 330 € 430\n",
      "First class  \n",
      "weekendDouble room € 205 € 280 € 345\n",
      "Single room € 265 € 375 € 485 Combine your visit to BUGA 23 in Mannheim with an event -\n",
      "ful and at the same time relaxing stay in Heidelberg and \n",
      "book our event-specific package.\n",
      "In addition to a day ticket for the German National Garden \n",
      "Show, this package includes a varied program in our at -\n",
      "tractive city. Benefit from the advantages of our Heidel -\n",
      "bergCard, which allows you, among other things, to visit \n",
      "the world-famous castle ruin as well as the Student Prison \n",
      "of the University of Heidelberg, the oldest university in \n",
      "Germany. Get to know the charm of the historic Old Town \n",
      "on a guided tour and enjoy a 3-course menu in a restau -\n",
      "rant in the heart of the Old Town setting - sit back and re-\n",
      "live the experience.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class \n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Day ticket for the German National Garden Show 2023  \n",
      " (BUGA, one-time access to the exhibition areas) \n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours)\n",
      "– 3-course menu in a restaurant in the Old Town\n",
      "Bookable from April 14 to October 8, 2023  \n",
      "BUGA flower pot  (German National Garden Show)\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Packages with overnight stay | 9\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 170 € 235 € 285 \n",
      "Single room € 220 € 310 € 400 \n",
      "First class  \n",
      "weekendDouble room € 185 € 255 € 325 \n",
      "Single room € 240 € 355 € 470 \n",
      "First classDouble room € 195 € 275 € 355 \n",
      "Single room € 260 € 395 € 530Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "10 | Packages with overnight stay\n",
      "Visit Heidelberg in summer and enjoy sunbathing in the \n",
      "meadows along the Neckar River, or explore the roman -\n",
      "tic Old Town in mild evening temperatures. In the warm \n",
      "sea son, life is exuberant in Heidelberg‘s twisty alleys. The \n",
      "many cozy cafés and pubs offer places in the summer air \n",
      "and in the middle of our city‘s busy activity. Many cultural \n",
      "highlights will make your summer in Heidelberg a memo -\n",
      "rable experience. Because the sun is shining so much \n",
      "nicer here, we will pay for the third night of your stay.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours) \n",
      "– Ticket for a trip on the solar boat “Neckarsonne“ \n",
      " (50 minutes)\n",
      "Bookable from July 21 to August 27, 2023  \n",
      "Summer Special – stay 3 nights, pay for 2\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 oder 3 nights\n",
      "ComfortDouble room € 120 € 185 \n",
      "Single room € 170 € 260 \n",
      "First class  \n",
      "weekendDouble room € 135 € 220 \n",
      "Single room € 190 € 340 \n",
      "First classDouble room € 145 € 245 \n",
      "Single room € 210 € 390 The city puzzle game “City & Quest” offers a truly unique \n",
      "opportunity to see the city through new eyes. Whether as \n",
      "a couple, with the family or with friends – the game pro-  \n",
      "vides the perfect combination of fun and exploration. The \n",
      "ideal group size is two to six people. An app for smart -\n",
      "phone or tablet is used to navigate during the puzzle tour \n",
      "and to input the solution. In addition, a City & Quest bag \n",
      "with equipment needed to solve the puzzles will be handed \n",
      "out. The game starts in the heart of Heidelberg’s Old Town. \n",
      "On the route of about three kilometers, you will find eleven \n",
      "other stations. All players must prove their creativity, co -\n",
      "operation and powers of deduction in order to master the \n",
      "tricky puzzles!Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Digital City & Quest scavenger hunt (App based) * \n",
      " (2.5 hours)\n",
      "Bookable all year round for 2 people or more  \n",
      "City & Quest – Heidelberg‘s interactive quiz\n",
      "* The app is required for the rally and can be downloaded in advance (via Android/IOS). The download is free. A deposit payment of € 50 in cash  \n",
      " for the City & Quest bag is obligatory. The deposit will be made at Hostel Lotte, Burgweg 3, 69117 Heidelberg.Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 110 € 175 € 240 \n",
      "Single room € 155 € 250 € 345 \n",
      "First class  \n",
      "weekendDouble room € 125 € 200 € 275 \n",
      "Single room € 180 € 300 € 420 \n",
      "First classDouble room € 135 € 225 € 310 \n",
      "Single room € 200 € 345 € 485 \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Packages with overnight stay | 11Would you like to complement your stay in Heidelberg with a  \n",
      "themed culinary experience on board of a ship? If so, come \n",
      "aboard and enjoy this package.\n",
      "You will stay overnight in a single or double room in a com -\n",
      "fort or first class hotel. In the evening, you can round off the \n",
      "day with a theme cruise on the ship including a menu. In ad -\n",
      "dition to theme evenings such as an Italian evening or a  \n",
      "Kurpfälzer evening, where typical regional dishes are served, \n",
      "a relaxed Grill & Chill cruise, or even a Captain‘s Dinner with \n",
      "wit and charm, you can choose from various dates and  \n",
      "theme cruises. From April to October, the package can also \n",
      "be booked including a public tour of the Old Town for an ad -\n",
      "ditional charge.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Themed boat trip including dinner on a ship of \n",
      " the “Weisse Flotte Heidelberg“ *\n",
      "Bookable all year round  \n",
      "AHOY - welcome aboard for  \n",
      "a culinary delight\n",
      "* Drinks are charged separately and paid on-site.\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "12 | Packages with overnight stay\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 180 € 235 € 295 \n",
      "Single room € 240 € 320 € 410 \n",
      "First class  \n",
      "weekendDouble room € 190 € 260 € 330 \n",
      "Single room € 250 € 360 € 470 \n",
      "First classDouble room € 200 € 280 € 360 \n",
      "Single room € 270 € 400 € 530\n",
      "An overview of all available \n",
      "dates and trips can be found here.Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Packages with overnight stay | 13\n",
      "Bookable from April to October 2023 for 4 people and more  \n",
      "Delectable Heidelberg\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 205 € 255 € 315 \n",
      "Single room € 250 € 340 € 425 \n",
      "First class  \n",
      "weekendDouble room € 220 € 285 € 355 \n",
      "Single room € 275 € 385 € 495 Besides the breathtaking scenery, Heidelberg offers a lot for \n",
      "all senses. This package will show you the culinary side of the \n",
      "city. A delightful tour through our Old Town will let you ex-\n",
      "plore the most deli  cious sides of Heidelberg. The exclusive \n",
      "tour “Delectable Heidelberg“ will let you enjoy a perfectly co -\n",
      "ordinated ex  perience. An aperitif matured under the sun of \n",
      "Baden, a regional 3-course menu in two traditional Heidel -\n",
      "berg restaurants, topped off with a sweet Heidelberg treat. \n",
      "Time and paths between the courses will be spiced up enter -\n",
      "tainingly with enjoyable stories. A ride in the solar boat will \n",
      "complete your stay with a unique view of the city. This is a \n",
      "tour for true connoisseurs who want to fall in love with Hei -\n",
      "delberg for good.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay \n",
      "– Participation in the public walking tour “Delectable \n",
      " Heidelberg” through the Old Town (3 hours, \n",
      " only in German) *\n",
      "– Ticket for a trip on the solar boat “Neckarsonne“  \n",
      " (50 minutes)\n",
      "* Times “Delectable Heidelberg“ tour, meeting point Marktplatz (Market Square), Herkulesbrunnen (Hercules Fountain):  \n",
      " starts at 5:30 pm, April 1/22, May 6/27, June 10/24, July 1/29, August 5/26, September 9/23 and October 7/14, 2023  \n",
      " Drinks are charged separately and paid on-site.14 | Packages with overnight stay\n",
      "Get to know Heidelberg from its quiet side and discover the \n",
      "unique natural scenery that makes our romantic city on  \n",
      "the Neckar River so worth living in. Starting from the Bis -\n",
      "marckplatz, along the famous Philosophenweg (Philoso -\n",
      "phers’ Walk) and, continuing through “Neuenheimer \n",
      "Schweiz”, an impressive nature reserve, you will reach the \n",
      "idyllic Benedictine Abbey Stift Neuburg. There you will have \n",
      "earned a cool, regional organic beer, brewed in the small \n",
      "“Brauerei zum Klosterhof” (the Abbey courtyard brewery) \n",
      "with the spring water of the monastery. Afterwards, you can \n",
      "either walk along the Neckar River back to the Old Town or \n",
      "take the boat back to the city center (for an additional fee, \n",
      "season April – October). An overnight stay including break -\n",
      "fast and the Heidelberg  CARD round off your unique stay.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Digital hiking route via Komoot * (1.5 hours, \n",
      " family-friendly beginner’s tour)\n",
      "– 1 bottle (1 liter) of regional organic beer from \n",
      " the Heidelberg Klosterhof brewery per person\n",
      "Bookable all year round for 2 people or more  \n",
      "Strolling around Heidelberg\n",
      "* App required Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 110 € 170 € 230\n",
      "Single room € 155 € 255 € 340 \n",
      "First class  \n",
      "weekendDouble room € 125 € 195 € 265\n",
      "Single room € 185 € 300 € 405 \n",
      "First classDouble room € 135 € 215 € 295 \n",
      "Single room € 205 € 340 € 470 \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deArrangements mit Übernachtung  | 15\n",
      "Bookable from November 2023 to March 2024 * \n",
      "Winter Dream – stay 3 nights, pay for 2\n",
      "In the winter months, Heidelberg proves itself to be a partic -  \n",
      "ularly inspiring destination. Our castle ruin and the twisty \n",
      "Old Town alleys are very atmospheric in the cold season, \n",
      "inviting you to a relaxed stroll before warming up again in \n",
      "one of the many restaurants or laid-back, cozy student \n",
      "pubs. The winter months are also full of exiting events and \n",
      "cultural highlights for a diverse city experience.Included services\n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Traditional “Heidelberger Studentenkuss“ \n",
      " (student’s kiss)\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 oder 3 nights\n",
      "ComfortDouble room € 105 € 165 \n",
      "Single room € 150 € 245 \n",
      "First class  \n",
      "weekendDouble room € 115 € 200 \n",
      "Single room € 180 € 320 \n",
      "First classDouble room € 125 € 225 \n",
      "Single room € 200 € 380 \n",
      "* except for Advent weekends\n",
      "16 | Packages with overnight stay\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.dePrice per person (in €)\n",
      "Hotel category Type of room 2 nights 3 nights\n",
      "ComfortDouble room € 205 € 265\n",
      "Single room € 295 € 380\n",
      "First class  \n",
      "weekendDouble room € 230 € 300\n",
      "Single room € 340 € 450\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "In the Advent season, the romantic charisma of our city is \n",
      "topped once again. From the end of November until just \n",
      "before Christmas Eve, the Christmas market covers the  \n",
      "entire Old Town. During these weeks, around 130 booths \n",
      "create a sea of lights, bring festive music and the scent of \n",
      "winter treats. A guided tour of the Christmas Market will  \n",
      "familiarize you with the magic and the special offers of our \n",
      "markets. A 3-course menu in our Christmas-themed Old \n",
      "Town makes your stay just perfect.Included services \n",
      "– 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Participation in the public Christmas Walking Tour \n",
      " on Saturday (1.5 hours)\n",
      "– Traditional 3-course menu in a restaurant \n",
      " in the Old Town\n",
      "Packages with overnight stay | 17\n",
      "Bookable at Advent weekends  \n",
      "Advent experienceBookable from January to June and September to December 2023  \n",
      "Theater and Orchestra  \n",
      "Heidelberg\n",
      "Our renowned theater and orchestra of Heidelberg  \n",
      "wel comes you to an exciting cultural package. The en -\n",
      "sembles offer something for everyone: musical theater, \n",
      "concerts, plays, dance and a dedicated child and youth \n",
      "theater. Choose from a diverse schedule with modern \n",
      "plays, popu  lar classics and exiting newcomers. The pack-\n",
      "age also in  cludes a 3-course menu in a restaurant in our \n",
      "beautiful Old Town. Lean back and reminisce about your \n",
      "experiences.\n",
      "Find the current schedule here.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Tickets for a performance at the \n",
      " “Theater and Orchestra Heidelberg” (category II)\n",
      "– Traditional 3-course menu in a restaurant \n",
      " in the Old Town\n",
      "18 | Packages with overnight stay\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.dePrice per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 160 € 220 € 270 \n",
      "Single room € 200 € 300 € 390\n",
      "First class  \n",
      "weekendDouble room € 175 € 245 € 315 \n",
      "Single room € 225 € 345 € 455 \n",
      "First classDouble room € 185 € 265 € 345 \n",
      "Single room € 255 € 395 € 515 \n",
      "Bookable from June 11 to July 30, 2023  \n",
      "Heidelberger  \n",
      "Schlossfestspiele  (Heidelberg Castle Festival)\n",
      "Sights you‘ve never seen before! The combination of the \n",
      "incredible flair of our castle ruin with its legendary pre -\n",
      "sentations and highclass concerts. Every summer, the \n",
      "Heidelberg Castle Festival draws many culture connois -\n",
      "seurs to our castle. With this package you become part of \n",
      "this fascination and it lets you be there for a play of your \n",
      "choice. A walking tour of the Old Town and a 3-course \n",
      "menu will bring you summer days you will surely remem -\n",
      "ber forever.\n",
      "Find the current schedule here.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Tickets for a performance of the Heidelberg Castle \n",
      " Festival (category II)\n",
      "– Traditional 3-course menu in a restaurant \n",
      " in the Old Town\n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours)\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Packages with overnight stay | 19\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 180 € 240 € 295 \n",
      "Single room € 220 € 320 € 405 \n",
      "First class  \n",
      "weekendDouble room € 190 € 265 € 325 \n",
      "Single room € 250 € 360 € 470 \n",
      "First classDouble room € 200 € 285 € 360 \n",
      "Single room € 270 € 400 € 530 \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "20 | Packages with overnight stay\n",
      "Bookable from March 17 to April 15, 2023  \n",
      "“Heidelberger Frühling”  (Heidelberg Spring)\n",
      "Music festival\n",
      "Music lovers’ hearts beat faster in Heidelberg, especially in \n",
      "the springtime. From March to April, the classical music \n",
      "scene celebrates a festival with international renown: With \n",
      "over 100 events and 47,000 visitors, the Heidelberg Spring is \n",
      "one of the largest festivals of its kind in Germany. On this \n",
      "journey you will be our guest at a concert. Choose your  \n",
      "favourite event from the top-class program and complete \n",
      "your concert evening with a culinary 3-course menu in our \n",
      "beautiful Old Town.\n",
      "The current festival program can be found here.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Tickets for a concert (category II) *\n",
      "– 3-course menu in a restaurant in the Old Town\n",
      "– Participation in the public walking tour of \n",
      " the Old Town (1.5 hours)\n",
      "Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 200 € 260 € 310 \n",
      "Single room € 240 € 330 € 430 \n",
      "First class  \n",
      "weekendDouble room € 215 € 280 € 350 \n",
      "Single room € 265 € 375 € 495 \n",
      "First classDouble room € 225 € 305 € 380 \n",
      "Single room € 285 € 415 € 555 \n",
      "* Except for opening and closing concert.\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de\n",
      "Accommodated in the Art-Nouveau ambience of the former \n",
      "public bath in Bergheim, you can get some fascinating in -\n",
      "sights into the “Anatomy of Happiness”. The KÖRPER  WELTEN \n",
      "museum has exhibited Gunter von Hagen‘s unique plastina -\n",
      "tions of human and animal bodies since September 2017. \n",
      "Preserved in life-like positions, the exhibits illustrate our  \n",
      "anatomy better than ever before. Come for an exciting  \n",
      "journey and learn what striving for happiness is all about.Included services \n",
      "– 1, 2 or 3 nights with breakfast in a single or double \n",
      " room in the hotel categories comfort or first class\n",
      "– HeidelbergCARD for the duration of your stay\n",
      "– Entrance ticket to the exhibition of the BODY \n",
      " WORLDS MUSEUM in the “Altes Hallenbad” \n",
      " Heidelberg with audio-guide *\n",
      "– Traditional 3-course menu in a restaurant \n",
      " in the Old Town\n",
      "Bookable all year round  \n",
      "BODY WORLDS MUSEUM  (KÖRPERWELTEN)\n",
      "“The Anatomy of Happiness“\n",
      "* Opening hours: Monday – Sunday 10:00 am – 6:00 pm (last admission 5:00 pm), closed on December 24. Subject to changes.Price per person (in €)\n",
      "Hotel category Type of room 1 night 2 nights 3 nights\n",
      "ComfortDouble room € 155 € 215 € 265\n",
      "Single room € 205 € 290 € 380\n",
      "First class  \n",
      "weekendDouble room € 170 € 240 € 310 \n",
      "Single room € 230 € 350 € 450\n",
      "First classDouble room € 180 € 260 € 340\n",
      "Single room € 250 € 390 € 510\n",
      "Packages with overnight stay | 21Participating Partner Hotels 2023\n",
      "Our package trips let you choose between the comfort and first class categories. The overview \n",
      "lists all participating partner hotels, classified by these categories. We are happy to support you \n",
      "in choosing your perfect accommodation.\n",
      "* do not participate in the “Winter Dream” and “Summer Special” packages  \n",
      "** not listed in this brochure Hotel page\n",
      "Bayrischer Hof 24\n",
      "Gasthaus Backmulde  * 26\n",
      "Hackteufel 26\n",
      "Hotel am Rathaus 27\n",
      "Hotel am Schloss * 28\n",
      "Hotel Heidelberg & Lounge 28\n",
      "Kulturbrauerei Heidelberg 29\n",
      "Staycity Aparthotel Heidelberg 31\n",
      "The Heidelberg Exzellenz Hotel  * 32Hotel page\n",
      "Hotel Zum Ritter St. Georg **\n",
      "ATLANTIC Hotel Heidelberg 24\n",
      "Bergheim 41 – Hotel im Alten Hallenbad 25\n",
      "Hilton Heidelberg  * 27\n",
      "NH Heidelberg 29\n",
      "Plaza Premium Heidelberg 30\n",
      "Qube Hotel Bahnstadt 30\n",
      "Qube Hotel Bergheim 30\n",
      "Rafaela Hotel Heidelberg  * 31\n",
      "22 | Participating Partner Hotels \n",
      "Hotel category comfort  Hotel category first classHotel facilities\n",
      " Restaurant  Charging station  \n",
      "   for electrical vehicles\n",
      " Bar  Free coach parking\n",
      " Conference rooms  Bicycle parking\n",
      " Elevator  Terrace / garden\n",
      " Air conditioning  Fitness room\n",
      " Wi-Fi  Sauna\n",
      " Garage  Swimming pool\n",
      " \n",
      " Parking  Pets welcome \n",
      "SR    Single Room        DR    Double RoomDistance to the Old Town\n",
      "1 Old Town \n",
      " Directly in the town center.\n",
      "2 City center \n",
      " The Old Town is within 5 – 15 minutes \n",
      " walking distance. \n",
      "3 Urban area \n",
      " The Old Town is within 20 – 30 minutes  \n",
      " walking distance and can be reached easily  \n",
      " by public transport within 5 – 10 minutes. \n",
      "4 Suburbs \n",
      " Districts within the city border; the Old Town  \n",
      " can be reached easily by public transport  \n",
      " within 15 – 20 minutes. \n",
      "Legend distances\n",
      " Highway Neckarmünzplatz  \n",
      "   (Tourist Information)\n",
      " Main station Nearest public transport stop \n",
      "Hotel rating under DEHOGA\n",
      "(German Hotel and Restaurant Association)\n",
      "Accommodation establishments which are not \n",
      "marked with stars have not undergone the \n",
      "voluntary rating process. No inference to their \n",
      "standard is intended. Tourist \n",
      "basic standards\n",
      "Standard \n",
      "middle standards\n",
      "Comfort \n",
      "raised standardsFirst Class \n",
      "high standards\n",
      "Luxury \n",
      "the highest standards \n",
      "Superior \n",
      "additional ratingAccommodations \n",
      "We will gladly advise you on our hotels and book your stay in Heidelberg. All the hotel information provided and the room \n",
      "prices are a reference only. We will make you an individual offer on request. Find a city map on pages 36 and 37. \n",
      "Accommodations  | 23\n",
      "Hotel category first class\n",
      "  \n",
      "  Arthotel Heidelberg \n",
      "Grabengasse 7, 69117 Heidelberg,  \n",
      "Phone +49 6221 6500601 Boutique Design Hotel – a combination of a her-\n",
      "itage listed building and an architecturally attractive \n",
      "modern building – unique in the heart of Heidel -\n",
      "berg’s Old Town. Our hotel has its own parking facil-  \n",
      "ities, and a few minutes’ walk away you will find the \n",
      "sights. Our rooms boast all the modern comforts, \n",
      "our restaurant ROMER an array of culinary delights \n",
      "and our function rooms the perfect atmo  sphere \n",
      "for your celebration. Heidelberg’s sights are right \n",
      "outside the door.\n",
      "Rooms 23 DR: € 139 – 232, SR: € 119 – 202,  \n",
      "1 Loft room (Suite): € 195 – 390\n",
      "Distance approx.     3.5 km     2.8 km     900 m\n",
      "  Peterskirche / Universitätsplatz 200 m1   5/200    € 15,50/24 h    24 | Hotels\n",
      "Hotels\n",
      "Bayrischer Hof  \n",
      "Rohrbacher Strasse 2, 69115 Heidelberg,  \n",
      "Phone +49 6221 8728803 This hotel is personally run and boasts an enviable \n",
      "central location in the heart of Heidelberg. Cross \n",
      "over Bismarckplatz (Bismarck Square) directly in \n",
      "front of the hotel and after 100 m you find yourself \n",
      "in the magic of Heidelberg’s Old Town with its \n",
      "romantic castle and quaint twisty lanes. \n",
      "Rooms 49 DR: from € 79, 7 SR: from € 69\n",
      "Distance approx.     4 km     1.5 km     2.5 km\n",
      "  Bismarckplatz 100 m2   € 12/24 h  ATLANTIC Hotel Heidelberg \n",
      "Europaplatz 1, 69115 Heidelberg,  \n",
      "Phone +49 421 944888535 (Pre-Opening)2 Exceptional 4- to 5-star comfort and individual full \n",
      "service await you at the northern German-influ -\n",
      "enced ATLANTIC Hotels Group. In close proximity \n",
      "to the new Heidelberg Congress Center and the \n",
      "train station, the 4-star-superior ATLANTIC Hotel \n",
      "Heidelberg will offer 310 stylish rooms and suites, a \n",
      "spa and wellness area, a restaurant, and a rooftop \n",
      "bar with a terrace and a view over Heidelberg and \n",
      "the surrounding area from mid 2023. State-of-  \n",
      "the-art conference rooms and a 350-square-meter \n",
      "flexible ballroom on the 13th floor round off the \n",
      "facilities.\n",
      "Rooms 310 DR: from € 199, SR: from € 179\n",
      "Distance approx.     1 km     0.1 km     4.1 km\n",
      "  Gadamerplatz 100 m3   8/600           \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deHotels  | 25\n",
      "Bergheim 41 – Hotel im Alten Hallenbad \n",
      "Bergheimer Strasse 41, 69115 Heidelberg, \n",
      "Phone +49 6221 7500405 The Bergheim 41 is a contemporary city hotel. \n",
      "Your perfect choice for both business and leisure \n",
      "accom  modation. The hotel has 32 comfort rooms, \n",
      "1 junior suite with sauna, rooftop garden with \n",
      "castle view and the Bergheim 41 Kaffeekultur (cof-\n",
      "fee shop). Short distances for shopping or sight -\n",
      "seeing are guaranteed as the pedestrian area and \n",
      "the Old Town are only 250 m away from the hotel.\n",
      "Rooms 31 DR: € 124 – 174, SR: € 109 – 159,  \n",
      "1 Junior Suite: € 280 – 330\n",
      "Distance approx.     3.9 km     2.4 km     2.1 km\n",
      "  Altes Hallenbad 50 m2    € 11,50/24 h    Berggasthof Königstuhl  \n",
      "Königstuhl 2, 69117 Heidelberg,  \n",
      "Phone +49 6221 41603004 Berggasthof Königstuhl is located on the summit \n",
      "of Heidelberg‘s local mountain and offers 62 \n",
      "rooms. The facilities include a restaurant, a hotel \n",
      "bar, a 24-hour reception and free Wi-Fi in all ar-\n",
      "eas. The 4-star property also provides a sauna \n",
      "and a terrace. Private parking (with and without \n",
      "charge) is available at the hotel. All rooms at the \n",
      "hotel are equipped with a desk and include a safe \n",
      "and minibar.\n",
      "Rooms 6 Economy Rooms: from € 99, 28 Classic Rooms: from \n",
      "€ 124, 24 Deluxe Rooms: from € 154, 4 Junior Suites: from € 174\n",
      "Distance approx.     4 km     3.6 km     1.6 km\n",
      "  Königstuhl 200 m4   3/150    € 15/24 h € 15/24 h     \n",
      "City Partner Hotel Holländer Hof \n",
      "Neckarstaden 66, 69117 Heidelberg,  \n",
      "Phone +49 6221 605006 Thanks to our unique location in the heart of the \n",
      "Old Town, our guests can enjoy a divine view of \n",
      "the Alte Brücke (Old Bridge), the Neckar River and \n",
      "the Philosophenweg (Philosophers’ Walk). Two dif -\n",
      "ferent breakfast options, our extensive breakfast \n",
      "buffet and the small business breakfast, as well as \n",
      "a range of air-conditioned rooms round off what \n",
      "we have to offer. You can reach all of Heidelberg’s \n",
      "top attractions within a few minutes on foot.\n",
      "Rooms 26 DR: from € 109, 12 SR: from € 91, 1 Suite: € 216 – 233\n",
      "Distance approx.     4 km     4 km     300 m\n",
      "  Alte Brücke 50 m1   \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deDer Europäische Hof Heidelberg \n",
      "Friedrich-Ebert-Anlage 1, 69117 Heidelberg,  \n",
      "Phone +49 6221 51507 Tradition meets emotion – spoil yourself in a wel -\n",
      "coming, family-run luxury hotel in its third and fourth \n",
      "generation. 100 rooms, 14 suites, 3 executive suites \n",
      "– all timelessly furnished with elegance and individu -\n",
      "ality. Enjoy French modern interpreted dish  es in  \n",
      "our fine-dining restaurant “Die Kurfürsten  stube” or  \n",
      "delight yourself with Mediterranean del  icacies in our \n",
      "summer restaurant & terrace. Find refreshment, rest \n",
      "and relaxation in our PANORAMA SPA. The von \n",
      "Kretschmann family welcomes you!\n",
      "Rooms 60 DR: € 258 – 470, 40 SR: € 209 – 347,  \n",
      "14 Junior Suites: € 474 – 566 , 3 Executive Suites: € 626 – 756\n",
      "Distance approx.     4 km     1.4 km     1.8 km\n",
      "  Bismarckplatz 350 m1   10/200    € 23/24 h       26 | Hotels\n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deGasthaus Backmulde  \n",
      "Schiffgasse 11, 69117 Heidelberg, \n",
      "Phone +49 6221 536608 We are sure you will feel right at home here – a place \n",
      "where both your soul and your taste-buds will be \n",
      "pampered. We will see to your needs and wishes, \n",
      "making your stay a relaxing and comfortable expe -\n",
      "rience. After a good night’s rest, you will be ready to \n",
      "enjoy everything our premises have to offer. Be our \n",
      "guest and leave your daily routine behind. It will be \n",
      "our pleasure to look after you!\n",
      "Rooms 3 DR: € 138, 17 Comfort DR: € 148, 5 SR: € 120 – 130,  \n",
      "2 Junior Suites: € 180 \n",
      "Distance approx.     3 km     4.1 km     700 m\n",
      "  Stadthalle 300 m1  2/100    € 15/24 h\n",
      "Hackteufel  \n",
      "Steingasse 7, 69117 Heidelberg, \n",
      "Phone +49 6221 9053809 The privately managed house with family-like at -\n",
      "mosphere, located at the core of Heidelberg’s Old \n",
      "Town between the Heiliggeistkirche (Church of the \n",
      "Holy Spirit) and the Alte Brücke (Old Bridge), invites \n",
      "you to enjoy stylish life. The 12 hotel rooms and  \n",
      "the holiday apartment have been lovingly and indivi -\n",
      "dually furnished. They come with LCD TV and free  \n",
      "Wi-Fi. The cozy restaurant and the wine tavern  \n",
      "welcome you with regional cuisine and a well-  \n",
      "stocked wine cellar. Welcome!\n",
      "Rooms 9 DR: from € 109, 2 SR: from € 85, Apartment: from € 189, \n",
      "Junior Suite: from € 189\n",
      "Distance approx.     3 km     3 km     800 m\n",
      "  Alte Brücke 100 m1     € 8/24 h   Heidelberg Marriott Hotel  \n",
      "Vangerowstrasse 16, 69115 Heidelberg,  \n",
      "Phone +49 6221 908010 Heidelberg Marriott Hotel is the perfect place for a \n",
      "few days of relaxation or your event. Idyllically loca -\n",
      "ted on the banks of the Neckar River, getting to and \n",
      "exploring Heidelberg is convenient. You can reach \n",
      "the Old Town on foot or by boat from the jetty at the \n",
      "hotel. Visit our sun terrace, the restaurant “Grill 16” \n",
      "or the bar, and relax in the sauna, the fitness center \n",
      "or by the pool. Highest hygiene stan  dards and free \n",
      "Wi-Fi are a matter of course. 248 rooms, 7 function \n",
      "rooms for max. 330 persons.\n",
      "Rooms 125 DR: from € 149, 64 Balcony rooms / Neckar River \n",
      "view: from € 169, 45 Executive Rooms: from € 209,  \n",
      "11 Junior Suites: from € 239, 3 Executive Suites: from € 439\n",
      "Distance approx.     500 m     800 m     3 km\n",
      "  Betriebshof 500 m3   7/330    € 25/24 h        \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deHilton Heidelberg  \n",
      "Kurfürsten-Anlage 1, 69115 Heidelberg, \n",
      "Phone +49 6221 352132011 Friendly hospitality in the center of Heidelberg! Newly \n",
      "opening in 2023, the hotel with its 244 elegant and \n",
      "high quality equipped rooms and suites benefits \n",
      "from a prime location. It is only a few minutes away \n",
      "from the banks of the Neckar River and the main  \n",
      "attractions such as the Old Town and Heidelberg \n",
      "Castle. After an exciting day you can indulge yourself \n",
      "in the hotel‘s own restaurant or enjoy the exclusive \n",
      "atmosphere in the hotel bar or executive lounge.\n",
      "Rooms 108 DR: from € 139, 79 Deluxe Rooms: from € 169,  \n",
      "47 Executive Rooms: from € 239, 4 Junior Suites: from € 289,  \n",
      "6 One Bedroom Suites: from € 339\n",
      "Distance approx.     4 km     1.2 km     2 km\n",
      "  Seegarten 10 m2   7/300    € 25/24 h   \n",
      "Hotel am Rathaus  \n",
      "Heiliggeiststrasse 1, 69117 Heidelberg,  \n",
      "Phone +49 6221 1473012 The hotel is situated in the heart of the Old Town, \n",
      "directly at the market square. The pedestrian  ised \n",
      "High Street and the Church of the Holy Spirit are \n",
      "just around the corner. All the historic sights as \n",
      "well as the original student pubs and Heidelberg \n",
      "Castle are also just a short walk away. Parking is \n",
      "available in the carpark P12 (200 m away).\n",
      "Rooms 14 DR: € 125 – 170, 3 SR: € 89 – 130,  \n",
      "1 Three-bed: € 155 – 189, 1 Four-bed: € 170 – 199 \n",
      "Distance approx.     3.5 km     3 km     350 m\n",
      "  Rathaus / Bergbahn 200 m1  € 10/24 h Hotels  | 27Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deHotel am Schloss \n",
      "Zwingerstrasse 20, 69117 Heidelberg, \n",
      "Phone +49 6221 1417013 Relax in the pleasant atmosphere of the Hotel am \n",
      "Schloss. From our rooms, you have a divine view \n",
      "over the Old Town of Heidelberg. On our roof ter -\n",
      "race, you can sit directly at the foot of Heidelberg \n",
      "Castle. Our breakfast buffet guarantees a good \n",
      "start into the day. There is a parking garage in the \n",
      "building.\n",
      "Rooms 15 DR: € 125 – 170, 4 SR: € 97 – 130,  \n",
      "2 Three-bed: € 155 – 180, 1 Four-bed: € 170 – 210 \n",
      "Distance approx.     4 km     4 km     600 m\n",
      "  Rathaus / Bergbahn 2 m1   € 10/24 h   \n",
      "Hotel Berger  \n",
      "Erwin-Rohde-Strasse 8, 69120 Heidelberg, \n",
      "Phone +49 6221 40160814 Located in Heidelberg’s central suburb of Neuen -\n",
      "heim, the family-run art nouveau villa invites you \n",
      "to come and relax around the heated outdoor \n",
      "pool in the peaceful garden setting. Our hotel and \n",
      "the nearby annex with apartments are only a few \n",
      "min utes from the Old Town, the university campus \n",
      "and hospitals. Come and enjoy the personal  \n",
      "atmosphere in our individually designed hotel \n",
      "rooms and if you are here for a longer stay, make \n",
      "yourself at home in our luxuriously designed \n",
      "apartments.\n",
      "Rooms 4 DR: € 120 – 160, 2 SR: € 95 – 105,  \n",
      "2 Family Rooms: from € 175\n",
      "Distance approx.     2.4 km     2 km     2.1 km\n",
      "  Mönchhofschule 100 m2  € 12/24 h   \n",
      "Hotel Heidelberg & Lounge \n",
      "Heuauer Weg 35 – 37, 69124 Heidelberg, \n",
      "Phone +49 6221 354441515 At the gates of the city, in the Kirchheim district, you \n",
      "will find a cozy atmosphere and an ideal location. \n",
      "The city center is easily reached within 20 minutes \n",
      "by public transport, jogging and cycling paths start \n",
      "at the hotel. You can park your bike in the bike  \n",
      "garage with e-bike charging possibility, for your car \n",
      "there is also an e-charging station. After an eventful \n",
      "day, relax in the lounge with delicious drinks, tarte \n",
      "flambée & dishes to share, on sunny days also on \n",
      "the terrace!\n",
      "Rooms 24 DR: from € 99, 10 SR: from € 75\n",
      "Distance approx.     5 km     4 km     7 km\n",
      "  Kirchheim Friedhof 50 m4          28 | HotelsInformation and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deHotel Krokodil  \n",
      "Kleinschmidtstrasse 12, 69115 Heidelberg,  \n",
      "Phone +49 6221 739297016 The Hotel Krokodil is situated in the peaceful West -\n",
      "stadt, only a short walk from the historic Old Town. \n",
      "This hotel offers quiet and spacious rooms that  \n",
      "really are value-for-money. Enjoy the pleasant  \n",
      "atmosphere and friendly service of a smaller, and \n",
      "more personal hotel. A delicious breakfast buffet \n",
      "awaits you in the morning. In our in-house restau -\n",
      "rant, you will be spoiled with regional specialties.\n",
      "Rooms 13 DR: from € 115, 3 SR: from € 90\n",
      "Distance approx.     2 km     500 m     2 km\n",
      "  Stadtbücherei 200 m2   € 12,50/24 h\n",
      "Kulturbrauerei Heidelberg  \n",
      "Leyergasse 6, 69117 Heidelberg, \n",
      "Phone +49 6221 50298017 The Kulturbrauerei Heidelberg hotel is located  \n",
      "at the foot of Heidelberg Castle in the heart of \n",
      "Heidelberg’s Old Town. Our rooms are spread \n",
      "across the old brewery building, the newly erec - \n",
      "ted annex and above the inn “Wirtshaus Zum \n",
      "Seppl”. All rooms have Wi-Fi and are non-smoking.\n",
      "Rooms 34 DR: € 141 – 199\n",
      "Distance approx.     7 km     4 km     100 m\n",
      "  Rathaus / Bergbahn 200 m1    € 15/24 h \n",
      "NH Heidelberg  \n",
      "Bergheimer Strasse 91, 69115 Heidelberg, \n",
      "Phone +49 6221 1327018 The NH Heidelberg in the city center of Heidelberg is \n",
      "the ideal starting point to discover one of the most \n",
      "romantic and beautiful cities in Germany and the  \n",
      "metropolitan region between the Rhine and Neckar \n",
      "River. Heidelberg is famous for its pioneering science \n",
      "and research, its history, well-known com  panies, and \n",
      "elite universities. The hotel has 168 rooms in both a \n",
      "new annex and a historic building, which once be-\n",
      "longed to the Heidelberg brewery.\n",
      "Rooms 158 DR: from € 109, 10 SR: from € 109\n",
      "Distance approx.     1 km     1 km     2.5 km\n",
      "  Volkshochschule 100 m2   16/400    € 24/24 h    Hotels  | 2930 | Hotels\n",
      "PLAZA Premium Heidelberg \n",
      "Sofienstrasse 6-8, 69115 Heidelberg, \n",
      "Phone +49 6221 435392019 The recently opened PLAZA Premium, which is  \n",
      "situated close to the historic Old Town, offers you \n",
      "the perfect place for all kinds of leisure activities \n",
      "and excursions. The central and quiet location of \n",
      "our hotel, is ideal for families, city and business \n",
      "travelers.\n",
      "Rooms 87 DR Comfort: from € 134, 70 DR Premium: from € 149, \n",
      "87 SR Comfort: from € 119, 70 SR Premium: from € 134,  \n",
      "10 Three-bed: from € 174, 1 Junior Suite: from € 214,  \n",
      "7 Suites: from € 244, 1 Presidential Suite: from € 304\n",
      "Distance approx.     5 km     1.7 km     1.8 km\n",
      "  Bismarckplatz 200 m2      € 17/24 h    \n",
      "Qube Hotel Bahnstadt \n",
      "Grüne Meile 21, 69115 Heidelberg, \n",
      "Phone +49 6221 63900020 All rooms at the new Qube Bahnstadt have ge -\n",
      "nerous glass sections that let in plenty of natural \n",
      "light. The hotel’s high-level equipment includes the \n",
      "latest presentation technology and fiberglass high-\n",
      "speed internet. Quality and design enjoy high priori -\n",
      "ty in all 84 rooms with their oak wood parquet floors, \n",
      "leather armchairs, walnut desks and natural stone \n",
      "bathtubs. The Qube Bahnstadt places special value \n",
      "on the gastronomy area with restaurant, bar and \n",
      "cozy lounge. The beautiful roof terrace is a special \n",
      "highlight.\n",
      "Rooms 84 DR: from € 108\n",
      "Distance approx.     700 m     700 m     4.6 km\n",
      "  Gadamerplatz 300 m3   4/100    € 14,80/24 h    \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.de Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deQube Hotel Bergheim  \n",
      "Bergheimer Strasse 74, 69115 Heidelberg, \n",
      "Phone +49 6221 18799021 The boutique hotel captivates with its sophisticated \n",
      "interior and sustainable features. The privately-run \n",
      "hotel Qube Bergheim places great importance on \n",
      "quality, service and friendliness. Excellent location \n",
      "in the town center. Expansive restaurant area with \n",
      "lounge and bar. Underground parking garage in \n",
      "the hotel.\n",
      "Rooms 26 DR: € 108 – 208, 11 SR: € 98 – 188,  \n",
      "16 Superior: € 138 – 228, 9 Deluxe: € 158 – 238\n",
      "Distance approx.     700 m     700 m     2.6 km\n",
      "  Volkshochschule 20 m2   2/70    € 14,80/24 h    sevenDays Hotel BoardingHouse \n",
      "Eppelheimer Strasse 14, 69115 Heidelberg, \n",
      "Phone +49 6221 7530023 The sevenDays is a unique combination of hotel and \n",
      "boarding house. Each studio has a balcony and a  \n",
      "fully equipped kitchenette with microwave oven,  \n",
      "induction hob and coffee machine. The studios are \n",
      "available for one night or longer stays, e. g. if you are \n",
      "here on a stay for weeks or months. In our break  fast \n",
      "restaurant, we offer a sumptuous breakfast buffet.\n",
      "Rooms 68 DR: from € 89, 68 SR: from € 79,  \n",
      "11 Penthouse: from € 99, 6 Apartments: from € 119 \n",
      "Distance approx.     500 m     300 m     4 km\n",
      "  Czernybrücke Süd 100 m3   € 10/24 h € 10/24 h    \n",
      "Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deRafaela Hotel Heidelberg \n",
      "Lutherstrasse 17, 69120 Heidelberg, \n",
      "Phone +49 6221 674330022 Central, idyllic, modern – Rafaela Hotel is located in \n",
      "direct proximity to the popular Neckar meadow and \n",
      "the Old Town with its tourist sights, as well as close \n",
      "to the Neuenheimer Feld with its research fa  cilities \n",
      "and hospitals. A small market square right outside \n",
      "of the hotel door beckons with gastronomy offers, \n",
      "outdoor chairs and French, Italian as well as sustain-  \n",
      "able specialties offers – down a calm side street. \n",
      "Parking places are available 300 m away in a public \n",
      "parking garage.\n",
      "Rooms 24 DR: € 109 – 183, 24 SR: € 90 – 162,  \n",
      "3 Family Rooms: € 113 – 216 \n",
      "Distance approx.     4 km     2.2 km     2 km\n",
      "  Brückenstrasse 240 m2  3/60     Hotels  | 31\n",
      "Staycity Aparthotel Heidelberg \n",
      "Speyerer Strasse 7-9, 69115 Heidelberg, \n",
      "Phone +49 6221 3600024 The Staycity Aparthotel Heidelberg convinces with its \n",
      "proximity to the historic Old Town. The hotel rooms \n",
      "are bright and generously furnished as well as air-\n",
      "conditioned and offer a separate shower and toilet \n",
      "area, hairdryer, LCD TV, safe, and free Wi-Fi. The \n",
      "apartments are additionally equipped with a kitche -\n",
      "nette and thus suitable for a longer stay. Reception \n",
      "and lobby bar are open 24/7. 90 hotel parking spaces \n",
      "with e-car charging stations.\n",
      "Rooms 215 DR: from € 80, 84 Apartments: from € 109\n",
      "Distance approx.     3 km     1 km     4 km\n",
      "  Montpellierbrücke 200 m3     € 15/24 h    Information and booking: +49 6221 5840 - 226, reservation@heidelberg-marketing.deBauernhof Koch  \n",
      "Holger und Marianne Koch, Bahnhofstrasse 50, 68535 Edingen-Neckarhausen, Phone +49 6203 85715, info@bauernhof-koch-edingen.de26\n",
      "Whether you are planning a vacation in the countryside, a city trip or a business \n",
      "stay, from our farm you can easily reach the entire metropolitan region. Our vaca -\n",
      "tion apartments are designed to be largely barrier-free and also offer sufficient \n",
      "space for families with children. They each have their own terrace with a small \n",
      "garden. All apartments are air-conditioned, parking spaces for our guests are \n",
      "available free of charge. \n",
      "Size 50 sqm, Rooms  2, Bedrooms  1, Beds  1 – 4, Price  from € 80 4\n",
      "Boardinghouse Luise5  \n",
      "Dr. Friedrich Betzer, Luisenstrasse 5, 69115 Heidelberg, Phone +49 6221 6556114, welcome@luise5-boardinghouse.de27\n",
      "Spacious apartments in a beautifully renovated historic building, in the direct \n",
      "neighborhood of the ATOS-Klinik hospital. Tastefully furnished, approx. 50 \n",
      "sqm for up to 4 self-caterers. 1 separate bedroom – 1 living / dining area with \n",
      "fully equipped kitchen and 1 extensible sofa bed – 1 daylit bathroom with \n",
      "shower / toilet – washing machine and dryer in the basement – free Wi-Fi – \n",
      "2 TVs per apartment – parking space at the house for a fee. \n",
      "Size 50 sqm, Rooms  2, Bedrooms  1, Beds  1 – 4, Price  from € 1292The Heidelberg Exzellenz Hotel  \n",
      "Rohrbacher Strasse 29, 69115 Heidelberg,  \n",
      "Phone +49 6221 915025 Our charming, individually appointed and personally \n",
      "run city hotel offers you a new lifestyle concept and \n",
      "an extraordinary design. The centrally located art \n",
      "nouveau villa is only a few minutes’ walk from \n",
      "Heidelberg’s Old Town. Some of our rooms have a \n",
      "fully equipped kitchenette. To make your stay more \n",
      "comfortable, we also have hotel parking facilities.\n",
      "Rooms 35 DR: from € 115, 13 SR: from € 82\n",
      "Distance approx.     3 km     2 km     2 km\n",
      "  Hans-Böckler-Strasse 50 m2\n",
      "1/30   € 15/24 h € 15/24 h \n",
      "Holiday Apartments, Aparthotels and Hostels32 | Hotels, Holiday Apartments, Aparthotels and HostelsFerienbauernhof & Gästehaus Fießer  \n",
      "Family Fießer, Kirchheimer Hof 7, 69124 Heidelberg, Phone +49 176 70709736, info@ferienbauernhof-heidelberg.de28\n",
      "In the middle of the field and yet in close proximity to the city lies our farm.  \n",
      "The old tobacco barn was converted into 4 modern, fully furnished vacation \n",
      "apartments. 1 – 5 persons can be accommodated. Around the farm there are \n",
      "many walking and cycling paths to get plenty of free space. The little guests also \n",
      "get their money’s worth: Small animals and a large outdoor playground make \n",
      "children’s hearts beat faster.\n",
      "Size 70 – 75 sqm, Rooms  3, Bedrooms  2, Beds  1 – 5, Price  from € 1004\n",
      "Ferienwohnung Peperoni Heidelberg  \n",
      "Ilona Jung, Rainweg 20, 69118 Heidelberg, Phone +49 6221 808332, c.i.jung@arcor.de29\n",
      "Our holiday apartment “Peperoni” is located in the green Ziegelhausen district,  \n",
      "offering good traffic connection to all sights and event locations. There are many \n",
      "hiking paths in close proximity, such as the Philosophenweg (Philosophers’ Walk) \n",
      "leading to Heidelberg’s Old Town. A bakery, an indoor swimming pool and the Fürs -\n",
      "tendamm bus stop are 200 m away. Bus lines 33, 34 and 37 will take you to any  \n",
      "relevant destination in Heidelberg every 20 minutes. For further information please \n",
      "also visit our website: www.peperoniheidelberg.jimdofree.com\n",
      "Size 42 sqm, Rooms  1, Bedrooms  1, Beds  1 – 4, Price  from € 594Holiday Apartments, Aparthotels and Hostels  | 33\n",
      "Ferienwohnung Rittmüller  \n",
      "Ursula Rittmüller, Stiftweg 28, 69118 Heidelberg, Phone +49 6221 801147, rainer.rittmueller@t-online.de30\n",
      "Our property is in a beautiful rural setting only 2 – 3 minutes’ walk from the \n",
      "forest, tennis courts, indoor pool with sauna, solarium and sunbathing area; \n",
      "hiking trails and direct bus connection to Heidelberg, the Old Town and the \n",
      "hospital. Both apartments have a kitchen, shower and WC, separate access, \n",
      "terrace with use of the garden, cable TV and Wi-Fi.\n",
      "Size 30 – 60 sqm, Rooms  1 – 3, Bedrooms  1 – 2, Beds  1 – 4, Price  from € 60 4\n",
      "Gästehaus Komm  \n",
      "Milvia Komm, Am Bächenbuckel 6, 69118 Heidelberg, Phone +49 6221 20602, gaestehaus-komm@t-online.de31\n",
      "Our accommodations are situated in a quiet area near the forest, offering stimula -\n",
      "ting walks. The holiday apartments have high-quality furnishings: fitted kitchen, \n",
      "bathrooms and toilet, including towels and bed linens, some with terrace, balcony \n",
      "and a lounge in the flower garden, SAT-TV and free Wi-Fi. Two direct bus connec -  \n",
      "tions will take you to the Old Town. Your dog is welcome here as well. We look  \n",
      "forward to your visit! \n",
      "Size 32 – 84 sqm, Rooms  1 – 2 with kitchen / bathroom, Bedrooms  0 / 1, Beds  2 / 4, Price  from € 60434 | Holiday Apartments, Aparthotels and Hostels\n",
      "Weingut Bauer  \n",
      "Ingrid Bauer, Dachsbuckel Winzerhof 1, 69126 Heidelberg, Phone +49 6221 381931, fewo@heidelberger-dachsbuckel.de34\n",
      "The six apartments of the Bauer Winery are modernly furnished and very \n",
      "quiet. In addition to a fully equipped kitchenette and internet access via Wi-Fi, \n",
      "all apartments have a large living area. Situated in the middle of the vineyards \n",
      "of the Heidelberg Dachsbuckel they offer the ideal starting point for a relax-\n",
      "ing vacation in Heidelberg.\n",
      "Size 45 – 80 sqm, Rooms  2 – 4, Bedrooms  1 – 3, Beds  3 – 7, Price  from € 65 4Heidelberg APHEARTMENTS  \n",
      "Marcus Weigl, Hauptstrasse 161, 69117 Heidelberg, Phone +49 152 53182564, mail@heidelberg-apheartments.com32\n",
      "In the heart of the Old Town you will find four beautiful, newly renovated \n",
      "vacation apartments, which are ideal for short trips as well as for long-term \n",
      "stays. Staying in a romantic baroque house, you can easily reach all sights \n",
      "and institutions by foot, go shopping, stroll around and end the day in one \n",
      "of the traditional restaurants – your vacation apartment and your dreamy \n",
      "comfortable bed are only a few steps away.\n",
      "Size 92–109 sqm, Rooms  3 – 4 with kitchen / bathroom, Bedrooms  2 – 3, Beds  1 – 7, Price  from € 130 1\n",
      "Zweite Heimat Heidelberg \n",
      "Monika Ihlenfeld, Werderstr. 43, Moltkestr. 7, Gaisbergstr. 29, Römerstr. 24, Heidelberg, Phone +49 6221 6561943, wohnen@zweiteheimatheidelberg.de35\n",
      "We want our guests to feel right at home. This is why we value high-quality \n",
      "living and furnishings. We have good traffic connections to all sights and \n",
      "event locations. You can cover your daily needs from nearby restaurants, \n",
      "cafés and stores. Equipment: TV, HiFi system, Wi-Fi, fully equipped kitchen, \n",
      "washing machine, tumble-dryer, bathtub or shower, toilet, towels, bed linen, \n",
      "terrace / balcony / conservatory.\n",
      "Size 45 – 95 sqm, Rooms  1 – 3 with kitchen / bathroom, Bedrooms  1 – 2,  \n",
      "Beds  1 – 6, Price  from € 89 32Steffis Hostel \n",
      "Stefanie Munz, Alte Eppelheimer Strasse 50, 69115 Heidelberg, Phone +49 6221 7782772, info@hostelheidelberg.de33\n",
      "You can find us just a few meters away from the Hauptbahnhof (main station), on \n",
      "the premises of the Landfriedkomplex (Landfried Complex), which are pro  tected \n",
      "as a monument. We offer a comfortable atmosphere and contact with travelers \n",
      "from around the world. You can choose between one of our lovingly furnished \n",
      "private rooms and a bed in a spacious room with multiple beds. All rooms share \n",
      "clean showers and toilets centrally situated along the hallway. Wi-Fi, parking place \n",
      "(24 h / € 8), bicycles, guest kitchen, lounge. \n",
      "Rooms 10 DR: from € 70, 4 SR: from € 60, 15 dormitories (3 – 10 beds): from € 243\n",
      "Further information about accommodations in Heidelberg  \n",
      "can be found on our website.Verkehrsverein Ziegelhausen e. V.  \n",
      " \n",
      "The beautiful landscape of the Neckar Valley east of \n",
      "Heidelberg, in the area of the district Ziegelhausen/\n",
      "Peterstal, already impressed well -known poets, scholars \n",
      "and composers in the 18th and 19th centuries.  \n",
      " \n",
      "Due to the very good transport connections, you can \n",
      "reach the sights, the old town, the famous Heidelberg \n",
      "Castle and the world -famous Heidelberg University \n",
      "Hospital in a short time.  \n",
      "We are happy to help you finding a suitable holiday \n",
      "home.  \n",
      "Verkehrsverein Ziegelhausen e. V.  \n",
      "Peterstaler Straße 1, 69118 Heidelberg  \n",
      "Telefon:      +49 (0)6221 -80 06 49  \n",
      "E-Mail:       info@verkehrsverein -ziegelhausen.de  \n",
      "Website:    www.verkehrsverein -ziegelhausen.de  \n",
      "Facebook: verkehrsverein.ziegelhausen  \n",
      "We are here for you:  \n",
      " \n",
      "Monday  10:00 am - 12:00 pm  \n",
      "Wednesday  10:00 am - 12:00 pm  \n",
      "       3:00 pm -   5:00 pm  \n",
      "Thursday     3:00 pm -   5:00 pm  \n",
      "Friday                 10:00 am - 12:00 pm  \n",
      "                  3:00 pm -   5:00 pm  \n",
      "Saturday           10:00 am -  12:00 pm   \n",
      "Heidelberg CARD\n",
      "Put Heidelberg in your pocket!\n",
      "With castle ticket including funicular railway\n",
      "—\n",
      "Combination ticket: one-time free admission  \n",
      "to the University Museum, the Student Prison and the special exhibition\n",
      "—\n",
      "Free travel on public transport in the city area\n",
      "—\n",
      "Discounts with attractive partners, on our public guided \n",
      "Old Town tours and city tours by bus with a castle tour.\n",
      "Available at all  \n",
      "Tourist Information\n",
      "Online order and\n",
      "detailed information at  \n",
      "www.heidelbergcard.comHeidelberg Marketing GmbH  \n",
      "www.heidelberg-marketing.com\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Familyfrom\n",
      "24 €36 | Overview map\n",
      "Overview map\n",
      "A 5\n",
      "A 656\n",
      "A 5Friedrich-Ebert-AnlageNeuenheimer LandstraßeZiegelhäuser Landstraßeuhsheimer Landstraße\n",
      "Bergheimer Straße\n",
      "Rohrbacher StraßeRöm\n",
      "erstraßeSpeyerer Straße\n",
      "Kirchheimer Weg\n",
      "Schwetzinger StraßeZiegelhausen\n",
      "SchlierbachAltstadtNeuenheimHandschuhsheim\n",
      "Bergheim\n",
      "Bahnstadt\n",
      "PfaffengrundWeststadtWieblingenNeuenheimer Feld\n",
      "Südstadt\n",
      "BoxbergRohrbachKirchheim\n",
      "EmmertsgrundHeidelberger \n",
      "SchlossMannheim\n",
      "14\n",
      "3\n",
      "2\n",
      "432\n",
      "Distance to the Old Town\n",
      "1 Old Town \n",
      " Directly in the town center.\n",
      "2 City center \n",
      " The Old Town is within \n",
      " 5 – 15 minutes walking distance.\n",
      "3 Urban area \n",
      " The Old Town is within 20 – 30  \n",
      " minutes walking distance and  \n",
      " can be reached easily by public  \n",
      " transport within 5 – 10 minutes.\n",
      "4 Suburbs \n",
      " Districts within the city border;  \n",
      " the Old Town can be reached  \n",
      " easily by public transport within  \n",
      " 15 – 20 minutes.1 14\n",
      "7204 17\n",
      "10232 15\n",
      "8215 18\n",
      "11243 16\n",
      "922619\n",
      "12\n",
      "1325Hotels\n",
      "Arthotel Heidelberg\n",
      "ATLANTIC Hotel Heidelberg\n",
      "Bayrischer Hof\n",
      "Berggasthof Königstuhl\n",
      "Bergheim 41 – \n",
      "Hotel im Alten Hallenbad\n",
      "City Partner Hotel Holländer Hof\n",
      "Der Europäische Hof Heidelberg\n",
      "Gasthaus Backmulde\n",
      "Hackteufel\n",
      "Heidelberg Marriott Hotel\n",
      "Hilton Heidelberg \n",
      "Hotel am Rathaus\n",
      "Hotel am SchlossHotel Berger\n",
      "Hotel Heidelberg & Lounge\n",
      "Hotel Krokodil\n",
      "Kulturbrauerei Heidelberg\n",
      "NH Heidelberg\n",
      "PLAZA Premium Heidelberg\n",
      "Qube Hotel Bahnstadt\n",
      "Qube Hotel Bergheim\n",
      "Rafaela Hotel Heidelberg\n",
      "sevenDays Hotel BoardingHouse\n",
      "Staycity Aparthotel Heidelberg\n",
      "The Heidelberg Exzellenz Hotel7\n",
      "2022316\n",
      "28\n",
      "342425\n",
      "153\n",
      "111951821 10\n",
      "33272214\n",
      "41869\n",
      "12\n",
      "131729\n",
      "3235b\n",
      "35a\n",
      "35c35d30 3126\n",
      "Map is not true to scale.City center map  | 37\n",
      "City center map\n",
      "UferstraßeZiegelhäuser Landstraße\n",
      "Hauptstraße\n",
      " PlöckPlöckPhilosophenweg      Hirschgasse\n",
      "Karlstor\n",
      "Heidelberger \n",
      "SchlossNeuenheimHandschuhsheim\n",
      "Bergheim\n",
      "WeststadtAlte Brücke\n",
      "AltstadtNeuenheimer LandstraßeHandschuhsheimer Landstraße\n",
      "Theodor-Heuss-Brücke\n",
      "Bismar ck-\n",
      "PlatzNeckarstaden\n",
      "Iqbal-Ufer\n",
      "Bergheimer Straße\n",
      "e\n",
      "Berliner StraßeErnst-W\n",
      "alz-Brücke\n",
      "Kurfürsten-AnlageRohrbacher Straße13\n",
      "3\n",
      "42\n",
      "2\n",
      "26\n",
      "3229\n",
      "3527\n",
      "333028\n",
      "3431Holiday Apartments, \n",
      "Aparthotels and Hostels\n",
      "Bauernhof Koch\n",
      "Boardinghouse Luise5\n",
      "Ferienbauernhof & Gästehaus Fießer\n",
      "Ferienwohnung Peperoni Heidelberg\n",
      "Ferienwohnung Rittmüller\n",
      "Gästehaus Komm\n",
      "Heidelberg APHEARTMENTS\n",
      "Steffis Hostel\n",
      "Weingut Bauer\n",
      "Zweite Heimat Heidelberg3\n",
      "23\n",
      "20\n",
      "2421625111921\n",
      "18522\n",
      "869\n",
      "12\n",
      "1317\n",
      "114\n",
      "7\n",
      "332732\n",
      "10\n",
      "35d35a35b\n",
      "35c4\n",
      "Map is not true to scale.38 | Public City Tours\n",
      "Public City Tours\n",
      "Walking Tour of the Old Town\n",
      "Fall in love with Heidelberg! Find out why Heidelberg is one \n",
      "of the most popular destinations in the world. Take a walking \n",
      "tour of the Old Town to get to know our city and its  \n",
      "fascinating sights and special features: the Heiliggeistkirche \n",
      "(Church of the Holy Spirit), Jesuits‘ quarter, Germany‘s oldest \n",
      "university or the Studentenkarzer (Student Prison). Become \n",
      "part of the romantic spirit of the twisty alleys and discover \n",
      "history around every corner.\n",
      "Price € 12, € 10 discount price *  \n",
      "Duration 1.5 hours  \n",
      "Times  \n",
      "April – October  \n",
      "German: daily 10:30 am, additionally Friday 6:00 pm and  \n",
      "Saturday 2:30 pm  \n",
      "English: Thursday – Saturday 10:30 am  \n",
      "November – March  \n",
      "German: Friday 2:30 pm and Saturday 10:30 am  \n",
      "Meeting Point Neckarmünzplatz, in front of the\n",
      "Tourist Information\n",
      " Note  During the Old Town festival “Heidelberger \n",
      "Herbst“ (Heidelberg Autumn) on September 30, 2023 \n",
      "there will be no Walking Tour of the Old Town at 2:30 pm.City and Castle Sightseeing Tour\n",
      "Open view on the “Heidelberg scenery“ – Act one: The fo  cus \n",
      "shifts to the romantic castle, which is majestically en  throned \n",
      "above the Karlsplatz. Enjoy a comfortable city tour by bus, \n",
      "accompanied by a tour guide, that leads you along the Ne -\n",
      "ckar River. Act two takes you up to Heidelberg Castle. In \n",
      "addition to an exterior tour, you will visit the in  ner courtyard \n",
      "and the famous Great Barrel. Take advan  tage of your castle \n",
      "ticket with a visit of the German Phar  macy Museum and the \n",
      "funicular ride back to the Old Town (Kornmarkt station).\n",
      "Price € 30, € 27 discount price * , € 28 with HDCARD  \n",
      "including castle ticket (funicular + castle courtyard  \n",
      "admission)  \n",
      "Duration  2 hours  \n",
      "Times  \n",
      "April – October  \n",
      "German: Friday, Saturday and Whitsunday 1:30 pm  \n",
      "November – March  \n",
      "German: Saturday 1:30 pm  \n",
      "Meeting Point  Neckarmünzplatz,  \n",
      "information board (bus stop)  \n",
      "Minimum number of participants  5 people \n",
      " Note  During the Old Town festival “Heidelberger \n",
      "Herbst“ (Heidelberg Autumn) on September 30, 2023 \n",
      "there will be no tour. \n",
      "Information and booking: +49 6221 5840 - 223/225, guide@heidelberg-marketing.de Information and booking: +49 6221 5840 - 223/225, guide@heidelberg-marketing.de* Eligibility criteria for the reduced price: school and university students (up to 28 years old), people with disabilities and a disabled person’s pass and \n",
      "owners of the ♥HDCARD. One accompanying person of a severely disabled person with the characteristic “B” in the severely disabled person’s identity \n",
      "card as well as of children and young people up to 18 years of age with a disability is free of charge. Children aged 6 to 14 pay a reduced price. Tickets \n",
      "are available at our Tourist Information.including  \n",
      "castle  \n",
      "ticketPublic City Tours | 39\n",
      "The University in the Old Town\n",
      "Founded by Prince Elector Ruprecht I. in 1386, the “Ruperto \n",
      "Carola” is Germany’s oldest university and one of the most \n",
      "venerable education facilities in Europe. The tour conveys \n",
      "not only the university’s history but also provides an insight \n",
      "into student life. It takes you from the university library via \n",
      "the Peterskirche (St. Peter’s Church), the oldest church in \n",
      "Heidelberg, to the Alte Aula (Old Auditorium) and the histor -\n",
      "ical Studentenkarzer (Student Prison). Visit the place that \n",
      "once made people suffer! From 1778 to 1914, students were \n",
      "punished here “for trivial offenses”.\n",
      "Price  € 15, discount price € 13 *  \n",
      "including admission to the Alte Aula (Old Auditorium)  \n",
      "and the Studentenkarzer (Student Prison)  \n",
      "Duration  1.5 – 2 hours  \n",
      "Times  \n",
      "April – October  \n",
      "German: Saturday 3:00 pm  \n",
      "Meeting Point  Neckarmünzplatz, in front of the  \n",
      "Tourist Information  \n",
      "Minimum number of participants  5 people\n",
      " Note  During the Old Town festival “Heidelberger \n",
      "Herbst“ (Heidelberg Autumn) on September 30, 2023 \n",
      "there will be no tour.Christmas Market Tour\n",
      "Take a walk through the Old Town and immediately per-\n",
      "ceive the Christmas spirit with the scent of roasted almonds \n",
      "and mulled wine. Lovingly arranged booths spread over \n",
      "various historical squares. The unique backdrop with Hei -\n",
      "delberg Castle towering above the Old Town creates an \n",
      "outstanding atmosphere and makes the Heidelberg Christ -\n",
      "mas Market one of the most fairy tale-like events in Germa -\n",
      "ny. The Advent-themed guided tour provides in  teresting \n",
      "information on the region’s Christmas and pre-Christmas \n",
      "traditions. The walk starts at the Neck  armünzplatz and pas -\n",
      "ses the most beautiful corners of the Old Town ending at \n",
      "the Universitätsplatz (University Square).\n",
      "Price  € 14, € 12 discount price *  \n",
      "Duration  1.5 hours  \n",
      "Times  \n",
      "Advent Saturdays December 2, 9, and 16, 2023  \n",
      "(German: 4:30 pm)  \n",
      "Meeting Point Neckarmünzplatz, in front of the  \n",
      "Tourist Information  \n",
      "Minimum number of participants  5 people\n",
      "Information and booking: +49 6221 5840 - 223/225, guide@heidelberg-marketing.deCastle ticket  \n",
      "including funicular  \n",
      "railway \n",
      "Two Heidelberg attractions go hand in hand: \n",
      "with the castle ticket you have the option of  \n",
      "reaching the romantic Heidelberg Castle com -\n",
      "fortably, eventful and in an environmentally-\n",
      "friendly way by taking the funicular railway. \n",
      "The funicular railway also combines two special \n",
      "features: the lower funicular railway to the  \n",
      "Molkenkur station is the most modern, while \n",
      "the upper one leading to the Königstuhl  \n",
      "(King‘s Seat) is the oldest cableway in Germany. 9 € * \n",
      "per personCastle ticket including funicular railway  | 41\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Gültig ab / Valid from\n",
      "Family\n",
      "Gültig ab / Valid fromYour benefit\n",
      "The castle ticket includes admission to the castle court -\n",
      "yard, the barrel cellar as well as the German Pharmacy  \n",
      "Museum and the fare for the funicular railway trip to the \n",
      "castle with a continuation of the trip to the Molkenkur  \n",
      "station (return trip including one stop)! \n",
      "Price per person*\n",
      "(valid for using the lower railway)\n",
      "€ 9 adult, € 4.50 discount price (pupils and univer  sity \n",
      "students up to 28 years of age and severely disabled \n",
      "people with appropriate ID)\n",
      "You can get the castle ticket including the funicular \n",
      "railway at the following places in Heidelberg\n",
      "– Tourist Information at the Hauptbahnhof (main station) \n",
      "– Tourist Information at the Neckarmünzplatz (Old Town) \n",
      "– Tourist Information in the Rathaus (town hall) at the  \n",
      " Marktplatz (Market Square, Old Town) \n",
      "– Checkout counter at the Kornmarkt and castle funicular  \n",
      " railway station \n",
      "– Heidelberg Castle checkout counter \n",
      "Group order\n",
      "For groups of 15 people or more, you can order castle \n",
      "tickets via our reservation department (dispatch by post \n",
      "within Germany). Please let us know the final number \n",
      "of participants two weeks before traveling (refund for  \n",
      "excess ordered tickets is not possible). \n",
      "Phone +49 6221 58 40-228\n",
      "gruppen@heidelberg-marketing.de\n",
      "Opening hours Heidelberg Castle\n",
      "Castle courtyard, Great Barrel\n",
      "All year: 9:00 am – 6:00 pm, \n",
      "last admission 5:30 pm\n",
      "German Pharmacy Museum\n",
      "April – October: 10:00 am – 6:00 pm, \n",
      "last admission 5:40 pm \n",
      "November – March: 10:00 am – 5:30 pm,\n",
      "last admission 5:10 pm\n",
      " Note  Information are subject to change.HeidelbergCARD\n",
      "The HeidelbergCARD makes your journey even easier.  \n",
      "Benefit from the following included services:\n",
      "– castle ticket including funicular railway \n",
      "– free use of public transport in Heidelberg\n",
      "– combo ticket (one-time free entrance to the\n",
      " University Museum, the Student Prison and the  \n",
      " special exhibition) \n",
      "– numerous discounts on tours, museums, leisure activities,  \n",
      " restaurants and shops. \n",
      "1 day – 24 € \n",
      "Valid from midnight to midnight on the \n",
      "day of validity. \n",
      "2 days – 26 € \n",
      "Valid all day on the first day until midnight \n",
      "the following day.\n",
      "4 days – 28 € \n",
      "Valid all day on the first day until \n",
      "midnight of the fourth day.\n",
      "Family 2 days – 57 € \n",
      "Valid all day on the first day until \n",
      "midnight the following day for a family \n",
      "(2 adults + up to 3 children or 1 adult \n",
      "+ up to 4 children under the age of 16). \n",
      "Funicular railway departure times\n",
      "Annual funicular railway maintenance \n",
      "expected March 6 – March 19, 2023\n",
      "Summer timetable\n",
      "valid from April 1 – November 1, 2023\n",
      "Kornmarkt – Castle\n",
      "9:00 am (every 10 minutes), last trip at 8:00 pm\n",
      "Castle – Kornmarkt\n",
      "9:03 am (every 10 minutes), last trip at 8:03 pm\n",
      "Winter timetable\n",
      "valid from November 2, 2023 – March 31, 2024\n",
      "Kornmarkt – Castle\n",
      "9:00 am (every 10 minutes), last trip at 5:10 pm\n",
      "Castle – Kornmarkt\n",
      "9:03 am (every 10 minutes), last trip at 5:43 pm\n",
      " Note  The lower funicular railway from the Kornmarkt station via the castle to the Molkenkur station is equipped to  \n",
      "facilitate disabled access. Strollers must be carried on the steps. The conductors are happy to assist you. We recommend \n",
      "that families with small children use a buggy. 42 | Events 2023\n",
      "Further events can be found on our website.  \n",
      "Dates and information are subject to change.Heidelberger Frühling  (Heidelberg Spring)\n",
      "March 17 - April 15, 2023\n",
      "One of the largest music festivals in Germany that be -\n",
      "longs to the top league of international festivals for classi -\n",
      "cal music and once again invites visitors to well over 100 \n",
      "events this year.\n",
      "www.heidelberger-fruehling.de/en/\n",
      "Heidelberger Schlossfestspiele\n",
      "(Heidelberg Castle Festival)\n",
      "June 11 - July 30, 2023\n",
      "The one-of-a-kind Renaissance buildings, the sleepy nooks \n",
      "and crannies, as well as the sprawling gardens and parks \n",
      "offer fascinating opportunities for the artistic work of the \n",
      "Heidelberg Theater. Squares and surfaces become stages, \n",
      "walls and corners backdrops.\n",
      "www.schlossfestspiele-heidelberg.de\n",
      "Heidelberg Castle Illuminations \n",
      "with Fireworks\n",
      "June 3, July 8 and September 2, 2023\n",
      "Bengali lights blaze three times every summer on the walls \n",
      "of the ruin in a picturesque and eternal manner. The illumi -\n",
      "nation of the beautiful facade of the castle is com  plemented \n",
      "by festive fireworks which bathe the Old Town in an impres -\n",
      "sive brilliance. \n",
      "www.heidelberg-marketing.com\n",
      "Summer at the River\n",
      "August 19 and 20, 2023\n",
      "Stroll, relax, enjoy the river. The city is moving closer to the \n",
      "water and invites you to linger and stroll along the banks of \n",
      "the Neckar River, with musical entertainment and a varied \n",
      "range of information and gastronomic offerings.\n",
      "www.heidelberg-marketing.comHeidelberger Herbst  (Heidelberg Autumn)\n",
      "September 30 - October 1, 2023\n",
      "One of the biggest Old Town festivals in the region features \n",
      "an artisan market, a giant flea market, numerous regional \n",
      "specialties and many bands providing entertainment in  \n",
      "various different squares in the Old Town.\n",
      "www.heidelberg-marketing.com\n",
      "Heidelberg Wine Village\n",
      "October 2 - 15, 2023\n",
      "Enjoy delicious local and regional wines in the heart of \n",
      "the Old Town. \n",
      "www.heidelberg-marketing.com\n",
      "Enjoy Jazz\n",
      "Beginning of October - Mid November 2023\n",
      "The festival offers diverse events for about seven weeks, with \n",
      "an emphasis not only on jazz, but also on other genres – in \n",
      "exclusive venues, e.g. in Heidelberg.\n",
      "www.enjoyjazz.de/en/\n",
      "Heidelberg Christmas Market\n",
      "November 27 - December 22, 2023\n",
      "Nestled in the Old Town, illuminated by the world-famous \n",
      "castle above, the Heidelberg Christmas Market invites you to \n",
      "take some time out and visit one of its six historic locations.\n",
      "“Winterwäldchen” (Winter Forest)\n",
      "and Heidelberg Ice Rink\n",
      "November 27, 2023 - Beginning of January \n",
      "2024\n",
      "At the Kornmarkt, the Winter Forest beckons visitors with its \n",
      "magical atmosphere, while one of Germany’s prettiest ice \n",
      "rinks offers ice-skating pleasure on the Karlsplatz.\n",
      "www.heidelberg-marketing.comEvents 2023\n",
      "A great program all year round.The German National \n",
      "Garden Show 2023 (BUGA)\n",
      "right here in our backyard!Heidelberg Marketing GmbH\n",
      "www.heidelberg-marketing.com\n",
      "Phone +49 6221 58-44444\n",
      "info@heidelberg-marketing.de\n",
      "  www.facebook.com/Heidelberg\n",
      "  www.instagram.com/Heidelberg4you\n",
      "... start to blossom!\n",
      "Get your BUGA-Bonus!\n",
      "Further events can be found on our website.  \n",
      "Dates and information are subject to change.44 | Good to know\n",
      "Good to know\n",
      "The central location of Heidelberg makes traveling easy with all modes of transport.\n",
      "Rail\n",
      "Heidelberg is well connected to the European long  \n",
      "distance network, as well as to the German Rail Network \n",
      "(ICE and IC / EC, www.bahn.de). Within the region and \n",
      "across its borders, the Rhine-Neckar Trans  port Associa-\n",
      "tion ensures the best connections with the “S-Bahn”  \n",
      "(www.vrn.de).\n",
      "Car / Bus\n",
      "The A5 / A6 highways (Autobahn) access large sections of \n",
      "the entire Rhine-Neckar region with inter-connected exits \n",
      "and connecting federal roads. The A5 / A656 highways have \n",
      "direct exits to Heidelberg.\n",
      "Airports\n",
      "– City Airport Mannheim (approx. 18 km)\n",
      "\t www.flugplatz-mannheim.de/en\n",
      "– Frankfurt Airport (approx. 80 km)\n",
      " www.frankfurt-airport.com/en \n",
      "– Baden-Airpark Flughafen\n",
      " Karlsruhe / Baden-Baden (approx. 90 km)\n",
      " www.baden-airpark.de/en/\n",
      "– Flughafen Stuttgart (approx. 120 km)\n",
      "\t www.flughafen-stuttgart.de\n",
      "– Frankfurt Hahn Airport (approx. 150 km)\n",
      " www.hahn-airport.de/en\n",
      "Environmental zone for cars and buses\n",
      "Heidelberg established an environmental zone in 2010, \n",
      "where only vehicles with certain exhaust standards are  \n",
      "admitted. Vehicles that are used in the environmental \n",
      "zone must have a sticker that marks the pollutant category.  \n",
      "Vehicles with a green pollutant sticker may drive in the en -\n",
      "vironmental zone. \n",
      "You can get more information on the low emission zone at\n",
      "www.heidelberg.de/umweltzoneTLS Shuttle Service Frankfurt Airport\n",
      "Simply and conveniently book your TLS transfer that will \n",
      "pick you up directly at your terminal at Frankfurt Airport \n",
      "and take you to your hotel in Heidelberg! Prices on \n",
      "request. \n",
      "HLS Heidelberg Limousine Service\n",
      "Book with us an exclusive limousine service, VIP first \n",
      "class service, cab service, airport transfer or shuttle \n",
      "service. Our trained chauffeurs will bring you safely to \n",
      "your destination. Price on request.\n",
      "RV / Camping\n",
      "– RV site \n",
      " Harbigweg 1 – 3, 69124 Heidelberg\n",
      " 48 parking spaces\n",
      " open all year round\n",
      " www.wohnmobilstellplatz-heidelberg.com \n",
      "– Camping Heidelberg \n",
      " Schlierbacher Landstrasse 151, 69118 Heidelberg\n",
      " quiet location, right beside the Neckar River\n",
      " open April – October\n",
      " www.camping-heidelberg.de \n",
      "– Camping Haide\n",
      " Ziegelhäuser Landstrasse 91, 69151 Neckargemünd\n",
      " 200 parking spaces, right beside the Neckar River\n",
      " open April – November\n",
      " www.camping-haide.de\n",
      " ”Nette Toilette” (Nice restrooms)\n",
      "You can use a total of 30 “nice restrooms” in the Old Town \n",
      "area without being forced to purchase or consume any -\n",
      "thing. Some restaurants and stores bear the “Nette Toilet -\n",
      "te” symbol on the entrance door.\n",
      "www.heidelberg.deGetting hereGood to know | 45\n",
      "Public passenger transport\n",
      "You can get information on the Heidelberg public trans -\n",
      "portation lines and fares around the clock from the service \n",
      "hotline +49 621 1077077, online at www.vrn.de  or at Rhein-\n",
      "Neckar-Verkehr GmbH’s (RNV) Customer Service Center at \n",
      "the Hauptbahnhof (main station), Kurfürsten-Anlage 62.\n",
      "Bicycle\n",
      "Heidelberg is a very bicycle-friendly town,\n",
      "where you can easily rent a bike:\n",
      " – Radolino bike rental\n",
      " www.radolino.de \n",
      "– radhof BERGHEIM “Bike im Bahnhof“\n",
      " www.fahrrad-heidelberg.de \n",
      "– DB-bike rental system in front of the main station \n",
      " www.callabike.de/en \n",
      "– Pedelec rental (E-bike),\n",
      " several locations\n",
      " www.rueckenwind-hd.org \n",
      "– Swapfiets\n",
      "\t www.swapfiets.de/en\t\n",
      "– Joyrides E-bike rental\n",
      " www.joyrides-rent.de\n",
      "VRNnextbike\n",
      "VRNnextbike has numerous bike stations in the metro -\n",
      "politan area Rhine-Neckar and operates around the \n",
      "clock. The stations are located at busy transport hubs \n",
      "near bus and tram stops. For further information:  \n",
      "Phone: +49 30 69205046, www.vrnnextbike.de/enCarsharing\n",
      "– Stadtmobil Rhein-Neckar\n",
      " Phone +49 621 12855585\n",
      " www.rhein-neckar.stadtmobil.de\n",
      "– eCarsharing Rhein-Neckar\n",
      " Phone +49 6221 3574974\n",
      " www.ecs-rn.de\n",
      "– AnyMove \n",
      " Phone +49 30 83795645\n",
      " www.anymove.app/heidelberg\n",
      "Taxi\n",
      "– Taxizentrale\n",
      " Phone +49 6221 302030\n",
      " www.taxizentrale-heidelberg.de\n",
      "– TaxiHDirekt\n",
      " Phone +49 6221 739090\n",
      " www.taxihdirekt.de\n",
      "e-Scooter\n",
      "At many locations throughout the city of Heidelberg, \n",
      "numerous e-scooters from various providers are avail-\n",
      "able for rent. To rent an e-scooter, register via the app of \n",
      "the respective rental company: \n",
      "– www.bolt.eu\n",
      "– www.li.me\n",
      "– www.tier.app/en\n",
      "– www.zeusscooters.comModes of transportation in Heidelberg\n",
      "Mobile in  \n",
      "HeidelbergRequest Form\n",
      "Please complete this form and send it to\n",
      "Heidelberg Marketing GmbH, Neuenheimer Landstrasse 5, 69120 Heidelberg/Germany\n",
      "Phone: +49 6221 5840-226, Fax: +49 6221 5840-222, reservation@heidelberg-marketing.de\n",
      "Point of contact\n",
      "Last name .....................................................................................\n",
      "First name .....................................................................................Title .................................................................................................Street .............................................................................................Zip code, city ................................................................................Country ..........................................................................................Phone .............................................................................................Fax ...................................................................................................E-Mail ..............................................................................................Dates ..............................................................................................Alternative dates .........................................................................\n",
      "Desired hotel category\n",
      " Comfort   First class\n",
      "Desired number of rooms\n",
      "Double rooms ..............................................................................\n",
      "Single rooms ................................................................................Desired package\n",
      "Lose your heart in Heidelb erg\n",
      "Crisscross through Heid elberg\n",
      "Heidelberg Castle Illumi nation\n",
      "BUGA flower pot (German Na tional Garden Show) \n",
      "Summer Special – stay 3 nig hts, pay for 2\n",
      "City & Quest – Heidelberg ‘s interactive quiz \n",
      "AHOY - welcome aboard \n",
      "for a culinary delight\n",
      "Delectable Heidelberg\n",
      "Strolling around Heidelbe rg\n",
      "Winter Dream – stay 3 nigh ts, pay for 2\n",
      "Advent experience\n",
      "Theater and Orchestra H eidelberg Heidelberger \n",
      "Schlossfestspiele \n",
      "(Heidelberg Castle Festival)\n",
      "„Heidelberger Frühling“ \n",
      "(Heidelberg Spring) Music  festival\n",
      "BODY WORLDS Museum (KÖRPERWELTEN) \n",
      "“The Anatomy of Happines s”\n",
      "Hotel stay without package\n",
      "Other comments / requests\n",
      "..........................................................................................................\n",
      "....................................................................................................................................................................................................................\n",
      "Place / Date ...................................................................................Signature .......................................................................................46 | Request Form\n",
      " Note  I  c o n fi r m  t h a t  I  h a v e  r e a d  t h e  f o r m  w i t h  t r a v e l e r ‘ s  i n f o r m a t i o n  f o r  a  p a c k a g e  t r i p  ( i n  a c c o r d a n c e  w i t h  §  6 5 1 a  G e r m a n  C i v i l  C o d e )  b e f o r e  b o o k i n g  a n d  \n",
      "acknowledge it.\n",
      " Note  Please note that you have no statutory withdrawal right from contracts on domestic tourism sights, in particular contracts on accommodations, \n",
      "guest tours and cultural events, after conclusion of the contract according to the law, but only a cancellation right – usually subject to fees – according to \n",
      "the agreed terms and conditions or the statutory provisions.\n",
      " Note  Please note that the detailed data protection provisions of Heidelberg Marketing GmbH can be found on our website https://www.heidelberg-marketing.de/en/  \n",
      "about-us/privacy-statement. With your signature, you confirm that you have understood and accepted the data protection provisions.Form sheet for information of the traveler  \n",
      "on a package tour  \n",
      "according to § 651a of the German Civil Code\n",
      "The most important rights according to policy (EU) 2015 / 2302\n",
      "– Travelers receive all essential information concerning the package tour before the conclusion of the package tour contract.  \n",
      "– At least one entrepreneur at a time shall be liable for proper rendering of all travel services included in a contract.  \n",
      "– The travelers will receive an emergency phone number or information on a point of contact through which they can contact the tour  \n",
      " operator or the travel agency.\n",
      "– The travelers can transfer the package tour to another person – within an appropriate period of time and potentially subject to  \n",
      " additional fees.\n",
      "– The price for the package tour must only be increased if certain costs (e.g. fuel prices) increase and if this is expressly stipulated in  \n",
      " the contract, and in any case no later than 20 days before the start of the package tour. If the price increase exceeds 8 % of the price  \n",
      " for the package tour, the traveler may withdraw from the contract. If a tour operator reserves the right to increase the price, the  \n",
      " traveler shall have the right to reduce the price if the corresponding costs reduce.\n",
      "– The travelers may withdraw from the contract without paying a revocation fee and shall be reimbursed in full for all payments if one  \n",
      " of the essential parts of the package tour, except for the price, is changed considerably. If the entrepreneur responsible for the  \n",
      " package tour cancels the package tour before it commences, the travelers shall have a claim to reimbursement for costs and,  \n",
      " potentially, compensation.\n",
      "– The travelers may withdraw from the contract without incurring any revocation fee if any extraordinary circumstances occur before  \n",
      " commencement of the package tour, e. g. if there are any security risks at the destination that are expected to impair the package  \n",
      " tour.\n",
      "– The travelers may also withdraw at any time before commencement of the package tour against payment of an appropriate and  \n",
      " reasonable revocation fee.\n",
      "– If essential parts of the package tour cannot be performed according to the agreement after commencement of the package tour,  \n",
      " the traveler shall be offered appropriate other provisions without additional costs. The traveler may withdraw from the contract  \n",
      " without paying any revocation fee (in the Federal Republic of Germany, this right is called “termination”), if services are not rendered  \n",
      " according to the contract and this has essential effects on rendering of the contractual package tour services and the tour operator  \n",
      " does not remedy this.\n",
      "– The traveler shall have a claim to price reduction and / or damages if the travel services are not rendered or not rendered properly.\n",
      "– The tour operator shall support the traveler if he/she is in trouble.\n",
      "– In case of insolvency of the tour operator or – in some Member States – the travel agent, payments shall be reimbursed. If the tour  \n",
      " operator or, if relevant, the travel agent, becomes insolvent after commencement of the package tour and if transport is part of  \n",
      " the package tour, return transport of the travelers is ensured. Heidelberg Marketing GmbH has taken out insolvency insurance  \n",
      " with R+V Allgemeine Versicherung AG. The travelers may contact R+V Allgemeine Versicherung AG (R+V Allgemeine Versicherung  \n",
      " AG, Abt. Kredit-Schaden, Raiffeisenplatz 1, 65189 Wiesbaden, Phone +49 611 5335859, Fax +49 611 5334500, Email: info@ruv.de,  \n",
      " www.ruv.de) if they are denied any services by Heidelberg Marketing GmbH due to insolvency.\n",
      "Website on which the policy (EU) 2015 / 2302 in the form transposed into national law can be found:\n",
      "www.umsetzung-richtlinie-eu2015-2302.de.The combination of travel services offered to you is a package tour within\n",
      "the meaning of directive (EU) 2015/2302.\n",
      "Therefore, you may claim all EU rights that apply to package tours. The\n",
      "company Heidelberg Marketing GmbH, Neuenheimer Landstrasse 5, D-69120 Heidelberg bears full responsibility for proper execution of the\n",
      "entire package tour. The company Heidelberg Marketing GmbH also has\n",
      "the legally required security for repayment of your claims and, if transport\n",
      "is included in the package tour, to ensure your return transport in case \n",
      "of its insolvency.Form sheet  | 47RIVERBOAT\n",
      "HEIDELBERGBoat trips on historic sloops\n",
      "Exclusive. Romantic. Classy.\n",
      "Trips starting from €249\n",
      "+49 172 - 57 38 08 9 |  info@riverboat-heidelberg.de\n",
      "WWW.RIVERBOAT-HEIDELBERG.DE |       riverboat.heidelbergwww.maerchenparadies.de\n",
      "Outdoor -Fun for Kids, \n",
      "Families and Friends\n",
      "MAR - NOV daily 10 a.m. till 6 p.m., Sundays and holidays until 7 p.m. \n",
      "Königstuhl 5, 69117 Heidelber g, free parking\n",
      "Take the funicular to the\n",
      "most famous ruin in the world.\n",
      "More at bergbahn-heidelberg.deJust get in and enjoy the\n",
      "magni/f_i  cent view. \n",
      "From Kornmarkt to Molkenkur and back.\n",
      "Includes entry to the Castle courtyard,\n",
      "the wine cellar and the German Pharmacy \n",
      "Museum.\n",
      "9 Euro incl. admission\n",
      "to the Castle\n",
      "courtyardCASTLE TICKET\n",
      "historicgermany.com\n",
      "  Crowned by a Castle  \n",
      "The romantic charm of Heidelberg, an ancient city along the Neckar River, \n",
      "lies in the flower filled balconies, bronze fountains and cozy cafés, where people \n",
      "exchange chocolate treats known as kisses.\n",
      "Heidelberg Castle, perched on a granite rock, rises high above the bumpy stone \n",
      "streets of the Old Town. The harmonic mixture between old and new, past and \n",
      "future is evident everywhere. History is what makes a destination so interesting. \n",
      "Each place has its own fascinating past. If you are an absolute lover of history \n",
      "and flair then you�ll adore the Historic Highlights of Germany cities.\n",
      "Great cities for History Buffs: \n",
      "www.historicgermany.com  Crowned by a Castle\n",
      "Photo credits: Heidelberg Marketing/Tobias Schwerdt historicgermany historicgermany\n",
      "Rostock\n",
      "Lübeck\n",
      "PotsdamOsnabrück\n",
      "Münster\n",
      "ErfurtAachen\n",
      "Bonn\n",
      "Koblenz\n",
      "Wiesbaden\n",
      "TrierWürzburg\n",
      "Heidelberg\n",
      "FreiburgRegensburg\n",
      "AugsburgTübingen\n",
      "HHoG_AD_HD2018_190x136_1008.indd   3 10.08.18   19:44experience a journey\n",
      "back in time\n",
      "Nuremberg Bayreuth Coburg Rothenburg o.d.T. Neckargemünd Schwetzingen\n",
      "Cobur g\n",
      "Schwetzingen\n",
      "AbenbergLichtenau\n",
      "Wolframs-\n",
      "EschenbachRothNurembergForchheimEbermannstadtAufseß\n",
      "HeiligenstadtMemmelsdorf\n",
      "Bamber g\n",
      "Pottenstein\n",
      "GößweinsteinWaischenfeld\n",
      "Muggen-\n",
      "dorfKulmbac h\n",
      "Bayreuth\n",
      "Rabenstein Castle\n",
      "Eglof fsteinKronac h\n",
      "Mannheim\n",
      "  Neckar -\n",
      "steinach NeckargemündHirsch-\n",
      "horn\n",
      "WeinsbergBad WimpfenGuttenberg \n",
      "Castle\n",
      "Heilbronn\n",
      "Schwäbisc h HallWalden-\n",
      "burg ÖhringenRothenbur g \n",
      "ob der Tauber\n",
      "ColmbergAnsbac hSeßlach\n",
      "Heidel-\n",
      "berg\n",
      "SinsheimNeckar -\n",
      "bischofsheimEbernPfarrweisachMaroldsweisach\n",
      "Rentweinsdorf\n",
      "SteinCadolzburgLangenzenn\n",
      "EberbachStreitberg\n",
      "Mosbach\n",
      "Hotel-Restaurant Hornberg CastleHeldburg\n",
      "Lauf a.d.Pegnitz\n",
      "Gundelsheim/Schlosshotel Horneck\n",
      "Kirchberg an der Ja gst\n",
      "Biohotel Schloss Kirchberg\n",
      "Die Bur  gen straße e.V. \n",
      "Allee 1 2 · 74072 Heil bronn  \n",
      "Phone +49 (0) 71 3 1 9 73501 -0  \n",
      "info@burgenstrasse.de \n",
      "www.castleroad.deeidelberg is situated along on the \n",
      "Castle Road which runs through \n",
      "southern Germany from Mannheim to Bayreuth. \n",
      "With idyllic landscapes, about 70 castles and \n",
      "palaces as well as countless other places of interest, \n",
      "the holiday route is the ideal destination for anyone \n",
      "who want to immerse themselves in bygone times.\n",
      "GERMANY\n",
      "MunichCologneBerlinHamburg\n",
      "Frankfurt a.M.\n",
      "MANNHEIMNUREMBERGBAYREUTH\n",
      "HEIDELBERGHere Germany begins \n",
      "being Italy„„\n",
      "Emperor Joseph II \n",
      "on his way to FrancfortTourismus Service \n",
      "Bergstrasse e.V .\n",
      "Tel +49 62 52 13 11 70www.diebergstrasse.de  |   www.land-des-roten-rieslings.de\n",
      "active & nature  |  experience & enjoyment  |  cities & culture\n",
      "©Hessen-Agentur BlofieldEXPERIENCE  BERGSTRASSE \n",
      "The holiday Route Bergstrasse with its connected hiking- and biking trails and its milde \n",
      "climate is situated between the rivers Rhein, Main and Neckar. More than 30 castles and \n",
      "palaces aswell as the UNESCO World Heritage Abbey of Lorsch invite for a stay.\n",
      "The fanciers of excellent wine and food won‘t miss anything. The destination between the \n",
      "cities Darmstadt and Heidelberg has a broad cultural choice. The wine growing-growing \n",
      "district Bergstrasse has much to experience – such as the rarity Red Riesling.… discover the town and beautiful\n",
      "landscape alongside the river Neckar\n",
      "and the Bergstraße in a completely\n",
      "new and innovative way.\n",
      "For more information: \n",
      "www.stadtsafari.com\n",
      "info@stadtsafari.com\n",
      "Fon 0621.43 715 418Discover Heidelberg\n",
      "with your Segway!\n",
      " Airport / Health / Wedding / Limousine & Sprinter\n",
      "Be it in an elegant limousine, a comfortable Sprinter or a classic  \n",
      "Mercedes-Benz vintage car: TLS is your trusted automotive partner for all \n",
      "travel connections to Heidelberg and in the Rhine-Neckar region. Our health \n",
      "shuttles are happy to drive patients with and without medical equipment to \n",
      "clinical treatments and check-up appointments. Barrier-free and e-wheelchair \n",
      "accessible rides can be provided on request. \n",
      "More information about TLS Heidelberg is available \n",
      "online at: www.tls-heidelberg.de  \n",
      "or by phone: +49 6221 770077\n",
      "We look forward to meeting you!TLS – Travel well with usHeidelberger\n",
      "Frühling\n",
      "Musikfestival\n",
      "March 17\n",
      "— April 15\n",
      "2023\n",
      "heidelberger-fruehling.de\n",
      "Heidelberg Marketing.indd   2Heidelberg Marketing.indd   2 20.09.2022   16:42:4620.09.2022   16:42:46MUSEUM  HEIDELBERG\n",
      "DAILY  \n",
      "10 AM – 6 PMWhat the BODY &\n",
      " \n",
      "are all aboutHeidelberger\n",
      "Frühling\n",
      "Musikfestival\n",
      "March 17\n",
      "— April 15\n",
      "2023\n",
      "heidelberger-fruehling.de\n",
      "Heidelberg Marketing.indd   2Heidelberg Marketing.indd   2 20.09.2022   16:42:4620.09.2022   16:42:46\n",
      "Two ideal destinations!\n",
      "technik-museum.de\n",
      "www.heidelberg.de/wlanFree Wi-Fi\n",
      "Heidelberg4You\n",
      "1. Choose network\n",
      "Heidelberg4you\n",
      "2. Connect\n",
      "Accept terms and conditions\n",
      "3. Let‘s go\n",
      "Join more than 200 hotspotsOpening \n",
      "2023: \n",
      "AlreA dy \n",
      "bookA ble!\n",
      "The North German-based ATLAn TiC Hotels  Group offers exceptional  \n",
      "4- to 5-star comfort and individual full service hotels – and they are \n",
      "coming to Heidelberg soon!\n",
      "right next to the new Heidelberg Congress Center and the railway station, \n",
      "the ATLA nTiC Hotel Heidelberg, opening mid-2023, will be providing  \n",
      "310 stylish rooms and suites, a spa and wellness area, a restaurant, and \n",
      "a rooftop bar with a terrace overlooking Heidelberg and near surroundings. \n",
      "State-of-the-art function rooms and a 350 sqm ballroom with flexible \n",
      "partition  walls located on the 13th floor complete the offer. \n",
      "reservations and general enquiries: heidelberg@atlantic-hotels.de\n",
      " Find out more about our 19 locations:\n",
      " atlantic-hotels.de/en\n",
      "ATlANTIC Hotels Management GmbH\n",
      "ludwig-r oselius-Allee 2, 28329 b remen, Germany\n",
      "“Heidelberg Trio“\n",
      "1 bottle of Riesling (Winery Clauer)\n",
      "1 bottle of Pinot Noir (Winery Hans Winter)\n",
      "1 bottle of Pinot Noir Rosé (Winery Bauer)\n",
      "Available at the Tourist Information \n",
      "Neckarmünzplatz.Heidelberg Marketing GmbH\n",
      "www.heidelberg-marketing.com\n",
      "  www.facebook.com/Heidelberg\n",
      "  www.instagram.com/Heidelberg4you\n",
      "special p rice\n",
      "€ 19.99\n",
      " instead o f\n",
      "€ 24.99 \n",
      "  \n",
      "General Terms and Conditions  \n",
      "Heidelberg Marketing GmbH, Package Travel \n",
      "Terms and Conditions  \n",
      " \n",
      "Dear guests,  \n",
      "We ask you to carefully read  the following Package Travel Terms and Conditions. To the \n",
      "extent effectively agreed, these Terms and Conditions become part of the contract for \n",
      "package travel concluded between the customer /  traveler (hereinafter referred to as \n",
      "the “Traveler“) and Heidel berg Marketing GmbH (hereinafter referred to as “HDM“). \n",
      "They supplement the statutory provisions contained in section 651a - y of the German \n",
      "Civil Code (Bürgerliches Gesetzbuch – “BGB“) and Articles 250 and 252 of the \n",
      "Introductory Act to the German Civil C ode (Einführungsgesetz zum BGB – “EGBGB“) and \n",
      "expand upon them. These Terms and Conditions apply exclusively to package \n",
      "travel arrangements from HDM. They do not apply to package travel \n",
      "arrangements for groups of 15 persons or more, to the arrangement of t hird -\n",
      "party services (e.g. guided tours and tickets), nor to contracts for \n",
      "accommodation services or making arrangements for such contracts.  \n",
      " \n",
      "1. Conclusion of the travel contract; obligations of the Traveler  \n",
      " \n",
      "1.1 The following applies to all booking channels:  \n",
      "a) The trip description and supplemental information provided by HDM for the \n",
      "respective trip, to the extent available to the Traveler at the time of booking, comprise \n",
      "the basis for an offer by HDM and the boo king made by the Traveler.  \n",
      "b) Travel agents and booking offices are not authorized by HDM to make agreements, \n",
      "to provide information, or make warranties that amend the agreed contents of the travel \n",
      "contract, extend beyond, or conflict with what is included  in the trip description and  / \n",
      "or services contractually agreed by HDM.  \n",
      "c) Information contained in hotel guides and similar listings that are not published by \n",
      "HDM is not binding upon HDM and its duty of performance to the extent not included \n",
      "within HDM‘s duty of performance by express agreement with the Traveler.  \n",
      "d) If the contents of the travel confirmation from HDM deviate from the contents of the \n",
      "booking, this comprises a new offer from HDM which is binding upon it for a period of \n",
      "ten days. A contract i s concluded on the basis of this new offer in the event that HDM \n",
      "has provided notice of the change in regard to the new offer, has satisfied its pre -\n",
      "contractual information obligations and the Traveler accepts such offer during the ten -\n",
      "day period referred to above by express statement to the travel agency or by making \n",
      "advance payment.  \n",
      "e) The pre -contractual information provided by the travel agent with regard to the \n",
      "essential features of the trip, the trip price and all additional costs, payment \n",
      "arrangements, minimum num ber of participants, and cancel ation fees (pursuant to \n",
      "Article 250 se ction 3 nos. 1, 3, to 5 and 7 EGBGB) do not become part of the package \n",
      "travel contract only if expressly agreed between the parties.  \n",
      "f) The Traveler is liable for all contractual obligations of other travelers for whom the \n",
      "Traveler makes a booking to the e xtent of the Traveler‘s own liability, provided the \n",
      "Traveler has made such an undertaking by express and separate agreement.  \n",
      "1.2 The following applies to bookings made verbally, by telephone, in writing, by email, \n",
      "or fax:  \n",
      "a) When making a booking, the Trav eler is making a binding offer to conclude a package \n",
      "travel contract with HDM. The Traveler is bound by the booking for three business days.  \n",
      "b) The contract is concluded upon receipt of the trip confirmation (acceptance notice) \n",
      "from HDM. Upon or immediatel y after conclusion of the contract, HDM will provide the \n",
      "Traveler with a travel confirmation  in compliance with legal requirements  on a durable \n",
      "medium (which permits the Traveler to save or store the confirmation unedited such \n",
      "that it will be accessible to  the Traveler for a reasonable period, e.g. on paper or by \n",
      "email), provided that the Traveler does not have a right to a trip confirmation in paper \n",
      "form pursuant to Article 250 section 6 subsection (1) second sentence EGBGB whilst the \n",
      "contract was conclude d in the simultaneous physical presence of both parties or outside \n",
      "of the business premises.  \n",
      "1.3 HDM notes that there is no right to cance lation under applicable law (section 312 \n",
      "subsection (7), section 312g subsection (2) first sentence no. 9 BGB) in the case of \n",
      "package travel contracts under sections 651a and 651c BGB concluded via distance sales \n",
      "(letters, catalogues, telephone calls, fax, email, or messages (SMS) sent via mobile \n",
      "network as well as radio, telemedia, and online services) but rather only th e statutory \n",
      "revocation and termination rights, in particular revocation pursuant to section 651h BGB \n",
      "(see also section 3) are availa ble. However, there is a cancel ation right if the contract for \n",
      "travel services under section 651a BGB is concluded outside o f business premises, \n",
      "unless the verbal negotiations upon which the formation of the contract is based were \n",
      "conducted on the basis of a preceding order by a consumer; there is like wise no right of \n",
      "cance lation in the latter case.  \n",
      " \n",
      " \n",
      " \n",
      " 2. Payment  \n",
      " \n",
      "2.1 HDM and the travel agency may only request or accept payments for the trip prior \n",
      "to the end of the package travel if an effective customer funds insurance contract is in \n",
      "place and the Traveler is provided a risk coverage certificate with the name and cont act \n",
      "information of the customer funds insurer in clear, understandable and highlighted \n",
      "fashion. Following conclusion of the contract, the trip price is due for payment four \n",
      "weeks prior to the commencement of travel in exchange for provision of the risk \n",
      "coverage certificate, provided the booking confirmation  / invoice does not reflect any \n",
      "other agreement. The entire trip price is immediately due for payment in the case of \n",
      "bookings made less than four weeks prior to the commencement of travel.  \n",
      "2.2 The provisi ons of section 2.1 notwithstanding, a risk coverage certificate needs not \n",
      "be provided as a prerequisite for payment falling due if the package travel offer does \n",
      "not include transportation to and  / or from the location at which the package travel \n",
      "services w ill be provided and, contrary to section 3.1, it has been agreed and noted in \n",
      "the trip confirmation, that the entire trip price is due for payment at the end of the \n",
      "package travel upon the end of travel without prior advance payment.  \n",
      "2.3 If the Traveler do es not make an advance payment and / or the final payment in \n",
      "accordance with the agreed payment terms, even though HDM is ready and able to \n",
      "provide the contractually agreed services, has satisfied its statutory information \n",
      "obligations, and the Traveler has  no statutory or contractual right of offsetting or \n",
      "retention and if the Traveler is responsible for the default of payment, HDM is entitled \n",
      "to revoke the package travel contract following a warning and grant of an appropriate \n",
      "grace period and the expirati on of this period , and charge the Traveler cance lation fees \n",
      "in accordance with section 3, unless the Traveler has a set -off or retention right at the \n",
      "time the payment falls due, or the Traveler is not at fault for the payment delay.  \n",
      " \n",
      "3. Cancel ation by the Traveler; rebooking  \n",
      " \n",
      "3.1 The Traveler may cancel the package travel contract at an y time before departure. \n",
      "Cancel ation must be communicated to HDM at the address set out below; if the trip has \n",
      "been booked via a travel agency, notice of cancel ation may also  be provided to the travel \n",
      "agency. It is advisable for the custo mer to provide notice of cancel ation in writing.  \n",
      "3.2 If the customer cancels prior to the commencement of travel or fails to begin travel, \n",
      "the tour operator loses the right to receive the trip  price. Instead, the tour operator may \n",
      "demand an appropriate compensation as far as i t is not at fault for the cance lation. HDM \n",
      "may not demand compensation if unavoidable, exceptional events that significantly \n",
      "impair the ability to provide the package trip or to transport persons to the destination \n",
      "occur at the or in its immediate vicinity; circumstances are deemed unavoidable and \n",
      "exceptional if they are not subject to the c ontrol of the party invoking such \n",
      "circumstances  and their results could not have been prevented even if all reasonable \n",
      "precautions had been taken.  \n",
      "3.3 HDM has defined the following fixed compensation levels under consideration of \n",
      "the period between notice of cance lation and the commencement of travel as well as \n",
      "under consideration of expected savings and the expected profits that may be earned \n",
      "by other use of the travel services. Compensation is comput ed based on the relevant \n",
      "cancel ation tier based on the t ime at which notice of the cancel ation is received:  \n",
      "a) Between the 27th and the 21st day prior to the commencement of travel, 20 % of the \n",
      "trip price  \n",
      "b) Between the 20th and 12th day prior to the commencement of travel, 40 % of the trip \n",
      "price  \n",
      "c) Between the 11th and the 3rd day prior to the commencement of travel, 60 % of the \n",
      "trip price  \n",
      "d) From the 2nd day prior to the commencement of travel and in the case of a no -show, \n",
      "90 % of the trip price.  \n",
      "3.4 We strongly encourage the purchase of a travel c ancel ation insurance as well as an \n",
      "insurance to cover return related expenses in the event of an accident or illness.  \n",
      "3.5 In any event, the Traveler is free to prove to HDM that HDM incurred no damages at \n",
      "all or damages that are significa ntly less than the  fixed cancel ation fees demanded by \n",
      "HDM.  \n",
      "3.6 A fixed compensation fee in accordance with section 3.3 shall not be deemed to have \n",
      "been fixed or agreed upon  to the extent HDM proves that HDM has incurred expenses \n",
      "that are significantly in excess of the applicable fixed compensation fee under section \n",
      "3.3. In such cases, HDM is obliged to specifically quantify and substantiate the amount \n",
      "of compensation demanded subject to consideration of saved expenses and the \n",
      "purchase of any other use of the travel serv ices.  \n",
      "3.7 If the tour operator is obliged to refund  the trip price following cance lation, section \n",
      "651h (2) BGB  shall remain unaffected . \n",
      "3.8 The foregoing is without prejudice to the customer‘s statutory right to demand that \n",
      "a third party take over the rights and duties under the package travel contract in lieu of General Terms and Conditions | 5758 | General Terms and Conditions\n",
      " \n",
      "  \n",
      "the Traveler pursuant to section 651e BGB by providing notice to the tour operato r on a \n",
      "durable medium. Such a declaration is timely in any event if received by the tour \n",
      "operator 7 days before the commencement of travel.  \n",
      "3.9 If any changes are made with regard to travel dates, accommodations, meal \n",
      "arrangements, or other services (booki ng changes) at the request of the Traveler after \n",
      "the conclusion of the contract, HDM may, up to the 31st day prior to the commencement \n",
      "of travel, impose a fee of € 15 without the Traveler‘s having a legal right to any such \n",
      "booking changes and only to the e xtent such changes are possible. Later booking \n",
      "changes are only possible subject to termination of the travel contract and rebooking in \n",
      "acco rdance with the terms of cancel ation set out above. The foregoing does not apply \n",
      "to requests for booking changes tha t result in only minor expenses, or if booking \n",
      "changes are necessary because HDM provided the Traveler no, insufficient or incorrect \n",
      "pre-contractual information as required under Article 250 section 3 EGBGB.  \n",
      " \n",
      "4. Duties of the Traveler  \n",
      " \n",
      "4.1 Travel documents : The customer is required to notify HDM or the travel agent from \n",
      "whom he booked the package travel if he does not receive the required travel \n",
      "documents (e.g. hotel or other vouchers) by the deadline indicated by HDM.  \n",
      "4.2 Notice of defects / demand for rel ief: \n",
      "a) The Traveler may demand relief if the package travel is not provided free of defects.  \n",
      "b) In the event that HDM could not provide relief due to a failure to provide notice of \n",
      "the defect for which the Traveler was at fault, the Traveler may not deman d a reduction \n",
      "in price under section 651m BGB or compensation for damages under section 651n BGB.  \n",
      "c) The Traveler is obliged to provide notice of defects immediately to HDM‘s local \n",
      "representative. If a local representative of HDM is neither available, nor contractually \n",
      "required, any defects in the package travel are to be reported to HDM at the contact \n",
      "office indicated by HDM. The booking confirmation will provide information regarding \n",
      "the availability of a local representative of HDM and  / or its contact office. However, the \n",
      "Traveler may also notify the travel agent from whom he booked the package travel of \n",
      "the defect.  \n",
      "d) The representative of HDM is commissioned to provide relief to the extent possible. \n",
      "However, such representative is not authorized to re cognize any claims.  \n",
      "4.3 Deadlines prior to termination: If the Traveler desires to terminate a package travel \n",
      "contract pursuant to section 651l BGB due to a major defect in the travel package of the \n",
      "type described in section 651i subsection (2) BGB, the Tr aveler is first required to \n",
      "provide HDM a reasonable period to provide relief. The foregoing does not apply only \n",
      "in cases where HDM refuses to provide relief or if immediate relief is required.  \n",
      " \n",
      " \n",
      "5. Limitation of liability  \n",
      " \n",
      "5.1 Contractual liability for da mages on the part of HDM that do not result from injury \n",
      "to life, limb or health, and are not the result of fault on the part of HDM, is limited to \n",
      "three times the trip price.  \n",
      "5.2 HDM is not liable for interruptions in performance, personal and material dam ages \n",
      "related to services t hat are only provided as third -party services (e.g. excursions offered \n",
      "by third parties, sporting events, thea ter visits, or exhibitions) if these services are \n",
      "explicitly labeled in the travel description and booking confirmation,  are clearly labeled \n",
      "as third -party services, including the ide ntity and address of the third -party contract \n",
      "partner, such that it is apparent to the Traveler that they are not part of the HDM \n",
      "package travel and may be selected separately. This is without prejudice to sections \n",
      "651b, 651c, 651w and 651y BGB.  \n",
      "5.3 Nonetheless, HDM is liable if, and to the extent that, damages result from a violation \n",
      "of notice, information or organizational duties on the part of HDM.  \n",
      " \n",
      " 6. Unused services  \n",
      " \n",
      "The Traveler has no right to a proportionate refund if the Traveler does not make use of \n",
      "specific elements of the package travel due to premature return related to an illness or \n",
      "other reasons for which HDM is not at fault. However, to the extent that very small sums \n",
      "are not involved, HDM will endeavour to obtain a refund from the service provider and \n",
      "repay the corresponding amounts to the Traveler as soon as, and to the extent that, such \n",
      "amounts are actually refunded to HDM from the individual service provi ders.  \n",
      " \n",
      "7.  Special regulations in connection with pandemics (in particular the Corona \n",
      "virus)  \n",
      " \n",
      "7.1 The parties agree that the agreed travel services shall always be provided by the \n",
      "respective service providers in compliance with and in accordance with the o fficial \n",
      "requirements and conditions applicable at the time of travel.  \n",
      "7.2 The Traveler agrees to comply with reasonable regulations or restrictions on use of \n",
      "the service providers when using travel services and to notify the tour guide and the \n",
      "service prov ider immediately in the event of typical symptoms of illness.  \n",
      " \n",
      "8. Choice of law and jurisdiction; information regarding consumer dispute \n",
      "settlement  \n",
      " \n",
      "8.1 In relation to Travelers who are not citizens of a member state of the European Union \n",
      "or Switzerland, t he parties agree to the exclusive application of German law to the either \n",
      "legal and contractual relationship between HDM and the Traveler. Such Travelers may \n",
      "only lodge suit against HDM at its place of domicile.  \n",
      "8.2 In the case of lawsuits lodged by HDM ag ainst Travelers and / or contractual \n",
      "partners to the package travel contract who are merchants, legal persons under public \n",
      "or private law and who maintain their residence or habitual place of abode outside of \n",
      "Germany, or whose residence or habitual place o f abode is unknown at the time a \n",
      "lawsuit is lodged, the location of HDM‘s domicile is the agreed place of jurisdiction.  \n",
      "8.3 With reference to the Act on Consumer Dispute Resolution (Gesetz über \n",
      "Verbraucherstreitbeilegung), HDM indicates that it will not pa rticipate in voluntary \n",
      "consumer dispute resolution. HDM will provide appropriate notice to the consumer \n",
      "should consumer dispute resolution become mandatory for HDM after these Terms and \n",
      "Conditions have been printed. HDM notes the European online dispute re solution \n",
      "platform  https://ec.europa.eu/consumers/odr/  with regard to all travel contracts \n",
      "concluded electronically.  \n",
      " \n",
      " \n",
      " \n",
      "© Copyright protection.  \n",
      "Noll | Hütten | Dukic Rechtsanwälte, Stuttgart | München, 2017 – 2023  \n",
      " \n",
      "Tourism agency:  \n",
      "Heidelberg Marketing GmbH  \n",
      "Managing director: Mathias Schiemer  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg, Germany  \n",
      " \n",
      "Phone:  +49 6221 5840  - 200 \n",
      "Telefax:  +49 6221 5840  - 222 \n",
      "info@heidelberg -marketing.de  \n",
      " \n",
      "Commercial register number: HRB 337405  \n",
      "Register court: AG Mannheim  \n",
      "VAT ID: DE226325597  \n",
      " \n",
      " \n",
      "   \n",
      "  \n",
      "General Terms and Conditions  \n",
      "Terms and Conditions for Guest  \n",
      "Accommodations and Agency Services  \n",
      " \n",
      "Dear guests,  \n",
      "Heidelberg Marketing GmbH, referred to hereinafter as “HDM,“ arranges \n",
      "accommodations at lodging establishments and from private renters, referred to \n",
      "hereinafter as “Hosts“, in Heidelberg and the vicinity based on current availability. To \n",
      "the extent validly agreed, the following Terms and Conditions shall become part of the \n",
      "lodging agreement concluded between the guest and the Host in the event of a booking, \n",
      "and provide terms that supplement the statutory rules applicable to the contractual \n",
      "relationship between the guest and the Host and to the contractual relationship \n",
      "between  the Host and HDM related to placement services. Accordingly, we request that \n",
      "you read these Terms and Conditions carefully.  \n",
      " \n",
      "1. Status of HDM  \n",
      " \n",
      "1.1. HDM is the operator of the respective websites and  / or publisher of the respective \n",
      "directory of accommodat ions, catalogues, flyers or other printed media and websites to \n",
      "the extent it is expressly listed as publisher / operator in such media.  \n",
      "1.2. To the extent that HDM arranges further services provided by the hosts, which are \n",
      "not a material part of the overall value of the hosts’ services, and represent neither a \n",
      "material feature of such combination of services by the host nor of HDM itself nor have \n",
      "been advertised as such, HDM is deemed merely to be an agent arranging \n",
      "acco mmodation services.  \n",
      "1.3. As an agent, HDM is deemed to be the provider of related travel services provided \n",
      "that the requirements for offering related travel services have been satisfied pursuant \n",
      "to the provisions of section 651w BGB.  \n",
      "1.4. Without prejudice  to the obligations of HDM as the provider of related travel \n",
      "services (in particular providing the legally required information sheet and obtaining a \n",
      "guarantee for customer funds in the event of collection activities by HDM) and the legal \n",
      "consequences of t he failure to comply with such statutory obligations, HDM is neither \n",
      "the tour operator nor a party of the contract with regard to any accommodation contract \n",
      "formed as the result of a booking provided that the requirements of 1.2 or 1.3 have been \n",
      "satisfied.  Accordingly, it is not liable for information provided by the host regarding \n",
      "prices and services, the provision of services itself as well as for any associated defects.  \n",
      " \n",
      "2. Contract formation  \n",
      " \n",
      "2.1 The following applies to all booking types:  \n",
      "a) The base s for the Host‘s offer and the g uest‘s booking consist of the description of \n",
      "the accommodations and supplemental information as contained in other materials on \n",
      "which the booking was made (e.g., description of the city, explanation of classification, \n",
      "etc.) t o the extent available to the guest at the time of booking.  \n",
      "b) HDM expressly notes that, in accordance with the statutory provisions (section 312g \n",
      "paragraph 2 sentence 1 n o. 9 of the German Civil Code [Bürgerliches Gesetzbuch – \n",
      "BGB]), there is no right of withdrawal in the case of contracts for lodging services that \n",
      "were concluded via distance selling (letters, catalogues, telephone calls, facsimile, \n",
      "emails, via messages sent on a cellular network [SMS] as well as via radio and \n",
      "telemedia). In such cases, th e statutory provisions applica ble to unused rental premises \n",
      "(section 537 BGB) (see also section 6 of these Terms and Conditions for Guest \n",
      "Accommodations) apply exclusively. However, there is a right of withdrawal if the \n",
      "lodging agreement was concluded off -premises.  \n",
      "2.2 The following applies to bookings made verbally, by telephone, in writing, by email \n",
      "or by fax:  \n",
      "a) By making a booking, the guest makes a binding offer to conclude a lodging \n",
      "agreement with the Host.  \n",
      "b) The contract is formed upon receipt of th e booking confirmation from the Host by the \n",
      "guest. There is no form requirement applicable to the booking confirmation such that \n",
      "confirmations made verbally or by telephone are binding for the guest. Generally, the \n",
      "Host or HDM sends an additional, written booking confirmation to the guest. However, \n",
      "bookings made by a guest verbally or by telephone shall also result in a binding contract \n",
      "if confirmed verbally or by telephone, even if the guest is not sent a corresponding \n",
      "written booking confirmation.  \n",
      "2.3 Section 2.2 notwithstanding, the following applies to bookings made online:  \n",
      "a) The process for making an online booking shall be explained to the guest at the \n",
      "relevant online portal. The guest has the ability to correct or delete information he / she \n",
      "has ente red, or to reset the entire online reservation form by means of a correction \n",
      "function, the use of which is explained to the guest. Contract languages for making an \n",
      "online booking are indicated.  b) To the extent the contract text is stored by the Host or in  the online booking system, \n",
      "the guest shall be informed of this and informed of the ability to access the contract text \n",
      "at a later time.  \n",
      "c) By clicking the “make binding reservation” button, the guest makes a binding offer \n",
      "to conclude a lodging agreement w ith the Host. The guest shall receive immediate \n",
      "electronic confirmation of his / her booking.  \n",
      "d) The transmission of an offer to conclude a contract by clicking on the “make binding \n",
      "reservation“ button does not confer upon the guest any right to the format ion of a \n",
      "lodging agreement in accordance with his / her booking information. Instead, the Host \n",
      "is free to decide whether or not to accept the guest‘s offer.  \n",
      "e) The contract is concluded when the guest receives the booking confirmation from the \n",
      "Host or HDM as its agent.  \n",
      "2.4 If the booking confirmation is provided in the form of a message on the screen (real -\n",
      "time booking) immediately after the guest makes the booking by clicking on the “make \n",
      "binding reservation“ button, the lodging agreement is concluded upon receipt and \n",
      "pres entation of this booking confirmation on the guest‘s screen without the \n",
      "requirement of an intervening notice that the booking has been received. In such cases, \n",
      "the customer is provided the option to save and print the booking confirmation. \n",
      "However, the bin ding nature of the lodging agreement does not depend on the \n",
      "circumstance that the guest has the option to save or print the booking confirmation.  \n",
      "The Host or HDM generally sends an additional, written booking confirmation to the \n",
      "guest by email, email attac hment, postal mail or fax. However, receipt of such additional \n",
      "booking confirmation is not a requirement for the lodging agreement to be binding.  \n",
      " \n",
      "3. Reservations  \n",
      " \n",
      "3.1 Non -binding reservations that entitle the guest to a right of withdrawal without \n",
      "charge are only permitted in the event of an express agreement to such effect with HDM \n",
      "or the Host.  \n",
      "3.2 If no reservation has been expressly agreed, a booking generally results in a legally \n",
      "binding contract concluded by and between the Host and the guest / client  in \n",
      "accordance with section 2 (c ontract formation).  \n",
      "3.3 If a non -binding reservation has been agreed with individual guests, the guest is \n",
      "required to notify HDM by the agreed date as to whether the reservation is to be \n",
      "considered a binding booking. If the guest fails to do so, the reservation shall be voided \n",
      "without any additional duty to provide notice on the part of HDM or the Host. If timely \n",
      "notice is given, the booking becomes binding, regardless of a booking confirmation \n",
      "subsequently issued by HDM or t he Host.  \n",
      " \n",
      "4. Pricing and services; price increases  \n",
      " \n",
      "4.1 The prices indicated in the booking basis (host directory, host offer, internet) are \n",
      "final prices and include statutory sales tax and all ancillary costs unless otherwise \n",
      "stipulated with respect to ancillary costs. Visitors‘ tax or fees for consumption -based \n",
      "goods and services (e.g.  electricity, gas, water, firewood) and for optional and additional \n",
      "goods and services may be incurred and listed separately.  \n",
      "4.2 Goods and services the Host is required to provide shall be based exclusively on the \n",
      "booking confi rmation together with the applicable brochure and  / or property \n",
      "description and any supplemental and express agreements made with the guest  / \n",
      "client. The guest  / client is advised to obtain any supplemental agreements in writing \n",
      "such as by fax, email or ot her informal written form.  \n",
      "4.3 Moreover, the Host may adjust its prices if the customer subsequently wishes to \n",
      "make changes to the number of rooms reserved, the Host‘s services or the length of the \n",
      "stay, and the Host consents to such changes.  \n",
      "4.4 The Host may charge a re -booking fee in the amount of € 15.00 for each change in \n",
      "the case of re -bookings (changes with regard to arrival or departure dates, length of \n",
      "stay, meal plan, booked additional goods and services or other supplemental goods and \n",
      "services) fo r which there is no legal right. The foregoing shall not apply in the event that \n",
      "such change is only minor.  \n",
      " \n",
      "5. Payment  \n",
      " \n",
      "5.1 The due date for payment is based on th e terms agreed with the guest /  client and \n",
      "noted in the booking confirmation. In the event that no special agreement has been \n",
      "made, the full price for the accommodations, including fees for ancillary costs and \n",
      "supplemental goods and services, shall be payable to the Host at the end of t he stay.  \n",
      "5.2 Payments may not be made in foreign currency or by collection -only check. Credit \n",
      "card payments are only permitted if this has been agreed or the Host offers this form of General Terms and Conditions | 5960 | General Terms and Conditions \n",
      " \n",
      "  \n",
      "payment in general as indicated on a posted notice. Payments at the end o f the stay may \n",
      "not be made via bank transfer.  \n",
      "5.3 If the guest does not make an agreed down payment and / or the remaining \n",
      "payment or does not make it in full within the specified period despite a reminder from \n",
      "the Host setting a reasonable deadline, altho ugh the Host is willing and able to properly \n",
      "provide the contractual services, no legal or contractual right of set -off or retention of \n",
      "the guest exists, and if the guest is responsible for the delay in payment, the Host shall \n",
      "be entitled to withdraw from the contract with the guest and to demand cancelation \n",
      "costs from him / her in accordance with section 6 of these terms and conditions.  \n",
      " \n",
      "6. Cance lation and no -show  \n",
      " \n",
      "The followin g applies in the event of cance lation and no -show unless otherwise agreed \n",
      "in ind ividual cases and noted in the booking confirmation:  \n",
      "6.1 Cancelation at no charge is possible up to 2 days prior to arrival in the case of \n",
      "bookings for up to 3 rooms.  \n",
      "6.2 For booki ngs of more than 3 rooms, cance lation of the reservation, in whole or in \n",
      "part, is only possible without incurring a charge up to 7 business days prior to arrival.  \n",
      "6.3 The following provisions shall apply in the case of a no -show or in the event it is no \n",
      "longer possible to cancel without incurring a charge:  \n",
      "a) In the event of cance lation, the Host‘s claim to payment of the agreed price for \n",
      "accommodations, including the meals component and fees for additional services, shall \n",
      "remain unaffected.  \n",
      "b) The Host shall undertake to make efforts to rent the cancel ed rooms to other guests \n",
      "within the scope of its normal business operations and without obligation make any \n",
      "special efforts as well as under consideration of the particular nature of the \n",
      "accommodations booked (e.g., non -smoking room, family room).  \n",
      "c) The Host is required to provide c redit for other rental of the room and, where this is \n",
      "not possible, for expenses saved.  \n",
      "d) In accordance with percentage rates approved by applicable jurisprudence for \n",
      "calculating saved expenses, the guest and  / or client shall undertake to pay the followi ng \n",
      "amounts to the lodging establishment, in each case based on the full price for lodging \n",
      "services (including all ancillary expenses):  \n",
      "in the case of accommodations without meals, 90 %;  \n",
      "in the case of overnight stays / breakfast, 80 %;  \n",
      "in the case of half- board, 70 %;  \n",
      "in the case of full board, 60 %.  \n",
      "e) The guest / client remains expressly entitled to provide evidence to the Host that the \n",
      "expenses saved by the latter are materially higher than the deductions provided for \n",
      "above, or that the lodging services or other goods and services were subject to othe r \n",
      "use. In the event of such proof, the guest / client is only obligated to pay the lowest \n",
      "relevant amount.  \n",
      "6.4 Purchase of a travel cancel ation insurance is highly recommended.  \n",
      "6.5 For administ rative reasons, notice of cance lation must be addressed to HDM (not \n",
      "the lodging establishment) and should be provided in writing, such as by fax, email or \n",
      "other informal written form, in the interest of the guest.  \n",
      " \n",
      "7. Arrival and departure  \n",
      " \n",
      "7.1 Guests are required to arrive at the agreed time or by 6:00 pm at the latest if no \n",
      "specific time has been agreed.  \n",
      "7.2 The following applies to later arrivals:  \n",
      "a) The guest shall undertake to inform the Host not later than the agreed date of arrival \n",
      "if the  guest will be arriving late or intends to arrive a day late in the case of \n",
      "accommodations booked for multiple days.  \n",
      "b) The Host is authorized to rent the accommodations to another guest if timely notice \n",
      "is not provided. The provisions of s ection 6.3 shall  apply mutatis mutandis for the period \n",
      "during which a room is vacant.  \n",
      "c) If the guest provides notice of late arrival, he / she shall undertake to pay the agreed \n",
      "charges less expenses saved by the Host in accordance with s ection 6.3, including for \n",
      "periods of non -occupancy for which a room was reserved, unless the Host is \n",
      "contractually or legally required to accept responsibility for the reasons for delayed \n",
      "occupancy.  \n",
      "7.3 Guests are required to vacate the accommodations at the agreed time, but not later \n",
      "than  12:00 pm on the date of departure if no specific time has been agreed. The Host \n",
      "may demand additional compensation as appropriate in the event that the \n",
      "accommodations are not vacated in a timely manner. The foregoing is without prejudice \n",
      "to the Host‘s abi lity to claim additional damages.  \n",
      " \n",
      "8. Duties of the customer; termination by the Host  \n",
      " \n",
      "8.1 Unless otherwise agreed, the accommodation may only be occupied by the guest \n",
      "for whom it was booked. Occupation by another person, in particular subletting in the \n",
      "case of commercial customers or, in particular, the transfer of blocks of rooms, is \n",
      "prohibited.  \n",
      "8.2 The guest shall undertake to treat the room and all furnishings, as well as all \n",
      "furnishings and fixtures at the lodging establishment itself, only as intended  and with \n",
      "care and, if posted (e.g., in the case of pools and saunas), only pursuant to the rules for \n",
      "use.  8.3 The guest shall undertake to immediately report any defects and malfunctions to \n",
      "the Host and request they be remedied. Notice of defects provided  solely to HDM is \n",
      "insufficient. The guest‘s right to assert claims may lapse, in whole or in part, if the guest \n",
      "is at fault for a failure to provide notice.  \n",
      "8.4 The guest may only terminate the contract in the event of substantial defects or \n",
      "malfunctions. Prior to termination, he  / she must provide the Host a reasonable period \n",
      "to remedy said defect as part of the notification of the same unless it is impossible for \n",
      "said defect to be remedied, the Host refuses to remedy the defect, if termination of the \n",
      "cont ract without notice is justified by a legitimate interest of the guest or that this would \n",
      "make it objectively unreasonable to expect the guest to continue with their stay.  \n",
      "8.5 Bringing and housing pets at the accommodations is only permitted if expressly \n",
      "agreed and only if the Host provides for such an option in the description. In the case of \n",
      "such an agreement, the guest is required to provide truthful information about the type \n",
      "and size of the pet. A breach of this obligation may provide the Host with rea sons to \n",
      "terminate the lodging agreement.  \n",
      "8.6 The Host may terminate the lodging agreement without observing a notice period \n",
      "if the guest, despite receiving a warning from the Host, continuously disrupts the Host‘s \n",
      "operations, other guests or the completion  of the stay, or if the guest acts in a manner \n",
      "that contravenes the contract to such a degree that the immediate dissolution of the \n",
      "contract is justified. In the event that the Host terminates the contract, the foregoing \n",
      "provisions regarding the obligation  to pay upon withdrawal by the guest shall apply \n",
      "mutatis mutandis with regard to the Host‘s right to payment.  \n",
      " \n",
      "9. Limitation of liability  \n",
      " \n",
      "9.1 The Host is liable without limitation, as far as the damage results from violation of \n",
      "an essential obligation, the performance of which was required for proper execution of \n",
      "the contract or the violation of which endangers achievement of the purpose of the  \n",
      "contract or the damage results from violation of life, body or health.  \n",
      "Apart from this, the Host‘s liability shall be limited to damage caused by the Host or its \n",
      "servants willfully or grossly negligently.  \n",
      "9.2 The potential Host‘s innkeeper‘s liability for  items brought by the guest in \n",
      "accordance with sections 701 et seq.  BGB shall remain unaffected by this provision . \n",
      "9.3 The Host is not liable for disruptions in connection with goods and services that were \n",
      "merely arranged for the guest  / client during the stay where it is clear that such goods \n",
      "and / or services are third -party services  (e.g., sporting events, theater  tickets, \n",
      "exhibitions, etc.). The foregoing applies mutatis mutandis to third -party goods and \n",
      "services arranged in combination with the reservation if and insofar as the same were \n",
      "expressly indicated as third -party services in the description and  / or book ing \n",
      "confirmation.  \n",
      " \n",
      "10. Special regulations in connection with pandemics (in particular the Corona \n",
      "virus)  \n",
      " \n",
      "10.1 The parties agree that the agreed travel services shall always be provided by the \n",
      "respective service providers in compliance with and in accordan ce with the official \n",
      "requirements and conditions applicable at the time of travel.  \n",
      "10.2 The guest agrees to comply with reasonable regulations or restrictions on use of \n",
      "HDM and the Hosts when using services and to notify the Host immediately in the event \n",
      "of typical symptoms of illness.  \n",
      " \n",
      "11. Alternative dispute resolution  \n",
      " \n",
      "11.1 In respect of the German Consumer Dispute Resolution Act (Gesetz über \n",
      "Verbraucherstreitbeilegung), HDM and the Host s advi se that neither HDM nor the Hosts \n",
      "currently participate in voluntary consumer dispute resolution.  \n",
      "11.2 HDM will provide the guest appropriate notice in the event consumer dispute \n",
      "resolution were to become obligatory for HDM or the  Hosts following the publication of \n",
      "these Terms and Conditions for Guest Accommodations  and Agency Services.  \n",
      "11.3 Please refer to the European online dispute resolution platform  \n",
      "https://ec.europa.eu/consumers/odr/ for all guest accommodation and agency \n",
      "services contracts concluded by electronic means.  \n",
      " \n",
      "12. Applicable law and place of jurisdiction  \n",
      " \n",
      "12.1 The contractual relationship between the guest and  / or client and the Host and  / \n",
      "or HDM is exclusively governed by German law. The foregoing shall apply in like manner \n",
      "to all other aspects of the legal relationship.  \n",
      "12.2 The gu est and  / or client may only file suit against the Host and  / or HDM at the \n",
      "location of their respective registered office.  \n",
      "12.3 The place of residence of the guest is determinative for suits brought by the Host \n",
      "and / or HDM against the guest and  / or clie nt. The Parties agree that the place of \n",
      "jurisdiction shall be the location of the Host‘s registered office for suits filed against \n",
      "guests and  / or clients that are merchants, legal entities under public law or private law \n",
      "or are persons whose residence or habitual place of abode is located outside of \n",
      "Germany or whose residence or habitual place of abode is unknown at the time a suit is \n",
      "filed.  \n",
      "12.4 The foregoing provisions shall not apply if and insofar as relevant and non -\n",
      "waivable provisions of European Uni on law or other international laws are applicable to \n",
      "the contract.   \n",
      "  \n",
      "payment in general as indicated on a posted notice. Payments at the end o f the stay may \n",
      "not be made via bank transfer.  \n",
      "5.3 If the guest does not make an agreed down payment and / or the remaining \n",
      "payment or does not make it in full within the specified period despite a reminder from \n",
      "the Host setting a reasonable deadline, altho ugh the Host is willing and able to properly \n",
      "provide the contractual services, no legal or contractual right of set -off or retention of \n",
      "the guest exists, and if the guest is responsible for the delay in payment, the Host shall \n",
      "be entitled to withdraw from the contract with the guest and to demand cancelation \n",
      "costs from him / her in accordance with section 6 of these terms and conditions.  \n",
      " \n",
      "6. Cance lation and no -show  \n",
      " \n",
      "The followin g applies in the event of cance lation and no -show unless otherwise agreed \n",
      "in ind ividual cases and noted in the booking confirmation:  \n",
      "6.1 Cancelation at no charge is possible up to 2 days prior to arrival in the case of \n",
      "bookings for up to 3 rooms.  \n",
      "6.2 For booki ngs of more than 3 rooms, cance lation of the reservation, in whole or in \n",
      "part, is only possible without incurring a charge up to 7 business days prior to arrival.  \n",
      "6.3 The following provisions shall apply in the case of a no -show or in the event it is no \n",
      "longer possible to cancel without incurring a charge:  \n",
      "a) In the event of cance lation, the Host‘s claim to payment of the agreed price for \n",
      "accommodations, including the meals component and fees for additional services, shall \n",
      "remain unaffected.  \n",
      "b) The Host shall undertake to make efforts to rent the cancel ed rooms to other guests \n",
      "within the scope of its normal business operations and without obligation make any \n",
      "special efforts as well as under consideration of the particular nature of the \n",
      "accommodations booked (e.g., non -smoking room, family room).  \n",
      "c) The Host is required to provide c redit for other rental of the room and, where this is \n",
      "not possible, for expenses saved.  \n",
      "d) In accordance with percentage rates approved by applicable jurisprudence for \n",
      "calculating saved expenses, the guest and  / or client shall undertake to pay the followi ng \n",
      "amounts to the lodging establishment, in each case based on the full price for lodging \n",
      "services (including all ancillary expenses):  \n",
      "in the case of accommodations without meals, 90 %;  \n",
      "in the case of overnight stays / breakfast, 80 %;  \n",
      "in the case of half- board, 70 %;  \n",
      "in the case of full board, 60 %.  \n",
      "e) The guest / client remains expressly entitled to provide evidence to the Host that the \n",
      "expenses saved by the latter are materially higher than the deductions provided for \n",
      "above, or that the lodging services or other goods and services were subject to othe r \n",
      "use. In the event of such proof, the guest / client is only obligated to pay the lowest \n",
      "relevant amount.  \n",
      "6.4 Purchase of a travel cancel ation insurance is highly recommended.  \n",
      "6.5 For administ rative reasons, notice of cance lation must be addressed to HDM (not \n",
      "the lodging establishment) and should be provided in writing, such as by fax, email or \n",
      "other informal written form, in the interest of the guest.  \n",
      " \n",
      "7. Arrival and departure  \n",
      " \n",
      "7.1 Guests are required to arrive at the agreed time or by 6:00 pm at the latest if no \n",
      "specific time has been agreed.  \n",
      "7.2 The following applies to later arrivals:  \n",
      "a) The guest shall undertake to inform the Host not later than the agreed date of arrival \n",
      "if the  guest will be arriving late or intends to arrive a day late in the case of \n",
      "accommodations booked for multiple days.  \n",
      "b) The Host is authorized to rent the accommodations to another guest if timely notice \n",
      "is not provided. The provisions of s ection 6.3 shall  apply mutatis mutandis for the period \n",
      "during which a room is vacant.  \n",
      "c) If the guest provides notice of late arrival, he / she shall undertake to pay the agreed \n",
      "charges less expenses saved by the Host in accordance with s ection 6.3, including for \n",
      "periods of non -occupancy for which a room was reserved, unless the Host is \n",
      "contractually or legally required to accept responsibility for the reasons for delayed \n",
      "occupancy.  \n",
      "7.3 Guests are required to vacate the accommodations at the agreed time, but not later \n",
      "than  12:00 pm on the date of departure if no specific time has been agreed. The Host \n",
      "may demand additional compensation as appropriate in the event that the \n",
      "accommodations are not vacated in a timely manner. The foregoing is without prejudice \n",
      "to the Host‘s abi lity to claim additional damages.  \n",
      " \n",
      "8. Duties of the customer; termination by the Host  \n",
      " \n",
      "8.1 Unless otherwise agreed, the accommodation may only be occupied by the guest \n",
      "for whom it was booked. Occupation by another person, in particular subletting in the \n",
      "case of commercial customers or, in particular, the transfer of blocks of rooms, is \n",
      "prohibited.  \n",
      "8.2 The guest shall undertake to treat the room and all furnishings, as well as all \n",
      "furnishings and fixtures at the lodging establishment itself, only as intended  and with \n",
      "care and, if posted (e.g., in the case of pools and saunas), only pursuant to the rules for \n",
      "use.  8.3 The guest shall undertake to immediately report any defects and malfunctions to \n",
      "the Host and request they be remedied. Notice of defects provided  solely to HDM is \n",
      "insufficient. The guest‘s right to assert claims may lapse, in whole or in part, if the guest \n",
      "is at fault for a failure to provide notice.  \n",
      "8.4 The guest may only terminate the contract in the event of substantial defects or \n",
      "malfunctions. Prior to termination, he  / she must provide the Host a reasonable period \n",
      "to remedy said defect as part of the notification of the same unless it is impossible for \n",
      "said defect to be remedied, the Host refuses to remedy the defect, if termination of the \n",
      "cont ract without notice is justified by a legitimate interest of the guest or that this would \n",
      "make it objectively unreasonable to expect the guest to continue with their stay.  \n",
      "8.5 Bringing and housing pets at the accommodations is only permitted if expressly \n",
      "agreed and only if the Host provides for such an option in the description. In the case of \n",
      "such an agreement, the guest is required to provide truthful information about the type \n",
      "and size of the pet. A breach of this obligation may provide the Host with rea sons to \n",
      "terminate the lodging agreement.  \n",
      "8.6 The Host may terminate the lodging agreement without observing a notice period \n",
      "if the guest, despite receiving a warning from the Host, continuously disrupts the Host‘s \n",
      "operations, other guests or the completion  of the stay, or if the guest acts in a manner \n",
      "that contravenes the contract to such a degree that the immediate dissolution of the \n",
      "contract is justified. In the event that the Host terminates the contract, the foregoing \n",
      "provisions regarding the obligation  to pay upon withdrawal by the guest shall apply \n",
      "mutatis mutandis with regard to the Host‘s right to payment.  \n",
      " \n",
      "9. Limitation of liability  \n",
      " \n",
      "9.1 The Host is liable without limitation, as far as the damage results from violation of \n",
      "an essential obligation, the performance of which was required for proper execution of \n",
      "the contract or the violation of which endangers achievement of the purpose of the  \n",
      "contract or the damage results from violation of life, body or health.  \n",
      "Apart from this, the Host‘s liability shall be limited to damage caused by the Host or its \n",
      "servants willfully or grossly negligently.  \n",
      "9.2 The potential Host‘s innkeeper‘s liability for  items brought by the guest in \n",
      "accordance with sections 701 et seq.  BGB shall remain unaffected by this provision . \n",
      "9.3 The Host is not liable for disruptions in connection with goods and services that were \n",
      "merely arranged for the guest  / client during the stay where it is clear that such goods \n",
      "and / or services are third -party services  (e.g., sporting events, theater  tickets, \n",
      "exhibitions, etc.). The foregoing applies mutatis mutandis to third -party goods and \n",
      "services arranged in combination with the reservation if and insofar as the same were \n",
      "expressly indicated as third -party services in the description and  / or book ing \n",
      "confirmation.  \n",
      " \n",
      "10. Special regulations in connection with pandemics (in particular the Corona \n",
      "virus)  \n",
      " \n",
      "10.1 The parties agree that the agreed travel services shall always be provided by the \n",
      "respective service providers in compliance with and in accordan ce with the official \n",
      "requirements and conditions applicable at the time of travel.  \n",
      "10.2 The guest agrees to comply with reasonable regulations or restrictions on use of \n",
      "HDM and the Hosts when using services and to notify the Host immediately in the event \n",
      "of typical symptoms of illness.  \n",
      " \n",
      "11. Alternative dispute resolution  \n",
      " \n",
      "11.1 In respect of the German Consumer Dispute Resolution Act (Gesetz über \n",
      "Verbraucherstreitbeilegung), HDM and the Host s advi se that neither HDM nor the Hosts \n",
      "currently participate in voluntary consumer dispute resolution.  \n",
      "11.2 HDM will provide the guest appropriate notice in the event consumer dispute \n",
      "resolution were to become obligatory for HDM or the  Hosts following the publication of \n",
      "these Terms and Conditions for Guest Accommodations  and Agency Services.  \n",
      "11.3 Please refer to the European online dispute resolution platform  \n",
      "https://ec.europa.eu/consumers/odr/ for all guest accommodation and agency \n",
      "services contracts concluded by electronic means.  \n",
      " \n",
      "12. Applicable law and place of jurisdiction  \n",
      " \n",
      "12.1 The contractual relationship between the guest and  / or client and the Host and  / \n",
      "or HDM is exclusively governed by German law. The foregoing shall apply in like manner \n",
      "to all other aspects of the legal relationship.  \n",
      "12.2 The gu est and  / or client may only file suit against the Host and  / or HDM at the \n",
      "location of their respective registered office.  \n",
      "12.3 The place of residence of the guest is determinative for suits brought by the Host \n",
      "and / or HDM against the guest and  / or clie nt. The Parties agree that the place of \n",
      "jurisdiction shall be the location of the Host‘s registered office for suits filed against \n",
      "guests and  / or clients that are merchants, legal entities under public law or private law \n",
      "or are persons whose residence or habitual place of abode is located outside of \n",
      "Germany or whose residence or habitual place of abode is unknown at the time a suit is \n",
      "filed.  \n",
      "12.4 The foregoing provisions shall not apply if and insofar as relevant and non -\n",
      "waivable provisions of European Uni on law or other international laws are applicable to \n",
      "the contract.  General Terms and Conditions | 61 \n",
      "  \n",
      "© Copyright protection.  \n",
      "Noll | Hütten | Dukic Rechtsanwälte, München | Stuttgart, 2020 – 2023 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Tourism agency:  \n",
      "Heidelberg Marketing GmbH  \n",
      "Managing director: Mathias Schiemer  \n",
      "Neuenheimer Landstraße 5  \n",
      "69120 Heidelberg, Germany  \n",
      " \n",
      "Phone:  +49 6221 5840  - 200 \n",
      "Telefax:  +49 6221 5840  - 222 \n",
      "info@heidelberg -marketing.de  \n",
      " \n",
      "Commercial register number: HRB 337405  \n",
      "Register court: AG Mannheim  \n",
      "VAT ID: DE226325597  \n",
      " \n",
      " \n",
      "  62 | General Terms and Conditions Imprint\n",
      "Heidelberg Marketing GmbH  \n",
      "Neuenheimer Landstrasse 5  \n",
      "69120 Heidelberg / Germany\n",
      "Phone +49 6221 58 40-200 \n",
      "Fax +49 6221 58 40-222 \n",
      "info@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.de\n",
      "The Heidelberg Marketing GmbH is a \n",
      "subsidiary of the City of Heidelberg\n",
      "Content  \n",
      "Heidelberg Marketing GmbH\n",
      "Layout  \n",
      "aB Grafik | Artem Bathauer \n",
      "www.a-b-grafik.de\n",
      "Photos\n",
      "Cover page, pages 3, 4, 5, 6 below, 7, 8, 9, 10, \n",
      "11 on the top, 12, 13, 14 on the top, 15, 16, 17, \n",
      "21, 22, 23, 38, 39 on the right, 40, 43, 45, 55 \n",
      "– Tobias Schwerdt  \n",
      "Pages 6 on the top, 56 – Christoph Düpper \n",
      "Page 11 below – TMBW / Stefan Kuhn \n",
      "Page 14 below – Heidelberg Marketing GmbH \n",
      "Page 18 – Theater und Orchester Heidelberg /  \n",
      "Waechter + Waechter  \n",
      "Page 19 – Theater und Orchester Heidelberg /  \n",
      "Susanne Reichardt  \n",
      "Page 20 on the top – Heidelberger Frühling / \n",
      "Susanne Reichardt  \n",
      "Page 20 below – studio visuell for \n",
      "Heidelberger Frühling  \n",
      "Pages 24 to 35 – Photos of the respective \n",
      "accommodations  \n",
      "Page 39 on the left – Achim Mende\n",
      "© Copyright 2023. All contents, in particular  \n",
      "texts, photographs and graphics, are protected \n",
      "by copyright. Unless expressly stated other  wise,  \n",
      "Heidelberg Marketing GmbH owns the copyright.Heidelberg  \n",
      "Marketing GmbH\n",
      "Neuenheimer Landstrasse 5  \n",
      "69120 Heidelberg / Germany\n",
      " \n",
      "Phone  +49 6221 58-44444  \n",
      "Fax +49 6221 58-40222 \n",
      "info@heidelberg-marketing.de  \n",
      "www.heidelberg-marketing.com\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        print(f\"Attempting to open: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:  # Open the PDF file\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text_content_of_pdf = ''\n",
    "\n",
    "                for page in pdf_reader.pages:\n",
    "                    text_content_of_pdf += page.extract_text() if page.extract_text() else ''\n",
    "\n",
    "                pdf_as_text.append(text_content_of_pdf)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")  # This will print out why the file couldn't be opened\n",
    "\n",
    "# Optionally print or process the text extracted from each PDF\n",
    "for text in pdf_as_text:\n",
    "    print(text)  # Print extracted text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of the Cleaning Steps:\n",
    "\n",
    "1. URL Removal: Extracted text might contain URLs which we remove using regular expressions.\n",
    "2. Page Number Removal: Often, PDF extraction includes footer elements like page numbers which we're removing.\n",
    "3. Metadata Removal: Copyright phrases and other metadata are common in PDFs.\n",
    "4. Non-ASCII Characters: Remove any characters that are not standard ASCII, which helps clean up characters that did not translate well from the PDF.\n",
    "5. Whitespace Normalization: Convert any sequence of whitespace characters to a single space, which helps in cleaning up the formatting.\n",
    "6. Punctuation and Special Characters: Remove punctuation which is often misplaced or misinterpreted during PDF extraction.\n",
    "7. Single Character Removal: Optionally, removing isolated single characters can clean up artifacts but should be used with caution as it can also remove valid data (like 'a' or 'I').\n",
    "8. Case Normalization: Convert text to lowercase to standardize it and simplify further processing.\n",
    "9. Trimming: Remove extra spaces at the beginning and end of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.8 MB 3.9 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.5/12.8 MB 5.2 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.8/12.8 MB 5.7 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.2/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 7.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.8/12.8 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 11.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 13.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.2/12.8 MB 14.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.4/12.8 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asha4\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\asha4\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heidelberg your new home for museum and public library included I lost my heart in Heidelberg Name of a 1927 musical by Fred Raymond music and Fritz Lhner Beda and Ernst Neubach words Welcome to Heidelberg Dear new citizen, I am pleased that you have chosen our city as your new home Heidelberg is known for its scenic beauty, its internationality and diversity Living in Heidelberg means having a home the city derives its high quality of life from the attractive mix of living and working, research and culture As a cosmopolitan city and Rainbow City, Heidelberg welcomes all people with open arms, regardless of their origin or sexual identity We want you to feel at home here and to that end, we support you in all areas of life Families with children have access to an excellent childcare network Schoolchildren can develop their talents to the full here, because Heidelberg is the best school location in Germany according to several studies Low income families receive targeted support in the form of a wide range of free services, and we also support trainees and students on their way, for example by subsidizing local public transport tickets and with the training house Eleven senior citizen centers are available as contact points for all older citizens A wide range of cultural, leisure and sporting activities, from theaters and half- marathons to festivals with top reputations, leaves nothing to be desired Climate protection plays a central role in Heidelberg for example, we promote the purchase of photovoltaic systems and electric cars, support climate friendly renovations and are proud of our forests and green spaces We are also proud of our many scientific and research institutions that take their ideas and solutions for a better future from Heidelberg to the world First and foremost is Ruperto Carola, our university of excellence, which attracts many young people to study in our city every year thanks to its international appeal We wish you a good start in our beautiful city and are happy to answer any questions you may have Your Prof Dr Eckart Wrzner Mayor Content A good place to live 8 First things first 12 Heidelberg in figures 14 Unique 15 times over 16 International city of culture 18 Top marks for school and childcare 24 Leading the way in climate protection 30 Transport for everyone 34 World class research 36 A safe city 38 Homes for everyone 40 Embracing diversity 44 Volunteers a vital asset for Heidelberg 46 Getting the public involved 47 Partners around the world 48 Imprint 51 A good place to live 9 8 A good place to live A good place to live Heidelberg is considered one of the most beautiful cities in Germany Every year, around 12 million people visit the world famous city alongside the river Neckar, to see the interplay between the castle, the old town and the river amid the hills of the Odenwald The about 160 , 000 citizens of Heidelberg appreciate Heidelbergs high quality of life Year after year, almost all respondents in the Heidelberg survey state that they love living in their city This is hardly surprising, given the citys many strengths A good place to live 11 10 A good place to live Family friendly Heidelberg supports families with offers that are among the best in Germany exemplary childcare, a choice of out standing schools and holiday programs, fun playgrounds, child friendly traffic planning and much more City of science Heidelberg University is one of the best in the world Together with the Uni versity Hospital, respected research institutes like the German Cancer Re-search Center, numerous research- related companies, and other higher education establishments, it has earned Heidelberg a worldwide reputation as a major center for science Cosmopolitan An estimated 56 , 000 people with a migration background are at home in Heidelberg They come from around 180 countries and enrich the vibrant community Good for recreation Because of its scenic location, its fa vorable climatic conditions and its urban flair Heidelberg has one of the highest leisure values in Germany Strong for business The city provides jobs for around 120 , 000 people Of these, around two- thirds are employed in the knowledge- intensive sector Dynamic urban development Housing is a top priority in Heidelberg The Bahnstadt is currently being devel oped as a pioneering new district It will provide living space for around 6 , 800 people and up to 6 , 000 jobs On four former sites of the U S Army, future quarters are growing with sev eral thousand apartments, space for innovative companies and high quality sports, cultural and leisure facilities Cultural mecca Heidelberg impresses with its wide range of cultural offerings in the areas of music, theater, dance, visual arts and film Heidelberg is also the first and so far only German speaking city of liter ature in the U NE SC O network of Cre ative Cities Voluntary work Heidelberg has a diverse and well- structured range of volunteers Their commitment is invaluable for success ful coexistence and social cohesion Involving people Heidelbergs citizens are systematically involved into municipal planning at an early stage They are thus important sources of ideas for the administration and the city council The citys project list, which provides information on planned projects, also enables early participation A safe city Heidelberg is safe thanks to a part nership with the state police and a broad based municipal public order service Eco friendly Heidelberg is an international pioneer in climate protection As early as 2015 , the United Nations named the city a Global Green City With the climate protection action plan, adopted in 2019 , Heidelberg aims to become climate neutral by 2030  First things first 13 12 First things first All services of Heidelberg City Administration can be accessed at the adminis trative offices, including registering your new place of residence, applying for a passport, I D or parking permit, changing the place of registration of your car, applying for a police clearance certificate Fhrungszeugnis or International Driving Permit, picking up yellow recycling bags or the latest waste collection calendar, applying for a Heidelberg Pass Heidelberg Pass, requesting child Care vouchers Betreuungsgutscheine or housing eligibility vouchers Wohnberechtigungsscheine, purchasing vouchers for the womens night taxi Frauen nachttaxischeine, and much more The addresses and open ing times of the administrative offices can be found at administration A wide range of citizens service information from A for Abfallkalender waste collection calendar to Z for Zulassungsformular car registration form is also available 24 hours a day on the internet at buergerservice  Homepage At you will find a comprehensive range of services and information From an online city map to a calendar of events to the latest news from the administration and the municipal council City Gazette Free of charge for every household On Wednesdays, the Heidelberg City Gazette lands in your mailbox Social media heidelberg.de heidelberg_de heidelberg_de Stadt Heidelberg All posts of the municipal social media channels at a glance at App The Mein Heidelberg app provides cur rent service information and tips The app can be downloaded free of charge at Important emergency numbers Police Dial 110 to reach the nearest police station Fire and rescue services Under the European emergency number 112 you can reach the fire de-partment and the rescue service for life-threatening cases of illness Citizen Service The Heidelberg citizens service Brgerservice team are on hand from 8 00 am 6 00 pm Monday to Friday to answer questions about the services of the City of Heidelberg and other Federal and state authori ties They can be reached on 06221 58 10580 or 115 , the central number for information on any German public authority The citizens service can also connect you to other services as necessary Citizen service in your local area Alternatively, you can access citizens services in person from the various administrative offices Brgermter around the city Brgeramt Altstadt Old Town Brgeramt BoxbergEmmertsgrund Brgeramt Handschuhsheim Brgeramt Kirchheim Brgeramt Mitte Central administrative office, covering the districts of Bahnstadt, Bergheim, Sdstadt and Weststadt Brgeramt Neuenheim Brgeramt Pfaffengrund Brgeramt Rohrbach Brgeramt Wieblingen Brgeramt Ziegelhausen Schlierbach First things first The city of Heidelberg offers you information and assistance for a quick orientation in your new home 12 Mio guests per year 120 000 workplaces Approximately 87% employed in the service sector65% of employees subject to social insurance contribu tions work in the knowl edge intensive service sector and in industrial high technology 38 000 students of which around 29 , 000 at Heidelberg University Heidelberg4you More than 200 locations are covered by the free public W LA N network Heidelberg in figures 15 14 Heidelberg in figures Heidelberg in figures 160 000 Inhabitants Heidelberg is the youngest city in Germany About 39% of the Heidelberg residents are younger than 30 years, around 16% older than 65 .34% university graduates of all employees subject to social insurance contributions By comparison, the national average is 18 .5% 109 km area 30% populated area 70% green space16 Unique 15 times over Unique 15 times over 17 Unique 15 times over Heidelberg has 15 districts, each with its own distinct identity The sheer variety of the citys districts is what makes Heidelberg such an attractive and vibrant place to live Residents can make their own mark on their local district by joining one of the many district associations and other groups With the Patrick Henry Village Heidelbergs 16th district is currently under development It is planned to create 5 , 000 jobs and housing for up to 10 , 000 people For further information, please visit the section Living in Heidelberg at  The weekly market in front of Tiefburg Castle in the Hand schuhsheim district is a popular weekend meeting place and an experience for all the senses with the regional produce on offer there Altstadt Old Town Bahnstadt Bergheim Boxberg Emmertsgrund Handschuhsheim Kirchheim Neuenheim Pfaffengrund Rohrbach Schlierbach Sdstadt Weststadt Wieblingen Ziegelhausen International city of culture 19 International city of culture If you have been living in Heidelberg for some time already, you will know that it lives up to its reputa tion As well as the beautiful surrounding scenery of the Odenwald hills, and the lovely river Neckar, the city offers a wide range of cultural and leisure facilities Heidelberg is a city that knows how to continue its great literary tradition in a contemporary way Heidelbergs tradi tion as the seat of the oldest university in Germany with important philoso phers, theologians and literary figures, is combined with Heidelbergs lively literary scene and above average num ber of libraries, bookstores, publishers and translators, publishing houses and translators For this reason, in December 2014 it became the first and only Ger man city to be included in the worlds largest cultural network, U NE SC Os Creative Cities Network, as a City of Literature cityofliterature With its extensive collections, the Kurpflzisches Museum , founded in 1879 , offers an astounding impres sion of the former Electoral Palatinate and its capital Heidelberg Learn more about the exhibitions and collections at As a new citizen, you can use the enclosed family voucher on your first visit to the Kurpflzische Museum You can redeem the enclosed family voucher for two adults and two children see page 49  The Mark Twain Center for Transatlantic Relations in Sdstadt is a cultural center and place of exchange on the history and future issues of German American relations At a historically significant location in the former commandants of fice, where the heart of the U S Armys headquarters in Europe beat, a multi media exhibition is on display where visitors can discover various facets of German American coexistence in Heidelberg with the help of digital aids and in a playful way For more information, visit twain center.com Culture and leisure Music, film, theater, dance, literature and art of all eras and styles, enter tainment and celebrations, sports and recreation Heidelberg offers a wide range of leisure activities The Theater und Orchester Heidel- berg i s a fi v e di v i s i o n t h e a t e r w i t h opera, concert, drama and dance, as well as its own ensemble for childrens and youth theater, whose capacity uti lization is among the best in Germany The theaters various festivals, such as the Heidelberger Stckemarkt, the Dance Biennale and the Heidelberg Castle Festival contribute to this You can find out more about the theater program at 18 International city of culture Old masters and modern artists the Kurpflzische Museum offers an exciting variety An overview of shopping opportu nities in Heidelberg is provided by the online platform heidelberg.de  There you will also find information on the citywide Danke Schein voucher If you want to stay up to date and well entertained, the municipal library is at your disposal with a wide range of literature and media With the library card, you can borrow a comprehen sive selection of books, magazines, C Ds, D VDs and electronic media As a new citizen, you will receive the card free of charge for three months upon presentation of the enclosed voucher see page 45 stadtbuecherei  Would you like to enjoy nature in your free time, learn something or visit famous sights There are numerous opportunities for all of these Among the special attrac tions are the world famous castle and the funicular to the Knigstuhl In the Heidelberg Zoo the largest zoo in the Metropolitan Region you can discover more than about 2 , 700 animals from all over the world Face to face with a tiger or lion, discover a red panda high up in the branches, watch elephant bulls bathing all this Friends of contemporary art can ex pect contemporary works in the hall of the Kunstverein directly next to the Kurpflzische Museum For more information, visit  With the Prinzhorn Collection and the Hassbecker Collection , Heidelberg has two internationally outstanding collections of works of outsider art with a focus on Art and Madness and Naive Painting established through the work of the two Heidelberg museums Sammlung Prinzhorn and Haus Cajeth Art and culture dealing with the les bian, gay, bisexual, trans, inter and queer lsbtiq for short community is offered annually by the Heidelberg Queer Festival  When it was founded in 2009 , it was the first festival for queer culture Today it is the largest and most renowned of its kind in Germany and one of the flagships of the Rainbow City Heidelberg Heidelberg also has a large number of in dependent institutions such as Halle02, Kulturfenster, Deutsch Amer ikanisches Institut, Interkul turelles Zentrum, Unterwegs- Theater and Kulturhaus Karlstor bahnhof  A list of all cultural institu tions, sports facilities and other leisure activities can be found under Life Leisure at  The many festivals also have a special appeal, including the classical music festivals Heidelberger Frh ling and Enjoy Jazz or the renowned International Film Festival Heidelberg Mann heim  Metropolink is also an inte gral part of the Heidelberg Sum mer For years, this nationally renowned festival of urban art has been actively contributing to the citys development and gives Heidelberg a modern face With Metropolinks Commis sary Germanys largest indoor gallery for street art there is now also a year round venue for urban art, culture, exchange and encounters At the municipal music and sing ing school, around 4 , 400 students re ceive instruction in singing, orchestra, choir, band and theory  Shopping Location Heidelberg Heidelberg is one of the most attrac tive shopping cities in Germany There are more than 500 retail stores and cafs in the city center from the old town to Bergheim and Neuenheim The approximately 1 4 kilometer long Hauptstrasse alone is home to more than 250 stores and restaurants The variety of small owner operated stores, the many side streets and the high quality of stay create a special shopping experience in the Old Town 20 International city of culture The former U S area Patrick Henry Village is the perfect setting for the popular Metropolink festival International city of culture 21 International city of culture 23 field, for beach volleyball courts, a large childrens playground as well as a skate facility, and there are oppor tunities for the trend sport slackline Recreation and relaxation can be found in any season in one of Heidelbergs five swimming pools , the more than 120 sports clubs and over 70 sports facilities and halls, as well as the alla hopp facility in Kirchheim In the Heidelberg Innovation Park hip on Speyerer Strae, the new S NP dome provides additional space for club and school sports The M LP Academics Heidelberg basketball team and the Rhein Neckar Lwen handball team also play there Major sporting events such as the Heidelberg Man triathlon, the S AS half marathon, international rugby tournaments, the rowing regatta and the Dragon Boat Cup attract recreational and pro fessional athletes and many spectators Other events, such as the Family Sports Day, round off the program The Explo Heidelberg on the grounds of the zoo offers fascinating scientific experiments to participate in The House of Astronomy on the Knig stuhl attracts children and adults alike to research into stars, galaxies, planets and black holes.and more is possible Around 500 , 000 visitors a year also experience seals, monkeys and exotic birds The Zoo School offers year round events for children, teens and adults, such as zoo lessons, zoo vacations and tours with a zoo ranger More info at www zoo-heidelberg.de Tours and chil drens birthday parties can be booked at In the city forest , there are 250 kilo meters of hiking trails, numerous rest and lookout points, themed and ad venture trails, the Knigstuhl walk ing course, designated mountain bike trails, the arboretums, the Pferchel forest adventure area for children and much more to discover The city forest is outstandingly sustainable Heidelberg has had the globally rec ognized P EF C seal of approval for its forest management since 2001  In 2015 , Heidelberg was the first city in Germany to receive the Recreational Forest certificate for the high rec reational quality of its entire forest in accordance with P EF C standards In 2018 , Heidelberg was allowed to call itself P EF C Forest Capital This was followed in 2021 by the P EF C designation Spa and Healing Forest likewise as the first German city The environmental education program Natrlich Heidelberg lets visitors experience nature and the forests around the city in a new way From March to December, guided tours, excursions and hands on activities for children, young people and adults take place The offers, including barbecue huts, can be booked directly at www natuerlich.heidelberg.de  The riverbank at the Neckar is also particularly popular as a play, sports, sunbathing and recreational area In addition, a part is used as a soccer 22 International city of culture The river banks at the Neckar are a popular spot to meet and relax Get out of the daily grind and into the fun In Heidelberg, you can swim in five swimming pools Whether its a sports pool in the middle of the city, an outdoor pool with a wide wave slide, or an indoor pool with a sauna everyone will find their personal favorite pool here Top marks for school and childcare 25 24 Top marks for school and childcare Top marks for school and childcare Heidelberg is an extremely family friendly city It offers the best childcare for very young children of any city in the state of Baden Wrttemberg, and its schools are outstanding Education Education has a long tradition in Heidel- berg, which is lived and further devel oped The spectrum extends far be yond the excellent range of courses offered by the university and colleges It includes services for all generations The city claims the top position na tionwide in early childhood education and is a nationwide pioneer in school- based learning For more information, visit  Heidelberg aims to give all its stu-dents the best school leaving qual ification possible For example, ad ditionalsystematically structured language support is provided at all Heidelberg elementary schools and the Marie Marcks School The Heidelberg School Support System H S provides support for lower achieving students, while the Hector Childrens Academy and the Hector Seminar are aimed at the highly gifted Of course, this also includes students with disabilities Heidelberg has devel oped viable support systems when it comes to inclusion Top marks for school and childcare 27 e V , which unites more than 50 youth associations, supports their concerns With the Feierbad , the city has cre ated another flexible offer for young people in Heidelberg On an event area in the Tiergartenbad, young people can celebrate together with free admission Social All people in Heidelberg should be able to live independently and in a self determined manner For this reason, the city offers a wide range of programs that give citizens on low incomes the chance to participate in social life For example, the Heidelberg Pass and the Heidelberg Pass provide free or reduced price admission to numerous cultural, sporting or other activities and free childcare in daycare centers from birth to school entry Both variants of the Heidelberg Pass can be applied for at the citizens registration offices A social ticket is also available for Heidelberg Pass holders This subsi dized season ticket allows citizens to use buses and trains at low prices This is available at the R NV customer center at the central station, and further informa tion is available at The city supports its citizens direct ly with subsidy programs for rented apartments and property and makes a contribution to affordable hous ing More at foerderprogramm Numerous advice centers in Heidelberg offer help in all situations Senior citizens Heidelberg is a city in which it is easy to grow old The pages at www heidelberg.desenioren provide an overview of the wide range of informa tion, advice and services on offer and the many ways of becoming and stay ing active In eleven Heidelberg dis-tricts, senior centers are a central point of contact for elderly people They offer a leisure program that Family Heidelberg offers a wide range of op tions for balancing family and career Even for the youngest children from the age of one, there are flexi ble opening hours, even all day, and excellently qualified specialist staff in the daycare centers Elementary schools offer lunch, homework, afternoon and vacation care The secondary school has the right profile for the next generation and is easy to reach Every educational qualification can be obtained at ei ther an all day or a half day school Heidelberg is a statewide pioneer in early childhood care and quality assurance 80 percent of elementary school children take advantage of the extensive after school care options There are childrens and youth centers in Heidelberg in practically every district and run by all providers The municipal Haus der Jugend House of Youth in Rmerstrae offers a cross district service There is plenty of room here for sports, art, dance and music, both during school hours and holiday time For more information, visit The youth associa tions of the two large churches, for example, organize camps, events and open meetings The Stadtjugendring 26 Top marks for school and childcare Place to be the strongly fre quented skate park is located right next to the riverside Top marks for school and childcare 29 28 Top marks for school and childcare ranges from memory training to hikes and lunches The city places a lot of emphasis on promoting the health of its older citizens and designs individual exercise programs with many partners With the Academy for the Elderly , Heidelberg also has a renowned inde pendent educational institution for people over 60 that offers courses ranging from astronomy to learning Chinese to creative writing For more information, visit The city of Heidelberg aims to enable older people to live independently in their own homes for as long as possi ble For example, numerous outpa tient social services offer house keeping or nursing services The Heidelberg Care Support Point provides advice and arranges outpa tient services phone 06221 58 49000 , pflegestuetzpunktheidelberg.de Information is also available at www heidelberg.depflege  Structural chang es can also help people stay at home Information on financing conversions, for example with the help of the citys Barrier free Lifetime Homes subsidy program, is available from the City of Heidelbergs housing counseling ser vice phone 06221 58 25300 , wohn-beratungheidelberg.de Those in need of care and their rela tives can use a traffic light system at , showing whether or which nursing homes in Heidelberg currently have places available A contact form ena bles initial contact with the relevant facilities The citys projects aim to empower older people, enable seniors to live as full and independent a life as possible in old age, and prevent loneliness Leading the way in climate protection 31 30 Leading the way in climate protection Heidelberg is committed to lasting environmental, nature, and climate protection and plays a pioneering role in this area internationally The framework for this is provided by the 30 point Climate Protection Action Plan, which covers all areas of life Leading the way in climate protection Climate Protection Action Plan Heidelberg has a pioneering role in environmental and climate protection, which the city intends to expand fur ther The goal Heidelberg wants to be climate neutral by 2050 at the lat est This means that the city wants to reduce C O2 emissions by 95 percent and cut the municipalitys energy re-quirements by half On the way to achieving this, Heidelberg launched a major climate protection action plan in November 2019 with the first 30 concrete proposals This action plan sets out targets and priorities within the Master Plan 100% Climate Protection www heidelberg.demasterplan100 The proposals affect all areas of life, from construction and housing, nu-trition and consumption to nature oriented urban design and mobility hd4climate The city can and wants to achieve these changes only together with its citizens and with stakeholders from business and society All new citizens are cordially invited to use their move here as a starting signal for more commitment to climate protection Energy consulting and funding programs Interested parties can easily deter mine their personal C O2 footprint using the Heidelberg C O2 Mirror at  Here you can find individual suggestions on how to save carbon dioxide The subsidy program for the rational use of energy promotes thermal insulation of the house and ventilation with heat re-covery There are also support programs for sustainable water management and environmentally friendly vehicles Infor mation is available at defoerderprogramm  The citys free energy advice hotline provides citizens with expert advice on all aspects of energy conservation and climate protection 06221 58 18141 and klimasuchtschutzheidelberg de It is supported by the Heidelberg- Rhein Neckar Kreis g Gmb H climate protection and energy consulting agency  For further information, visit Anyone interested in generating electric ity from solar energy on their own roof can get free personal advice whether as an owner, landlord, tenant or compa ny information on the solar campaign at  The installation of photovoltaic systems on roofs and facades is also being pro moted to encourage private individu als, housing associations, businesses and farmers to exploit the solar power potential in Heidelberg For more in formation, visit foerderprogramm Leading the way in climate protection 33 Nature and landscape When it comes to preserving biodiver sity, ecological aspects play the leading role in Heidelberg For example, as part of the species protection plan, mainte nance measures are carried out in eco-logically valuable areas of the district to preserve the habitat of rare animal and plant species In addition, with the help of the program for biotope networking, connecting structures in the form of meadows are created in agriculturally used areas Animals can use these as migration corridors More information on these and other con servation activities is available at www heidelberg.denaturundlandschaft  Sustainable consumption With projects relating to the topic of sustainable consumption, the City of Heidelberg would like to encour age citizens to become aware of their responsibility as consumers and to contribute to sustainable development in their everyday purchasing decisions Among other things, Heidelberg has been designated a Fair Trade Town  Since 2021 , on the initiative of the city, producers of regional food from Hei delberg and the region have been advertising their products with the new Genial regional label In addition to its commitment to fair trade and regional products, the city also wants to promote the use of organic foods As part of the Bio in Heidelberg campaign, various projects are being implemented to this end together with educational institutions as well as fa cilities along the value chain from farmers to retailers to restaurants Two shopping guides with tips on sus tainable purchasing and consump tion alternatives are available at www heidelberg.debio  Passive house construction Bahn stadt With Bahnstadt, Heidelberg has launched an outstanding climate pro tection project that serves as a global role model Passive house construction is the standard for the entire district so far, this is unique in Germany Heidel berg aims to become a climate neutral city by 2050 at the latest Sustainable energy With its 20202030 energy concept, Stadtwerke Heidelberg is expanding a climate friendly energy supply The wood fired combined heat and power plant, which supplies the Bahnstadt district with heat and electricity, was a first building block Further plants and an energy and future storage facility are being built in the Pfaffengrund energy park Education for sustainable devel opment The Education for Sustainable Develop ment program teaches children about the impact of their own actions on the future Numerous projects ensure that E SD topics are integrated into everyday school life In the E-Team Project , for example, students work to protect the climate Mobility projects motivate children to travel in a healthy and environmentally friendly way At the popular Natrlich Heidelberg series of events, children and adults learn how important nature conser vation and environmental protection are Information is available at www heidelberg.debne 32 Leading the way in climate protection The Bahnstadt district is characterized by passive house construction and is an international lighthouse project Little people, big impact It is never too early to be a climate activist Transport for everyone 35 34 Transport for everyone Transport for everyone Environmentally conscious mobility is important to the city Heidelberg is therefore promoting the expansion of local public transport and the cycling network With the Heidelberg Mobility Network , the street car network is being comprehensively expanded, including the reconstruction of the central station stop and the new streetcar to Bahnstadt Since January 2019 , Heidelberg has been the first municipality in the region to use electric buses in the city center Local public transport Just under a third of all Heidelberg residents primarily use public trans-portation With the Heidelberg Mobility Network, the streetcar network has been extensively expanded in recent years, including the construction of a new streetcar route through Bahnstadt and the modernization of the central station stop For more information, visit For public transport timetable informa tion, visit  Since 2016 , the city of Heidelberg has been promoting the switch from car to public transport with an annual ticket for local public transport for citizens who voluntarily deregister their vehicle Cycle paths 33 percent of Heidelberg residents reg ularly use bicycles in the city center Awarded the title of Bicycle Friendly Municipality by the state, Heidelberg is a model municipality for the Rad K UL TU R initiative of the Baden Wrt temberg Ministry of Transport and In frastructure The city wants to further increase its attractiveness for cyclists and launched the bike offensive a whole bundle of measures to improve cycling Good connections to highways and air traffic Even distant destinations can be reached quickly and easily By car, you can be on the Autobahn 5 in just a few minutes Within just 50 minutes, you can reach Frankfurt Airport and Stuttgart in 90 minutes With the long distance train service, the journey is even faster In addition, long distance buses serve numerous destinations throughout Germany World class research 37 World class research Heidelberg is one of the most important centers of science worldwide Heidelberg is one of the worlds top locations for up and coming industries such as biotechnology and organic elec tronics Excellent cutting edge research clusters and renowned research insti tutes such as the European Laboratory for Molecular Biology E MB L, the Ger man Cancer Research Center D KF Z and three Max Planck Institutes contribute to Heidelbergs outstanding worldwide reputation as science hub As Heidelberg Mannheim Health Life Science Alliance , the two medical faculties and university hospitals as well as the renowned research institutions in the region form a unique alliance in Germany The goal to make the Rhine Neckar region an international center of excellence in life sciences, medical technology and health care industry This should also raise the already first class patient care onto a new level Since 1901 , 57 personalities whose lives are connected to Heidelberg, have re ceived the Nobel Prize The most recent one, the Nobel Prize of Chemistry, was awarded to Professor Dr Stefan W Hell for his development of super-resolution microscopy 34 percent of all those who work and pay statutory social security contribu tions in the city hold a degree Both Heidelberg and the Rhine Neckar metropolitan region are characterized by close cooperation between science and business This is reflected in a large number of interdisciplinary research centers such as the Heidelberg Tech nology Park, the Industry on Campus projects between Heidelberg University and different companies or in the office and laboratory buildings Sky Labs and Sky Angle in the new Bahnstadt district The exceptional role of science in Hei delberg also benefits other aspects of Heidelberg life enabling daycare centers and schools to offer special ac tivities, creating exciting local jobs and profiting those working in the creative industries The city wants to leverage this effect further The International Building Exhibition Heidelberg I BA is dedicated to building and exploring the knowledge city of the future with its motto Knowledge creates City With the new Heidelberg Congress Center in the Bahnstadt district, con gresses in science and business will find a new home This will enable the Stadthalle in the old town to concen trate on its strengths as a concert and cultural center again The Stadthalle is currently undergoing extensive ren ovation Founded in 1386 , the University of Heidelberg is the oldest university in Germany Its leading role in the academic scene in Germany and abroad is reflect ed in its classification and promotion by central and state government as an university of excellence According to the Shanghai ranking, Ruperto Carola is reg ularly the best ranked German university and counts among the worlds Top 50 38 , 000 students take advantage of the courses offered by the various universi ties, cooperative education institutes and other higher education establishments in Heidelberg including the Heidelberg University of Education P H Heidel berg, the private S RH University and the University of Jewish Studies 36 World class research The European Molecular Biology Laboratory E MB L, one of the worlds best known biological research laboratories, has its headquarters in Heidelberg A safe city Fighting fires, saving lives, ensuring traffic safety, settling disputes there are many aspects to making sure that Heidelberg is a safe place to be around Youth fire brigade and volunteer fire department In addition to the professional fire department with its more than 116 full time firefighters, more than 400 citizens of the city of Heidelberg are involved on a voluntary basis in eight departments of the volunteer fire department The childrens and youth fire brigade has around 220 members, around a third of whom are girls In addition to firefighting lessons, they meet for joint activities Working together For a safe Heidelberg and for an op timal response when disaster strikes, the city administration, police and res cue services work closely together The municipal public order service Kommunaler Ordnungsdienst, for example, makes sure that bars and shops in the old town close on time and responds to disturbances of the peace The municipal enforcement service Gemeindevollzugsdienst monitors parking and adherence to the rules of the road To protect against flooding, the citys civil engineering office works not only with the fire department and the municipal public order service but also with the relief organiziation D LR G as well as the civil protection organ ization Technisches Hilfswerk and, if necessary, the Red Cross, the Malteser emergency service and the Army Coping with disasters Tips are available in the Disasters Alarm guidebook, which is available free of charge and can be download ed from  Official warnings can be received directly on your smartphone via the warning app N IN A The app also provides warnings of severe weather or flood situations as well as advice on the correct reaction to such weather events It is available free of charge for Android and i OS Always think about the people around you Inform them in case of an emer gency situation and support them if they need help The Heidelberg Fire Department Around the clock, the men and women of the Heidelberg Fire Department are on duty 24 hours a day to provide quick and effective help in emergencies They are called out 2 , 500 times a year to fight fires, rescue citizens from road accidents and, increasingly, help people affected by extreme weather situations On aver age, they save between 20 to 25 people a year out of life threatening situations Doing this job properly requires inten sive training, including annual training sessions in the fire stations special gas mask training facility, as well as state- of the art equipment Three turntable ladders help rescue people from higher floors In the districts of Pfaffengrund, Wieblin gen and Ziegelhausen, new fire stations have been built In October 2020 , the new integrated control room started operations, coordination all calls to the emergency number 112 in Heidelberg and the Rhein Neckar region It is the operations center for non police emergency response for Heidelberg and the Rhine Neckar district 38 A safe city More than 600 members of the D LR G Heidelberg are committed to improving safety on and along the Neckar River A safe city 39 Homes for everyone 41 Fighting discrimination It is important to the City of Heidelberg that all of its residents live together harmoniously The Office for Equal Opportunity is the point of contact for anyone who wants to take action against discrimination or who has ex perienced it themselves whether on basis of their place of origin, a disa bility, age, sexual identity or gender Further information as well as advice and support are available by calling 06221 58 15550 , by e-mailing antidiskri-minierungheidelberg.deand looking online at in cases of domestic violence The Heidelberg Intervention Model against violence in relationships H IM supports those affected by domestic violence to escape the cycle of violence Women and children can turn to the Womens Intervention Center for help and counseling The Mens Intervention Center and the mens hotline offer counseling and therapy for perpetrators of violence as well as support and counseling for men who have experienced violence themselves Further information is available at www heidelberg.dehim 40 A safe city Homes for everyone Since 2012 , just under 3 , 900 new apartments have been built in Heidelberg This has been made pos sible by the construction of new neighborhoods such as Bahnstadt as well as the reuse of former U S military sites Dynamic growth Heidelberg is a popular place to live and work However, this popularity puts a strain on the housing market To create sufficient living space for people of all income groups, the city council unanimously adopted the Housing action program An annual 800 new apartments are to be built in the com ing years In addition, the city runs two subsidy schemes for tenants and buyers and builds special family friend ly apartments in the new district of Bahnstadt Moreover, the development of affordable housing on the former U S military sites is a top priority Bahnstadt is particularly popular with families and is Heidelbergs district with the most children.housing The first residents moved here in the summer of 2016  The plans for former U S hospital also consist mainly of residential buildings On the site of Patton Barracks, the Heidelberg In novation Park is being developed and Patrick Henry Village is to become a model city district of the future for up to 15 , 000 people For further informa tion, visit homedevelopconversion I BA Heidelberg Knowledge creates city The Heidelberg International Building Exhibition I BA Knowledge Creates City illuminates urban planning in the knowledge city of the future over a period of ten years until 2022  The I BA covers locations throughout the whole city and many areas of life One of the projects, which involves a substantial public consultation element, is the development of a concept for the further use of Heidelbergs largest conversion area Patrick Henry Village For further information, visit heidelberg.deenglish  Bahnstadt A lively mix of housing, science and business The new district of Bahnstadt has model character it is the largest passive house settlement in the world With a vibrant mix of housing, science and commerce, the district creates new spaces to work and live in Ultimately, Bahnstadt will be home for around 6 , 800 people and pro vide jobs for 6 , 000 , mainly in research and science based companies With a total area of 116 hectares, the Bahnstadt is larger than the entire old town of Heidelberg and one of the largest urban development projects in Germany In addition to green and open spaces, streets and watercourses as well as kindergartens, an elementary school, a community center, a cinema and a local shopping center have been created The new Europaplatz will connect the district with the central station The new conference center is also being built here For further information, visit The former U S Army sites A once in a lifetime opportunity The withdrawal of the U S Army repre-sents a unique opportunity for Heidel berg 180 hectares are now free for new developments for affordable housing, cultural and recreational offers, inno vative scientific institutions, attractive business premises and much more There are five former U S sites to be redeveloped Mark Twain Village Campbell Barracks 43 3 hectares in Sdstadt, U S Hospital 9 3 hectares in Rohrbach as well as Patton Barracks 14 8 hectares, Patrick Henry Village 97 2 hectares and the Airfield 15 6 hectares in Kirchheim The site in the southern part of the city is primarily used for affordable 42 Homes for everyone Homes for everyone 43 Future in a nutshell the Heidelberg Innovation Parc hip on the premises of the former Patton Barracks Patrick Henry Village P HV is to be further developed into a sustainable 16th district and become a model location for the use of digital technologies, innovative mobility concepts and climate neutral energy supply Embracing diversity 45 44 Embracing diversity As a Rainbow City , Heidelberg is taking decisive steps to strengthen L GB TI Q Since 2016 , the members of the Sexual and Gender Diversity Round Table have been advising the city administration Around 15 local L GB TI Q initiatives contribute their expertise here in order to work out fundamental and current problems of L GB TI Q and to counteract exclusion and disadvantage through appropriate recommendations With projects such as the Trans Action Weeks Rhine Neckar, the cities of Heidelberg and Mannheim offer a plat form to raise awareness and educate about trans persons and the diversity of gender identities and their forms of expression With the Intercultural Center , Heidelberg is creating a place for cultural participation that promotes cohesion in the citys society and the integration of people with a history of migration through various participation formats and an intercultural program With the Interreligious Dialogue , the city facilitates a successful exchange of religions in Heidelberg In order to ensure that the interests of foreign residents are also heard, the Migration Advisory Council has had an advisory function in the city council for more than 20 years Further information can be found at integration  The City of Heidelberg is intensively committed to the equal partici pation and self determination of people with disabilities This includes barrier free communication as well as comprehensive counseling and care services, the Wohnberatung housing counseling as a specialist office for barrier free planning, building, and living, or the improvement of mobility, for example through the wheelchair cab The municipal representative for the disabled is the primary contact person for concerns of people with disabilities and their relatives and also advises the administration on matters of inclusion Contact behindertenbeauftragteheidelberg.de, 06221 58 15590  Further information at  Embracing diversity Heidelberg is diverse people from 180 nations, with different gender and sexual identities, old and young, people with and without disabilities live here With numerous offers, measures and projects, the city is working with many partners to ensure that all people here have the same opportunities and can feel at home in Heidelberg Getting the public involved 47 46 Volunteers a vital asset for Heidelberg Heidelberg values its volunteers with the Civic Engagement Coordination Office Koordinierungsstelle Brgeren gagement as part of the mayors of fice, there is a central contact point for voluntary work in the city hall Once a year, it awards the honorary plaque Ehrenamtsmedaille for outstanding civic commitment The volunteer agency Freiwilligen- Agentur of the Der P AR IT TI SC HE charity is an important municipal part ner and an important contact for citi zens who would like to get involved An online database of volunteering opportunities can be found at www freiwilligenagentur-heidelberg.de The City of Heidelberg wants citizens to be involved from as early a stage as possible in decision making on municipal projects, for example the redevelopment of the former U S Army sites and the master plan for the Neuenheimer Feld area To ensure the smooth running of this process, the city has drawn up guidelines for public participation Leitlinien fr mitgestaltende Brgerbeteiligung in consultation with the public The people of Heidelberg also have access to a planning list Vorhabenliste that is updated regularly with details of current and forthcoming municipal projects As well as indicating the stage a project has reached, the list also includes information on project costs and names relevant contacts In this way, it facilitates dialogue between the city and its residents form an early stage of a project You can direct any questions you have about public participation in Heidelberg to the mu nicipal coordination point Koordinierungss telle For more information, visit berg.deinvolvement Volunteers a vital asset for Heidelberg For more than 15 years, the city administration has been supporting voluntary work Getting the public involved Getting the public even more involved in municipal decision- making is very important to the City of Heidelberg The Civic Engagement Coordination Office is happy to answer any questions about vol untary work in Heidel berg Please visit www heidelberg.debuerger- engagement for further information 48 Partners around the world C AM BR ID GE M ON TP EL LI ER B AU TZ EN R EH OV OT KU MA MO TO HA NG ZH OU S IM FE RO PO LP AL O A LT OPartners around the world Heidelberg entertains partnerships with eight cities on three continents Contribution towards international understanding As an international city with almost 12 million guests a year and about 45 , 000 citizens with a migrant back ground, Heidelberg has eight partner ships with Palo Alto in the U S, with Hangzhou in China, with Montpellier in France, with Cambridge in Great Britain, Rehovot in Israel, Bautzen in Germany, Simferopol on the Crime an peninsula and with Kumamoto in Japan Palo Alto and Hangzhou are Heidel bergs youngest partner cities Palo Alto in the U S state of California is considered the capital of Silicon Valley Hangzhou is the location of major scientific and educational in stitutions and is particularly strong in the high tech fields of biomedicine, informatics and digital new media In addition, there are friendly relations with the cities of Mostar Bosnia Her zegovina, Jelenia Gra Poland and Heidelberg South AfricaGauteng Province Granting free entry to the Kurpflzisches Museum valid for 2 adults and 2 children Can be exchanged for a library card for the Public Library Stadtbcherei valid for 3 months Voucher Voucher Discover the past The unique collections at Kurpflzisches Museum, from art, handicrafts and archaeological finds to fascinating insights into Heidelbergs history, take visitors on a thrilling journey of discovery through the millennia We look forward to your visit Reading, listening, watching Free library card valid for 3 months At our library you will find 230 , 000 books, more than 300 international magazines and newspapers, C Ds and D VDs, superb premises, help and advice, 46 opening hours per week, a 24 hour digital library service, a mobile library, events, digital workstations, a library caf, user information and parking area Heidelberg Public Library Poststrae 15 69115 Heidelberg Phone 06221 58 36100 stadtbuechereiheidelberg.de times Tuesday to Friday 10 00 am 8 00 pm Saturdays 10 00 am 4 00 pm Nearest public transport Stadtbcherei, Rmerstrae Kurpflzisches Museum Heidelberg Hauptstrae 97 69117 Heidelberg Phone 06221 58 34020 kurpfaelzischesmuseumheidelberg.de times Tuesday to Sunday 10 00 am 6 00 pm Mondays Closed Nearest public transport Universittsplatz, Peterskirche Imprint City of Heidelberg Marktplatz 10 69117 Heidelberg Text Public Relations Office Design City of Heidelberg, Brand Communication Photos Tobias Dittmer Cover and Pages 8 , 28 , 30 , 34 , 46 Julian Beekmann Pages 3 , 4 , 6 Steffen Diemer Pages 16 , 18 , 22 , 41 , 50 o Tobias Schwerdt Page 19 u D NA Collective Page 21 Sven Ehlers Stadtwerke H D Page 23 Envato.com Page 24 Philipp Rothe Page 26 Christian Buck Pages 32 , 33 , 42 , 49 u Kinga Lubowiecka E MB L Page 36 Lukas Rbenacker Page 38 Lukas Schtz D LR G H D Page 39 K CA P Page 43 Uwe Anspach Page 44 Gattner K MH Page 49 o 2020 City of Heidelberg All right reserved Reprints including excerpts only with the explicit per mission of the City of Heidelberg Edition 1st Edition, June 2022 Links Heidelberg City Administration City of Heidelberg website Online citizens service Calendar of events Heidelberg App City of Heidelberg on social media of Heidelberg Marktplatz 10 69117 Heidelberg Phone 06221 58 10580 Fax 06221 58 10900 stadtheidelberg.de\n",
      "City of Heidelberg Municipal Integration Plan Summary City of Heidelberg Municipal Integration Plan Summary Page 2 Foreword by Dr Eckart Wrzner , Oberbrgermeister Lord Mayor Heidelberg is the first local authority in Germany to carry out a representative surve y on the current situation of migrants in the city We wanted to find out more about the circumstances, points of view and attitudes of a group of people which makes up more than a quarter of our resident urban population Were proud of the findings from this survey as most of those questioned do not sense that there are any serious problems with integration They also afford us a complex insight into the current situation of migrants in our city We have conducted a targeted evaluation of the results and are concentrating our resources on those areas where there is a particular need for action We want to be more effective in capturing migrants many and varied talents and potential for the local community and providing them with the prerequisites for li ving as members of our multi ethnic German society who are involved in it and make an active contribution to it without forgetting their cultural roots Foreword by Wolfgang Erichson, Brgermeister fr Integration, Chancengleichheit und Brgerdienste, Deputy Mayor for Integration, Equal Opportunities and Citizens Services The world in one city The philosopher, Karl Jaspers, who worked in Heidelberg coined the phrase Heidelberg a spiritual life form and chose the image of The world in one cit y for Heidelberg He wanted to make it clear to us that the image of a city is characterised not just by its architecture and monuments but in particular by its people For us, this means investigating and scrutinising the measures currently carried out i n the area of integration With the Heidelberg Municipal Integration Plan, we want to spell out that Heidelbergers with and without a history of migration have the same interests in common There is no clash of interests between indigenous people and immi grants In fact, they have a common interest in the successful inclusion of people with a history of immigration and in their full involvement in the opportunities available in education, employment and for their own advancement This requires everyone indigenous people as well as immigrants to get involved in everyday life in the citys districts and clubs, at school and in the workplace This doesnt mean that were leaving it as a vague statement of intent we have agreed targets and measures and set priorities in a number of areas The key thing is that we concentrate our efforts particularly on the section of immigrants who suffer from significant integration needs Our aim is to ensure that our endeavours are targeted at meeting these needs To this end, we must actively confront obstacles to integration such as insufficient German language skills, unemployment and dropping out of educational and professional training We will monitor developments on a regular and structured basis and carry out progress checks I would particularly like to thank all those people who have provided critical support and been actively involved in the process of drafting this plan and have thus contributed to its success Foreword Michael Mwa Alli Madi, Vorsitzende r des AuslnderratsMigrationsrats Chair of the Foreig ners CouncilMigrants Council If you open your door, youll have a big house-sogoes an African saying In a figurative sense, Heidelberg is a big house in many ways The city can put one reaso n for its bigness down to the way in which for hundreds of years it has opened its door to strangers not just to tourists but also to those who make a lasting contribution Its current reputation, its academic excellence and its wealth would be inconce ivable without migration The Municipal Integration Plan now underway is an ambitious attempt to realise a consistent City of Heidelberg Municipal Integration Plan Summary Page 3 integration policy by means of a coherent steering instrument Measurable targets and transparency will ensure that there is no regressi on through complacency The Foreigners CouncilMigrants Council is a central interface for reaching the people and is an interface for the Citys beneficent measures as well However, more structures are needed beyond that, such as strengthening migr ant community organisations and creating a central intercultural centre as a meeting place and advice centre The Foreigners CouncilMigrants Council welcomes the Municipal Integration Plan and will provide critical and constructive support in its imple mentation in the interests of the people with a migration background who live here City of Heidelberg Municipal Integration Plan Summary Page 4 1  The City of Heidelbergs potential and strategy There are two ways of looking at Heidelberg as a cosmopolitan and multicultural city of science where two -thirdsof its migrants have university entrance qualifications there is the approach based on potential and the perspective which looks at shortcomings The City of Heidelberg is pursuing the strategy of continuing to translate proven approaches into sustai nable structures In this task, its actions are guided by the following statements We promote the participation of all citizens We act to prevent exclusion We address disadvantaged target groups We choose interculturally sensitive pathways We alr eady live out our commitment to tolerance Heidelberg is a home for all We strengthen involvement in communal life We monitor changes and measure our successes We support people who seek asylum here 2  The process so far Integration is embedded a s a guiding principle in the 2015 Heidelberg City Development Plan known as S TE P 2015 which since 1997 has provided the City of Heidelberg with guidelines and targets for responsible local authority policy The essential process steps taken by the City w hen drafting the current Municipal Integration Plan were 1  Stock -takeof the ongoing projects and activities which are available in Heidelberg specifically for people with a migration background 2  Development of recommendations for action in selected area s by committed groups 3  Survey on the current situation of people with a migration background in Heidelberg 3  The current situation of migrants in Heidelberg In order to find out more about the people who come from different cultural groups, the City of Heidelberg arranged to have a representative survey carried out by the Sinus Sociovision Institute The population covered by this investigation were all those people who had a migration background and were resident in Heidelberg from the age of 18  The main results of the survey were The proportion of migrants from highly industrialised countries is disproportionately high in Heidelberg compared to the national average Heidelbergs migrants are significantly younger with 61 per cent between 30 and 60 years of age and there are more people in employment and fewer retired people among them than in the control group of Heidelbergs overall population Compared to the urban population overall, there is a higher proportion of lower net household incomes amon g migrants but also a higher proportion of higher income earners hence, the range of incomes is wider Two thirds of Heidelbergs migrants have qualified for university entrance or passed exams at a similar level a willingness to work hard and a desire f or social advancement feature prominently in the migrant community City of Heidelberg Municipal Integration Plan Summary Page 5 It is clear that the precarious background rooted in tradition to which nationally 47 per cent of migrants belong is considerably smaller in Heidelberg at 17 per cent On the other hand, economically and socially elevated backgrounds are much more prevalent than for the national average In particular, an intellectual and cosmopolitan background is absolutely dominant among Heidelbergs migrants at 48 per cent The results of the survey are available online at the City of Heidelberg website 4  The aim of the City of Heidelberg successful integration Integration requires both a willingness to accept by the majority society and a willingness to integrate on the part of the migrants It is a two sidedprocess Only when we regard the values of the constitution as a common foundation will we be able to cre ate equality of opportunity for all members of society As shown in the Sinus survey, because the composition of the various migrant groups in Heidelberg is becoming increasingly complex, work based around target groups will be a key factor in successfull y carrying out effective integration work The Municipal Integration Plan provides a new opportunity to honour the obligations which Heidelberg undertook back in 2007 with the signing of the E U Charter for Equality of Women and Men in Local Life 5  Area s for action by the City of Heidelberg in respect of integration In line with the nature of integration policy as an across -the board task, the integration measures are widely distributed across the various departments of the City of Heidelberg Six areas for action which relate to integration were selected 5 1 Education and language support General datafacts and figures The level of education in the City of Heidelberg is high However, the school -leavingqualifications of pupils with a migration ba ckground do not correspond to the Citys impressive statistics only 21 per cent qualified for university entrance Abitur , while 18 per cent gained the secondary school leaving qualification leading to H EF E Realschulabschluss and 44 per cent gained th e basic school -leavingqualification Hauptschulabschluss 17 per cent left school without any school leaving qualifications The figure for German pupils was just 3 3 per cent The City of Heidelbergs strengths Early years education, language support and musical education Kommunale Bildungslandschaft Municipal Education Landscape greater networking between local authorities and schools Heidelberger Untersttzungssystem Schule Heidelberg School Support System extracurricular service for under performingchildren Schulkinder helfen Schulkindern Schoolchildren Help Schoolchildren migrants who have succeeded in education act as learning buddies to pupils General education services including the Zweite Heimat Second Home programme run by the Heidelberg Adult Education Centre, education vouchers and the wide range of foreign language literature available at the Public Library Aims Organise integration through education as a two sidedprocess by the majority society and City of Heidelberg Municipal Integration Plan Summary Page 6 migrants whereby both sides are included in the education and support services Implementation planned for 2011 Extend the successful Kinder lernen Deutsch language support programme for the 20102011 academic year to year groups 3 and 4  Gain the supp ort of Heidelbergs schools in improving educational opportunities and establishing fairness in education Implemented since Autumn 2010 Provide greater availability of cultural education activities, such as those of the Music and Singing School, to fami lies with a history of migration and in particular those on low incomes Implemented in 2011 Provide support for adult migrants to enable them to get involved in general education and thus impart the idea of life -longlearning i.e supplementary German language courses, foreign language guided tours in museums and the Public Library implemented in 2011  Ensure recognition of the family language of children with a migration background as a resource and as providing potential and support for multilingua lism among these children and young people in order to maintain and extend language diversity Implemented since 2010 Buddy and mentoring projects 5 2 Training and the job market General datafacts and figures Also i n Heidelberg, foreigners are considera bly disadvantaged compared to Germans in respect of access to employment and are more likely to be affected by long -termunemployment The number of unemployed foreigners has increased since the end of 2008 and currently stands at 10 5 per cent as of 30 0 4 2010  Furthermore, only 7 8 per cent of apprentices in Heidelberg do not have German nationality The City of Heidelbergs strengths Personalised support plans through skills analysis at compulsory secondary schools and special schools Heidelbergs Jugendberufshelfer youth careers advisers supervise pupils at compulsory secondary schools and pupils who receive support from Grade 8 and upwards Support is provided for foreign youngsters in par ticular through the Azubi -Fonds project Specialis t language training at the University of Education for young people with a migration background from the Grade 7 and upwards  The Treff miteinander Come Together project provides services which support personal development and educational and profes sional integration The pilot programme Talent e fr die Metropol region Kooperatives bergangsmanagement Schule und Beruf K M Talent for the Metropolitan Region School to Job Transfer Management Cooperation aims to increase the number of pupils transferring from school to training Companies whose owners have a migration background are recruited as training providers through the Ausbildungsverbund Heidelberg Heidelberg Training Alliance  Aims Increase the training capacity of foreign co mpanies and vocational training opportunities for disadvantaged young people with a migration background Implemented 01 11 2009 31 10 2010 Provide individual support for under performingyoung people from compulsory secondary schools, special school s and vocational training colleges when entering work Implemented since 2010 Make use of local heroes demonstrate opportunities for the professional and social integration of migrants To be implemented in the long term Open up prospects for establ ishing new businesses for Heidelbergers with a history of migration City of Heidelberg Municipal Integration Plan Summary Page 7 who are trained in Germany Implemented from 2011 onwards Ease shortage of skilled workers and integrate skilled people with a history of migration and foreign school -leavingqualificati ons into the job market Implemented from 2011 onwards 5 3 Health and welfare systems The City of Heidelbergs strengths Heidelberg joined Germanys Healthy Cities network in 1991  The Trink dich fit und schlau Drink Yourself Fit and Smart projec t has made drinking water an integral part of the school day at Heidelbergs primary and special schools The Sarah Wiener Foundation project-Frgesunde Kinder und was Vernnftiges zu e ssen Healthy Children and Eating Properly aims to correct th e poor nutrition practices of the fast food generation H EI KE Keiner fllt durchs Netz H EI KE Heidelberger Kinderschutz Engag ement is the name of a new collaborative project between the city and the university to strengthen child protection Parents to be can call at the Frhe Hilfen Early Assistance drop in centre if they are worried about the demands of parenting or need help or advice Since 1993 , the Migration und Gesundheit Migration and Health Working Group has provided the coordin ation for improving health care for migrants Aims Enable trustful contact with doctors in medical dialogues, also in the mother tongue To be implemented in the medium term Produce a leaflet for migrants to improve their knowledge and involvement in t he German health and education systems To be implemented in the medium term Improve the health care of all migrants through a survey of foreign language skills among doctors specialising in general medicine, psychiatry and psychotherapy as well as the respective authorised specialist personnel Implemented in 2011 5 4 Neighbourhoods and voluntary work General datafacts and figures The places where foreign inhabitants live are distributed unevenly across the city They are concentrated in the Ber gheim, Altstadt, Emmertsgrund, Boxberg and Rohrbach districts Nearly a third of todays migrants in Heidelberg arrived between 2000 and 2008 and they come from Eastern Europe and the U SA in particular The satisfaction among Heidelbergers with a migration background with where they live is generally high at 84 per cent however, in the 30 44 age group an above averagenumber tend to be dissatisfied with their current housing arrangements The additional demand for housing in Heidelberg is estimated to be a round 8 , 000 homes by 2020  The City of Heidelbergs strengths The City of Heidelberg advocates adequate housing provision, socially responsible housing development and a social infrastructure The District Management Team in Emmertsgrund wants to develop measures together with the people to upgrade the housing and living conditions in Emmertsgrund The Women the Future Workshops are extending dialogue right across cultural boundaries A series of information events for senior citizens in Turkish informs migrants of stimulating activities and out -patientand in patientservices for older people in Heidelberg The Citys senior citizens centres are meeting places for senior citizens of different nationalities The lter werden in der neuen Heimat Agei ng in a New Homeland Network organises various joint events for migrants and Germans every year City of Heidelberg Municipal Integration Plan Summary Page 8 Since 1997 , the Freiwilligen Brse Heidelberg Heidelberg Volunteers Exchange has operated as a local drop -incentre to promote volunteering in Heidelberg Aims Actively support migrants involvement in volunteering as a result of the approval of the Volunteering Concept To be implemented in the short term Increase identification with the districts of the city and encourage sharing between various popul ation groups Implemented in 2011 Provide more support for migrant community organisations through the provision of additional specialist advice and education services in the medium term and an Intercultural Centre in the long term Mediation between th e expectations and principles of social co -existenceof the different population groups, including steering services, sponsorship schemes and neighbourhood events Implemented in 2011 5 5 Sport The City of Heidelbergs strengths The Integration durch Dialog und Bewegung Integration through Dialogue and Movement Project organises special sports activities for women and men and for children and young people The Kick dich schlau Play It Smart Project aims to support the development of social skills as well as academic performance The Sport fest der Kulturen Sports from All Cultures event provides for insights into other cultures In Heidelberg, many sports clubs are actively involved in integration Aims Create greater transparency in existing activities Implemented in 2011 Provide support for club membership for children and young people from low income families through the Heidelberg Pass  Since Autumn 2010 Increase intercultural skills in sport provide support for participat ion of migrants and in particular migrants in sports clubs Implemented from 2011 onwards 5 6 Culture General datafacts and figures The leisure and cultural landscape in Heidelberg is very diverse with some 70 to 100 or more events per day ranging from exhibitions, world music and visual arts right through to dance theatre Heidelbergs support for culture amounts to a sum of 130 per inhabitant and therefore leads comparable German local authorities The City of Heidelbergs strengths Intercultural activities at the German -American Institute and at the Eine Welt Zentrum One World Centre integration takes place here through sharing and mutual interest Heidelbergs educational institutions, such as the Music and Singing School, Public Library, Office for Cultural Affairs, Kurpflzisches Museum and many others provide access to cultural life The numerous projects of the City of Heidelbergs Z WI NG ER3 Childrens and Youth Theatre make a drama out of life for the target group of adolescent migrants Since 1995 , the Karlstorbahnhof has provided a wide range of activities as a socio -cultural centre and in the intercultural sector City of Heidelberg Municipal Integration Plan Summary Page 9 Aims Provide more networking of existing cultural institutions, awareness -raisingand training for participants in cultural work on intercultural issues Implemented since 2010 Monitor ongoing projects and implement findings compiled during the course of the projects Implemented since the end of 2010 Develop information specific to the target groups about cultural and othe r activities in order to extend participation in cultural life To be implemented in the medium term 6  Over archingissues of integration work 6 1 Intercultural openness Local government is one of the first points of contact in Heidelberg for many mi grants and it strives to develop an intercultural orientation This is supported by a series of professional development training courses on intercultural skills In addition to local government, the Info Caf International or I CI also offers a range of i nformation to newly arrived foreign students 6 2 P R work Integral to our P R work are information and briefings on integration activities and promoting Heidelbergs image as a cosmopolitan and tolerant city The City of Heidelberg promotes integration, op enness and a willingness to be accommodating among the public Reports on integration issues and the organisation of events help to make integration come alive in everyday life 6 3 Heidelberg City of Business and City of Science Heidelberg is well served as a business location by its very high proportion of service industries, mainly characterised by science and research, minimal dependence on economic cycles, stable employment, reputation and image as well as its high proportion of small businesses Thi s is also to the benefit of people with a history of immigration, as the city is able to offer them excellent opportunities for their training and careers The Prognos Zukunftsatlas Branchen 2009 confirmed the City of Heidelberg as having excellent future potential, in particular in the areas of health science and corporate and research services The city has large pioneering projects in the form of the new Bahnstadt District Development and Campus I I The Ruprecht -Karl University was founded in 1386 and is Germanys oldest university thus, it has an international reputation and is the largest employer in the city Other higher education institutions in Heidelberg are the University of Education with its Intercultural Centre of Excellence, S RH Private University Heidelberg and the Hochschule fr Jdische Studien College of Jewish Studies  Heidelberg is also the headquarters of the Academy of Sciences Humanities and numerous major international research institutions, such as the European Molecular Biology Laboratory E MB L, the National Centre for Tumour Diseases N CT, the worlds most modern Ion -Beam Therapy Centre H IT, the German Cancer Research Centre D KF Z and four Max Planck Institutes for Comparative Public Law and International Law, As tronomy, Nuclear Physics and Medical Research Furthermore, the Heidelberg Technology Park is home to more than 80 companies and scientific institutions and around 1 , 400 employees and provides a centre of innovation which is recognised worldwide 6 4 Interfaith dialogue City of Heidelberg Municipal Integration Plan Summary Page 10 Taking part in interfaith dialogue in Heidelberg are the two Christian churches as well as the Jewish community, the Turkish Islamic Culture Association and the Deputy Mayor responsible for integration and equal opportunities It aims to be a platform for an equal, respectful but also critical exchange of views as well as encounter and cooperation in everyday life As well as the Africa Service, the Latino Prayer Circle, the Persian Bible Study Hour, the French language tudes Bibliques an d the Franciscan Childrens Drum Group, the work of the Migration skirche Immigrant Church Project of the Chapel Community outreach ministry should also be highlighted As well as tasks such as the honorary management, pastoral care and theological supp ort of migrant groups, a main focus is the strengthening and networking of womens groups 6 5 Educational sponsorship projects The regional service centre of the Aktion zusammen wachsen of the Parittischer Wohlfahrtsverband currently supports 16 exi sting and newly established sponsorship projects of various associations, clubs, foundations and institutions in Heidelberg which provide comprehensive assistance from authority sponsors and learning support right through to integration support 6 6 Principle demands of the Foreigners CouncilMigrants Council Intercultural centremeeting house In order to improve the coordination of integration measures, an intercultural centre is to be established providing a central point for information, specialis t advice, educational services and an information and communication portal for people with a migration background and migrant community organisations Specialist advice and educational services for migrant community organisations The migrant community or ganisations are to be included in the Citys integration work to a greater extent than before through advice, support and a range of training A specialist advice centre can bring greater professionalism to the work of the organisations Education and la nguage support The language support services of the migrant community organisations are to be regarded as supporting multilingualism in a globally oriented society Additional resources are to be made available for language support services in order to be able to provide services above B1 level There needs to be a greater focus on vocational content here 7  Steering the integration work 7 1 The steering process at the City Administration As an across -the board task, integration requires all the bodies involved to work together The implementation of the agreed integration needs to be steered centrally The steering body is comprised of the local council, the Committee for Integration and Equal Opportunities, the Foreigners CouncilMigrants Council an d the Lord Mayor as the Leader of the Council and is supported at departmental level by the Department of Family, Social Welfare and Culture and the Department of Integration, Equal Opportunities and Citizens Services The implementation of the integratio n measures is also the responsibility of the respective Heads of Office and of our collaborative partners The sharing of experience which is coordinated by the Brgeramt Local Administration Office 1 and the joint development of measures are being exten ded through a steering group which has yet to be set up 7 2 Networking of local actors 1 As of 1st May 2011 , the Equal Opportunities Offi ce has been responsible for this City of Heidelberg Municipal Integration Plan Summary Page 11 In practice, the success of integration activities depends on how these different measures, projects and approaches are linked together The City of Heidelberg will i nvestigate where greater coordination will be required in future Increased transparency between the actors will be a key aim in respect of reducing parallel structures and consolidating resources for specific purposes 7 3 Impact monitoring In view of the fact that the quality requirements for projects are growing at the same time as resources are becoming scarcer, integration activities need to be designed to be especially efficient Central to this will be the demands for concrete results Good impact monitoring requires a willingness to discuss the work on the part of the experts involved, and as a result more precise knowledge about the results and impact over the course of the project can be incorporated into the further planning and implementation, thereby contributing to improving the steering and the prospects for success This monitoring system will be set up by the end of 2011 with the participation of the Foreigners CouncilMigrants Council and in transparent cooperation with the offices invo lved 8  Acknowledgements and looking ahead The City of Heidelberg considers it to be of great importance to take this opportunity to express its thanks to all those who have supported the development of the Municipal Integration Plan 1  Language and education Subgroup Children, School and Family Spokeswoman Mrs Prof Dr Ingrid Dietrich, Pdagogische Hochschule, Interkulturelles Kompetenzzentrum University of Education, Intercultural Centre of Excellence Mrs Hlya Amhari, Auslnde rratMigrationsrat Foreigners CouncilMigration Council Mrs Dr Orietta Angelucci von Bogdandy, H IP PY Mr Nicolas Apfel -Totaro, Jugendgemei nderat Youth Council Mrs Yvonne Bedbur, Pdagogische Hoc hschule, Interkulturelles Kompetenzzentrum University of Education, Intercultural Centre of Excellence Herr Malte Burmester, Jugendgemeinderat City Youth Council Mr Giuseppe Cibella Mrs Ulrike Duchrow, Asylarbeitskreis Asylum Working Group Mrs Renate Emer, Kinder -und Jugendamt Office of Children and Youth Mrs Birgit Fliedner, Kinder -und Jugendamt Office of Children and Youth Mrs Anja Kegler, Kinder und Jugendamt Office of Children and Youth Mrs Renate Kneise, Bezirksbeirtin Member of the District Advisory Council Mrs Dr Marianne Laurig, H IP PY Mrs Catherine Mechler -Dupouey, Interku ltureller Elternverein, AuslnderratMigrationsrat Intercultural Parents Association, Foreigners CouncilMigrants Council Mrs Susanne Meyer, pd -aktiv Mrs Barbara Mn ch, Fachberaterin fr Grundschulen expert advisor for elementary schools Mrs Dr Maria Susana Oder -Pea, Ausl nderratMigrationsrat Foreigners CouncilMigrants Council Mr Sotirios Papadopoulos -Herzhauser, AuslnderratMigrationsrat Foreigners CouncilMigrants Council Mrs Ute Salize, pd -aktiv Mrs Dubravka Santak, Zentrum fr Inte gration durch Bildung Mrs Nora Schnberger, Pan -Afrikanische-OrganisationPan African Organisation Mrs Silvia Selke, Pdagogische Hoc hschule, Interk ulturelles Kompetenzzentrum University of Education, Intercultural Centre of Excellence Mrs Nathalie Sommer, Heidelberg Intern ational School Mrs Margarete Zwink -Eisele, Internation ale Gesamtschule Subgroup Youth and Adults Spokeswoman Mrs Dr Luitgard Nipp -Stolzenburg, Volkshochschule City of Heidelberg Municipal Integration Plan Summary Page 12 Mrs Magdalena Adamczyk, Alpha -Aktiv Sprachschule Mr Nicolas Apfel -Totaro, Jugendgemei nderat City Youth Council Mr Karl Heinz Bareuther, Internationaler Bund Mr Malte Burmester, Jugendgemeinderat City Youth Council Mrs Claudia Emmendrfer Brler, Volk shochschule Mrs Renate Kneise, Bezirksbeirtin Member of the District Advisory Council Mrs Nadine Marschik, Diakonisches Werk Mrs Regine Mitternacht, Stadtbcherei Public Library Mr Michael Weigel, Heidelberger Pd agogium Mrs Antje von Wolff, Kinderbeauftragte Stadtteil Wieblingen Ombudswoman for Children Mrs Yaldir Zleyha, Alpha -Aktiv Sprac hschule 2  Training, job market and higher education Spokesman Mr zkan E rgen, Jugendagentur e G Mr Malte Burmester, Jugendgemeinderat City Youth Council Mrs Drthe Domzig, Amt fr Chance ngleichheit Equal Oppurtunities Office Mrs Viktoria Engelhart, Internationaler Bund Mr Dr Maximilian Eberius, Deutsch -Polnische Gesellschaft German -Polish Association Mr Dr Nihat Genc, Verein zur Frderung des Gedankenguts Atatrks Kemalist Thought Association Mr Joachim Hahn, Amt fr Stadtentwic klung und Statistik Office for Urban Development and Statistics Mr Alexander Hornschuch, Agentur fr Arbeit Employment Agency Mr Jens Katzenberger, Verein zur berufl ichen Integration und Qualifizierung V BI Association for professional Integration and Qualification Mrs Anna Kloppenburg, Akademisches Auslandsam t der Universitt International Office of the University Mr Siegfried Khler, S RH Berufliche R ehabilitation Berufsfrderungswerk Heidelberg Mr Karl Heinz Lhr, Job Center Mrs Sonja Mechler, Heidelberger Dienste Mr Jrg Schmidt -Rohr, V BI Mr Heinz Schorr, Industrie -und Handelskammer Rhein -Neckar, Geschftsstelle He idelberg Rhine -Neckar Chamber of Industry and Commerce, Heidelberg Field Office Mr Leopold belhr, Kreishandwerke rschaft Local Council of Skilled Crafts 3  Family, healt h and welfare systems Spokeswoman Mrs Christine Khl, Gesun dheitsamt Heidelberg Heidelberg Public Health Office Mr Wolfgang Blam, Gesundheitsamt Public Health Office Mr Sylla Bachir Mrs Aysel Celep -Monz Mr Dr Ulrich Deutschmann, Kultur kreis Emmertsgrund -Boxberg Mrs Marion Duscha, Heidelberger Selbs thilfebro Mrs Anja Dhring, Bi Be Z -Bildungint egriert, Beratung erffnet Zukunft Mrs Wiebke Hartmann, Asylarbeit skreis Asylum Working Group Medi Netz Rhein -Neckar-Mr Dirk Ho fmann, Amt fr Sport und Gesundheitsfrderung Office of Sports and Health Promotion Mrs Birgit Kurz, Diakonisches Werk Mrs Annemarie Lerch, Kinderschutzbund Association for the Protection of Children Mr Prof Dr Bernard -M Mechler, Ausl nderratMigrationsrat Foreigners CouncilMigration Council Mrs Iris Mhlhausen, Kinderschutzbund Association for the Protection of Children Mrs Gler Olgun, Trkisch -Islamischer Kulturverein Turkish Islamic Cultur e Association Mr Choukri Rasc ho, Initiative zur Integr ation kurdischer Migranten Initiative for the Integration of Kurdish Migrants Mrs Dr Semra Serdaroglu -Baloch, Inte rnationales Frauen -und Familienzentrum International Womens and Childrens Centre City of Heidelberg Municipal Integration Plan Summary Page 13 Mrs Sadje Srer, Trkis ch-Islamischer Ku lturverein Turkish Islamic Cultur e Association Mrs Susanne Vlker, Bi Be Z 4  Neighbourhoods and voluntary work Spokesman Mr Prof Dr Martin Albert, S RH Hochschule Heidelberg Mrs Annette Diefenbacher, Amt fr Sozi ales und Seni oren Office of Social Matters and Senior Citizens Mr Dr Maximilian Eberius, Deutsch -Polnische Gesellschaft German -Polish Association Mr Karl Emer, Caritasverband Mrs Brbel Fabig, Amt fr Soziales und Senioren Office of Social Matters and S enior Citizens Mrs Heidi Farrenkopf, Diakonisches Werk Mrs Heidi Flassak, Kinderbeauftragte SdstadtWeststadt Ombudswoman for Children Mrs Ulrike Jessberger, Kulturkreis Emmertsgrund -Boxberg, Bezirksbeirtin Boxberg Member of the District Ad visory Council Mr Gerald Kraus, Gesellschaft fr Grund -und Hausbesitz G GH-Mrs Annette Kritzer, Asylarbeitskreis Asylum Working Group Mrs Desiree Knsberg, S RH Hochschule Mrs Ulli Lemann, Freiwilligen Brse Mrs Cecilia Lima -Wst Mrs Dr Heidrun Mollenkopf, Kulturkreis Emmertsgrund -Boxberg, Bezirksbeirtin Emmertsgrund Member of the District Advisory Council Mr Christoph Nestor, Mieterverein Mrs Gabriele Riedke -Dschangaei, Seni orenzentrum Rohrbach Mr Ernst Schwemmer, Arbeitsgemei nschaft Stadtteilvereine Working Group of District Residents Associations Mrs Gudrun Sidrassi -Harth, Asylarbeit skreis Asylu m Working Group Mrs Dr Karin Weinmann -Abel, Kulturkreis Emmertsgrund -Boxberg, Em Box Info Mr Dr Edgar Wu nder, Geographisches Institut Geographical Institute 5  Sport and culture Spokesman Mr Dr Hamdi Galal El -Din, Car itasverband Sportkreis Mrs Alexandra Eberhard, Kulturamt Office for Cultural Affairs Mrs Cornelia Gans, T SG Ziegelhausen Mr Reiner Greulich, Polizeidirektion Police Headquarters Mrs Michaela Gnter, Caritasverband Mrs Saadet Kirici, Trkisch -Islamischer Kulturverein Turkish Islamic Cultur e Association Mr Rainer Rmer, T SG 78 Heidelberg Mr Karlheinz Schrumpf , Turnerbund Rohrbach Mr Ulrich Sudhlter, Budo -Club Emmertsgrund -Boxberg Mrs Ingrid Wolschin, Kulturhaus Karlsto rbahnhof Furthermore the Department of the Lord Mayor 01 , the Personnel and Organis ation Office 11 , the Office for Urban Developm ent and Statistics 12 , the Public Relations Office 13 , the Local Administration Office 15 , the Equal Opportunities Office 16 , the Office for Schools and Education 40 , the Office of Cultural Affairs 41 , the Theatre and Orchestra 44 , the Public Library 45 , the Municipal School of Music and Singing 46 , the Office of Social Welfare and Senior Citizens 50 , the Children and Youth Office 51 , the Office of Sports Facilities and Health Promotion 52 , the City Planning Office 61 , the Landscap e Architects and Forestry Office 67 , the Office of Economic Development and Employment 80 , the Gesellschaft fr Grund und Hausbesitz, the Karlstorbahnhof Cultur al Centre and Heidelberg Technolog y Park have made major contributions to the process of in tegrat ion as an across -the board task  The City of Heidelberg will update the Municipal Integration Plan The aim will be to monitor the implementation of the measures and modify the aims according to changes in the overall conditions In the course of im plementation, the City of Heidelberg will define performance indicators in order to gauge the success of the measures As part of this process, the City of City of Heidelberg Municipal Integration Plan Summary Page 14 Heidelberg will continue to involve the Citys internal and external experts in order to take accoun t of different perspectives, take on board the experience gained and live out intercultural diversity in its own practice as well  Impressum Herausgeberin Stadt Heidelberg Brgeramt und Amt fr Chancengleichheit Telefon 06221 58 15530 email chancengleichheitheidelberg.de Postfach 10 55 20 69045 Heidelberg Stand 10  Februar 2011 bersetzung Bro Kbe Obere Seegasse 29 69124 Heidelberg\n",
      "German Courses in Heidelberg for Immigrants Table of Contents 1  Learning German in Heidelberg 4 2  German Course Offer 6 2 1 Affordable or Free of Charge Language Learning Opportunities by the City of Heidelberg 6 2 2 Free of Charge German Courses and Exam Preparation by Associations and Charitable Institutions 8 2 3 Courses Certified by the Federal Office for Migration and Refugees 12 2 3 1 Integration Course 12 2 3 2 Integration Course for Parents 16 2 3 3 German for Professional Purposes 20 2 3 4 Integration Course Providers in Heidelberg 24 3  Legal Notice 27 Every new language is like an open window that shows a new view of the world and expands your attitude to wards life Frank Harris 1856 1931 Irish American author, journalist, publisher and editor Learning German in Heidelberg 5 4 Learning German in Heidelberg 1  Learning German in Heidelberg In this brochure, you will find an overview of affordable language learning opportunities offered by the City of Heidelberg Stadt Heidelberg, free of charge German courses offered by charitable institutions and fee based German courses that are certified by the German Feder al Office for Migration and Refugees Bundesamt fr Migration und Flchtlinge, B AM F Fee based German courses that are not certified by the Federal Office for Migration and Refugees are not listed in this brochure Some of these courses are listed in an overview compiled by Migration Hub Heidelberg in heidelberg The present overview is not exhaustive It is updated regularly If you have any suggestions for changes, you may contact us via the following mail address sprachfoerderungheidelberg.de This brochure is intended to provide immigrants and professionals working in migration with an overview Affordable or Free of Charge Language Learning 7 Opportunities by the City of Heidelberg6 German Course Offer 2  German Course Offer 2 1 Affordable or Free of Charge Language Learning Opportuni- ties by the City of Heidelberg What does the City of Heidelberg Offer in Terms of Language Learning Anyone coming to Heidelberg to apply for asylum qualifies for financial support from the City to partici pate in a language course at the Adult Education Center Volkshochschule, V HS The course takes a mini mum of four weeks and comprises 100 course hours Participants will learn the basics for day to day commu nication The City of Heidelberg pays for the course Participants may also progress to an advanced course The advanced course also comprises 100 course hours Who is eligible to participate in a language course paid for by the City of Heidelberg You are eligible if you are in preliminary accommodations by the City of Heidelberg, are not of school age, have been living in Germany for a maximum of 15 months, qualify for benefits according to Germanys Asylum- Seekers Benefits Act Asylbewerberleistungsgesetz Applications for these benefits need to be submitted to the Office of Social Welfare and Senior Citizens Amt fr Soziales und Senioren of the City of Heidelberg These courses are intended for everyone who is not eligible to participate in a B AM F integration course How much are the language courses The first language course is free of charge for partici pants The Citys Office of Social Welfare and Senior Citizens pays a part of it The rest can be payed using the Heidelberg Pass Everyone eligible to receive benefits according to the German Asylum Seekers Benefits Act qualifies to receive the Heidelberg Pass The advanced course costs E UR 32  The rest is payed for using an educational voucher by the City of Heidelberg which is funded by donations More information Stadt Heidelberg Amt fr Soziales und Senioren Bergheimer Strae 155 , 69115 Heidelberg Ms Kobs, Phone 49 6221 58 37392 or Mr Frhlich, Phone 49 6221 58 37260 sozialamtheidelberg.de Free of Charge German Courses by Associations and Charitable Institutions 9 Caf Talk Offer in cooperation with Evangelische Kirche Heidelberg, the initiative Weststadt sagt J A, Asyl arbeitskreis Heidelberg e V and Diakonisches Werk Evangelische Kirche Heidelberg Contact Caf Talk Phone 49 176 52085027 or 49 159 04867912 infocaf-talk.com Vangerowstrae 5 , 69115 Heidelberg Caritasverband Heidelberg e V Language courses by volunteers for women from a refugee background Contact and Course Location Caritasverband Heidelberg e V Phone 49 6221 727 81 91 fluechtlingssozialdienstcaritas heidelberg.de Flchtlingsunterkunft Hardtstrae 101 69124 Heidelberg Caritasverband Heidelberg e V Diakonisches Werk der Evangelischen Kirche Heidelberg Volunteers offer language courses for the inhabitants of the refugee arrival centre Ankunftszentrum of the state of Baden Wrttemberg in Patrick Henry Village P HV Contact and Course Location Mr Theisen and Ms Straub Phone 49 6221 739 58 69 phvehrenamtcaritasdiakoniehd.de Independent social counseling and advice on administrative procedures Ankunftszentrum Heidelberg North Gettysburg Avenue 4517 , 69124 Heidelberg8 Free of Charge German Courses by Associations and Charitable Institutions 2 2 Free of Charge German Courses and Exam Preparation by Associations and Charitable Institutions For more information you may reach out to the contact persons listed below The following language learning opportunities are not certified Asylarbeitskreis Heidelberg e V Volunteers support immigrants in learning German they offer language courses, individual 11 language tutoring and help in preparing for exams Contact Ms Sommer Phone 49 6221 182797 asylarbeitskreis-heidelbergt-online.de Plck 101 , 69117 Heidelberg Course Location Flchtlingsunterkunft Henkel Teroson Strae 14 16 69123 Heidelberg Former Hotel Metropol Alte Eppelheimer Strae 80 69115 Heidelberg Welt Haus kleiner Meetingraum Willy Brandt Platz 5 69115 Heidelberg Free of Charge German Courses by Associations and Charitable Institutions 11 10 Free of Charge German Course by Associations and Charitable Institutions Evangelische Gemeinde Bonhoeffer Two language courses in small groups for women childcare is available during class hours of one of the courses Contact Dr Bindseil Phone 49 6221 712248 christiane.bindseilkbz.ekiba.de Evangelische Bonhoeffer Gemeinde Oppelner Strae 2 , 69 124 Heidelberg Diakonisches Werk der Evangelischen Kirche Heidelberg Voluntary integration guides support immigrants in learn ing German by offering individual language tutoring 11  The Sprachmittler Netzwerk Heidelberg Network of In- terpreters Heidelberg offers language support and interprets during official appointments or counseling sessions Contact Ms Arnold Phone 49 6221 53750 integrationsbegleiterdwhd.de Karl Ludwig Strae 6 , 69117 Heidelberg Frauen Forum Emmertsgrund Free of charge German courses for women and girls Contact and Course Location Ms Oedel frauen.initiativegmx.de Information between 11 30 am and 12 00 pm except during school holidays Emmertsgrundpassage 31 , im Jugendcaf 69126 Heidelberg Internationales Frauen-und Familienzentrum Heidelberg e V Contact and Course Location Ms Lindner Phone 49 6221 182334 deutschkursifz-heidelberg.de Theaterstrae 16 , 69117 Heidelberg Interkulturelles Frauencaf, T ES e V Stadtteilma nagement Emmertsgrund Free of charge German courses for women Contact and Course Location Ms Bertolo Phone 49 6221 1394016 stadtteilmanagementemmertsgrund.de Emmertsgrundpassage 11b, 69126 Heidelberg Rotary Club Heidelberg Alte Brcke and Volks- hochschule Heidelberg V HS, project Deutsch Paten German buddy project Voluntary mentors offer additional support for V HS stu- dents of German Contact Ms Reichenbach Volkshochschule Heidelberg Phone 49 6221 911960 reichenbachvhs hd.de Integration Course 13 12 Courses Certified by the Federal Office for Migration and Refugees 2 3  Courses Certified by the Federal Office for Migration and Refugees 2 3 1 Integration Course What is the integration course of the Federal Office for Migration and Refugees B AM F The general B AM F integration course comprises 700 course hours It consists of a language course with 600 course hours on important day to day topics, and an orientation course with 100 course hours, explaining the legal system, history and culture of Germany In addition, there are special types of integration courses for specific groups These are literacy courses, integration courses for young adults, intensive integration courses for quick learners, integration courses for women, integration courses for parents, integration courses for students learning an additional alphabet as well as integration courses for people with disabilities What is the goal of a B AM F integration course The integration course is set to enable participants to speak on a B1 level after completion and to pass the test Life in Germany If you pass these tests, you will receive the Zertifikat Integrationskurs Certificate Integration Course Among others, this certificate is helpful, when people with a migratory background want to apply for a residence title or apply for naturalization and it helps to integrate into the job market Who can participate in a B AM F integration course Ethnic German resettlers, newly immigrated people with a residence title, people from other countries who have been living in Germany for some time, E U citizens, asylum applicants with good prospects to remain, people whose deportation has been temporarily suspended and who have a residence permit You can be allowed to participate in an integra tion course voluntary participation or you can be obligated to participate in an integration course obligatory participation by the Jobcenter, the Em- ployment Agency Agentur fr Arbeit or by official executive bodies of the German Asylum Seekers Benefits Act How much is the integration course Every course hour costs E UR 2 29  Final examinations are free of charge If you receive support like the citizens benefits Brgergeld, social security Sozialhilfe, hous- ing benefits Wohngeld or benefits according to the Asy- lum Seekers Benefits Act, you may apply for reim bursement You may also be eligible for a partial refund of traveling expenses If you pass the test and had to pay for it yourself, you are eligible for a 50% refund, pro- vided you have successfully completed the course within two years You will need to submit an application to that end Ethnic German resettlers do not need to pay a course fee Do you offer childcare during class hours Your language school will advise you on the options for childcare and will, if applicable, inform you about in tegration courses for parents that include childcare offers See also page 16  More information B AM F Regionalkoordinatorin Integration Ms Saka Referat 52 B, A S Karlsruhe Bundesamt fr Migration und Flchtlinge Pfizerstrae 1 , 76139 Karlsruhe Phone 49 911 943 80 269 nuray.sakabamf.bund.de or from any integration course provider in Hei- delberg and online ThemenIntegrationZugewanderte Teilneh- mendeIntegrationskurseintegra tionskurse node.htmljsessionid4 CC1 C39 E89 CB- F3 B61 A278 CF8378830 BB.internet271 Please submit your application to Bundesamt fr Migration und Flchtlinge, Regionalstelle Karlsruhe, Ref 610 , Pfizerstrae 1 , Gebude F, 76139 Karlsruhe Your local integration course providers will help you with your application Integration Course 15 14 Integration Course How do I participate in a B AM F integration course Application to participate in an integration course to be submitted to the Federal O ce for Migra- tion and Refugees vol- untary participation With the Permission Berechtigungsschein or Obligation Cer- ti cate Verp ichtungsschein you will receive information on the assessment language test You need to take this test to be allowed to participate in the integration course Participants will then be told where they can attend the course This course will match their language skills Participants will start the course offered by the provider that was allocated to them or that they have chosen You will nd an overview of all integration course providers in Heidelberg in this brochure Participants will attend the general integration course or a special type of integration course and need to pass the nal examinations Zertifikat Integrationskurs Certificate Integration Course The certi cate comes with legal advantages It is for example required to receive a settlement permit and is helpful if you would like to get naturalized Obligation Certi cate from immigration author ities, Jobcenter, Employ- ment Agency etc oblig- atory participationor Flipchart with exercises in German in a German course Integration Course for Parents 17 16 Integration Course for Parents 2 3 2 Integration Course for Parents What is an integration course for parents An integration course for parents is set to allow parents to attend an integration course, while their children aged between 0 and 5 will be looked after by professional childcare providers on the premises The course con sists of a language course with 900 and an orientation course with 100 units that parents can attend part time over a longer period The language courses revolve around day to day family situations and the German educational system This allows parents to learn how they can best support their child The orientation course centers around laws, values, politics and the history of Germany This course is offered by V HS Heidelberg in cooperation with and with the financial support of the Office of Equal Opportunities Amt fr Chancengleichheit of the City of Heidelberg What is the goal of an integration course for parents To complete the integration course for parents you will take the German Test for Immigrants Deutschtest fr Zuwanderer , D TZ while the test Life in Germany Leben in Deutschland, Li D will conclude the orientation course Participants receive the Zertifikat Integrationskurs Cer- tificate Integration Course, provided they have reached the B1 level and have successfully passed the orientation test Integration Course for Parents 19 18 Integration Course for Parents Who is eligible to attend the integration course for parents Ethnic German resettlers with children, newly immigrated people with a residence title and with children, people with children from other countries who have been living in Germany for some time, E U citizens with children, asylum applicants with good prospects to remain and with children How much is the integration course for parents with childcare Every course hour costs E UR 2 29  Final examinations are free of charge If you receive support like citizens benefits Brgergeld, social security Sozialhilfe, hous- ing benefits Wohngeld or benefits according to the Asylum Seekers Benefits Act, you may apply for reim bursement You may also be eligible for a partial refund of traveling expenses If you pass the test and had to pay for it yourself, you are eligible for a 50% refund provided you have successfully completed the course within two years You will need to submit an applica tion to that end Ethnic German resettlers do not need to pay a course fee Childcare is free of charge for all participants More information Volkshochschule Heidelberg e V Bergheimer Strae 76 69115 Heidelberg Ms Reichenbach Phone 49 6221 911960 reichenbachvhs hd.de German for Professional Purposes 21 20 German for Professional Purposes 2 3 3 German for Professional Purposes What is a German for professional purposes B AM F course and what is its goal German for professional purposes builds on the integra tion courses Participants learn German used in a work context There are different modules Basic module 400 and 500 units for the general professional language, modules for subject specific contents 300 units and modules specifically for people who are undergoing the procedure to have their foreign professional qualifi- cation recognized 600 units You may also combine the language course with qualification measures of the Federal Employment Agency Bundesagentur fr Arbeit Who can participate in a German for professional purposes B AM F course First Condition Participants belong to one of the following groups Immigrants, including refugees that are undergoing the procedure to have their foreign professional qualification recognized and have good prospects to remain, people whose deportation has been temporarily sus pended according to the German Residence Act Aufenthaltsgesetz Section 60a Subsection 2 Clause 3 , E U citizens, Germans with a migratory background Second Condition Participants also belong to one of the following groups People who are registered as jobseekers, as unem ployed or as looking for an apprenticeship andor people receiving citizens benefits or unemployment benefits benefits according to the German Social Code, Sozialgesetzbuch I I or I II people in employment, people who are undergoing the procedure to have their foreign professional qualification recognized and apprenticestrainees Third Condition Participants have already completed an integration course and or they speak German at B1 level but need more language skills for their job The Emplyoment Agency, the Jobcenter and in some cases the B AM F will decide on who will participate in the course German for professional purposes How much is the course In general, German for professional purposes is free of charge If have a job and you do not receive any other social benefits, you need to pay 50% of the fees currently E UR 2 42 per course hour More information B AM F, Ms Lang Regionalkoordinatorin Berufsbezogene Sprachfrderung Wolframstrae 62 , 70191 Stuttgart Phone 49 911 943 73942 B SK Stuttgartbamf.bund.de Agentur fr Arbeit Heidelberg Kaiserstrae 69 71 , 69115 Heidelberg Phone 49 800 4555500 Jobcenter Heidelberg Speyerer Strae 6 , 69115 Heidelberg jobcenter heidelbergjobcenter ge.de22 German for Professional Purposes German for Professional Purposes 23 Integration Course Providers in Heidelberg 25 24 Integration Course Providers in Heidelberg 2 3 4 Integration Course Providers in Heidelberg German for Professional Purposes Integration Course Integration Course for Parents Berlitz Deutschland Gmb H Mr  Lieder Phone 49 6221 164004 christian.liederberlitz.de Sofienstrae 7a, 69115 Heidelberg B BQ Bildung und Berufliche Qualifizierung g Gmb H Mr  Knanbi Phone 49 6221 8907725 knanbi.marouanebiwe.de Eppelheimer Strae 13 , 69115 Heidelberg FU Academy of Languages g Gmb H Ms Wagner Phone 49 6221 912034 tatjana.wagnerfuu.de Hauptstrae 1 , 69117 Heidelberg Heidelberger Dienste g Gmb H Ms Engelsberg Phone 49 157 71461202 engelsberghddienste.de Hospitalstrae 5 , 69115 Heidelberg Heidelberger Pdagogium Mr  Weigel Phone 49 6221 45680 infoheidelberger-paedagogium.de Schrderstrae 22a, 69120 Heidelberg Internationaler Bund e V Ms Egelhof Phone 49 6221 3169531 Sprachkurse Heidelbergib.de Belfortstrae 2 , 69115 Heidelberg Offer includes an integration course for young people between 16 and 26 U SS Gmb H Ms Weibach 49 6221 9987840 deutschuss.de Englerstrae 6 , 69126 Heidelberg Volkshochschule Heidelberg e V Ms Trschmann Qataoui Phone 49 6221 911988 dafvhs-hd.de Bergheimer Strae 76 , 69115 Heidelberg Gebrden Verstehen e Kfr Only language courses specifically for the deaf Ms Fll Phone 49 6221 7287478 heidelberggebaerdenverstehen.de Legal Notice Stadt Heidelberg Amt fr Chancengleichheit Fachbereich Integration Sprachfrderung Bergheimer Strae 69 69115 Heidelberg sprachfoerderungheidelberg.de Layout Stadt Heidelberg, Markenkommunikation Translation Susanne J Schneider Photos Cover, pages 5 , 12 , 26 Annette Schiffmann, Asylarbeitskreis Heidelberg e V Page 4 Heidelberg Marketing Gmb H Pages 67 , 13 Philipp Rothe Pages 1617 , 1819 , 2021 , 2223 ridoshutterstock.com 2023 Stadt Heidelberg  Reproduction in part or in whole only allowed with the express authorization of the City of Heidelberg Stadt Heidelberg Edition Oktober 2023 , 3rd edition Stadt Heidelberg Bergheimer Strae 69 69115 Heidelberg Phone 49 6221 58 15500 Fax 49 6221 58 49160 chancengleichheit heidelberg.de fr Chancengleichheit\n",
      "Bahnstadt The place to be in the science city of Heidelberg www bahnstadt.heidelberg.de Dear reader, On a site that was once a marshaling yard for freight trains, we are today developing one of the worlds largest passive house settlement Bahnstadt Located in central Heidelberg, the district builds on the tradition of European cities of science, offering an attractive mix of residential life and research, leisure and culture, all in the same district 6 , 000 jobs are being created here, predominantly in research and science based companies On an area covering more than 100 hectares, this zero emission district is one of the largest urban development projects in Germany As a science hub, Bahnstadt projects a unique sense of dynamism, attracting both high tech companies with their research and development depart- ments and private universities The offi ce and laboratory buildings Sky Labs and Sky Angle created by the foundation Max Jarecki Stiftung are hothouses for innovation as is the Innovation Lab of the leading edge cluster Organic Electronics The Heidelberg Technology Park offers outstand- ing development opportunities for start ups at two locations in Bahnstadt Furthermore, the new conference center will provide a home for interna tional congresses and trade fairs in the science city of Heidelberg Nearby, the Heidelberg Innovation Park H IP is a breeding ground for innovation in the fi elds of I T, digital media and bioinformatics Spacious green areas, excellent transport links and numerous child daycare facilities, leisure activities and shops make Bahnstadt a highly attractive working environment Other research locations in Heidelberg can be reached in just a few minutes This brochure is intended to provide an overview of the Bahnstadt science hub I hope you enjoy learning more about Heidelbergs district of the future Yours sincerely Prof Dr Eckart Wrzner Mayor Jrgen Odszuck First Deputy Mayor4 Living researching developing Living researching developing 5 Living researching developing 6 , 800 people will be living in the district An estimated 2 billion euros are being invested 50% of Bahnstadt residents are younger than 30 years old Science and commerce, residential living and culture have been closely interwoven in Heidelberg life for centuries This tradition is being continued in Bahnstadt The concept is a successful one An urban environment with daycare centers, a school and shops, excellent transport links, energy effi cient buildings, green areas Heidelbergs Bahnstadt reconciles the needs of employers and employees in a single location It provides suitable commercial premises with appropriate infrastructure for knowl- edge intensive and research based companies, as well as trades and retail enterprises Bahnstadt is one of the biggest urban development projects in Germany and one of the worlds largest passive house settle ment The new district is situated next to Heidelbergs main railway station It is just a few minutes by public transport to other science centers such as Neuen- heimer Feld and the Old Town In future there will be 6 , 800 people living in the district, and 6 , 000 people working there Bahnstadt is growing rapidly, with 4 , 300 people al- ready living here by the middle of 2019  Parents have eight daycare centers to choose from for their chil- dren Another daycare facility is under construction Many shops are already open The B community center at Gadamerplatz is a central meeting point Popular in Heidelberg and beyond, the Halle02 events venue stages concerts, exhibitions and other unique cultural events in what used to be a freight depot All buildings in the Bahnstadt district are constructed in compliance with the passive house standard This dramatically reduces the buildings energy require ments Remaining energy needs are met in an environ mentally responsible manner A fi rst step in this re gard was the construction by Heidelbergs public utility company of a wood fi red C HP plant, which has been in operation since 2013  The C HP plant produces enough electricity and heat to make Bahnstadt a zero emission district Bahnstadt stands for quality in all respects in par- ticular for individual quality of life This is refl ected in the high demand for living space 2 , 521 residences with a living area totaling 163 , 844 square meters have already been built Virtually all residential prop erties in Bahnstadt have been sold or let 1 Bauhaus D IY store 41 , 000 square meters, of which 20 , 000 square meters are sales area 2 Residential quarters in third phase of construction with urban element and urban view L BB W Immobilien 3 Tankturm water tower at Bauhaus Cultural and events center and architectural offi ce 4 Residential property in second phase of construction 5 Promenade in second phase of construction with farmyard themed playground 6 Heidelberg Village cross generation residential quarter 7 First section of tramway along the Green Mile 8 M EI LE N S TE IN 185 apartments, daycare facility, commercial premises, offi ces, restaurant and hotel G GH 9 B3 at Gadamerplatz School, community center and daycare facility I BA project 10 Luxor Filmpalast cinema complex F TB Englert Gmb H 11 Residential quarters in fi rst phase of construction 12 Promenade in fi rst phase of construction Recreation area with the two themed playgrounds fi re department and I CE13 Student ats, campus accommodation and micro apartments 14 Zollhofgarten daycare facility in the freight halls 15 Campus Gardens quarter 370 apartments i Live Heidelberg Gmb H 16 Junges Wohnen Young living 106 rental apartments S OK A building 17 halle02 cultural center in the former freight halls 18 Sky Labs offi ce and laboratory building 19 , 000 square meters for research oriented companies, including Heidelberg Engineering, Reckitt Benckiser and the international Schiller University 19 Second tramway section through the Green Mile and Czernyring 20 New fi re station 21 Schwetzinger Terrasse Recreation area with daycare facility of the same name 22 Colours quarter 11 , 600 square meters for offi ce, commercial and residential use 23 Sky Angle offi ce and laboratory building 16 , 000 square meters for research oriented and science based companies 24 Zollhofgarten recreation area25 Stadttor offi ce building Approx 11 , 000 square meters of rental premises 26 Main station, Bahnstadt exit Direct access to Bahnstadt from the station concourse 27 Heidelberg Technology Park with two of its fi ve locations including the Innovation Lab Gmb H research platform of leading edge cluster Organic Electronics 28 Stadttor Ost offi ce building cbs Corporate Business Solutions, Proaesthetic clinic, outpatient surgery center, medical and physiotherapy practices, offi ce areas 11 , 000 square meters 29 X XX L and Mmax furniture stores 30 Recreation area at Promenade 31 Spitzes Eck recreation area 32 Apartments und offi ce building Grne Meile Projektentwicklung Gmb H 33 Pfaffengrunder Terrasse recreation area 34 Health Center Bahnstadt 7 , 500 square meters for use as medical and specialist practices 35 Westarkaden shopping center Supermarkets, drugstore, specialist retailers, restaurants, cafs, daycare facility and apartments Unmssig Bautrgergesellschaft36 Gneisenaustrae cycle path and footbridge 37 Pfi tzenmeier Premium Plus Resort Aqua Dome and restaurants 38 Kopernikusquartier Apartments, stores and offi ces 39 Redesigned Czernyplatz and Czernyring 40 Residential building Approx 160 apartments, mainly three to fi ve rooms 41 New conference center 42 Europaplatz lively quarter with offi ces, residential properties, restaurants, commercial premises and a conference hotel Gustav Zech Stiftung 43 District cooling delivered to the conference center, buildings at Europaplatz and near offi ces Projects realized Projects in planning under construction Heidelbergs Bahnstadt A district with a successful mix of residential living, research and commerce 29 1 2 233234 31 3314 1519 4039 59 8 1310 11 121735 1636 24 1841 21 2046 30 27 2273 2542 282637 38 436 Heidelbergs Bahnstadt Heidelbergs Bahnstadt 78 Heidelberg International hotspot for science Heidelberg International hotspot for science 9 Heidelberg International hotspot for science 22 hectare Bahnstadt Campus 35 , 000 m Sky Labs and Sky Angle Heidelberg enjoys an outstanding reputation worldwide as a pres- tigious location for scientifi c research The reasons lie partly in the citys long history, but also in the fact that science is part of the urban environment It has been closely interwoven with Heidelbergs economy and daily life for six hundred years This tradition continues with the creation of the Bahn stadt campus On a site covering around 22 hectares, the campus offers state of the art spaces and facilities for pioneering knowledge based companies and research institutions for example in the fi elds of life sciences, biotechnology, I CT or energy and environ mental sciences As with the city as a whole, the Bahn stadt campus stands for openness, dialog, and creativity in a vibrant urban environment Today seven out of ten people in Heidelberg are employed in scientifi c research or by high tech companies The unemploy ment rate is consistently below fi ve percent In par- ticular, researchers and scientists value the urban en- vironment Heidelberg provides, and the opportunities it offers for a lunchtime walk from the laboratory to a nearby caf The Bahnstadt campus is no different In addition, several science and biotech companies have made Bahnstadt their base They are part of the Heidelberg Technology Park, which has created ideal conditions and services for biotech and science companies in Heidelberg The heart of the campus project was set up by the not for profi t foundation Max Jarecki Heidelberg- Stiftung The foundation provided capital with a view to working with the city of Heidelberg to promote interdisciplinary cooperation and knowledge transfer The organizations founder Dr Henry Jarecki, who studied medicine at Heidelberg University in the 1950s, was immediately enthused by the idea of developing a campus for new research oriented companies on the Bahnstadt site It all started with the Sky Labs building at Zollhof- garten, which has become the most visible landmark of Bahnstadt on account of its height, striking facade, and unique architectural design featuring two pro- jecting upper fl oors But Sky Labs also points the way in terms of Bahnstadts research ambitions The striking building offers state of the art labs and re search facilities and perfectly embodies the successful interplay between science and industry in this for ward looking district A second project fi nanced by the Max Jarecki founda tion, the Sky Angle laboratory and offi ce building, is in close proximity to Sky Labs A new conference center in Bahnstadt will in future provide the perfect venue for meetings and conferences Ideally located, the center is directly adjacent to the main station with tram connections to the historic old town With opening scheduled for 2023 , the conference center is a fl agship project for the city of Heidelberg as a hub for science and business Close to the Bahnstadt site on land formerly occupied by the Patton Barracks, the city is also developing the Heidelberg Innovation Park hip, which will be a hotspot for innovative companies in the fi elds of I T, digital media and bioinformatics As a multifunc tional offi ce and laboratory building, the Business Development Center Organic Electronics offers ideal conditions for young high tech companies 1 , 800 Conference center seats in the main hall10 Attractive mix for companies Attractive mix for companies 11 Attractive mix for companies Approx 250 rooms at the conference hotel at Europaplatz The new district is an equally attractive proposition for high tech and research companies, service providers and B2 B companies, retailers and trades The vibrant mix makes Bahnstadt attractive to cor porations and small traders alike In contrast to most monostructured offi ce centers, companies fi nd themselves here in a vibrant and inspiring environ ment Employees have the chance to live close to their place of work There are eight daycare centers of fering an excellent range of childcare services Shopping is quick and easy with a range of stores for everyday needs In addition, Bahnstadt boasts all the traditional hard location factors Transport connections are ideal Heidelberg and the Rhine Neckar metropolitan region offer huge potential in terms of skilled employees And Bahnstadt is specifi cally designed to the passive house standard, which also applies to all offi ce, lab- oratory and commercial property Consequently, energy costs are lower than in conventionally con structed buildings Innovative and research oriented companies value the very favorable location and positive environ ment close to other research based companies and facilities In the direct vicinity are Bahnstadt Campus and Czernyring, two of the fi ve locations of the Heidelberg Technology Park In addition, top inter national companies and a private university have located in close proximity Bahnstadt is also the home to the leading edge cluster Organic Electronics with its research platform Inno- vation Lab Gmb H The core of the 5 , 000 square meter offi ce and laboratory space is a 650 square meter cleanroom laboratory Over 190 researchers from various disciplines work here on innovations in the fi eld of printed electronics, including digital dental imprints, intelligent glass and intelligent fl oor mats The Stadttor building also demonstrates that the right underlying conditions are in place Many com- panies have already been here for some time The Heidelberg company io-consultants moved its head- quarters to the Stadttor offi ce building in Bahnstadt in early 2013  This is the ideal location for io con- sultants it provides an inspiring environment for staff and an attractive and contemporary ambience for customers The Stadttor Ost complex is located in the imme diate vicinity The two buildings on the corner of Speyerer Strae and Rudolf Diesel Strae opened for occupancy in the fall of 2018  The offering to tenants includes modern offi ces and conference rooms as well as co working spaces Worlds fi rst passive house cinema 15screens open air area12 Attractive mix for companies Attractive mix for companies 13 6 , 000 jobs are being created in Bahnstadt Other companies to have relocated to Bahnstadt also benefi t from the excellent underlying condi- tions including the Luxor Filmpalast cinema complex with 15 screens and outdoor area, sev- eral hotels and the D IY store Bauhaus The retail park is also home to the X XX L and Mmax furniture stores The new Westarkaden Heidelberg shopping center provides an offering of retail and food outlets covering a total area of 11 , 700 square meters The offering includes a grocery super market, a discount grocery store, a drugstore, a pharmacy, a hair salon and restaurants The First Steps daycare center is also opening another location there for up to 40 toddlers In addi tion, the center offers offi ce space, 284 apart ments, and a two storied parking garage The retail outlets are scheduled to open from the beginning of 2020  The entire project is to be completed in the spring of 2020  A lively quarter is emerging on the south side of the main station, featuring offi ces, residential properties, shops, restaurants and a high class conference hotel The fi rst buildings are sched uled for completion in 2021  The new Europa- platz will form the center of this quarter The new Convention Center opposite Europa- platz will in future provide a venue for national and international conferences in Heidelberg, the city of science There are plans for up to 1 , 800 seats in the main hall and 800 seats in the small hall The opening is scheduled for the beginning of 2023  Bahnstadt also builds on Heidelbergs tradition as a high quality location for medicine and medical technology, in particular with a planned medical center, accommodation for patients families, and research into appliances for ophthal mological diagnostics by Heidelberg Engineer- ing Heidelberg is one of the worlds leading medical locations, with 13 hospitals and a reha bilitation facility The University Hospital Heidelberg with its medical faculty is the citys largest employer, with around 13 , 700 staff including 1 , 700 doctors It is one of the three largest hospitals in Germany The medical faculty is ranked number one in Germany 14 A place for all lifestyles A place for all lifestyles 15 A place for all lifestyles 4 , 300 people are already living in Bahnstadt One in eight households receives housing assistance from the city Every 4 days a new Bahnstadt resident is born Single people have different needs to families with children the elderly have different lifestyles to students A young offi ce worker for a creative I T company pursues different leisure activities compared to an experienced scientist engaged in lab research But they are all Bahnstadt residents and help give the new quarter its unique character 16 A place for all lifestyles A place for all lifestyles 17 Bahnstadt reflects the diversity of life plans that comes with such a mixed resident structure Lifestyles are reflected in the architecture, the public outdoor spaces, the squares, and the housing From traditional and spacious apartments to airy townhouses and villas Bahnstadt has an accommo dation type to meet every requirement Yet as varied as it is, the properties for the future 6 , 800 residents all have one thing in common Each one is compact in size and offers privacy in the form of a garden, a spacious green inner courtyard, or a fifth floor roof terrace Generously proportioned and quality designed outdoor areas also provide excellent opportunities for time spent in the open air Bahn stadt meets all the requirements for improving everyones quality of life Living in Bahnstadt means having a sense of wellbeing inside ones own four walls and in an attractive residential district 18 First class education and daycare facilities First class education, daycare and community facilities 8 daycare centers are already operational The B community hall has seating for 200 people Heidelbergs long tradition in education is being practiced and developed in Bahnstadt The city of Heidelberg is ranked top in Germany for early learning initiatives for children The rate of daycare provision for young children under three years old is 54 , 9 percent Every child between three and six years old has a kindergarten place and around 87 percent of all Heidelberg primary school children also receive afternoon daycare following lessons at school According to the German Learning Atlas of the Bertelsmann Foundation, Heidelberg is one of the top locations for schooling in Germany In addition, many educational institutions offer a range of activities to promote lifelong learning These high social standards clearly apply to Bahnstadt in particular, where residents seek specifi cally to reconcile career and family Centrally located on Gadamerplatz, the B community center forms the heart of Bahnstadt One of eight child daycare center in the district opened here An inclusive all day primary school with gymnasium and canteen offers a fresh outlook in the fi eld of education The model school provides teaching for children with and without disability The B building also benefi ts from a community center, which is open to adults and the elderly as a meeting place for different generations Its location at the heart of Bahnstadt illustrates the importance the future district attaches to education and social needs Creating links between daycare facilities and primary school ensures a smooth transition from pre-school to primary school education The importance attached to education is refl ected also in the architectural design of the B building Instead of relying on off the peg concepts, the de- velopers applied modern fi ndings based in architec tural psychology A key role is played here by room and building dimensions these establish the learning environment that infl uences learners And, of course, the climate protecting passive house standard is also an integral aspect of building design The B project was implemented in a public private partner ship by the municipal construction and service com- pany and is a project of the Heidelberg International Architecture Exhibition I BA The B community center is a central point of contact for residents of the district A large hall with a stage provides space for events Group and seminar rooms offer space for additional activities, and a public caf encourages visitors to stay and chat 150 the elementary schoolpupils attend First class education and daycare facilities 1920 Short distances in the science city Short distances in the science city 21 Short distances in the science city 2 2 km of tram lines through Bahnstadt 3 5 km of cycle paths 3tram stops in Bahnstadt The new district is situated next to Heidelbergs main railway station The station concourse has been specially extended so that platforms are also easily and quickly accessible from the Bahnstadt district via the stations future south entrance A more mobile district than Bahnstadt Unlikely A new 2 2 kilometer stretch of tramway links Bahnstadt with the other districts of Heidelberg and various research locations such as the Neuenheimer Feld cam- pus and the Old Town Two lines will in future connect the district to the citys tram network For car drivers, too, Bahnstadt is ideally situated The A5 highway is just a few minutes away via the highway feeder road the Walldorf highway intersection and S AP are around 15 minutes drive and it takes around 30 minutes to drive to Karlsruhe and 50 minutes to Frankfurt International Airport Frankfurt and Stuttgart are also easily accessible as workplaces by rail even as a daily commute But in a city of short distances like Heidelberg, the bicy- cle is king As a result, Bahnstadt will see the construc tion of 3 5 kilometers of cycle paths In addition, the fi rst cycle and pedestrian bridge linking the southern areas of the city is already complete An additional cycle bridge over the railway lines and accessing northern parts of the city is planned Both bridges are compo nents of the main cycle route between Neuen- heimer Feld and southern parts of the city Long stay and resident parking is in underground car parks and on individual plots and even more is being done to make mobility in Bahnstadt environmentally compatible Charging points for electric cars have been installed in underground car parks and at Gad- amerplatz 22 District with world class reference model status District with world class reference model status 23 District with world class reference model status Up to 100% sustainable All energy and heat supplied entirely from renewable energies Bahnstadt is one of the worlds largest passive house settlement It far exceeds all legal specifi cations in Germany in particular concerning energy saving regulations Bahnstadt received the Passive House Award 2014 from the Passive House Institute of Darmstadt in recognition of its international reference model status Thanks to its Bahnstadt district, Heidelberg also received the Global Green City Award in 2015 from the United Nations in New York C O2 emission in passive house buildings is less than half that of conventional buildings Constructors and developers are advised on energy issues in order to ensure compliance with the stringent requirements of the passive house energy standard This includes advising and discussion of funding possibilities the city of Heidelberg supports developers by award ing grants, for example Furthermore, Bahnstadt is the largest area in Heidelberg to be fully equipped with the smart meter concept Intelligent electricity meters offer each household the chance to gain a better over view of their energy consumption and costs And it goes a step further Bahnstadt has adopted a concept that supports natural soil functions This means an increased proportion of rainwater is able to evaporate, improving the urban climate In addi tion, some of the rainwater can seep away into con- struction fi elds This in turn promotes local ground water formation The non-use of environmentally valuable outdoor areas also demonstrates that Bahnstadt is not just about building houses for the future Furthermore, biodi- versity has an important role to play in Bahnstadt Around 3 , 600 sand and wall lizards were moved to specially created new habitats when construction work started 5 656 5 Haupt bahnhof Schloss Bahnstadt Neckar KarlsruheBasel Stuttgart Mannheim Mannheim Frankfurt am Main Frankfurt am Main Frankfurt International Airport Heidelberg K AR T OG RA PH IE P eh Sc hefcik0 500 1000 m Im Neuenheimer Feld University campus 2 5 km Campus University Old Town 3 km24 Facts figures Contact Facts figures Living researching developing Bahnstadt is a district that boasts residential property, research facilities, commercial and leisure activities and jobs in keeping with Heidelbergs tradition At the same time, it is one of the worlds largest passive house settlement and one of the biggest urban development projects in Germany Location Former freight and marshaling depot to the southwest of Heidelberg city center The area also includes former military sites that have become available Total area 102 , 5 hectars building area 61% green area and open spaces 18% traffic network 21% First residents arrived in June 2012 There are currently more than 4 300 people living in Bahnstadt as at year middle 2019 Target 6 , 800 residents and 3 , 700 residential units Jobs 6 , 000 Project term 2008 to 2022 Development trust Deutsche Stadt-und Grundstcksentwicklungsgesellschaft mb H Co K G D SK City partners Entwicklungsgesellschaft Heidelberg Gmb H Co K G E GH Private and public investment E UR O 2 billion estimate, of which around E UR 300 million on infrastructure Bahnstadt research hub The district is centrally situated close to the citys main railway station and with access to the Heidelberg tram network The highway network is accessible in just a few minutes Research hub 25 Contact Stadtverwaltung Heidelberg Geschftsstelle Bahnstadt Telefon 06221 58 20250 bahnstadtheidelberg.de Amt fr Wirtschaftsfrderung und Wissenschaft Telefon 06221 58 30000 , wifoeheidelberg.de Amt fr ffentlichkeitsarbeit Telefon 06221 58 12000 oeffentlichkeitsarbeitheidelberg.de Partner Entwicklungsgesellschaft Heidelberg Gmb H Co K G E GH Telefon 06221 718660 infoegh-bahnstadt.de Vertrieb Wohnen Gewerbe S Immobilien Heidelberg Gmb H infos immo hd.de Telefon 06221 511 5500 Entwicklungstreuhnder D SK Deutsche Stadt-und Grundstcksentwicklungsgesellschaft mb H Co K G Telefon 06221 99849 20 Research Innovation Technologiepark Heidelberg Gmb H technologieparkheidelberg.de Telefon 06221 502 5715 Innovation Park hip 1 km26 Publishing information Publishing information City of Heidelberg City Hall, Marktplatz 10 D 69117 Heidelberg Text Public Relations Office Layout Mayors Office Photo credits Christian Buck Page 4 , 5 , 13 , 14 , 15 , 16 , 17 , 19 Philipp Rothe Page 3 , 18 Klaus Venus Page 6 , 7 D EG EL O Architekten Page 8 Thilo Ross Page 9 Winking Froh Architekten B DA Page 10 , 11 Unmssig Bautrgergesellschaft Page 12 , 13 City of Heidelberg Page 16 Steffen Diemer Cover, Page 16 , 20 , 21 , 22 , 23 Issue 3rd Edition, September 2019 Public Relations Office City of Heidelberg City of Heidelberg Marktplatz 10 69117 Heidelberg Phone 49 6221 58 12000 Fax 49 6221 58 12900 oeffentlichkeitsarbeitheidelberg.de\n",
      "Heidelberg CA RD Heidelberg in your pocket Gltig ab Valid from 4 Gltig ab Valid from 2 Family Gltig ab Valid from 2 Gltig ab Valid from 1 Information and service hotline Please refer to the opening hours of the Tourist Information on our website  The H DC AR D is available at Tourist Information at the main station, at the Neckarmnzplatz and in the town hall at the Marktplatz Market Square Kthe Wohlfahrt store at the Universittsplatz University Square, Hauptstrae 124 Heidelberg Youth Hostel Tiergartenstrae 5 numerous Heidelberg hotels49 6221 58 44444 I NF OR MA TI ON T IC KE TS infoheidelberg marketing.de marketing.com Heidelberg CA RD Benefit from numerous advantages in the city area with the H DC AR D  The following services are included in the H DC AR D castle ticket including funicular railway free travel on public transport in the city area Combo ticket one time admission to the University Museum, the Student Prison and the special exhibition many other discounts for guided tours, museums, leisure activities, gastronomy and shopping H DC AR D for pupils, students and trainees up to the age of 28 with I D, including discounted castle ticket Partners sponsors1 day 24 20 discount price Valid from midnight to midnight on the day of validity 2 days 26 22 discount price Valid all day on the first day until midnight the following day 4 days 28 24 discount price Valid all day on the first day until midnight of the fourth day 2 days family 57 Valid all day on the first day until midnight the following day for a family 2 adults and up to 3 children or 1 adult and up to 4 children under the age of 16  Gltig ab Valid from 2 Family Gltig ab Valid from 4 Gltig ab Valid from 1 Gltig ab Valid from 2 The H DC AR D includes Castle ticket funicular railway The castle ticket includes admission to the castle court yard, barrel cellar as well as to the German Pharmacy Museum, and the ride on the funicular railway to the cas tle with continuation to the Molkenkur station round trip including one stop Opening hours Heidelberg Castle 900 am 600 pm castle courtyard, barrel cellar, last entrance 530 pm Opening hours German Pharmacy Museum April October 1000 am 600 pm, last entrance 540 pm November March 1000 am 530 pm, last entrance 510 pm Timetable funicular railway Annual funicular railway maintenance March 6 March 19 , 2023 Summer timetable April 1 November 1 , 2023 Kornmarkt Castle 900 am every 10 minutes, last trip 800 pm Castle Kornmarkt 903 am every 10 minutes, last trip 803 pm Winter timetable November 2 , 2023 March 31 , 2024 Kornmarkt Castle 900 am every 10 minutes, last trip 510 pm Castle Kornmarkt 903 am every 10 minutes, last trip 543 pm subject to changeCombo ticket of the university One time admission to the University Museum, the Student Prison and the special exhibition For more information about opening hours, please click here  Free rides on public transport The H DC AR D entitles holders to travel on all the Rhine Neckar public transport systems V RN buses, trams and authorized trains in the case of D B R E, R B and S Bahn trains, all 2nd class in the greater Heidelberg area Heidelberg Wabe 125 fare zone for the duration of validity Further benefits Dicounts for guided tours, bus tours, boat trips as well as museums, stores and restaurants are listed on the following pages If no district is indicated, the service partner is located in the Old Town City Tours and Excursions City and Castle Sightseeing Tour A discovery tour by bus to all the major sights and points of in terest in the city The central attraction is the visit to Heidelberg Castle Enjoy an exciting guided tour of this world famous ruin In addition to an exterior tour, you will explore the in ner courtyard and the Great Barrel approx 1 hour bus tour, 1 hour castle tour, funicular ride back to the Old Town Prices 30 , 27 discount price , with H DC AR D 28 including castle ticket funicular castle courtyard admission Duration 2 hours April October Friday, Saturday and Whitsunday 130 pm German There is no tour during the Old Town Festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023  November March Saturday 130 pm German Meeting point Neckarmnzplatz, information board bus stop Reservation required 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Reservation required 49 6221 5840 223 - 225 , guideheidelberg marketing.de Walking Tour of the Old Town This tour features major attractions as well as historic narrow alleys and squares the many faces of Heidelbergs Old Town, a whole range of magical settings, all of which have their sto ries to tell Prices 12 , 10 discount price , with H DC AR D 10 Duration 1 5 hours April October daily 1030 am, additionally Friday 600 pm and Saturday 230 pm German, Thursday Saturday 1030 am English There is no tour during the Old Town Festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023 at 230 pm November March Friday 230 pm and Saturday 1030 am German Meeting point Neckarmnzplatz, in front of the Tourist Information Christmas Market Walking Tour Discover the most beautiful corners of the Old Town during this entertaining guided tour and learn interesting facts about the Christmas market and the regions Christmas traditions The tour ends convivially at a mulled wine booth Prices 14 , 12 discount price , with H DC AR D 12 Duration 1 5 hours Advent Saturdays December 2 , 9 , and 16 , 2023 , 430 pm German Meeting point Neckarmnzplatz, in front of the Tourist Information Reservation required 49 6221 5840 223 - 225 , guideheidelberg marketing.de Segway Tour Highly Philosophical Discover Heidelberg in an unusual way driving pleasure at its best Get to know the city from an entirely new perspec tive Prices 59 , with H DC AR D 54 including helmet fee and segway licence Duration 1 34 hours February and November daily 100 pm German English, depending on weather conditions March October daily 930 am, 100 pm and 400 pm German English Meeting point Neckarmnzplatz Reservation required 49 6221 5840 223 - 225 , guideheidelberg marketing.de Cabriobus Sightseeing Tour The open roof of the convertible bus offers completely new perspectives Enjoy an unrestricted view of your beau tiful surroundings, even if it rains An audio guide in mul tiple languages keeps you informed Prices 12 , 7 11 discount price , with H DC AR D 11 Duration 40 minutes Languages German, English, Chinese, Dutch, French, Italian, Japanese, Korean, Russian, Spanish March, November and December daily 1000 am 400 pm, departure every full hour April October daily 1000 am 500 pm, departure every half and full hour There is no tour during the Old Town Festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023  Meeting point Karlsplatz Carls Square, information board bus stopNaturally Heidelberg Neuenheimer Schweiz, Mausbachstollen, Felsenmeer, Klima- oase Kohlhof not only the city itself, but also Heidelbergs natural environment has special highlights to offer The nature guides of the environmental education and experience platform Natrlich Heidelberg Naturally Heidelberg lead you on the traces of the Geo Naturepark to romantic places beyond the touristic hotspots Information on topics, dates and meeting points can be obtained via the online booking portal at Prices with H DC AR D 2 reduced Eligibility criteria for the reduced price school and university students up to 28 years old, people with disabilities and a disabled persons pass One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons I D as well as of children and young people up to 18 years with a disability is free of charge Tickets are available at our Tourist Information Children younger than 6 years are free of charge, children aged 6 to 12 pay 7  People with disabilities and a disabled persons pass as well as owners of the H D C AR D pay 11  Leisure Time With the H DC AR D you receive a 10% discount City Quest City rally with a kick for your team, Dr Nadja Pentzlin Phone 49 176 38003103 Mrchenparadies Heidelberg Knigstuhl 5 1 discount for one park entrance Valid for ticket holders and their own children Radhof Bergheim Bergheimer Strae 101 10% discount on bike rentalrepair Paddle Tours Heidelberg Kayak tours in and around Heidelberg Phone 49 151 61466148 benighauspaddle-tours.de Discount on all kayak tours Solar boat Neckarsonne Landing pier Karl Theodor- Brcke Alte Brcke, Old Bridge Discount on the 50 minute Heidelberg round trip Weisse Flotte Heidelberg Landing pier Stadthalle Heidelberg Discount on the castle to ur from Heidelberg to Neckarsteinach and return Zoo Heidelberg Experience life live Tiergartenstrae 3 Neuenheimer Feld 10% disc ount on the regular zoo entrance fee at the ticket office Museums and Exhibitions With the H DC AR D you receive a 20% discount on the admission price at the following museums Documentation and Cultural Center of German Sinti and Roma Bremeneckgasse 2 20% discount on events like concerts Krperwelten Museum Body Worlds Museum Altes Hallenbad Heidelberg Poststrae 36 5 and Bergheimer Strae 41 Bergheim Discount on the admission price for the single ticket for adults Kurpflzisches Museum Heidelberg Palatinate Museum, Hauptstrae 97 Museum Haus Cajeth Primitive Malerei Art of Outsiders Haspelgasse 12 Museum Sammlung Prinzhorn Prinzhorn Collection University Psychiatric Hospital, Vostrae 2 Bergheim prinzhorn.de Vlkerkundemuseum V PS T Hauptstrae 235 Palais Weimar vpst.de Culture With the H DC AR D you receive a 20% discount on tickets Augustinum Heidelberg Cinema, concerts, shows and more Jaspersstrae 2 Emmertsgrund Enjoy Jazz Festival for Jazz and More, numerous event locations in and around Heidelberg Discount at the box office for events that are not sold out International Music Festival Heidelberger Frhling March 17 April 15 , 2023 Tickets 49 6221 5840 044 10% discount on tickets only Kulturhaus Karlstorbahnhof Concerts, club culture, readings, cabaret theater Marlene Dietrich Platz 3 Sdstadt 10% discount on event tick ets excluding rentals and special events The offer is redeemable at the evening box office Karlstorbahnhof Theater and Orchestra Heidelberg Musical theater, concert, drama, dance as well as theater for children and young people and various festivals Tickets 49 6221 58 20000 The discount applies to perfor mances by the theater incl the Heidelberger Schlossfestspiele Heidelberg Castle Festival, not valid for special events Gastronomy Show your H DC AR D and receive a discount at the following providers Bubbles Heidelberg Hauptstr 177 2 Bubble Waffle Wra ps for the price of 1 Cocktailcaf Regie Theaterstrae 2 Phone 49 6221 652226 20% discount on cocktails Gasthaus Backmulde Schiffgasse 11 Phone 49 6221 53660 backmulde.de A glass of prosecco free of charge in combination with a dinner Hans Hirschs Kurpfalzbru Hauptstrae 190 Phone 49 6221 5991142 One beer 0 , 3 l with each main course Hotel Bar at the Leonardo Heidelberg City Center Bergheimer Strae 63 Bergheim Phone 49 6221 5080 10% discount at the hotel bar Leo Bar at the Leonardo Heidelberg Pleikartsfrster Strae 101 Kirchheim Phone 49 6221 7880 10% discount at the hotel bar My Currywurst Rohrbacher Str 2 Bergheim Phone 49 6221 7250175 A large menu for the price of the small menu My Currywurst Hauptstrae 166 Phone 49 6221 5991401 A large menu for the price of the small menu R AD A Coffee Rsterei Untere Strae 21 Phone 49 6221 1805585 10% on hot drinks Restaurant Zum Roten Ochsen Hauptstrae 217 Phone 49 6221 20977 A glass of apple win e ros as aperitif free of charge Schlemmermeyer Hauptstrae 31 Phone 49 6221 20959 10% discount on the entire range Schlosshotel Molkenkur Klingenteichstrae 31 Phone 49 6221 654080 10% discount at the hote l restaurant Yolicious Frozen Yogurt Ziegelgasse 26 Phone 49 6221 3544670 Pay 1 small cup in cluding 1 topping receive 1 basic cup including 2 toppings Shopping With the H DC AR D you receive a 10% discount on your purchase from the following providers Bofinger Hauptstr 33 Discount on regular ite ms Not valid on already discounted products Bofinger Femme Sofienstrae 17 Bergheim Discount on regular ite ms Not valid on already discounted products Bofinger Men Sofienstrae 15 Bergheim Discount on regular ite ms Not valid on already discounted products Brauerei zum Klosterhof Stiftweg 4 Ziegelhausen zum klosterhof.de Applies only to organic beer Bro und Schreibwaren Knoblauch Gmb H Plck 2 Chocolaterie St Anna No 1 St.-Anna Gasse 1 st anna.de Applies only to purchase of goods Eau de Wald Kettengasse 2 The gin can be tasted for free Heidelberg Bonbon Manufactory Steingasse 4 Discount on bonbons Heimat Beautiful local products Kettengasse 3 i A M design manufactory graphic studio Hauptstrae 152 Kthe Wohlfahrt Hauptstrae 124 3% discount Lindt Boutique Marktplatz 3 Excluding discounted items M AJ A Mieder-, Wsche-, Bademoden Neugasse 13 feine waesche.de Napapijri Heidelberg Hauptstrae 100 Optik Dieterich Friedrich Ebert Platz 1 dieterich.de Perfumery Frosch Hauptstrae 140 Excluding services, discounted items and selected brands Trendy Bags Heidelberg Hauptstrae 179 Discount on travel luggage V IU Heidelberg Hauptstr 95 Discount on all glasses Yabis Cashmere Untere Strae 9 Tourist Information With the H DC AR D you get a 10% discount on all souvenirs and 30% discount on clothes Tourist Information at the main station Willy Brandt Platz 1 Bergheim Phone 49 6221 58 44444 Tourist Information at the Neckarmnzplatz Neckarmnzplatz Phone 49 6221 58 44444 Tourist Information in the Rathaus town hall Marktplatz Phone 49 6221 58 44444 Imprint Heidelberg Marketing Gmb H Neuenheimer Landstrae 5 69120 Heidelberg Phone 49 6221 5840 200 Fax 49 6221 5840 209 infoheidelberg-marketing.de marketing.com The Heidelberg Marketing Gmb H is a subsidiary of the City of Heidelberg Content Heidelberg Marketing Gmb H Photos cover page, pages 6 , 7 , 8 , 10 , 11 , 12 , 13 , 17 , 20 Tobias Schwerdt page 9 Stadtsafari page 14 Augustinum Hei delberg Christian Topp page 15 Theater and Orchestra Heidelberg Sebastian Bhler page 19 Christoph Dpper 2020 2023 All contents, in particular texts, pho tographs and graphics, are protected by  Unless expressly stated otherwise, Heidelberg Marketing Gmb H owns the  Kornmarkt Cabriobus City tour Neckarmnzplatz Molkenkur Knigstuhl Bergbah n funicular railwayKarlstorAltstadt Bussemergasse Kl Mantelgasse Groe Mantelgasse Haspelgasse Wehrsteg Floring Krmergasse Mittelbadgasse Apothekergasse Fischerg  Semmelsg Steingasse Leyerg Obere Neckarstrae Mnchg asse Dreiknigsstr Kettengasse Schulgasse Grabengasse Sandgasse Theaterstra e Friedrichstr ae Mrzgasse Akademiestra e Neugasse Rohrbacher Stra e Bismarckstrae Nadlerstr ae St An na Gasse Fahrtgasse Thibautstra e Pfaffengasse Am Brckentor Zoo Heiliggeiststr Marstallstr ae Schiffgasse Bauamtsgasse Ziegelgasse Brunnengasse Bienenstrae Karpfengasse Unter e Stra e Fischmarkt Marsiliusplat z Richard Hauser- Platz Stadthalle Friedrich- Ebert Platz Bismar ck- Platz Rathaus Kornmarkt Schlangenweg Unter e Neckarstra e Landfriedstr ae Neuenheimer Landstr ae Ziegelhuser Landstr ae Neckarstaden Friedrich Ebert Anlage Gaisbergtunnel Schloss berg tunnel Friedrich Ebert Anlag e Kurfrsten Anlage Neckarstaden Schurmanstr ae Uferstr ae Neuenheimer Landstrae Plck Plck Hauptstr ae Bergheimer St rae Bahnhofstra e Hauptstr ae Merianstr ae Ingrimstr ae Zwingerstr ae Un t Fauler Pelz Ober er Fauler Pelz Neue Schlo ssstr ae Theodor Heuss Brcke Karl Theodor- Brcke Brckenstrae Neue Schlossstrae Mrc henpar adies Fairy T ale Par adiseSchlierbach Rohrbach Neuenheim Neckarwiese Hirschgasse Alte Brcke Old BridgeFugngerbergan g pedestrian cr ossing Karlsplatz Neckarmnz- platz Marktplatz Universitts- platz Schloss Solar po wered boat Neckar ferry Liselotte Heidelberge r Schloss CastleNeuenheimer Feld Kliniken Hospital Hauptbahnhof main stationKloster Benedictine abbe y Stift Neubur g Philosophenweg Philosophers Walk Kirchheim Weststadt Ziegelhausen Weisse Flotte Footpath Pier Bus tours Public toilets Tourist Information HD Card saleonly entry and exit Parking garage Meeting point Funicular railway Bus parking\n",
      "Rohan Chopra, Aniruddha M Godbole, Nipun Sadvilkar, Muzaffar Bashir Shah, Sohom Ghosh, and Dwight Gunning Confidently design and build your own N LP projects with this easy to understand practical guide The Natural Language Processing Workshop The Natural Language Processing Workshop 2020 Packt Publishing  No part of this course may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews Every effort has been made in the preparation of this course to ensure the accuracy of the information presented However, the information contained in this course is sold without warranty, either express or implied Neither the authors, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this course Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this course by the appropriate use of capitals However, Packt Publishing cannot guarantee the accuracy of this information Authors Rohan Chopra, Aniruddha M Godbole, Nipun Sadvilkar, Muzaffar Bashir Shah, Sohom Ghosh, and Dwight Gunning Reviewers Ankit Bhatia, Nagendra Nagaraj, Nimish Narang, Sumit Kumar Raj, Tom Taulli, and Ankit Verma Managing Editor Saumya Jha Acquisitions Editors Royluis Rodrigues, Kunal Sawant, Sneha Shinde, Archie Vankar, and Karan Wadekar Production Editor Roshan Kawale Editorial Board Megan Carlisle, Samuel Christa, Mahesh Dhyani, Heather Gopsill, Manasa Kumar, Alex Mazonowicz, Monesh Mirpuri, Bridget Neale, Dominic Pereira, Shiny Poojary, Abhishek Rane, Brendan Rodrigues, Erol Staveley, Ankita Thakur, Nitesh Thakur, and Jonathan Wray First published August 2020 Production reference 1130820 I SB N 978 1 80020 842 1 Published by Packt Publishing Ltd Livery Place, 35 Livery Street Birmingham B3 2 PB, U KTable of Contents Preface i Chapter 1 Introduction to Natural Language Processing 1 Introduction 2 History of N LP 3 Text Analytics and N LP 3 Exercise 1 01 Basic Text Analytics 5 Various Steps in N LP 8 Tokenization 8 Exercise 1 02 Tokenization of a Simple Sentence 9 Po S Tagging 10 Exercise 1 03 Po S Tagging 11 Stop Word Removal 12 Exercise 1 04 Stop Word Removal 13 Text Normalization 15 Exercise 1 05 Text Normalization 16 Spelling Correction 17 Exercise 1 06 Spelling Correction of a Word and a Sentence 18 Stemming 20 Exercise 1 07 Using Stemming 20 Lemmatization 22 Exercise 1 08 Extracting the Base Word Using Lemmatization 23 Named Entity Recognition N ER 24 Exercise 1 09 Treating Named Entities 25 Word Sense Disambiguation 26 Exercise 1 10 Word Sense Disambiguation 27 Sentence Boundary Detection 28 Exercise 1 11 Sentence Boundary Detection 29 Activity 1 01 Preprocessing of Raw Text 30 Kick Starting an N LP Project 31 Data Collection 32 Data Preprocessing 32 Feature Extraction 32 Model Development 33 Model Assessment 33 Model Deployment 33 Summary 33 Chapter 2 Feature Extraction Methods 35 Introduction 36 Types of Data 36 Categorizing Data Based on Structure 37 Categorizing Data Based on Content 39 Cleaning Text Data 40 Tokenization 41 Exercise 2 01 Text Cleaning and Tokenization 42 Exercise 2 02 Extracting n grams 44 Exercise 2 03 Tokenizing Text with Keras and Text Blob 47 Types of Tokenizers 50 Exercise 2 04 Tokenizing Text Using Various Tokenizers 51 Stemming 58 Regexp Stemmer 58 Exercise 2 05 Converting Words in the Present Continuous Tense into Base Words with Regexp Stemmer 59 The Porter Stemmer 60 Exercise 2 06 Using the Porter Stemmer 60 Lemmatization 61 Exercise 2 07 Performing Lemmatization 62 Exercise 2 08 Singularizing and Pluralizing Words 63 Language Translation 64 Exercise 2 09 Language Translation 65 Stop Word Removal 66 Exercise 2 10 Removing Stop Words from Text 66 Activity 2 01 Extracting Top Keywords from the News Article 67 Feature Extraction from Texts 68 Extracting General Features from Raw Text 68 Exercise 2 11 Extracting General Features from Raw Text 69 Exercise 2 12 Extracting General Features from Text 72 Bag of Words Bo W 80 Exercise 2 13 Creating a Bag of Words 80 Zipfs Law 83 Exercise 2 14 Zipfs Law 84 Term Frequency Inverse Document Frequency T FI DF 89 Exercise 2 15 T FI DF Representation 89 Finding Text Similarity Application of Feature Extraction 91 Exercise 2 16 Calculating Text Similarity Using Jaccard and Cosine Similarity 92 Word Sense Disambiguation Using the Lesk Algorithm 95 Exercise 2 17 Implementing the Lesk Algorithm Using String Similarity and Text Vectorization 96 Word Clouds 98 Exercise 2 18 Generating Word Clouds 99 Other Visualizations 102 Exercise 2 19 Other Visualizations Dependency Parse Trees and Named Entities 102 Activity 2 02 Text Visualization 104 Summary 105 Chapter 3 Developing a Text Classifier 107 Introduction 108 Machine Learning 108 Unsupervised Learning 108 Hierarchical Clustering 110 Exercise 3 01 Performing Hierarchical Clustering 111 k means Clustering 118 Exercise 3 02 Implementing k means Clustering 119 Supervised Learning 124 Classification 124 Logistic Regression 125 Exercise 3 03 Text Classification Logistic Regression 126 Naive Bayes Classifiers 130 Exercise 3 04 Text Classification Naive Bayes 131 k nearest Neighbors 136 Exercise 3 05 Text Classification Using the k nearest Neighbors Method 137 Regression 141 Linear Regression 141 Exercise 3 06 Regression Analysis Using Textual Data 142 Tree Methods 148 Exercise 3 07 Tree Based Methods Decision Tree 149 Random Forest 154 Gradient Boosting Machine and Extreme Gradient Boost 155 Exercise 3 08 Tree Based Methods Random Forest 157 Exercise 3 09 Tree Based Methods X GBoost 162 Sampling 167 Exercise 3 10 Sampling Simple Random, Stratified, and Multi-Stage 168 Developing a Text Classifier 173 Feature Extraction 173 Feature Engineering 173 Removing Correlated Features 173 Exercise 3 11 Removing Highly Correlated Features Tokens 174 Dimensionality Reduction 179 Exercise 3 12 Performing Dimensionality Reduction Using Principal Component Analysis 180 Deciding on a Model Type 185 Evaluating the Performance of a Model 186 Exercise 3 13 Calculating the R MS E and M AP E of a Dataset 189 Activity 3 01 Developing End to End Text Classifiers 191 Building Pipelines for N LP Projects 192 Exercise 3 14 Building the Pipeline for an N LP Project 192 Saving and Loading Models 194 Exercise 3 15 Saving and Loading Models 194 Summary 198 Chapter 4 Collecting Text Data with Web Scraping and A PIs 201 Introduction 202 Collecting Data by Scraping Web Pages 202 Exercise 4 01 Extraction of Tag Based Information from H TM L Files 204 Requesting Content from Web Pages 208 Exercise 4 02 Collecting Online Text Data 208 Exercise 4 03 Analyzing the Content of Jupyter Notebooks in H TM L Format 211 Activity 4 01 Extracting Information from an Online H TM L Page 214 Activity 4 02 Extracting and Analyzing Data Using Regular Expressions 215 Dealing with Semi Structured Data 216 J SO N 216 Exercise 4 04 Working with J SO N Files 218 X ML 220 Exercise 4 05 Working with an X ML File 222 Using A PIs to Retrieve Real Time Data 224 Exercise 4 06 Collecting Data Using A PIs 224 Extracting data from Twitter Using the O Auth A PI 226 Activity 4 03 Extracting Data from Twitter 228 Summary 229 Chapter 5 Topic Modeling 231 Introduction 232 Topic Discovery 232 Exploratory Data Analysis 233 Transforming Unstructured Data to Structured Data 233 Bag of Words 234 Topic Modeling Algorithms 235 Latent Semantic Analysis L SA 235 L SA How It Works 236 Key Input Parameters for L SA Topic Modeling 237 Exercise 5 01 Analyzing Wikipedia World Cup Articles with Latent Semantic Analysis 238 Dirichlet Process and Dirichlet Distribution 244 Latent Dirichlet Allocation L DA 245 L DA How It Works 245 Measuring the Predictive Power of a Generative Topic Model 246 Exercise 5 02 Finding Topics in Canadian Open Data Inventory Using the L DA Model 247 Activity 5 01 Topic Modeling Jeopardy Questions 252 Hierarchical Dirichlet Process H DP 253 Exercise 5 03 Topics in Around the World in Eighty Days 254 Exercise 5 04 Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe 259 Practical Challenges 266 State of the Art Topic Modeling 266 Activity 5 02 Comparing Different Topic Models 267 Summary 268 Chapter 6 Vector Representation 271 Introduction 272 What Is a Vector 272 Frequency Based Embeddings 273 Exercise 6 01 Word Level One Hot Encoding 277 Character Level One Hot Encoding 283 Exercise 6 02 Character One Hot Encoding Manual 284 Exercise 6 03 Character Level One Hot Encoding with Keras 286 Learned Word Embeddings 293 Word2 Vec 293 Exercise 6 04 Training Word Vectors 294 Using Pre Trained Word Vectors 301 Exercise 6 05 Using Pre Trained Word Vectors 302 Document Vectors 309 Uses of Document Vectors 310 Exercise 6 06 Converting News Headlines to Document Vectors 310 Activity 6 01 Finding Similar News Article Using Document Vectors 316 Summary 316 Chapter 7 Text Generation and Summarization 319 Introduction 320 Generating Text with Markov Chains 320 Markov Chains 320 Exercise 7 01 Text Generation Using a Random Walk over a Markov Chain 322 Text Summarization 327 Text Rank 327 Key Input Parameters for Text Rank 329 Exercise 7 02 Performing Summarization Using Text Rank 329 Exercise 7 03 Summarizing a Childrens Fairy Tale Using Text Rank 333 Activity 7 01 Summarizing Complaints in the Consumer Financial Protection Bureau Dataset 337 Recent Developments in Text Generation and Summarization 338 Practical Challenges in Extractive Summarization 340 Summary 340 Chapter 8 Sentiment Analysis 343 Introduction 344 Why Is Sentiment Analysis Required 344 The Growth of Sentiment Analysis 345 The Monetization of Emotion 345 Types of Sentiments 345 Emotion 345 Key Ideas and Terms 347 Applications of Sentiment Analysis 348 Tools Used for Sentiment Analysis 349 N LP Services from Major Cloud Providers 349 Online Marketplaces 350 Python N LP Libraries 350 Deep Learning Frameworks 351 The textblob library 352 Exercise 8 01 Basic Sentiment Analysis Using the textblob Library 352 Activity 8 01 Tweet Sentiment Analysis Using the textblob library 354 Understanding Data for Sentiment Analysis 356 Exercise 8 02 Loading Data for Sentiment Analysis 356 Training Sentiment Models 360 Activity 8 02 Training a Sentiment Model Using T FI DF and Logistic Regression 361 Summary 362 Appendix 365 Index 425 Prefaceii Preface About the Book Do you want to learn how to communicate with computer systems using Natural Language Processing N LP techniques, or make a machine understand human sentiments Do you want to build applications like Siri, Alexa, or chatbots, even if you would have never done it before With The Natural Language Processing Workshop , you can expect to make consistent progress as a beginner, and get up to speed in an interactive way, with the help of hands on activities and fun exercises The book starts with an introduction to N LP you will study different approaches to N LP tasks, and perform exercises in Python to understand the process of preparing datasets for N LP models Next, you will use advanced N LP algorithms and visualization techniques to collect datasets from open websites, and to summarize and generate random text from a document In the final chapters, you will use N LP to create a chatbot that detects positive or negative sentiment in text documents such as movie reviews By the end of this book, you will be equipped with the essential N LP tools and techniques you need to solve common business problems that involve processingtext Audience This book is for beginner to mid level data scientists, machine learning developers, and N LP enthusiasts A basic understanding of machine learning and N LP is required to help you grasp the topics in this workshop more quickly About the Chapters Chapter 1 , Introduction to Natural Language Processing , starts by defining natural language processing and the different types of natural language processing tasks, using practical examples for each type This chapter also covers the process of structuring and implementing a natural language processing project Chapter 2 , Feature Extraction Methods , covers basic feature extraction methods from unstructured text These include tokenization, stemming, lemmatization, and stopword removal We also discuss observations we might see from these extraction methods and introduce Zipfs Law Finally, we discuss the Bag of Words model and Term Frequency Inverse Document Frequency T F I DF About the Book iii Chapter 3 , Developing a Text Classifier , teaches you how to create a simple text classifier with feature extraction methods covered in the previous chapters Chapter 4 , Collecting Text Data with Web Scraping and A PIs , introduces you to web scraping and discusses various methods of collecting and processing text data from online sources, such as H TM L and X ML files and A PIs Chapter 5 , Topic Modeling , introduces topic modeling, an unsupervised natural language processing technique that groups documents according to topic You will see how this is done using Latent Dirichlet Allocation L DA, Latent Semantic Analysis L SA, and Hierarchical Dirichlet Processes H DP Chapter 6 , Vector Representation , discusses the importance of representing text as vectors, and various vector representations, such as Word2 Vec and Doc2 Vec Chapter 7 , Text Generation and Summarization, teaches you two simple natural language processing tasks creating text summaries and generating random text with statistical assumptions and algorithms Chapter 8 , Sentiment Analysis , teaches you how to detect sentiment in text, using simple techniques Sentiment analysis is the use of computer algorithms to detect whether the sentiment of text is positive or negative Conventions Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy U RLs, user input, and Twitter handles are shown as follows We find that the summary for the Wikipedia article is much more coherent than the short story We can also see that the summary with a ratio of 0 20 is a subset of a summary with a ratio of 0 25  Words that you see on the screen, for example, in menus or dialog boxes, also appear in the text like this On this page, click on Keys option to access the secret keys A block of code is set as follows text_after_twentytext_after_twenty.replacen, text_after_twentyre.subrs, ,text_after_twenty New terms and important words are shown like this A Markov chain consists of a state space and a specific type of successor function.iv Preface Long code snippets are truncated and the corresponding names of the code files on Git Hub are placed at the top of the truncated code The permalinks to the entire code are placed below the code snippet It should look as follows Exercise 7 01 .ipynb 1 H AN DL E wn 2 L IN K httpst.cow 3 S PE CI AL_ CH AR S ltltamp 4 P AR An 5 def cleantext 6 text re.subH AN DL E, , text 7 text re.subL IN K, , text 8 text re.subS PE CI AL_ CH AR S, , text 9 text re.subP AR A, n, text The full code can be found at  Code Presentation Lines of code that span multiple lines are split using a backslash  When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line For example history model.fitX, y, epochs100, batch_size5, verbose1, validation_split0 2 , shuffleFalse Comments are added into code to help explain specific bits of logic Single line comments are denoted using the symbol, as follows Print the sizes of the dataset printNumber of Examples in the Dataset , X.shape printNumber of Features for each example , X.shape Multi line comments are enclosed by triple quotes, as shown below Define a seed for the random number generator to ensure the result will be reproducible seed 1 np.random.seedseed random.set_seedseedAbout the Book v Setting up Your Environment Before we explore the book in detail, we need to set up specific software and tools In the following section, we shall see how to do that Installation and Setup Jupyter notebooks are available once you install Anaconda on your system Anaconda can be installed for Windows systems using the steps available at anaconda.comanacondainstallwindows  For other systems, navigate to the respective installation guide from anaconda.comanacondainstall  These installations will be executed in the C drive of your system You can choose to change the destination Installing the Required Libraries Open Anaconda Prompt and follow the steps given here to get your system ready We will create a new environment on Anaconda where we will install all the required libraries and run our code 1  To create a new environment, run the following command conda create namenlp 2  To activate the environment, type the following conda activate nlp For this course, whenever you are asked to open a terminal, you need to open Anaconda Prompt, activate the environment, and then proceed 3  To install all the libraries, download the environment file from and run the following command pip install frequirements.txt 4  Jupyter notebooks allow us to run code and experiment with code blocks To start Jupyter Notebook, run the following inside the nlp environment jupyter notebook A new browser window will open up with the Jupyter interface You can now navigate to the project location and run Jupyter Notebook.vi Preface Installing Libraries pip comes pre installed with Anaconda Once Anaconda is installed on your machine, all the required libraries can be installed using pip , for example, pip install numpy  Alternatively, you can install all the required libraries using pip install r requirements.txt  You can find the requirements.txt file at  The exercises and activities will be executed in Jupyter Notebooks Jupyter is a Python library and can be installed in the same way as the other Python libraries that is, with pip install jupyter , but fortunately, it comes pre installed with Anaconda To open a notebook, simply run the command jupyter notebook in the Terminal or Command Prompt Accessing the Code Files You can find the complete code files of this book at  You can also run many activities and exercises directly in your web browser by using the interactive lab environment at  we have tried to support interactive versions of all activities and exercises, but we recommend a local installation as well for instances where this support is not available If you have any issues or questions about installation, please email us at workshopspackt.com  Overview In this chapter, you will learn the difference between Natural Language Processing N LP and basic text analytics You will implement various preprocessing tasks such as tokenization, lemmatization, stemming, stop word removal, and more By the end of this chapter, you will have a deep understanding of the various phases of an N LP project, from data collection to model deployment Introduction to Natural Language Processing12 Introduction to Natural Language Processing Introduction Before we can get into N LP in any depth, we first need to understand what natural language is To put it in simple terms, it is a means for us to express our thoughts and ideas To define it more specifically, language is a mutually agreed upon set of protocols involving wordssounds that we use to communicate with each other In this era of digitization and computation, we are constantly interacting with machines around us through various means, such as voice commands and typing instructions in the form of words Thus, it has become essential to develop mechanisms by which human language can be comprehended accurately by computers N LP helps us do this So, N LP can be defined as a field of computer science that is concerned with enabling computer algorithms to understand, analyze, and generate natural languages Lets look at an example You have probably interacted with Siri or Alexa at some point Ask Alexa for a cricket score, and it will reply with the current score The technology behind this is N LP Siri and Alexa use techniques such as Speech to Text with the help of a search engine to do this magic As the name suggests, Speech to Text is an application of N LP in which computers are trained to understand verbally spoken words N LP works at different levels, which means that machines process and understand natural language at different levels These levels are as follows Morphological level This level deals with understanding word structure and word information Lexical level This level deals with understanding the part of speech of the word Syntactic level This level deals with understanding the syntactic analysis of a sentence, or parsing a sentence Semantic level This level deals with understanding the actual meaning of a sentence Discourse level This level deals with understanding the meaning of a sentence beyond just the sentence level, that is, considering the context Pragmatic level This level deals with using real world knowledge to understand the sentence History of N LP 3 History of N LP N LP is a field that has emerged from various other fields such as artificial intelligence, linguistics, and data science With the advancement of computing technologies and the increased availability of data, N LP has undergone a huge change Previously, a traditional rule based system was used for computations, in which you had to explicitly write hardcoded rules Today, computations on natural language are being done using machine learning and deep learning techniques Consider an example Lets say we have to extract the names of some politicians from a set of political news articles So, if we want to apply rule based grammar, we must manually craft certain rules based on human understanding of language Some of the rules for extracting a persons name can be that the word should be a proper noun, every word should start with a capital letter, and so on As we can see, using a rule based system like this would not yield very accurate results Rule based systems do work well in some cases, but the disadvantages far outweigh the advantages One major disadvantage is that the same rule cannot be applicable in all cases, given the complex and nuanced nature of most language These disadvantages can be overcome by using machine learning, where we write an algorithm that tries to learn a language using the text corpus training data rather than us explicitly programming it to do so Text Analytics and N LP Text analytics is the method of extracting meaningful insights and answering questions from text data, such as those to do with the length of sentences, length of words, word count, and finding words from the text Lets understand this with an example Suppose we are doing a survey using news articles Lets say we have to find the top five countries that contributed the most in the field of space technology in the past 5 years So, we will collect all the space technology related news from the past 5 years using the Google News A PI Now, we must extract the names of countries in these news articles We can perform this task using a file containing a list of all the countries in the world 4 Introduction to Natural Language Processing Next, we will create a dictionary in which keys will be the country names and their values will be the number of times the country name is found in the news articles To search for a country in the news articles, we can use a simple word regex After we have completed searching all the news articles, we can sort the country names by the values associated with them In this way, we will come up with the top five countries that contributed the most to space technology in the last 5 years This is a typical example of text analytics, in which we are generating insights from text without getting into the semantics of the language It is important here to note the difference between text analytics and N LP The art of extracting useful insights from any given text data can be referred to as text analytics N LP, on the other hand, helps us in understanding the semantics and the underlying meaning of text, such as the sentiment of a sentence, top keywords in text, and parts of speech for different words It is not just restricted to text data voice speech recognition and analysis also come under the domain of N LP It can be broadly categorized into two types Natural Language Understanding N LU and Natural Language Generation N LG  A proper explanation of these terms is provided here N LU N LU refers to a process by which an inanimate object with computing power is able to comprehend spoken language As mentioned earlier, Siri and Alexa use techniques such as Speech to Text to answer different questions, including inquiries about the weather, the latest news updates, live match scores, and more N LG N LG refers to a process by which an inanimate object with computing power is able to communicate with humans in a language that they can understand or is able to generate human understandable text from a dataset Continuing with the example of Siri or Alexa, ask one of them about the chances of rainfall in your city It will reply with something along the lines of, Currently, there is no chance of rainfall in your city It gets the answer to your query from different sources using a search engine and then summarizes the results Then, it uses Text to Speech to relay the results in verbally spoken words So, when a human speaks to a machine, the machine interprets the language with the help of the N LU process By using the N LG process, the machine generates an appropriate response and shares it with the human, thus making it easier for humans to understand the machine These tasks, which are part of N LP, are not part of text analytics Lets walk through the basics of text analytics and see how we can execute it in Python Text Analytics and N LP 5 Before going to the exercises, lets define some prerequisites for running the exercises Whether you are using Windows, Mac or Linux, you need to run your Jupyter Notebook in a virtual environment You will also need to ensure that you have installed the requirements as stated in the requirements.txt file on  Exercise 1 01 Basic Text Analytics In this exercise, we will perform some basic text analytics on some given text data, including searching for a particular word, finding the index of a word, and finding a word at a given position Follow these steps to implement this exercise using the following sentence The quick brown fox jumps over the lazy dog 1  Open a Jupyter Notebook 2  Assign a sentence variable the value The quick brown fox jumps over the lazy dog  Insert a new cell and add the following code to implement this sentence The quick brown fox jumps over the lazy dog sentence 3  Check whether the word quick belongs to that text using the following code def find_wordword, sentence return word in sentence find_wordquick, sentence The preceding code will return the output True 4  Find out the index value of the word fox using the following code def get_indexword, text return text.indexword get_indexfox, sentence The code will return the output 16 6 Introduction to Natural Language Processing 5  To find out the rank of the word lazy , use the following code get_indexlazy, sentence.split This code generates the output 7 6  To print the third word of the given text, use the following code def get_wordtext,rank return text.split get_wordsentence, 2 This will return the output brown 7  To print the third word of the given sentence in reverse order, use the following code get_wordsentence, 2 This will return the output nworb 8  To concatenate the first and last words of the given sentence, use the following code def concat_wordstext This method will concat first and last words of given text words text.split first_word words last_word words return first_word last_word concat_wordssentence Note The triple quotes shown in the code snippet above are used to denote the start and end points of a multi line code comment Comments are added into code to help explain specific bits of logic The code will generate the output Thedog  Text Analytics and N LP 7 9  To print words at even positions, use the following code def get_even_position_wordstext words text.split return for i in rangelenwords if i% 2 0 get_even_position_wordssentence This code generates the following output 10  To print the last three letters of the text, use the following code def get_last_n_letterstext, n return text get_last_n_letterssentence, 3 This will generate the output dog 11  To print the text in reverse order, use the following code def get_reversetext return text get_reversesentence This code generates the following output god yzal eht revo spmuj xof nworb kciuq eh T 12  To print each word of the given text in reverse order, maintaining their sequence, use the following code def get_word_reversetext words text.split return .join for word in words get_word_reversesentence This code generates the following output eh T kciuq nworb xof spmuj revo eht yzal god8 Introduction to Natural Language Processing We are now well acquainted with basic text analytics techniques Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, lets dive deeper into the various steps and subtasks in N LP Various Steps in N LP we have talked about the types of computations that are done with natural language Apart from these basic tasks, you can also design your own tasks as per your requirements In the coming sections, we will discuss the various preprocessing tasks in detail and demonstrate each of them with an exercise To perform these tasks, we will be using a Python library called N LT K Natural Language Toolkit  N LT K is a powerful open source tool that provides a set of methods and algorithms to perform a wide range of N LP tasks, including tokenizing, parts of speech tagging, stemming, lemmatization, and more Tokenization Tokenization refers to the procedure of splitting a sentence into its constituent partsthe words and punctuation that it is made up of It is different from simply splitting the sentence on whitespaces, and instead actually divides the sentence into constituent words, numbers if any, and punctuation, which may not always be separated by whitespaces For example, consider this sentence I am reading a book Here, our task is to extract wordstokens from this sentence After passing this sentence to a tokenization program, the extracted wordstokens would be I, am, reading, a, book, and  this example extracts one token at a time Such tokens are called unigrams  N LT K provides a method called word_tokenize , which tokenizes given text into words It actually separates the text into different words based on punctuation and spaces between words To get a better understanding of tokenization, lets solve an exercise based on it in the next section Various Steps in N LP 9 Exercise 1 02 Tokenization of a Simple Sentence In this exercise, we will tokenize the words in a given sentence with the help of the N LT K library Follow these steps to implement this exercise using the sentence, I am reading N LP Fundamentals 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries and download the different types of N LT K data that we are going to use for different tasks in the following exercises from nltk import word_tokenize, download download In the preceding code, we are using N LT Ks download method, which downloads the given data from N LT K N LT K data contains different corpora and trained models In the preceding example, we will be downloading the stop word list, punkt , and a perceptron tagger, which is used to implement parts of speech tagging using a structured algorithm The data will be downloaded at nltk_datacorpora in the home directory of your computer Then, it will be loaded from the same path in further steps 3  The word_tokenize method is used to split the sentence into words tokens We need to add a sentence as input to the word_tokenize method so that it performs its job The result obtained will be a list, which we will store in a word variable To implement this, insert a new cell and add the following code def get_tokenssentence words word_tokenizesentence return words 4  In order to view the list of tokens generated, we need to view it using the print function Insert a new cell and add the following code to implement this printget_tokensI am reading N LP Fundamentals This code generates the following output 10 Introduction to Natural Language Processing We can see the list of tokens generated with the help of the word_tokenize method Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will see another pre processing step Parts of Speech Po S tagging  Po S Tagging In N LP, the term Po S refers to parts of speech Po S tagging refers to the process of tagging words within sentences with their respective Po S We extract the Po S of tokens constituting a sentence so that we can filter out the Po S that are of interest and analyze them For example, if we look at the sentence, The sky is blue, we get four tokens, namely The, sky, is, and blue, with the help of tokenization Now, using a Po S tagger , we tag the Po S for each wordtoken This will look as follows The preceding format is an output of the N LT K pos_tag method It is a list of tuples in which every tuple consists of the word followed by the Po S tag D T Determiner N N Noun, common, singular or mass V BZ Verb, present tense, third person singular J J Adjective For the complete list of Po S tags in N LT K, you can refer to Po S tagging is performed using different techniques, one of which is a rule based approach that builds a list to assign a possible tag for each word Various Steps in N LP 11 Po S tagging finds application in many N LP tasks, including word sense disambiguation, classification, Named Entity Recognition N ER , and coreference resolution For example, consider the usage of the word planted in these two sentences He planted the evidence for the case and He planted five trees in the garden We can see that the Po S tag of planted would clearly help us in differentiating between the different meanings of the sentences Lets perform a simple exercise to understand how Po S tagging is done in Python Exercise 1 03 Po S Tagging In this exercise, we will find out the Po S for each word in the sentence, I am reading N LP Fundamentals  We first make use of tokenization in order to get the tokens Later, we will use the pos_tag method, which will help us find the Po S for each wordtoken Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import word_tokenize, pos_tag 3  To find the tokens in the sentence, we make use of the word_tokenize method Insert a new cell and add the following code to implement this def get_tokenssentence words word_tokenizesentence return words 4  Print the tokens with the help of the print function To implement this, add a new cell and write the following code words get_tokensI am reading N LP Fundamentals printwords This code generates the following output 12 Introduction to Natural Language Processing 5  Well now use the pos_tag method Insert a new cell and add the following code def get_poswords return pos_tagwords get_poswords This code generates the following output I, P RP, am, V BP, reading, V BG, N LP, N NP, Fundamentals, N NS In the preceding output, we can see that for each token, a Po S has been allotted Here, P RP stands for personal pronoun , V BP stands for verb present , V GB stands for verb gerund , N NP stands for proper noun singular , and N NS stands for noun plural  Note To access the source code for this specific section, please refer to  You can also run this example online at  We have learned about assigning appropriate Po S labels to tokens in a sentence In the next section, we will learn about stop words in sentences and ways to deal with them Stop Word Removal Stop words are the most frequently occurring words in any language and they are just used to support the construction of sentences and do not contribute anything to the semantics of a sentence So, we can remove stop words from any text before an N LP process, as they occur very frequently and their presence does not have much impact on the sense of a sentence Removing them will help us clean our data, making its analysis much more efficient Examples of stop words include a, am, and, the, in, of, and more In the next exercise, we will look at the practical implementation of removing stop words from a given sentence Various Steps in N LP 13 Exercise 1 04 Stop Word Removal In this exercise, we will check the list of stop words provided by the nltk library Based on this list, we will filter out the stop words included in our text 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import download downloadstopwords from nltk import word_tokenize from nltk.corpus import stopwords 3  In order to check the list of stop words provided for English , we pass it as a parameter to the words function Insert a new cell and add the following code to implement this stop_words stopwords.wordsenglish 4  In the code, the list of stop words provided by English is stored in the stop_ words variable In order to view the list, we make use of the print function Insert a new cell and add the following code to view the list printstop_words This code generates the following output Figure 1 1 List of stop words provided by English 14 Introduction to Natural Language Processing 5  To remove the stop words from a sentence, we first assign a string to the sentence variable and tokenize it into words using the word_tokenize method Insert a new cell and add the following code to implement this sentence I am learning Python It is one of the most popular programming languages sentence_words word_tokenizesentence Note The code snippet shown here uses a backslash to split the logic across multiple lines When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line 6  To print the list of tokens, insert a new cell and add the following code printsentence_words This code generates the following output I, am, learning, Python, ., It, is, one, of, the, most, popular, programming, languages 7  To remove the stop words, we need to loop through each word in the sentence, check whether there are any stop words, and then finally combine them to form a complete sentence To implement this, insert a new cell and add the following code def remove_stop_wordssentence_words, stop_words return .joinword for word in sentence_words if word not in stop_words 8  To check whether the stop words are filtered out from our sentence, print the sentence_no_stops variable Insert a new cell and add the following code to print printremove_stop_wordssentence_words,stop_words This code generates the following output I learning Python It one popular programming languages Various Steps in N LP 15 As you can see in the preceding code snippet, stop words such as am, is, of, the, and most are being filtered out and text without stop words is produced as output 9  Add your own stop words to the stop word list stop_words.extend printremove_stop_wordssentence_words,stop_words This code generates the following output learning Python  popular programming languages As we can see from the output, now words such as I, It, and One are removed as we have added them to our custom stop word list We have learned how to remove stop words from given text Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will focus on normalizing text Text Normalization There are some words that are spelled, pronounced, and represented differentlyfor example, words such as Mumbai and Bombay, and U S and United States Although they are different, they refer to the same thing There are also different forms of words that need to be converted into base forms For example, words such as does and doing, when converted to their base form, become do Along these lines, text normalization is a process wherein different variations of text get converted into a standard form We need to perform text normalization as there are some words that can mean the same thing as each other There are various ways of normalizing text, such as spelling correction, stemming, and lemmatization, which will be covered later For a better understanding of this topic, we will look into a practical implementation of text normalization in the next section 16 Introduction to Natural Language Processing Exercise 1 05 Text Normalization In this exercise, we will normalize some given text Basically, we will be trying to replace select words with new words, using the replace function, and finally produce the normalized text replace is a built in Python function that works on strings and takes two arguments It will return a copy of a string in which the occurrence of the first argument will be replaced by the second argument Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to assign a string to the sentence variable sentence I visited the U S from the U K on 22 10 18 3  We want to replace U S with United States , U K with United Kingdom , and 18 with 2018  To do so, use the replace function and store the updated output in the normalized_sentence variable Insert a new cell and add the following code to implement this def normalizetext return text.replaceU S, United States .replaceU K, United Kingdom .replace- 18 , 2018 4  To check whether the text has been normalized, insert a new cell and add the following code to print it normalized_sentence normalizesentence printnormalized_sentence The code generates the following output I visited the United States from the United Kingdom on 22 10 2018 5  Add the following code normalized_sentence normalizeU S and U K are two superpowers printnormalized_sentence The code generates following output United States and United Kingdom are two superpowers Various Steps in N LP 17 In the preceding code, we can see that our text has been normalized Note To access the source code for this specific section, please refer to  You can also run this example online at  Over the next sections, we will explore various other ways in which text can be normalized Spelling Correction Spelling correction is one of the most important tasks in any N LP project It can be time consuming, but without it, there are high chances of losing out on important information Spelling correction is executed in two steps 1  Identify the misspelled word, which can be done by a simple dictionary lookup If there is no match found in the language dictionary, it is considered to be misspelled 2  Replace it or suggest the correctly spelled word There are a lot of algorithms for this task One of them is the minimum edit distance algorithm, which chooses the nearest correctly spelled word for a misspelled word The nearness is defined by the number of edits that need to be made to the misspelled word to reach the correctly spelled word For example, lets say there is a misspelled word, autocorect Now, to make it autocorrect, we need to add one r, and to make it auto, we need to delete 6 characters, which means that autocorrect is the correct spelling because it requires the fewest edits We make use of the autocorrect Python library to correct spellings autocorrect is a Python library used to correct the spelling of misspelled words for different languages It provides a method called spell , which takes a word as input and returns the correct spelling of the word Lets look at the following exercise to get a better understanding of this 18 Introduction to Natural Language Processing Exercise 1 06 Spelling Correction of a Word and a Sentence In this exercise, we will perform spelling correction on a word and a sentence, with the help of Pythons autocorrect library Follow these steps in order to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import word_tokenize from autocorrect import Speller 3  In order to correct the spelling of a word, pass a wrongly spelled word as a parameter to the spell function Before that, you have to create a spell object of the Speller class using langen to signify the English language Insert a new cell and add the following code to implement this spell Spellerlangen spellNatureal This code generates the following output Natural 4  To correct the spelling of a sentence, first tokenize it into tokens After that, loop through each token in sentence , autocorrect the words, and finally combine the words Insert a new cell and add the following code to implement this sentence word_tokenizeNtural Luanguage Processin deals with the art of extracting insightes from Natural Languaes 5  Use the print function to print all tokens Insert a new cell and add the following code to print the tokens printsentence This code generates the following output Ntural, Luanguage, Processin, deals, with, the, art, of, extracting, insightes, from, Natural, LanguaesVarious Steps in N LP 19 6  Now that we have got the tokens, loop through each token in sentence , correct the tokens, and assign them to a new variable Insert a new cell and add the following code to implement this def correct_spellingtokens sentence_corrected .joinspellword for word in tokens return sentence_corrected 7  To print the correct sentence, insert a new cell and add the following code printcorrect_spellingsentence This code generates the following output Natural, Language, Procession, deals, with, the, art, of, extracting, insights, from, Natural, Languages In the preceding code snippet, we can see that most of the wrongly spelled words have been corrected But the word Processin was wrongly converted into Procession  It should have been Processing  This happened because to change Processin to Procession or Processing , an equal number of edits is required To rectify this, we need to use other kinds of spelling correctors that are aware of context Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will look at stemming, which is another form of text normalization 20 Introduction to Natural Language Processing Stemming In most languages, words get transformed into various forms when being used in a sentence For example, the word product might get transformed into production when referring to the process of making something or transformed into products in plural form It is necessary to convert these words into their base forms, as they carry the same meaning in any case Stemming is the process that helps us to do so If we look at the following figure, we get a perfect idea of how words get transformed into their base forms Figure 1 2 Stemming of the word product To get a better understanding of stemming, lets perform a simple exercise In this exercise, we will be using two algorithms, called the porter stemmer and the snowball stemmer, provided by the N LT K library The porter stemmer is a rule based algorithm that transforms words to their base form by removing suffixes from words The snowball stemmer is an improvement over the porter stemmer and is a little bit faster and uses less memory In N LT K, this is done by the stem method provided by the Porter Stemmer class Exercise 1 07 Using Stemming In this exercise, we will pass a few words through the stemming process so that they get converted into their base forms Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import stem Various Steps in N LP 21 3  Now pass the following words as parameters to the stem method To implement this, insert a new cell and add the following code def get_stemsword,stemmer return stemmer.stemword porter Stem stem Porter Stemmer get_stemsproduction,porter Stem 4  When the input is production , the following output is generated product 5  Similarly, the following code would be used for the input coming  get_stemscoming,porter Stem We get the following output come 6  Similarly, the following code would be used for the input firing  get_stemsfiring,porter Stem When the input is firing , the following output is generated fire 7  The following code would be used for the input battling  get_stemsbattling,porter Stem If we give the input battling , the following output is generated battl 8  The following code will also generate the same output as above, for the input battling stemmer stem Snowball Stemmerenglish get_stemsbattling,stemmer The output will be as follows battl22 Introduction to Natural Language Processing As you have seen while using the snowball stemmer, we have to provide the language as english  We can also use the stemmer for different languages such as Spanish, French, and many more From the preceding code snippets, we can see that the entered words are converted into their base forms Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will focus on lemmatization , which is another form of text normalization Lemmatization Sometimes, the stemming process leads to incorrect results For example, in the last exercise, the word battling was transformed to battl , which is not a word To overcome such problems with stemming, we make use of lemmatization Lemmatization is the process of converting words to their base grammatical form, as in battling to battle, rather than just randomly axing words In this process, an additional check is made by looking through a dictionary to extract the base form of a word Getting more accurate results requires some additional information for example, Po S tags along with words will help in getting better results In the following exercise, we will be using Word Net Lemmatizer , which is an N LT K interface of Word Net Word Net is a freely available lexical English database that can be used to generate semantic relationships between words N LT Ks Word Net Lemmatizer provides a method called lemmatize , which returns the lemma grammatical base form of a given word using Word Net To put lemmatization into practice, lets perform an exercise where well use the lemmatize function Various Steps in N LP 23 Exercise 1 08 Extracting the Base Word Using Lemmatization In this exercise, we will use the lemmatization process to produce the proper form of a given word Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import download downloadwordnet from nltk.stem.wordnet import Word Net Lemmatizer 3  Create an object of the Word Net Lemmatizer class Insert a new cell and add the following code to implement this lemmatizer Word Net Lemmatizer 4  Bring the word to its proper form by using the lemmatize method of the Word Net Lemmatizer class Insert a new cell and add the following code to implement this def get_lemmaword return lemmatizer.lemmatizeword get_lemmaproducts With the input products , the following output is generated product 5  Similarly, use the input as production now get_lemmaproduction With the input production , the following output is generated production 6  Similarly, use the input as coming now get_lemmacoming With the input coming , the following output is generated coming24 Introduction to Natural Language Processing Hence, we have learned how to use the lemmatization process to transform a given word into its base form Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will look at another preprocessing step in N LP named entity recognition N ER  Named Entity Recognition N ER N ER is the process of extracting important entities, such as person names, place names, and organization names, from some given text These are usually not present in dictionaries So, we need to treat them differently The main objective of this process is to identify the named entities such as proper nouns and map them to categories, which are already defined For example, categories might include names of people, places, and so on N ER has found use in many N LP tasks, including assigning tags to news articles, search algorithms, and more N ER can analyze a news article and extract the major people, organizations, and places discussed in it and assign them as tags for new articles In the case of search algorithms, lets suppose we have to create a search engine, meant specifically for books If we were to submit a given query for all the words, the search would take a lot of time Instead, if we extract the top entities from all the books using N ER and run a search query on the entities rather than all the content, the speed of the system would increase dramatically To get a better understanding of this process, well perform an exercise Before moving on to the exercise, let me introduce you to chunking, which we are going to use in the following exercise Chunking is the process of grouping words together into chunks, which can be further used to find noun groups and verb groups, or can also be used for sentence partitioning Various Steps in N LP 25 Exercise 1 09 Treating Named Entities In this exercise, we will find the named entities in a given sentence Follow these steps to implement this exercise using the following sentence We are reading a book published by Packt which is based out of Birmingham 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import download from nltk import pos_tag from nltk import ne_chunk from nltk import word_tokenize downloadmaxent_ne_chunker downloadwords 3  Declare the sentence variable and assign it a string Insert a new cell and add the following code to implement this sentence We are reading a book published by Packt which is based out of Birmingham 4  To find the named entities from the preceding text, insert a new cell and add the following code def get_nertext i ne_chunkpos_tagword_tokenizetext, binaryTrue return get_nersentence This code generates the following output , TreeN E, 26 Introduction to Natural Language Processing In the preceding code, we can see that the code identifies the named entities Packt and Birmingham and maps them to an already defined category, N NP  Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will focus on word sense disambiguation, which helps us to identify the right sense of any word Word Sense Disambiguation Theres a popular saying A man is known by the company he keeps Similarly, a words meaning depends on its association with other words in a sentence This means two or more words with the same spelling may have different meanings in different contexts This often leads to ambiguity Word sense disambiguation is the process of mapping a word to the sense that it should carry We need to disambiguate words based on the sense they carry so that they can be treated as different entities when being analyzed The following figure displays a perfect example of how ambiguity is caused due to the usage of the same word in different sentences Figure 1 3 Word sense disambiguation One of the algorithms to solve word sense disambiguation is the Lesk algorithm It has a huge corpus in the background generally Word Net is used that contains definitions of all the possible synonyms of all the possible words in a language Then it takes a word and the context as input and finds a match between the context and all the definitions of the word The meaning with the highest number of matches with the context of the word will be returned Word Sense Disambiguation 27 For example, suppose we have a sentence such as We play only soccer in a given text Now, we need to find the meaning of the word play in this sentence In the Lesk algorithm, each word with ambiguous meaning is saved in background synsets In this case, the word play will be saved with all possible definitions Lets say we have two definitions of the word play 1  Play Participating in a sport or game 2  Play Using a musical instrument Then, we will find the similarity between the context of the word play in the text and both of the preceding definitions using text similarity techniques The definition best suited to the context of play in the sentence will be considered the meaning or definition of the word In this case, we will find that our first definition fits best in context, as the words sport and game are present in the preceding sentences In the next exercise, we will be using the Lesk module from N LT K It takes a sentence and the word as input, and returns the meaning or definition of the word The output of the Lesk method is synset , which contains the I D of the matched definition These I Ds can be matched with their definitions using the definition method of wsd.synsetword  To get a better understanding of this process, lets look at an exercise Exercise 1 10 Word Sense Disambiguation In this exercise, we will find the sense of the word bank in two different sentences Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import nltk nltk.downloadwordnet from nltk.wsd import lesk from nltk import word_tokenize 3  Declare two variables, sentence1 and sentence2 , and assign them with appropriate strings Insert a new cell and the following code to implement this sentence1 Keep your savings in the bank sentence2 Its so risky to drive over the banks of the road28 Introduction to Natural Language Processing 4  To find the sense of the word bank in the preceding two sentences, use the Lesk algorithm provided by the nltk.wsd library Insert a new cell and add the following code to implement this def get_synsetsentence, word return leskword_tokenizesentence, word get_synsetsentence1,bank This code generates the following output Synsetsavings_bank.n 02 5  Here, savings_bank.n 02 refers to a container for keeping money safely at home To check the other sense of the word bank, write the following code get_synsetsentence2,bank This code generates the following output Synsetbank.v 07 Here, bank.v 07 refers to a slope in the turn of a road Thus, with the help of the Lesk algorithm, we were able to identify the sense of a word in whatever context Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will focus on sentence boundary detection , which helps detect the start and end points of sentences Sentence Boundary Detection Sentence boundary detection is the method of detecting where one sentence ends and another begins If you are thinking that this sounds pretty easy, as a period  or a question mark denotes the end of a sentence and the beginning of another sentence, then you are wrong There can also be instances where the letters of acronyms are separated by full stops, for instance Various analyses need to be performed at a sentence level detecting the boundaries of sentences is essential Sentence Boundary Detection 29 An exercise will provide us with a better understanding of this process Exercise 1 11 Sentence Boundary Detection In this exercise, we will extract sentences from a paragraph To do so, well be using the sent_tokenize method, which is used to detect sentence boundaries The following steps need to be performed 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import nltk from nltk.tokenize import sent_tokenize 3  Use the sent_tokenize method to detect sentences in some given text Insert a new cell and add the following code to implement this def get_sentencestext return sent_tokenizetext get_sentencesWe are reading a book Do you know who is the publisher It is Packt Packt is based out of Birmingham This code generates the following output We are reading a book Do you know who is the publisher It is Packt., Packt is based out of Birmingham 4  Use the sent_tokenize method for text that contains periods  other than those found at the ends of sentences get_sentencesMr Donald John Trump is the current president of the U SA Before joining politics, he was a businessman The code will generate the following output Mr Donald John Trump is the current president of the U SA., Before joining politics, he was a businessman 30 Introduction to Natural Language Processing As you can see in the code, the sent_tokenize method is able to differentiate between the period  after Mr and the one used to end the sentence We have covered all the preprocessing steps that are involved in N LP Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, using the knowledge we have gained, lets perform an activity Activity 1 01 Preprocessing of Raw Text We have a text corpus that is in an improper format In this activity, we will perform all the preprocessing steps that were discussed earlier to get some meaning out of the text Note The text corpus, file.txt , can be found at this location After downloading the file, place it in the same directory as the notebook Follow these steps to implement this activity 1  Import the necessary libraries 2  Load the text corpus to a variable 3  Apply the tokenization process to the text corpus and print the first 20 tokens 4  Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus 5  Apply Po S tags to each of the corrected tokens and print them 6  Remove stop words from the corrected token list and print the initial 20 tokens 7  Apply stemming and lemmatization to the corrected token list and then print the initial 20 tokens Kick Starting an N LP Project 31 8  Detect the sentence boundaries in the given text corpus and print the total number of sentences Note The solution for this activity can be found via this link  We have learned about and achieved the preprocessing of given data By now, you should be familiar with what N LP is and what basic preprocessing steps are needed to carry out any N LP project In the next section, we will focus on the different phases of an N LP project Kick Starting an N LP Project We can divide an N LP project into several sub-projects or phases These phases are completed in a particular sequence This tends to increase the overall efficiency of the process, as memory usage changes from one phase to the next An N LP project has to go through six major phases, which are outlined in the following figure Figure 1 4 Phases of an N LP project 32 Introduction to Natural Language Processing Suppose you are working on a project in which you need to classify emails as important and unimportant We will explain how this is carried out by discussing each phase in detail Data Collection This is the initial phase of any N LP project Our sole purpose is to collect data as per our requirements For this, we may either use existing data, collect data from various online repositories, or create our own dataset by crawling the web In our case, we will collect different email data We can even get this data from our personal emails as well, to start with Data Preprocessing Once the data is collected, we need to clean it For the process of cleaning, we will make use of the different preprocessing steps that we have learned about in this chapter It is necessary to clean the collected data to ensure effectiveness and accuracy In our case, we will follow these preprocessing steps 1  Converting all the text data to lowercase 2  Stop word removal 3  Text normalization, which will include replacing all numbers with some common term and replacing punctuation with empty strings 4  Stemming and lemmatization Feature Extraction Computers understand only binary digits 0 and 1  As such, every instruction we feed into a computer gets transformed into binary digits Similarly, machine learning models tend to understand only numeric data Therefore, it becomes necessary to convert text data into its equivalent numerical form To convert every email into its equivalent numerical form, we will create a dictionary of all the unique words in our data and assign a unique index to each word Then, we will represent every email with a list having a length equal to the number of unique words in the data The list will have 1 at the indices of words that are present in the email and 0 at the other indices This is called one hot encoding We will learn more about this in coming chapters Summary 33 Model Development Once the feature set is ready, we need to develop a suitable model that can be trained to gain knowledge from the data These models are generally statistical, machine learning based, deep learning based, or reinforcement learning based In our case, we will build a model that is capable of differentiating between important and unimportant emails Model Assessment After developing a model, it is essential to benchmark it This process of benchmarking is known as model assessment In this step, we will evaluate the performance of our model by comparing it to others This can be done by using different parameters or metrics These parameters include precision, recall, and accuracy In our case, we will evaluate the newly created model by seeing how well it performs at classifying emails as important and unimportant Model Deployment This is the final stage for most industrial N LP projects In this stage, the models are put into production They are either integrated into an existing system or new products are created by keeping this model as a base In our case, we will deploy our model to production, so that it can classify emails as important and unimportant in real time Summary In this chapter, we learned about the basics of N LP and how it differs from text analytics We covered the various preprocessing steps that are included in N LP, such as tokenization, Po S tagging, stemming, lemmatization, and more We also looked at the different phases an N LP project has to pass through, from data collection to model deployment In the next chapter, you will learn about the different methods of extracting features from unstructured text, such as T F I DF and bag of words You will also learn about N LP tasks such as tokenization, lemmatization, and stemming in more detail Furthermore, text visualization techniques such as word clouds will be introduced Overview In this chapter, you will be able to categorize data based on its content and structure You will be able to describe preprocessing steps in detail and implement them to clean up text data You will learn about feature engineering and calculate the similarity between texts Once you understand these concepts, you will be able to use word clouds and some other techniques to visualize text Feature Extraction Methods236 Feature Extraction Methods Introduction In the previous chapter, we learned about the concepts of Natural Language Processing N LP and text analytics We also took a quick look at various preprocessing steps In this chapter, we will learn how to make text understandable to machine learning algorithms As we know, to use a machine learning algorithm on textual data, we need a numerical or vector representation of text data since most of these algorithms are unable to work directly with plain text or strings But before converting the text data into numerical form, we will need to pass it through some preprocessing steps such as tokenization, stemming, lemmatization, and stop word removal So, in this chapter, we will learn a little bit more about these preprocessing steps and how to extract features from the preprocessed text and convert them into vectors We will also explore two popular methods for feature extraction Bag of Words and Term Frequency Inverse Document Frequency, as well as various methods for finding similarity between different texts By the end of this chapter, you will have gained an in depth understanding of how text data can be visualized Types of Data To deal with data effectively, we need to understand the various forms in which it exists First, lets explore the types of data that exist There are two main ways to categorize data by structure and by content, as explained in the upcoming sections Types of Data 37 Categorizing Data Based on Structure Data can be divided on the basis of structure into three categories, namely, structured, semi structured, and unstructured data, as shown in the following diagram Figure 2 1 Categorization based on content These three categories are as follows Structured data This is the most organized form of data It is represented in tabular formats such as Excel files and Comma Separated Value C SV files The following image shows what structured data usually looks like Figure 2 2 Structured data The preceding table contains information about five people, with each row representing a person and each column representing one of their attributes 38 Feature Extraction Methods Semi structured data This type of data is not presented in a tabular structure, but it can be transformed into a table Here, information is usually stored between tags following a definite pattern X ML and H TM L files can be referred to as semi structured data The following screenshot shows how semi structured data can appear Figure 2 3 Semi structured data Types of Data 39 The format shown in the preceding screenshot is called markup language format Here, the data is stored between tags, hierarchically It is a universally accepted format, and there are a lot of parsers available that can convert this data into structured data Unstructured data This type of data is the most difficult to deal with Machine learning algorithms would find it difficult to comprehend unstructured data without any loss of information Text corpora and images are examples of unstructured data The following image shows what unstructured data looks like Figure 2 4 Unstructured data This is called unstructured data because if we want to get employee details from the preceding text snippet with our program, we will not be able to do so by simple parsing We have to make our algorithm understand the semantics of the language to make it able to extract information from this Categorizing Data Based on Content Data can be divided into four categories based on content, as shown in the following diagram Figure 2 5 Categorizing data based on structure 40 Feature Extraction Methods Lets look at each category here Text data This refers to text corpora consisting of written sentences This type of data can only be read An example would be the text corpus of a book Image data This refers to pictures that are used to communicate messages This type of data can only be seen Audio data This refers to voice recordings, music, and so on This type of data can only be heard Video data A continuous series of images coupled with audio forms a video This type of data can be seen as well as heard With that, we have learned about the different types of data and their categorization on the basis of structure and content When dealing with unstructured data, it is necessary to clean it first In the next section, we will look into some of the preprocessing steps for cleaning data Cleaning Text Data The text data that we are going to discuss here is unstructured text data, which consists of written sentences Most of the time, this text data cannot be used as it is for analysis because it contains some noisy elements, that is, elements that do not really contribute much to the meaning of the sentence at all These noisy elements need to be removed because they do not contribute to the meaning and semantics of the text If theyre not removed, they can not only waste system memory and processing time, but also negatively impact the accuracy of the results Data cleaning is the art of extracting meaningful portions from data by eliminating unnecessary details Consider the sentence, He tweeted, Live coverage of General Elections available at this.tvshowge2019 __ Please tune in  In this example, to perform N LP tasks on the sentence, we will need to remove the emojis, punctuation, and stop words, and then change the words into their base grammatical form Cleaning Text Data 41 To achieve this, methods such as stopword removal, tokenization, and stemming are used We will explore them in detail in the upcoming sections Before we do so, lets get acquainted with some basic N LP libraries that we will be using here Re This is a standard Python library thats used for string searching and string manipulation It contains methods such as match , search , findall , split , and sub , which are used for basic string matching, searching, replacing, and more, using regular expressions A regular expression is nothing but a set of characters in a specific order that represents a pattern This pattern is searched for in the texts textblob This is an open source Python library that provides different methods for performing various N LP tasks such as tokenization and Po S tagging It is similar to nltk , which was introduced in Chapter 1 , Introduction to Natural Language Processing  It is built on the top of nltk and is much simpler as it has an easier to use interface and excellent documentation In projects that do not involve a lot of complexity, it should be preferable to nltk  keras This is an open source, high level neural network library thats was developed on top of another neural network library called Tensor Flow  In addition to neural network functionality, it also provides methods for basic text processing and N LP tasks Tokenization Tokenization and word tokenizers were briefly described in Chapter 1 , Introduction to Natural Language Processing  Tokenization is the process of splitting sentences into their constituents that is, words and punctuation Lets perform a simple exercise to see how this can be done using various packages 42 Feature Extraction Methods Exercise 2 01 Text Cleaning and Tokenization In this exercise, we will clean some text and extract the tokens from it Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import the re package import re 3  Create a method called clean_text that will delete all characters other than digits, alphabetical characters, and whitespaces from the text and split the text into tokens For this, we will use the text which matches with all non alphanumeric characters, and we will replace all of them with an empty string def clean_textsentence return re.subr_, , sentence.split 4  Store the sentence to be cleaned in a variable named sentence and pass it through the preceding function Add the following code to this implement sentence Sunil tweeted, Witnessing 70th Republic Day of India from Rajpath, New Delhi Mesmerizing performance by Indian Army Awesome airshow india_official indian_army India 70th Republic_ Day For more photos ping me sunilphotoking.com clean_textsentenceCleaning Text Data 43 The preceding command fragments the string wherever any blank space is present The output should be as follows Figure 2 6 Fragmented string 44 Feature Extraction Methods With that, we have learned how to extract tokens from text Often, extracting each token separately does not help For instance, consider the sentence, I do not hate you, but your behavior Here, if we process each of the tokens, such as hate and behavior, separately, then the true meaning of the sentence would not be comprehended In this case, the context in which these tokens are present becomes essential Thus, we consider n consecutive tokens at a time n grams refers to the grouping of n consecutive tokens together Note To access the source code for this specific section, please refer to  You can also run this example online at  Next, we will look at an exercise where n grams can be extracted from a given text Exercise 2 02 Extracting n grams In this exercise, we will extract n grams using three different methods First, we will use custom defined functions, and then the nltk and textblob libraries Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import the re package and create a custom defined function, which we can use to extract n-grams Add the following code to do this import re def n_gram_extractorsentence, n tokens re.subr_, , sentence.split for i in rangelentokens-n1 printtokens In the preceding function, we are splitting the sentence into tokens using regex, then looping over the tokens, taking n consecutive tokens at a time Cleaning Text Data 45 3  If n is 2 , two consecutive tokens will be taken, resulting in bigrams To check the bigrams, we pass the function the text and with n2 Add the following code to do this n_gram_extractorThe cute little boy is playing with the kitten., 2 The preceding code generates the following output 4  To check the trigrams, we pass the function with the text and with n3 Add the following code to do this n_gram_extractorThe cute little boy is playing with the kitten., 3 The preceding code generates the following output 5  To check the bigrams using the nltk library, add the following code from nltk import ngrams listngramsThe cute little boy is playing with the kitten .split, 246 Feature Extraction Methods The preceding code generates the following output The, cute, cute, little, little, boy, boy, is, is, playing, playing, with, with, the, the, kitten 6  To check the trigrams using the nltk library, add the following code listngramsThe cute little boy is playing with the kitten..split, 3 The preceding code generates the following output The, cute, little, cute, little, boy, little, boy, is, boy, is, playing, playing, with, the, with, the, kitten 7  To check the bigrams using the textblob library, add the following code pip install -Utextblob from textblob import Text Blob blob Text BlobThe cute little boy is playing with the kitten blob.ngramsn2 The preceding code generates the following output , Word List, Word List, Word List, Word List, Word List, Word List, Word ListCleaning Text Data 47 8  To check the trigrams using the textblob library, add the following code blob.ngramsn3 The preceding code generates the following output , Word List, Word List, Word List, Word List, Word List, Word List In this exercise, we learned how to generate n grams using various methods Note To access the source code for this specific section, please refer to  You can also run this example online at  Exercise 2 03 Tokenizing Text with Keras and Text Blob In this exercise, we will use keras and textblob to tokenize texts Follow these steps to complete this exercise 1  Open a Jupyter Notebook and insert a new cell 2  Import the keras and textblob libraries and declare a variable named sentence , as follows from keras.preprocessing.text import text_to_word_sequence from textblob import Text Blob sentence Sunil tweeted, Witnessing 70th Republic Day of India from Rajpath, New Delhi Mesmerizing performance by Indian Army Awesome airshow india_official indian_army India 70th Republic_ Day For more photos ping me sunilphotoking.com 48 Feature Extraction Methods 3  To tokenize using the keras library, add the following code def get_keras_tokenstext return text_to_word_sequencetext get_keras_tokenssentence The preceding code generates the following output Figure 2 7 Tokenization using Keras Cleaning Text Data 49 4  To tokenize using the textblob library, add the following code def get_textblob_tokenstext blob Text Blobtext return blob.words get_textblob_tokenssentence The preceding code generates the following output Figure 2 8 Tokenization using textblob With that, we have learned how to tokenize texts using the keras and textblob libraries Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will discuss the different types of tokenizers 50 Feature Extraction Methods Types of Tokenizers There are different types of tokenizers that come in handy for specific tasks Lets look at the ones provided by nltk one by one Whitespace tokenizer This is the simplest type of tokenizer It splits a string wherever a space, tab, or newline character is present Tweet tokenizer This is specifically designed for tokenizing tweets It takes care of all the special characters and emojis used in tweets and returns clean tokens M WE tokenizer M WE stands for Multi-Word Expression Here, certain groups of multiple words are treated as one entity during tokenization, such as United States of America, Peoples Republic of China, not only, and but also These predefined groups are added at the beginning with mwe methods Regular expression tokenizer These tokenizers are developed using regular expressions Sentences are split based on the occurrence of a specific pattern a regular expression Word Punct Tokenizer This splits a piece of text into a list of alphabetical and non alphabetical characters It actually splits text into tokens using a fixed regex , that is, w  Now that we have learned about the different types of tokenizers, in the next section, we will carry out an exercise to get a better understanding of them Cleaning Text Data 51 Exercise 2 04 Tokenizing Text Using Various Tokenizers In this exercise, we will use different tokenizers to tokenize text Perform the following steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and the following code to import all the tokenizers and declare a variable sentence from nltk.tokenize import Tweet Tokenizer from nltk.tokenize import M WE Tokenizer from nltk.tokenize import Regexp Tokenizer from nltk.tokenize import Whitespace Tokenizer from nltk.tokenize import Word Punct Tokenizer sentence Sunil tweeted, Witnessing 70th Republic Day of India from Rajpath, New Delhi Mesmerizing performance by Indian Army Awesome airshow india_official indian_army India 70th Republic_ Day For more photos ping me sunilphotoking.com 3  To tokenize the text using Tweet Tokenizer , add the following code def tokenize_with_tweet_tokenizertext Here will create an object of tweet Tokenizer tweet_tokenizer Tweet Tokenizer Then we will call the tokenize method of tweet Tokenizer which will return token list of sentences return tweet_tokenizer.tokenizetext tokenize_with_tweet_tokenizersentence Note The symbol in the code snippet above denotes a code comment Comments are added into code to help explain specific bits of logic 52 Feature Extraction Methods The preceding code generates the following output Figure 2 9 Tokenization using Tweet Tokenizer As you can see, the hashtags, emojis, websites, and Twitter I Ds are extracted as single tokens If we had used the white space tokenizer, we would have got hash, dots, and the symbol as separate tokens Cleaning Text Data 53 4  To tokenize the text using M WE Tokenizer , add the following code def tokenize_with_mwetext mwe_tokenizer M WE Tokenizer mwe_tokenizer.add_mweIndian, Army return mwe_tokenizer.tokenizetext.split tokenize_with_mwesentence The preceding code generates the following output Figure 2 10 Tokenization using the M WE tokenizer 54 Feature Extraction Methods In the preceding screenshot, the words Indian and Army, which should have been treated as a single identity, were treated separately This is because Army not Army is treated as a token Lets see how this can be fixed in the next step 5  Add the following code to fix the issues in the previous step tokenize_with_mwesentence.replace, The preceding code generates the following output Figure 2 11 Tokenization using the M WE tokenizer after removing the sign Here, we can see that instead of being treated as separate tokens, Indian and Army are treated as a single entity Cleaning Text Data 55 6  To tokenize the text using the regular expression tokenizer, add the following code def tokenize_with_regex_tokenizertext reg_tokenizer Regexp TokenizerwS return reg_tokenizer.tokenizetext tokenize_with_regex_tokenizersentence The preceding code generates the following output Figure 2 12 Tokenization using the regular expression tokenizer 56 Feature Extraction Methods 7  To tokenize the text using the whitespace tokenizer, add the following code def tokenize_with_wsttext wh_tokenizer Whitespace Tokenizer return wh_tokenizer.tokenizetext tokenize_with_wstsentence The preceding code generates the following output Figure 2 13 Tokenization using the whitespace tokenizer Cleaning Text Data 57 8  To tokenize the text using the Word Punct tokenizer, add the following code def tokenize_with_wordpunct_tokenizertext wp_tokenizer Word Punct Tokenizer return wp_tokenizer.tokenizetext tokenize_with_wordpunct_tokenizersentence The preceding code generates the following output Figure 2 14 Tokenization using the Word Punct tokenizer 58 Feature Extraction Methods In this section, we have learned about different tokenization techniques and their nltk implementation Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, we are ready to use them in our programs Stemming In many languages, the base forms of words change when theyre used in sentences For example, the word produce can be written as production or produced or even producing, depending on the context The process of converting a word back into its base form is known as stemming It is essential to do this, because without it, algorithms would treat two or more different forms of the same word as different entities, despite them having the same semantic meaning So, the words producing and produced would be treated as different entities, which can lead to erroneous inferences In Python, Regexp Stemmer and Porter Stemmer are the most widely used stemmers Lets explore them one at a time Regexp Stemmer Regexp Stemmer uses regular expressions to check whether morphological or structural prefixes or suffixes are present For instance, in many cases, verbs in the present continuous tense the present tense form ending with ing can be restored to their base form simply by removing ing from the end for example, playing becomes play Lets complete the following exercise to get some hands on experience with Regexp Stemmer  Cleaning Text Data 59 Exercise 2 05 Converting Words in the Present Continuous Tense into Base Words with Regexp Stemmer In this exercise, we will use Regexp Stemmer on text to convert words into their basic form by removing some generic suffixes such as ing and ed To use nltk s regex_stemmer , we have to create an object of Regexp Stemmer by passing the regex of the suffix or prefix and an integer, min , which indicates the minimum length of the stemmed string Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and import Regexp Stemmer from nltk.stem import Regexp Stemmer 3  Use regex_stemmer to stem each word of the sentence variable Add the following code to do this def get_stemstext Creating an object of Regexp Stemmer, any string ending with the given regex ing will be removed regex_stemmer Regexp Stemmering, min4 The below code line will convert every word into its stem using regex stemmer and then join them with space return .joinregex_stemmer.stemwd for wd in text.split sentence I love playing football get_stemssentence The preceding code generates the following output I love play football60 Feature Extraction Methods As we can see, the word playing has been changed into its base form, play  In this exercise, we learned how we can perform stemming using nltk s Regexp Stemmer  Note To access the source code for this specific section, please refer to  You can also run this example online at  The Porter Stemmer The Porter stemmer is the most common stemmer for dealing with English words It removes various morphological and inflectional endings such as suffixes, prefixes, and the plural s from English words In doing so, it helps us extract the base form of a word from its variations To get a better understanding of this, lets carry out a simple exercise Exercise 2 06 Using the Porter Stemmer In this exercise, we will apply the Porter stemmer to some text Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import nltk and any related packages and declare a sentence variable Add the following code to do this from nltk.stem.porter import sentence Before eating, it would be nice to sanitize your hands with a sanitizerCleaning Text Data 61 3  Now, well make use of the Porter stemmer to stem each word of the sentence variables def get_stemstext ps_stemmer Porter Stemmer return .joinps_stemmer.stemwd for wd in text.split get_stemssentence The preceding code generates the following output befor eating, it would be nice to sanit your hand wash with a sanit Note To access the source code for this specific section, please refer to  You can also run this example online at  Porter Stemmer is a generic rule based stemmer that tries to convert a word into its basic form by removing common suffixes and prefixes of the English language Though stemming is a useful technique in N LP, it has a severe drawback As we can see from this exercise, we find that, while eating has been converted into eat which is its proper grammatical base form, the word sanitize has been converted into sanit which is not the proper grammatical base form This may lead to some problems if we use it To overcome this issue, there is another technique we can use called lemmatization Lemmatization As we saw in the previous section, there is a problem with stemming It often generates meaningless words Lemmatization deals with such cases by using vocabulary and analyzing the words morphologies It returns the base forms of words that can be found in dictionaries Lets walk through a simple exercise to understand this better 62 Feature Extraction Methods Exercise 2 07 Performing Lemmatization In this exercise, we will perform lemmatization on some text Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import nltk and its related packages, and then declare a sentence variable Add the following code to implement this import nltk from nltk.stem import Word Net Lemmatizer from nltk import word_tokenize nltk.downloadwordnet nltk.downloadpunkt sentence The products produced by the process today are far better than what it produces generally 3  To lemmatize the tokens, we extracted from the sentence, add the following code lemmatizer Word Net Lemmatizer def get_lemmastext lemmatizer Word Net Lemmatizer return .joinlemmatizer.lemmatizeword for word in word_tokenizetext get_lemmassentence The preceding code generates the following output The product produced by the process today are far better than what it produce generally Cleaning Text Data 63 With that, we learned how to generate the lemma of a word The lemma is the correct grammatical base form They use the vocabulary to match the word to its correct nearest grammatical form Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will deal with other kinds of word variations by looking at singularizing and pluralizing words using textblob  Exercise 2 08 Singularizing and Pluralizing Words In this exercise, we will make use of the textblob library to singularize and pluralize words in the given text Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import Text Blob and declare a sentence variable Add the following code to implement this from textblob import Text Blob sentence Text BlobShe sells seashells on the seashore To check the list of words in the sentence, type the following code sentence.words The preceding code generates the following output Word List64 Feature Extraction Methods 3  To singularize the third word in the sentence, type the following code def singularizeword return word.singularize singularizesentence.words The preceding code generates the following output seashell 4  To pluralize the fifth word in the given sentence, type the following code def pluralizeword return word.pluralize pluralizesentence.words The preceding code generates the following output seashores Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, in the next section, we will learn about another preprocessing task language translation Language Translation You might have used Google Translate before, which gives the exact translation of a word in another language this is an example of language translation or machine translation In Python, we can use Text Blob to translate text from one language into another Text Blob provides a method called translate , in which you have to pass text in the source language The method will return the translated word in the destination language Lets look at how this is done Cleaning Text Data 65 Exercise 2 09 Language Translation In this exercise, we will make use of the Text Blob library to translate a sentence from Spanish into English Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import Text Blob , as follows from textblob import Text Blob 3  Make use of the translate function of Text Blob to translate the input text from Spanish to English Add the following code to do this def translatetext,from_l,to_l en_blob Text Blobtext return en_blob.translatefrom_langfrom_l, toto_l translatetextmuy bien,from_les,to_len The preceding code generates the following output Text Blobvery well With that, we have seen how we can use Text Blob to translate from one language to another Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will look at another preprocessing task stop word removal 66 Feature Extraction Methods Stop Word Removal Stop words, such as am, the, and are, occur frequently in text data Although they help us construct sentences properly, we can find the meaning even if we remove them This means that the meaning of text can be inferred even without them So, removing stop words from text is one of the preprocessing steps in N LP tasks In Python, nltk , and textblob , text can be used to remove stop words from text To get a better understanding of this, lets look at an exercise Exercise 2 10 Removing Stop Words from Text In this exercise, we will remove the stop words from a given text Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import nltk and declare a sentence variable with the text in question from nltk import word_tokenize sentence She sells seashells on the seashore 3  Define a remove_stop_words method and remove the custom list of stop words from the sentence by using the following lines of code def remove_stop_wordstext,stop_word_list return .joinword for word in word_tokenizetext if word.lower not in stop_word_list custom_stop_word_list remove_stop_wordssentence,custom_stop_word_list The preceding code generates the following output sells seashells seashoreCleaning Text Data 67 Thus, we have seen how stop words can be removed from a sentence Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next activity, well put our knowledge of preprocessing steps into practice Activity 2 01 Extracting Top Keywords from the News Article In this activity, you will extract the most frequently occurring keywords from a sample news article Note The new article thats being used for this activity can be found at  The following steps will help you implement this activity 1  Open a Jupyter Notebook 2  Import nltk and any other necessary libraries 3  Define some functions to help you load the text file, convert the string into lowercase, tokenize the text, remove the stop words, and perform stemming on all the remaining tokens Finally, define a function to calculate the frequency of all these words 4  Load news_article.txt using a Python file reader into a single string 5  Convert the text string into lowercase 6  Split the string into tokens using a white space tokenizer 68 Feature Extraction Methods 7  Remove any stop words 8  Perform stemming on all the tokens 9  Calculate the frequency of all the words after stemming Note The solution for this activity can be found via this link  With that, we have learned about the various ways we can clean unstructured data Now, lets examine the concept of extracting features from texts Feature Extraction from Texts As we already know, machine learning algorithms do not understand textual data directly We need to represent the text data in numerical form or vectors To convert each textual sentence into a vector, we need to represent it as a set of features This set of features should uniquely represent the text, though, individually, some of the features may be common across many textual sentences Features can be classified into two different categories General features These features are statistical calculations and do not depend on the content of the text Some examples of general features could be the number of tokens in the text, the number of characters in the text, and so on Specific features These features are dependent on the inherent meaning of the text and represent the semantics of the text For example, the frequency of unique words in the text is a specific feature Lets explore these in detail Extracting General Features from Raw Text As we have already learned, general features refer to those that are not directly dependent on the individual tokens constituting a text corpus Lets consider these two sentences The sky is blue and The pillar is yellow Here, the sentences have the same number of words a general featurethat is, four But the individual constituent tokens are different Lets complete an exercise to understand this better Feature Extraction from Texts 69 Exercise 2 11 Extracting General Features from Raw Text In this exercise, we will extract general features from input text These general features include detecting the number of words, the presence of wh words words beginning with wh, such as what and why and the language in which the text is written Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import the pandas library and create a Data Frame with four sentences Add the following code to implement this import pandas as pd from textblob import Text Blob df pd Data FrameThe interim budget for 2019 will be announced on 1st February., Do you know how much expectation the middle class working population is having from this budget, February is the shortest month in a year., This financial year will end on 31st March df.columns df.head The preceding code generates the following output Figure 2 15 Data Frame consisting of four sentences 70 Feature Extraction Methods 3  Use the apply function to iterate through each row of the column text, convert them into Text Blob objects, and extract words from them Add the following code to implement this def add_num_wordsdf df df.applylambda x lenText Blobstrx.words return df add_num_wordsdf The preceding code generates the following output 0 11 1 15 2 8 3 8 Name number_of_words, dtype int64 The preceding code line will print the number_of_words column of the Data Frame to represent the number of words in each row 4  Use the apply function to iterate through each row of the column text, convert the text into Text Blob objects, and extract the words from them to check whether any of them belong to the list of wh words that has been declared Add the following code to do so def is_presentwh_words, df The below line of code will find the intersection between set of tokens of every sentence and the wh_words and will return true if the length of intersection set is non zero df df.applylambda x True if lensetText Blobstrx words.intersectionwh_words 0 else False return df Feature Extraction from Texts 71 wh_words setwhy, who, which, what, where, when, how is_presentwh_words, df The preceding code generates the following output 0 False 1 True 2 False 3 False Name is_wh_words_present, dtype bool The preceding code line will print the is_wh_words_present column that was added by the is_present method to df, which means for every row, we will see whether wh_word is present 5  Use the apply function to iterate through each row of the column text, convert them into Text Blob objects, and detect their languages def get_languagedf df df.applylambda x Text Blobstrx.detect_language return df get_languagedf The preceding code generates the following output 0 en 1 en 2 en 3 en Name language, dtype object With that, we have learned how to extract general features from text data Note To access the source code for this specific section, please refer to  You can also run this example online at  Lets perform another exercise to get a better understanding of this 72 Feature Extraction Methods Exercise 2 12 Extracting General Features from Text In this exercise, we will extract various general features from documents The dataset that we will be using here consists of random statements Our objective is to find the frequency of various general features such as punctuation, uppercase and lowercase words, letters, digits, words, and whitespaces Note The dataset that is being used in this exercise can be found at this link 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd from string import punctuation import nltk nltk.downloadtagsets from nltk.data import load nltk.downloadaveraged_perceptron_tagger from nltk import pos_tag from nltk import word_tokenize from collections import Counter 3  To see what different kinds of parts of speech nltk provides, add the following code def get_tagsets tagdict loadhelptagsetsupenn_tagset.pickle return listtagdict.keys tag_list get_tagsets printtag_listFeature Extraction from Texts 73 The preceding code generates the following output Figure 2 16 List of Po S 4  Calculate the number of occurrences of each Po S by iterating through each document and annotating each word with the corresponding pos tag Add the following code to implement this This method will count the occurrence of pos tags in each sentence def get_pos_occurrence_freqdata, tag_list Get list of sentences in text_list text_list data.text create empty dataframe feature_df pd Data Framecolumnstag_list for text_line in text_list get pos tags of each word pos_tags j for i, j in pos_tagword_tokenizetext_line create a dict of pos tags and their frequency in given sentence row dictCounterpos_tags feature_df feature_df.appendrow, ignore_indexTrue feature_df.fillna0, inplaceTrue return feature_df tag_list get_tagsets data pd.read_csv datadata.csv, header0 feature_df get_pos_occurrence_freqdata, tag_list feature_df.head 74 Feature Extraction Methods The preceding code generates the following output Figure 2 17 Number of occurrences of each Po S in the sentence 5  To calculate the number of punctuation marks, add the following code def add_punctuation_countfeature_df, data feature_df data applylambda x lensetx.intersection setpunctuation return feature_df feature_df add_punctuation_countfeature_df, data feature_df.head The add_punctuation_count method will find the intersection of the set of punctuation marks in the text and punctuation sets that were imported from the string module Then, it will find the length of the intersection set in each row and add it to the num_of_unique_punctuations column of the Data Frame The preceding code generates the following output 0 0 1 0 2 1 3 1 4 0 Name num_of_unique_punctuations, dtype int64 Feature Extraction from Texts 75 6  To calculate the number of capitalized words, add the following code def get_capitalized_word_countfeature_df, data The below code line will tokenize text in every row and create a set of only capital words, ten find the length of this set and add it to the column number_of_capital_words of dataframe feature_df data applylambda x lenword for word in word_tokenizestrx if word.isupper return feature_df feature_df get_capitalized_word_countfeature_df, data feature_df.head The preceding code will tokenize the text in every row and create a set of words consisting of only capital words It will then find the length of this set and add it to the number_of_capital_words column of the Data Frame The preceding code generates the following output 0 1 1 1 2 1 3 1 4 1 Name number_of_capital_words, dtype int64 The last line of the preceding code will print the number_of_capital_words column, which represents the count of the number of capital letter words in each row 76 Feature Extraction Methods 7  To calculate the number of lowercase words, add the following code def get_small_word_countfeature_df, data The below code line will tokenize text in every row and create a set of only small words, then find the length of this set and add it to the column number_of_small_words of dataframe feature_df data applylambda x lenword for word in word_tokenizestrx if word.islower return feature_df feature_df get_small_word_countfeature_df, data feature_df.head The preceding code will tokenize the text in every row and create a set of only small words, then find the length of this set and add it to the number_of_ small_words column of the Data Frame The preceding code generates the following output 0 4 1 3 2 7 3 3 4 2 Name number_of_small_words, dtype int64 The last line of the preceding code will print the number_of_small_words column, which represents the number of small letter words in each row 8  To calculate the number of letters in the Data Frame, use the following code def get_number_of_alphabetsfeature_df, data feature_df data applylambda x lench for ch in strx if ch.isalpha return feature_df Feature Extraction from Texts 77 feature_df get_number_of_alphabetsfeature_df, data feature_df.head The preceding code will break the text line into a list of characters in each row and add the count of that list to the number_of_alphabets columns This will produce the following output 0 19 1 18 2 28 3 14 4 13 Name number_of_alphabets, dtype int64 The last line of the preceding code will print the number_of_columns column, which represents the count of the number of alphabets in each row 9  To calculate the number of digits in the Data Frame, add the following code def get_number_of_digit_countfeature_df, data The below code line will break the text line in a list of digits in each row and add the count of that list into the columns number_of_digits feature_df data applylambda x lench for ch in strx if ch.isdigit return feature_df feature_df get_number_of_digit_countfeature_df, data feature_df.head The preceding code will get the digit count from each row and add the count of that list to the number_of_digits columns The preceding code generates the following output 0 0 1 0 2 0 3 0 4 0 Name number_of_digits, dtype int6478 Feature Extraction Methods 10  To calculate the number of words in the Data Frame, add the following code def get_number_of_wordsfeature_df, data The below code line will break the text line in a list of words in each row and add the count of that list into the columns number_of_digits feature_df data applylambda x lenword_tokenizestrx return feature_df feature_df get_number_of_wordsfeature_df, data feature_df.head The preceding code will split the text line into a list of words in each row and add the count of that list to the number_of_digits columns We will get the following output 0 5 1 4 2 9 3 5 4 3 Name number_of_words, dtype int64 11  To calculate the number of whitespaces in the Data Frame, add the following code def get_number_of_whitespacesfeature_df, data The below code line will generate list of white spaces in each row and add the length of that list into the columns number_of_white_spaces feature_df data applylambda x lench for ch in strx if ch.isspace return feature_df Feature Extraction from Texts 79 feature_df get_number_of_whitespacesfeature_df, data feature_df.head The preceding code will generate a list of whitespaces in each row and add the length of that list to the number_of_white_spaces columns The preceding code generates the following output 0 4 1 3 2 7 3 3 4 2 Name number_of_white_spaces, dtype int64 12  To view the full feature set we have just created, add the following code feature_df.head We will be printing the head of the final Data Frame, which means we will print five rows of all the columns We will get the following output Figure 2 18 Data Frame consisting of the features we have created With that, we have learned how to extract general features from the given text Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, lets explore how we can extract unique features 80 Feature Extraction Methods Bag of Words Bo W The Bag of Words Bo W model is one of the most popular methods for extracting features from raw texts In this technique, we convert each sentence into a vector The length of this vector is equal to the number of unique words in all the documents This is done in two steps 1  The vocabulary or dictionary of all the words is generated 2  The document is represented in terms of the presence or absence of all words A vocabulary or dictionary is created from all the unique possible words available in the corpus all documents and every single word is assigned a unique index number In the second step, every document is represented by a list whose length is equal to the number of words in the vocabulary The following exercise illustrates how Bo W can be implemented using Python Exercise 2 13 Creating a Bag of Words In this exercise, we will create a Bo W representation for all the terms in a document and ascertain the 10 most frequent terms In this exercise, we will use the Count Vectorizer module from sklearn , which performs the following tasks Tokenizes the collection of documents, also called a corpus Builds the vocabulary of unique words Converts a document into vectors using the previously built vocabulary Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import the necessary libraries and declare a list corpus Add the following code to implement this import pandas as pd from sklearn.feature_extraction.text import Count Vectorizer Feature Extraction from Texts 81 3  Use the Count Vectorizer function to create the Bo W model Add the following code to do this def vectorize_textcorpus Will return a dataframe in which every row will ,be vector representation of a document in corpus param corpus input text corpus return dataframe of vectors bag_of_words_model Count Vectorizer performs the above described three tasks on the given data corpus dense_vec_matrix bag_of_words_model fit_transformcorpus.todense bag_of_word_df pd Data Framedense_vec_matrix bag_of_word_df.columns sortedbag_of_words_model vocabulary_ return bag_of_word_df corpus Data Science is an overlap between Arts and Science, Generally, Arts graduates are right brained and Science graduates are left brained, Excelling in both Arts and Science at a time becomes difficult, Natural Language Processing is a part of Data Science df vectorize_textcorpus df.head The vectorize_text method will take a document corpus as an argument and return a Data Frame in which every row will be a vector representation of a document in the corpus 82 Feature Extraction Methods The preceding code generates the following output Figure 2 19 Data Frame of the output of the Bo W model 4  Create a Bo W model for the 10 most frequent terms Add the following code to implement this def bow_top_ncorpus, n Will return a dataframe in which every row will be represented by presence or absence of top 10 most frequently occurring words in data corpus param corpus input text corpus return dataframe of vectors bag_of_words_model_small Count Vectorizermax_featuresn bag_of_word_df_small pd Data Frame bag_of_words_model_small.fit_transform corpus.todense bag_of_word_df_small.columns sortedbag_of_words_model_small.vocabulary_ return bag_of_word_df_small df_2 bow_top_ncorpus, 10 df_2.head In the preceding code, we are checking the occurrence of the top 10 most frequent words in each sentence and creating a Data Frame out of it Feature Extraction from Texts 83 The preceding code generates the following output Figure 2 20 Data Frame of the output of the Bo W model for the 10 most frequent terms Note To access the source code for this specific section, please refer to  You can also run this example online at  In this section, we learned what Bo W is and how to can use it to convert a sentence or document into a vector Bo W is the easiest way to convert text into a vector however, it has a severe disadvantage This method only considers the presence and absence of words in a sentence or documentnot the frequency of the wordstokens in a document If we are going to use the semantics of any sentence, the frequency of the words plays an important role To overcome this issue, there is another feature extraction model called T FI DF, which we will discuss later in this chapter Zipfs Law According to Zipfs law, the number of times a word occurs in a corpus is inversely proportional to its rank in the frequency table In simple terms, if the words in a corpus are arranged in descending order of their frequency of occurrence, then the frequency of the word at the ith rank will be proportional to 1i Figure 2 21 Zipfs law 84 Feature Extraction Methods This also means that the frequency of the most frequent word will be twice the frequency of the second most frequent word For example, if we look at the Brown University Standard Corpus of Present Day American English, the word the is the most frequent word its frequency is 69 , 971 , while the word of is the second most frequent with a frequency of 36 , 411  As we can see, its frequency is almost half of the most frequently occurring word To get a better understanding of this, lets perform a simple exercise Exercise 2 14 Zipfs Law In this exercise, we will plot both the expected and actual ranks and frequencies of tokens with the help of Zipfs law We will be using the 20newsgroups dataset provided by the sklearn library, which is a collection of newsgroup documents Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import the necessary libraries from pylab import import nltk nltk.downloadstopwords from sklearn.datasets import fetch_20newsgroups from nltk import word_tokenize from nltk.corpus import stopwords import matplotlib.pyplot as plt import re import string from collections import Counter Add two methods for loading stop words and the data from the newsgroups_data_sample variable def get_stop_words stop_words stopwords.wordsenglish stop_words stop_words liststring.printable return stop_words Feature Extraction from Texts 85 def get_and_prepare_datastop_words This method will load 20newsgroups data and and remove stop words from it using given stop word list param stop_words return newsgroups_data_sample fetch_20newsgroupssubsettrain tokenized_corpus word.lower for sentence in newsgroups_data_sample for word in word_tokenize re.subr_, , sentence if word.lower not in stop_words return tokenized_corpus In the preceding code, there are two methods get_stop_words will load stop word list from nltk data, while get_and_prepare_data will load the 20newsgroups data and remove stop words from it using the given stop word list 3  Add the following method to calculate the frequency of each token def get_frequencycorpus, n token_count_di Countercorpus return token_count_di.most_commonn The preceding method uses the Counter class to count the frequency of tokens in the corpus and then return the most common n tokens 4  Now, call all the preceding methods to calculate the frequency of the top 50 most frequent tokens stop_word_list get_stop_words corpus get_and_prepare_datastop_word_list get_frequencycorpus, 5086 Feature Extraction Methods The preceding code generates the following output Figure 2 22 The 50 most frequent words of the corpus Feature Extraction from Texts 87 5  Plot the actual ranks of words that we got from frequency dictionary and the ranks expected as per Zipfs law Calculate the frequencies of the top 10 , 000 words using the preceding get_frequency method and the expected frequencies of the same list using Zipfs law For this, create two listsan actual_frequencies and an expected_frequencies list Use the log of actual frequencies to downscale the numbers After getting the actual and expected frequencies, plot them using matplotlib def get_actual_and_expected_frequenciescorpus freq_dict get_frequencycorpus, 1000 actual_frequencies expected_frequencies for rank, tup in enumeratefreq_dict actual_frequencies.appendlogtup rank 1 if rank 0 else rank expected frequency 1rank as per zipfs law expected_frequencies.append1 rank return actual_frequencies, expected_frequencies def plotactual_frequencies, expected_frequencies plt.plotactual_frequencies, g, expected_frequencies, ro plt.show We will plot the actual and expected frequencies actual_frequencies, expected_frequencies get_actual_and_expected_frequenciescorpus plotactual_frequencies, expected_frequencies88 Feature Extraction Methods The preceding code generates the following output Figure 2 23 Illustration of Zipfs law So, as we can see from the preceding output, both lines have almost the same slope In other words, we can say that the lines or graphs depict the proportionality of two lists Note To access the source code for this specific section, please refer to  You can also run this example online at  Feature Extraction from Texts 89 Term Frequency Inverse Document Frequency T FI DF Term Frequency Inverse Document Frequency T FI DF is another method of representing text data in a vector format Here, once again, well represent each document as a list whose length is equal to the number of unique wordstokens in all documents corpus, but the vector here not only represents the presence and absence of a word, but also the frequency of the wordboth in the current document and the whole corpus This technique is based on the idea that the rarely occurring words are better representatives of the document than frequently occurring words Hence, this representation gives more weightage to the rarer or less frequent words than frequently occurring words It does so with the following formula Figure 2 24 T FI DF formula Here, term frequency is the frequency of a word in the given document Inverse document frequency can be defined as logDdf, where df is document frequency and D is the total number of documents in the background corpus Now, lets complete an exercise and learn how T FI DF can be implemented in Python Exercise 2 15 T FI DF Representation In this exercise, we will represent the input texts with their T FI DF vectors We will use a sklearn module named Tfidf Vectorizer , which converts text into T FI DF vectors Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import all the necessary libraries and create a method to calculate the T FI DF of the corpus Add the following code to implement this from sklearn.feature_extraction.text import Tfidf Vectorizer def get_tf_idf_vectorscorpus tfidf_model Tfidf Vectorizer vector_list tfidf_model.fit_transformcorpus.todense return vector_list 90 Feature Extraction Methods 3  To create a T FI DF model, write the following code corpus Data Science is an overlap between Arts and Science, Generally, Arts graduates are right brained and Science graduates are left brained, Excelling in both Arts and Science at a time becomes difficult, Natural Language Processing is a part of Data Science vector_list get_tf_idf_vectorscorpus printvector_list In the preceding code, the get_tf_idf_vectors method will generate T FI DF vectors from the corpus You will then call this method on a given corpus The preceding code generates the following output Figure 2 25 T FI DF representation of the 10 most frequent terms Finding Text Similarity Application of Feature Extraction 91 The preceding output represents the T FI DF vectors for each row As you can see from the results, each document is represented by a list whose length is equal to the unique words in the corpus and in each list vector The vector contains the T FI DF values of the words at their corresponding index Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will solve an activity to extract specific features from texts Finding Text Similarity Application of Feature Extraction So far in this chapter, we have learned how to generate vectors from text These vectors are then fed to machine learning algorithms to perform various tasks Other than using them in machine learning applications, we can also perform simple N LP tasks using these vectors Finding the string similarity is one of them This is a technique in which we find the similarity between two strings by converting them into vectors The technique is mainly used in full text searching There are different techniques for finding the similarity between two strings or texts They are explained one by one here Cosine similarity The cosine similarity is a technique to find the similarity between the two vectors by calculating the cosine of the angle between them As we know, the cosine of a zero degree angle is 1 meaning the cosine similarity of two identical vectors is 1 , while the cosine of 180 degrees is 1meaning the cosine of two opposite vectors is 1  Thus, we can use this cosine angle to find the similarity between the vectors from 1 to - 1  To use this technique in finding text similarity, we convert text into vectors using one of the previously discussed techniques and find the similarity between the vectors of the text This is calculated as follows Figure 2 26 Cosine similarity 92 Feature Extraction Methods Here, A and B are two vectors, A B is the dot product of two vectors, and A and B are the magnitude of two vectors Jaccard similarity This is another technique thats used to calculate the similarity between the two texts, but it only works on Bo W vectors The Jaccard similarity is calculated as the ratio of the number of terms that are common between two text documents to the total number of unique terms present in those texts Consider the following example Suppose there are two texts Text 1 I like detective Byomkesh Bakshi Text 2 Byomkesh Bakshi is not a detective he is a truth seeker The common terms are Byomkesh, Bakshi, and detective The number of common terms in the texts is three The unique terms present in the texts are I, like, is, not, a, he, is, truth, and seeker So, the number of unique terms is nine Therefore, the Jaccard similarity is 39 0 3  To get a better understanding of text similarity, we will complete an exercise Exercise 2 16 Calculating Text Similarity Using Jaccard and Cosine Similarity In this exercise, we will calculate the Jaccard and cosine similarity for a given pair of texts Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from sklearn.metrics.pairwise import cosine_similarity lemmatizer Word Net LemmatizerFinding Text Similarity Application of Feature Extraction 93 3  Create a function to extract the Jaccard similarity between a pair of sentences by adding the following code def extract_text_similarity_jaccardtext1, text2 This method will return Jaccard similarity between two texts after lemmatizing them param text1 text1 param text2 text2 return similarity measure lemmatizer Word Net Lemmatizer words_text1 lemmatizer.lemmatizeword.lower for word in word_tokenizetext1 words_text2 lemmatizer.lemmatizeword.lower for word in word_tokenizetext2 nr lensetwords_text1.intersectionsetwords_text2 dr lensetwords_text1.unionsetwords_text2 jaccard_sim nr dr return jaccard_sim 4  Declare three variables named pair1 , pair2 , and pair3 , as follows pair1 pair2 Once upon a time there lived a king., Who is your queen pair3 5  To check the Jaccard similarity between the statements in pair1 , write the following code extract_text_similarity_jaccardpair1,pair1 The preceding code generates the following output 0 1428571428571428594 Feature Extraction Methods 6  To check the Jaccard similarity between the statements in pair2 , write the following code extract_text_similarity_jaccardpair2,pair2 The preceding code generates the following output 0 0 7  To check the Jaccard similarity between the statements in pair3 , write the following code extract_text_similarity_jaccardpair3,pair3 The preceding code generates the following output 0 6 8  To check the cosine similarity, use the Tfidf Vectorizer method to get the vectors of each text def get_tf_idf_vectorscorpus tfidf_vectorizer Tfidf Vectorizer tfidf_results tfidf_vectorizer.fit_transformcorpus todense return tfidf_results 9  Create a corpus as a list of texts and get the T FI DF vectors of each text document Add the following code to do this corpus , pair1, pair2, pair2, pair3, pair3 tf_idf_vectors get_tf_idf_vectorscorpus 10  To check the cosine similarity between the initial two texts, write the following code cosine_similaritytf_idf_vectors,tf_idf_vectors The preceding code generates the following output arrayFinding Text Similarity Application of Feature Extraction 95 11  To check the cosine similarity between the third and fourth texts, write the following code cosine_similaritytf_idf_vectors,tf_idf_vectors The preceding code generates the following output array 12  To check the cosine similarity between the fifth and sixth texts, write the following code cosine_similaritytf_idf_vectors,tf_idf_vectors The preceding code generates the following output array So, in this exercise, we learned how to check the similarity between texts As you can see, the texts He is desperate and Is he not desperate returned similarity results of 0 80 meaning they are highly similar, whereas sentences such as Once upon a time there lived a king and Who is your queen returned zero as their similarity measure Note To access the source code for this specific section, please refer to  You can also run this example online at  Word Sense Disambiguation Using the Lesk Algorithm The Lesk algorithm is used for resolving word sense disambiguation Suppose we have a sentence such as On the bank of river Ganga, there lies the scent of spirituality and another sentence, Im going to withdraw some cash from the bank Here, the same wordthat is, bankis used in two different contexts For text processing results to be accurate, the context of the words needs to be considered In the Lesk algorithm, words with ambiguous meanings are stored in the background in synsets  The definition that is closer to the meaning of a word being used in the context of the sentence will be taken as the right definition Lets perform a simple exercise to get a better idea of how we can implement this 96 Feature Extraction Methods Exercise 2 17 Implementing the Lesk Algorithm Using String Similarity and Text Vectorization In this exercise, we are going to implement the Lesk algorithm step by step using the techniques we have learned so far We will find the meaning of the word bank in the sentence, On the banks of river Ganga, there lies the scent of spirituality We will use cosine similarity as well as Jaccard similarity here Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd from sklearn.metrics.pairwise import cosine_similarity from nltk import word_tokenize from sklearn.feature_extraction.text import Tfidf Vectorizer from sklearn.datasets import fetch_20newsgroups import numpy as np 3  Define a method for getting the T FI DF vectors of a corpus def get_tf_idf_vectorscorpus tfidf_vectorizer Tfidf Vectorizer tfidf_results tfidf_vectorizer.fit_transform corpus.todense return tfidf_results 4  Define a method to convert the corpus into lowercase def to_lower_casecorpus lowercase_corpus return lowercase_corpus 5  Define a method to find the similarity between the sentence and the possible definitions and return the definition with the highest similarity score def find_sentence_definitionsent_vector,defnition_vectors This method will find cosine similarity of sentence with the possible definitions and return the one with highest similarity score along with the similarity score result_dict Finding Text Similarity Application of Feature Extraction 97 for definition_id,def_vector in definition_vectors.items sim cosine_similaritysent_vector,def_vector result_dict sim definition sortedresult_dict.items, keylambda x x, reverseTrue return definition,definition 6  Define a corpus with random sentences with the sentence and the two definitions as the top three sentences corpus On the banks of river Ganga, there lies the scent of spirituality, An institute where people can store extra cash or money., The land alongside or sloping down to a river or lake What you do defines you, Your deeds define you, Once upon a time there lived a king., Who is your queen, He is desperate, Is he not desperate 7  Use the previously defined methods to find the definition of the word bank lower_case_corpus to_lower_casecorpus corpus_tf_idf get_tf_idf_vectorslower_case_corpus sent_vector corpus_tf_idf definition_vectors def1corpus_tf_idf, def2corpus_tf_idf definition_id, score find_sentence_definitionsent_vector,definition_vectors printThe definition of word is with similarity of  formatbank,definition_id,score You will get the following output The definition of word bank is def2 with similarity of 0 1441913068627889798 Feature Extraction Methods As we already know, def2 represents a riverbank So, we have found the correct definition of the word here In this exercise, we have learned how to use text vectorization and text similarity to find the right definition of ambiguous words Note To access the source code for this specific section, please refer to  You can also run this example online at  Word Clouds Unlike numeric data, there are very few ways in which text data can be represented visually The most popular way of visualizing text data is by using word clouds A word cloud is a visualization of a text corpus in which the sizes of the tokens words represent the number of times they have occurred, as shown in the following image Figure 2 27 Example of a word cloud Finding Text Similarity Application of Feature Extraction 99 In the following exercise, we will be using a Python library called wordcloud to build a word cloud from the 20newsgroups dataset Lets go through an exercise to understand this better Exercise 2 18 Generating Word Clouds In this exercise, we will visualize the most frequently occurring words in the first 1 , 000 articles from sklearn s fetch_20newsgroups text dataset using a word cloud Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Import the necessary libraries and dataset Add the following code to do this import nltk nltk.downloadstopwords import matplotlib.pyplot as plt plt.rc Params 200 from sklearn.datasets import fetch_20newsgroups from nltk.corpus import stopwords from wordcloud import Word Cloud import matplotlib as mpl mpl.rc Params 200 3  Write the get_data method to fetch the data def get_datan newsgroups_data_sample fetch_20newsgroupssubsettrain text strnewsgroups_data_sample return text100 Feature Extraction Methods 4  Add a method to remove stop words def load_stop_words other_stopwords_to_remove n, n, , , n Lines, n I,n stop_words stopwords.wordsenglish stop_words.extendother_stopwords_to_remove stop_words setstop_words return stop_words 5  Add the generate_word_cloud method to generate a word cloud object def generate_word_cloudtext, stopwords This method generates word cloud object with given corpus, stop words and dimensions wordcloud Word Cloudwidth 800 , height 800 , background_color white, max_words200, stopwords stopwords, min_font_size 10 .generatetext return wordcloud 6  Get 1 , 000 documents from the 20newsgroup data, get the stop word list, generate a word cloud object, and finally plot the word cloud with matplotlib text get_data1000 stop_words load_stop_words wordcloud generate_word_cloudtext, stop_words plt.imshowwordcloud, interpolationbilinear plt.axisoff plt.showFinding Text Similarity Application of Feature Extraction 101 The preceding code generates the following output Figure 2 28 Word cloud representation of the first 10 articles So, in this exercise, we learned what word clouds are and how to generate word clouds with Pythons wordcloud library and visualize this with matplotlib Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will explore other visualizations, such as dependency parse trees and named entities 102 Feature Extraction Methods Other Visualizations Apart from word clouds, there are various other ways of visualizing texts Some of the most popular ways are listed here Visualizing sentences using a dependency parse tree Generally, the phrases constituting a sentence depend on each other We depict these dependencies by using a tree structure known as a dependency parse tree For instance, the word helps in the sentence God helps those who help themselves depends on two other words These words are God the one who helps and those the ones who are helped Visualizing named entities in a text corpus In this case, we extract the named entities from texts and highlight them by using different colors Lets go through the following exercise to understand this better Exercise 2 19 Other Visualizations Dependency Parse Trees and Named Entities In this exercise, we will look at two of the most popular visualization methods, after word clouds, which are dependency parse trees and using named entities Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import spacy from spacy import displacy python -mspacy download en_core_web_sm import en_core_web_sm nlp en_core_web_sm.load 3  Depict the sentence God helps those who help themselves using a dependency parse tree with the following code doc nlpGod helps those who help themselves displacy.renderdoc, styledep, jupyterTrueFinding Text Similarity Application of Feature Extraction 103 The preceding code generates the following output Figure 2 29 Dependency parse tree 4  Visualize the named entities of the text corpus by adding the following code text Once upon a time there lived a saint named Ramakrishna Paramahansa His chief disciple Narendranath Dutta also known as Swami Vivekananda is the founder of Ramakrishna Mission and Ramakrishna Math doc2 nlptext displacy.renderdoc2, styleent, jupyterTrue The preceding code generates the following output Figure 2 30 Named entities Note To access the source code for this specific section, please refer to  You can also run this example online at  Now that you have learned about visualizations, we will solve an activity based on them to gain an even better understanding 104 Feature Extraction Methods Activity 2 02 Text Visualization In this activity, you will create a word cloud for the 50 most frequent words in a dataset The dataset we will use consists of random sentences that are not clean First, we need to clean them and create a unique set of frequently occurring words Note The text_corpus.txt file thats being used in this activity can be found at  Follow these steps to implement this activity 1  Import the necessary libraries 2  Fetch the dataset 3  Perform the preprocessing steps, such as text cleaning, tokenization, and lemmatization, on the fetched data 4  Create a set of unique words along with their frequencies for the 50 most frequently occurring words 5  Create a word cloud for these top 50 words 6  Justify the word cloud by comparing it with the word frequency that you calculated Note The solution for this activity can be found via this link  Summary 105 Summary In this chapter, you have learned about various types of data and ways to deal with unstructured text data Text data is usually extremely noisy and needs to be cleaned and preprocessed, which mainly consists of tokenization, stemming, lemmatization, and stop word removal After preprocessing, features are extracted from texts using various methods, such as Bo W and T FI DF These methods convert unstructured text data into structured numeric data New features are created from existing features using a technique called feature engineering In the last part of this chapter, we explored various ways of visualizing text data, such as word clouds In the next chapter, you will learn how to develop machine learning models to classify texts using the feature extraction methods you have learned about in this chapter Moreover, different sampling techniques and model evaluation parameters will be introduced Overview This chapter starts with an introduction to the various types of machine learning methods, that is, the supervised and unsupervised methods You will learn about hierarchical clustering and k means clustering and implement them using various datasets Next, you will explore tree based methods such as random forest and X GBoost Finally, you will implement an end to end text classifier in order to categorize text on the basis of its content Developing a Text Classifier3108 Developing a Text Classifier Introduction In the previous chapters, you learned about various extraction methods, such as tokenization, stemming, lemmatization, and stop word removal, which are used to extract features from unstructured text We also discussed Bag of Words and Term Frequency Inverse Document Frequency T FI DF  In this chapter, you will learn how to use these extracted features to develop machine learning models These models are capable of solving real world problems, such as detecting whether sentiments carried by texts are positive or negative, predicting whether emails are spam or not, and so on We will also cover concepts such as supervised and unsupervised learning, classifications and regressions, sampling and splitting data, along with evaluating the performance of a model in depth This chapter also discusses how to load and save these models for future use Machine Learning Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead Machine learning algorithms are fed with large amounts of data that they can work on to build a model This model is later used by businesses to generate solutions that help them analyze data and build strategies for the future For example, a beverage production company can make use of multiple datasets to better understand the trends of their products consumption over the course of a year This would help them reduce wastage and better predict the requirements of their consumers Machine learning is further categorized into unsupervised and supervised learning Lets explore these two terms in detail Unsupervised Learning Unsupervised learning is the method by which algorithms learn patterns within data that is not labeled Since labels supervisors are absent, it is referred to as unsupervised learning In unsupervised learning, you provide the algorithm with the feature data and it learns patterns from the data on its own Machine Learning 109 Unsupervised learning is further classified into clustering and association Clustering Clustering is the process of combining objects into groups called clusters For example, if there are 50 students who need to be categorized based on their attributes, we do not use any specific attributes to create segments Rather, we try to learn the hidden patterns that exist in their attributes and categorize them accordingly This process is known as cluster analysis or clustering one of the most popular types of unsupervised learning When handed a set of text documents, we can divide them into groups that are similar with the help of clustering A common example of clustering could be when you search for a term on Google and similar pages or links are recommended These recommendations are powered by document clustering Association Another type of unsupervised learning is association rule mining We use association rule mining to obtain groups of items that occur together frequently The most common use case of association rule mining is to identify customers buying patterns For example, in a supermarket, customers who tend to buy milk and bread generally tend to buy cheese This information can be used to design supermarket layouts An application of association rule mining in Natural Language Processing N LP is to find similar words for example, outstanding , excellent , and superb are all synonyms of good  Association rule mining can easily find patterns like this in any N LP dataset However, the detailed theoretical explanations of these algorithms are beyond the scope of this chapter Lets explore the different types of clustering In particular, we will be talking about hierarchical and k means clustering, and the different scenarios in which they should be used However, before we dive into those, its important to understand the concept of distance metrics, which is what we use to create clusters and identify similar data points The most common distance metric is Euclidean, which is calculated as follows Figure 3 1 Formula for Euclidean distance In the case of machine learning, p and q are different data points in the dataset and pi, qi are the different features of those data points 110 Developing a Text Classifier Hierarchical Clustering Hierarchical clustering algorithms group similar objects together to create a cluster with the help of a dendrogram  In this algorithm, we can vary the number of clusters as per our requirements First, we construct a matrix consisting of distances between each pair of instances data points After that, we construct a dendrogram a representation of clusters in the form of a tree based on the distances between them We truncate the tree at a location corresponding to the number of clusters weneed For example, imagine that you have 10 documents and want to group them into a number of categories based on their attributes the number of words they contain, the number of paragraphs, punctuation, and so on and do not have any fixed number of categories in mind This is a use case of hierarchical clustering Lets assume that we have a dataset containing the features of the 10 documents Firstly, the distances between each pair of documents from the set of 10 documents are calculated After that, we construct a dendrogram and truncate it at a suitable position to get a suitable number of clusters Figure 3 2 Output dendrogram after performing hierarchical clustering Machine Learning 111 In the preceding graph, we can perform a truncation at distance 3 5 to get two clusters or at 2 5 to get three clusters, depending on the requirements To create a dendrogram using scikit learn, we can use the following code import scipy.cluster.hierarchy as sch dendrogram sch.dendrogramsch.linkageX, methodward plt.titleDendrogram plt.show Here, X is the dataset that we want to perform hierarchical clustering with Lets perform an exercise to understand how we can implement this Exercise 3 01 Performing Hierarchical Clustering In this exercise, we will analyze the text documents in sklearns fetch_20newsgroups dataset The 20 newsgroups dataset contains news articles on 20 different topics We will make use of hierarchical clustering to classify the documents into different groups Once the clusters have been created, we will compare them with their actual categories Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from sklearn.datasets import fetch_20newsgroups from scipy.cluster.hierarchy import ward, dendrogram import matplotlib as mpl from scipy.cluster.hierarchy import fcluster from sklearn.metrics.pairwise import cosine_similarity import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter112 Developing a Text Classifier from pylab import import nltk import warnings warnings.filterwarningsignore 3  Download a list of stop words and the Wordnet corpus from nltk  Insert a new cell and add the following code to implement this nltk.downloadstopwords stop_wordsstopwords.wordsenglish stop_wordsstop_wordsliststring.printable nltk.downloadwordnet lemmatizerWord Net Lemmatizer 4  Specify the categories of news articles we want to fetch to perform our clustering task We will use three categories For sale, Electronics, and Religion Add the following code to do this categories misc.forsale, sci.electronics, talk.religion.misc 5  To fetch the dataset, add the following lines of code news_data fetch_20newsgroupssubsettrain, categoriescategories, shuffleTrue, random_state42, download_if_missingTrue 6  To view the data of the fetched content, add the following code news_data Machine Learning 113 The preceding code generates the following output Figure 3 3 The first five news articles 7  To check the categories of news articles, insert a new cell and add the following code printnews_data.target The target is the variable that we predict by making use of the rest of the variables in a dataset The preceding code generates the following output Here, 0 refers to misc.forsale , 1 refers to sci.electronics , and 2 refers to talk.religion.misc 8  To store news_data and the corresponding categories in a pandas Data Frame and view it, write the following code news_data_df pd Data Frametext news_data, category news_data.target news_data_df.head 114 Developing a Text Classifier The preceding code generates the following output Figure 3 4 Text corpus of news data corresponding to the categories in a Data Frame 9  To count the number of occurrences of each category appearing in this dataset, write the following code news_data_df.value_counts The preceding code generates the following output 1 591 0 585 2 377 Name category, dtype int64 10  Use a lambda function to extract tokens from each text of the news_data_ df Data Frame Check whether any of these tokens is a stop word, lemmatize the ones that are not stop words, and then concatenate them to recreate the sentence Make use of the join function to concatenate a list of words into a single sentence To replace anything other than letters, digits, and whitespaces with blank space, use a regular expression re Add the following code to do this news_data_df news_data_df .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx if word.lower not in stop_words Machine Learning 115 11  Create a T FI DF matrix and transform it into a Data Frame Add the following code to do this tfidf_model Tfidf Vectorizermax_features200 tfidf_df pd Data Frametfidf_model.fit_transform news_data_df.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 5 T FI DF representation as a Data Frame 12  Calculate the distance using the sklearn library from sklearn.metrics.pairwise import euclidean_distances as euclidean dist 1 euclideantfidf_df 13  Now, create a dendrogram for the T FI DF representation of documents import scipy.cluster.hierarchy as sch dendrogram sch.dendrogramsch.linkagedist, methodward plt.xlabelData Points plt.ylabelEuclidean Distance plt.titleDendrogram plt.show 116 Developing a Text Classifier The preceding code generates the following output Figure 3 6 Truncated dendrogram Here, you can see that a cluster count of four seems optimal 14  Use the fcluster function to obtain the cluster labels of the clusters that were obtained by hierarchical clustering k4 clusters fclustersch.linkagedist, methodward, k, criterionmaxclust clusters The preceding code generates the following output array, dtypeint32 Machine Learning 117 15  Make use of the crosstab function of pandas to compare the clusters we have obtained with the actual categories of news articles Add the following code to implement this news_data_df clusters pd.crosstabnews_data_df .replace0misc.forsale, 1sci.electronics, 2talk.religion.misc, news_data_df .replace1 cluster_1, 2 cluster_2, 3 cluster_3, 4 cluster_4 The preceding code generates the following output Figure 3 7 Crosstab between actual categories and obtained clusters Using the preceding image, we can analyze the high level patterns that the clustering algorithm found to group the articles into one of the four clusters As you can see, cluster 2 has mostly religion related articles, while cluster 3 consists of primarily sales related articles The other two clusters do not have a proper distinction The reason for this could be that the model figured out that words related to religion and for sale appeared frequently in the articles that were classified into those respective clusters, while the articles on electronics consist of mostly generic words Note To access the source code for this specific section, please refer to  You can also run this example online at 118 Developing a Text Classifier One major disadvantage of hierarchical clustering is scalability Using hierarchical clustering for large datasets is very difficult for such cases, we can use k means clustering Let us explore how this works k means Clustering In this algorithm, we segregate the given instances data points into k number of groups here, k is a natural number First, we choose k centroids We assign each instance to its nearest centroid, thereby creating k groups This is the assignment phase, which is followed by the update phase In the update phase, new centroids for each of these k groups are calculated The data points are reassigned to their nearest newly calculated centroids The assignment phase and the update phase are carried on repeatedly until the assignment of data points no longer changes For example, suppose you have 10 documents You want to group them into three categories based on their attributes, such as the number of words they contain, the number of paragraphs, punctuation, and the tone of the document In this case, we will assume that k is 3 that is, we want to create these three groups Firstly, three centroids need to be chosen In the initialization phase, each of these 10 documents is assigned to one of these three categories, thereby forming three groups In the update phase, the centroids of the three newly formed groups are calculated To decide the optimal number of clusters that is, k, we execute k means clustering for various values of k and note down their performances sum of squared errors We try to select a small value for k that has the lowest sum of squared errors This method is called the elbow method  The scikit learn library can be used to perform k-means in Python using the following code from sklearn.cluster import K Means kmeans K Meansn_clusters4 kmeans.fitX clusters kmeans.predictX Here, we create the base model using the kmeans class of scikit learn Then, we train the model using the fit function The trained model can then be used to get clusters using the predict function, where X represents a Data Frame of independent variables Lets perform an exercise to get a better understanding of k means clustering Machine Learning 119 Exercise 3 02 Implementing k means Clustering In this exercise, we will create four clusters from text documents in sklearns fetch_20newsgroups text dataset using k means clustering We will compare these clusters with the actual categories and use the elbow method to obtain the optimal number of clusters Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd from sklearn.datasets import fetch_20newsgroups import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadstopwords nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore import seaborn as sns sns.set import numpy as np from scipy.spatial.distance import cdist from sklearn.cluster import K Means 3  To use stop words for the English language and the Word Net corpus for lemmatization, add the following code stop_words stopwords.wordsenglish stop_words stop_words liststring.printable lemmatizer Word Net Lemmatizer120 Developing a Text Classifier 4  To specify the categories of news articles, add the following code categories misc.forsale, sci.electronics, talk.religion.misc 5  Use the following lines of code to fetch the dataset and store it in a pandas Data Frame news_data fetch_20newsgroupssubsettrain, categoriescategories, shuffleTrue, random_state42, download_if_missingTrue news_data_df pd Data Frametext news_data, category news_data.target 6  Use the lambda function to extract tokens from each text of the news_ data_df Data Frame Discard the tokens if theyre stop words, lemmatize them if theyre not, and then concatenate them to recreate the sentence Use the join function to concatenate a list of words into a single sentence and use the regular expression method re to replace anything other than alphabets, digits, and whitespaces with a blank space Add the following code to do this news_data_df news_data_df .applylambda x .join lemmatizer.lemmatizeword.lower for word in word_tokenize re.subr_, , strx if word.lower not in stop_words 7  Use the following lines of code to create a T FI DF matrix and transform it into a Data Frame tfidf_model Tfidf Vectorizermax_features200 tfidf_df pd Data Frametfidf_model .fit_transform news_data_df.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.headMachine Learning 121 The preceding code generates the following output Figure 3 8 T FI DF representation as a Data Frame 8  Use the K Means function of sklearn to create four clusters from a T FI DF representation of news articles Add the following code to do this kmeans K Meansn_clusters4 kmeans.fittfidf_df y_kmeans kmeans.predicttfidf_df news_data_df y_kmeans 9  Use pandas crosstab function to compare the clusters we have obtained with the actual categories of the news articles Add the following code to do this pd.crosstabnews_data_df .replace0misc.forsale, 1sci.electronics, 2talk.religion.misc, news_data_df .replace0 cluster_1, 1 cluster_2, 2 cluster_3, 3 cluster_4 122 Developing a Text Classifier The preceding code generates the following output Figure 3 9 Crosstab between the actual categories and obtained clusters xxx From the figure above, you can see, cluster 2 has majorly religion related articles and cluster 4 has mostly for sale related articles The other two clusters do now have a proper distinction but cluster 3 has majority of the electronic articles 10  Finally, to obtain the optimal value of k that is, the number of clusters, execute the k means algorithm for values of k ranging from 1 to 6  For each value of k, store the distortionthat is, the mean of the distances of the documents from their nearest cluster center Look for the value of k where the slope of the plot changes rapidly Add the following code to implement this distortions K range1, 6 for k in K kmean Model K Meansn_clustersk kmean Model.fittfidf_df distortions.appendsumnp.mincdist tfidf_df, kmean Model.cluster_centers_, euclidean, axis1 tfidf_df.shape plt.plotK, distortions, bx plt.xlabelk plt.ylabelDistortion plt.titleThe Elbow Method showing the optimal number of clusters plt.show Machine Learning 123 The preceding code generates the following output Figure 3 10 Optimal clusters represented in a graph using the elbow method From the preceding graph, we can conclude that the optimal number of clusters is 2  Note To access the source code for this specific section, please refer to  You can also run this example online at  We have seen how unsupervised learning can be implemented in Python Now, let us talk about supervised learning 124 Developing a Text Classifier Supervised Learning Unlike unsupervised learning, supervised learning algorithms need labeled data They learn how to automatically generate labels or predict values by analyzing various features of the data provided For example, say you have already starred important text messages on your phone, and you want to automate the task of going through all your messages daily considering they are important and marked already This is a use case for supervised learning Here, messages that have been starred previously can be used as labeled data Using this data, you can create two types of models that are capable of the following Classifying whether new messages are important Predicting the probability of new messages being important The first type is called classification, while the second type is called regression Lets learn about classification first Classification Say you have two types of food, of which type 1 tastes sweet and type 2 tastes salty, and you need to determine how an unknown food will taste using various attributes of the food such as color, fragrance, shape, and ingredients This is an instance ofclassification Here, the two classes are class 1 , which tastes sweet, and class 2 , which tastes salty The features that are used in this classification are color, fragrance, the ingredients used to prepare the dish, and so on These features are called independent variables The class sweet or salty is called a dependent variable Formally, classification algorithms are those that learn patterns from a given dataset to determine classes of unknown datasets Some of the most widely used classification algorithms are logistic regression, Naive Bayes, k nearest neighbor, and tree methods Lets learn about each of them Supervised Learning 125 Logistic Regression Despite having the term regression in it, logistic regression is used for probabilistic classification In this case, the dependent variable the outcome is binary, which means that the values can be represented by 0 or 1  For example, consider that you need to decide whether an email is spam or not Here, the value of the decision the dependent variable, or the outcome can be considered to be 1 if the email is spam otherwise, it will be 0  No other outcome is possible The independent variables that is, the features will consist of various attributes of the email, such as the number of occurrences of certain keywords and so on We can then make use of the logistic regression algorithm to create a model that predicts if the email is spam 1 or not 0 , as shown in the following graph Figure 3 11 Example of logistic regression Here, the decision boundary is created by training a logistic regression model that helps us classify spam emails 126 Developing a Text Classifier The scikit learn library can be used to perform logistic regression in Python using the following code from sklearn.linear_model import Logistic Regression log_reg Logistic Regression log_reg.fitX,y predicted_labels log_reg.predictX predicted_probability log_reg.predict_probaX Here, we create the base model using the Logistic Regression class of scikit learn Then, we train the model using the fit function The trained model can then be used to make predictions, and we can also get probability estimates for each class using the predict_proba function Here, X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables Exercise 3 03 Text Classification Logistic Regression In this exercise, we will classify reviews of musical instruments on Amazon with the help of the logistic regression classification algorithm Note To download the dataset, visit  Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadpunktSupervised Learning 127 nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  Read the data file in J SO N format using pandas Add the following code to implement this review_data pd.read_json datareviews_ Musical_ Instruments_5.json, linesTrue review_data.head The preceding code generates the following output Figure 3 12 Data stored in a Data Frame 4  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than alphabetical characters, digits, and whitespaces with blank space Add the following code to implement this lemmatizer Word Net Lemmatizer review_data review_data .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx 128 Developing a Text Classifier 5  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text  Add the following code to implement this review_datacleaned_review_text, review Text, overall.head The preceding code generates the following output Figure 3 13 Review texts before and after cleaning, along with their overall scores 6  Create a T FI DF matrix and transform it into a Data Frame  Add the following code to do this tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform review_data.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 14 A T FI DF representation as a Data Frame Supervised Learning 129 7  The following lines of code are used to create a new column target, which will have 0 if the overall parameter is less than 4 , and 1 otherwise Add the following code to implement this review_data review_data.apply lambda x 0 if x4 else 1 review_data.value_counts The preceding code generates the following output 1 6938 0 3323 Name target, dtype int64 8  Use sklearns Logistic Regression function to fit a logistic regression model on the T FI DF representation of these reviews after cleaning them Add the following code to implement this from sklearn.linear_model import Logistic Regression logreg Logistic Regression logreg.fittfidf_df,review_data predicted_labels logreg.predicttfidf_df logreg.predict_probatfidf_df The preceding code generates the following output array0 57146961 , 0 68579907 , 0 56068939 , , 0 65979968 , 0 5495679 , 0 21186011 9  Use the crosstab function of pandas to compare the results of our classification model with the actual classes target , in this case of the reviews Add the following code to do this review_data predicted_labels pd.crosstabreview_data, review_data130 Developing a Text Classifier The preceding code generates the following output Figure 3 15 Crosstab between actual target values and predicted labels Here, we can see 1543 instances with the target label 0 that are correctly classified and 1780 such instances that are wrongly classified Furthermore, 6312 instances with the target label 1 are correctly classified, whereas 626 such instances are wrongly classified Note To access the source code for this specific section, please refer to  You can also run this example online at  we have seen how to implement logistic regression now, lets look at Nave Bayes classification Naive Bayes Classifiers Just like logistic regression, a Naive Bayes classifier is another kind of probabilistic classifier It is based on Bayes theorem, which is shown here Figure 3 16 Bayes theorem In the preceding formula, A and B are events and PB is not equal to 0  PAB is the probability of event A occurring, given that event B is true Similarly, PBA is the probability of event B occurring, given that event A is true PB is the probability of the occurrence of event B Supervised Learning 131 Say there is an online platform where hotel customers can provide a review for the service provided to them The hotel now wants to figure out whether new reviews on the platform are appreciative in nature or not Here, PA the probability of the review being an appreciative one, while PB the probability of the review being long Now, we have come across a review that is long and want to figure out the probability of it being appreciative To do that, we need to calculate PAB PBA will be the probability of appreciative reviews being long From the training dataset, we can easily calculate PBA, PA, and PB and then use Bayes theorem to calculate PAB Similar to logistic regression, the scikit learn library can be used to perform nave Bayes classification and can be implemented in Python using the following code from sklearn.naive_bayes import Gaussian NB nb Gaussian NB nb.fitX,y predicted_labels nb.predictX predicted_probability nb.predict_probaX Here, we created the base model using the Gaussian NB class of scikit learn Then, we trained the model using the fit function The trained model can then be used to make predictions we can also get probability estimates for each class using the predict_proba function Here, X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables Exercise 3 04 Text Classification Naive Bayes In this exercise, we will classify reviews of musical instruments on Amazon with the help of the Nave Bayes classification algorithm Follow these steps to implement this exercise Note To download the dataset for this exercise, visit 1  Open a Jupyter Notebook 132 Developing a Text Classifier 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  Read the data file in J SO N format using pandas Add the following code to implement this review_data pd.read_json datareviews_ Musical_ Instruments_5.json, linesTrue review_data.head The preceding code generates the following output Figure 3 17 Data stored in a Data Frame Supervised Learning 133 4  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than alphabets, digits, and whitespaces with blank space Add the following code to implement this lemmatizer Word Net Lemmatizer review_data review_data .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx 5  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text  Add the following code to implement this review_datacleaned_review_text, review Text, overall.head The preceding code generates the following output Figure 3 18 Review texts before and after cleaning, along with their overall scores 6  Create a T FI DF matrix and transform it into a Data Frame Add the following code to do this tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform review_data.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head 134 Developing a Text Classifier The preceding code generates the following output Figure 3 19 A T FI DF representation as a Data Frame 7  The following lines of code are used to create a new column target, which will have the value 0 if the overall parameter is less than 4 , and 1 otherwise Add the following code to implement this review_data review_data .applylambda x 0 if x4 else 1 review_data.value_counts The preceding code generates the following output 1 6938 0 3323 Name target, dtype int64 8  Use sklearns Gaussian NB function to fit a Gaussian Naive Bayes model on the T FI DF representation of these reviews after cleaning them Add the following code to do this from sklearn.naive_bayes import Gaussian NB nb Gaussian NB nb.fittfidf_df,review_data predicted_labels nb.predicttfidf_df nb.predict_probatfidf_df The preceding code generates the following output array9.97730158e- 01 , 3 .63599675e 09 , 9 .45692105e 07 , , 2 .46001047e 02 , 3 .43660991e 08 , 1 .72767906e 27 The preceding screenshot shows the predicted probabilities of the input tfidf_df dataset Supervised Learning 135 9  Use the crosstab function of pandas to compare the results of our classification model with the actual classes target , in this case of the reviews Add the following code to do this review_data predicted_labels pd.crosstabreview_data, review_data The preceding code generates the following output Figure 3 20 Crosstab between actual target values and predicted labels Here, we can see 2333 instances with the target label 0 that are correctly classified and 990 such instances that have been wrongly classified Furthermore, 4558 instances with the target label 1 have been correctly classified, whereas 2380 such instances have been wrongly classified Note To access the source code for this specific section, please refer to  You can also run this example online at  Well explore k nearest neighbors in the next section 136 Developing a Text Classifier k nearest Neighbors k nearest neighbors is an algorithm that can be used to solve both regression and classification In this chapter, we will focus on the classification aspect of the algorithm as it is used for N LP applications Consider, for instance, the saying birds of a feather flock together This means that people who have similar interests prefer to stay close to each other and form groups This characteristic is called homophily  This characteristic is the main idea behind the k nearest neighbors classification algorithm To classify an unknown object, k number of other objects located nearest to it with class labels will be looked into The class that occurs the most among them will be assigned to itthat is, the object with an unknown class The value of k is chosen by running experiments on the training dataset to find the most optimal value When dealing with text data for a given document, we interpret nearest neighbors as other documents that are the most similar to the unknown document We can make use of the scikit learn library to implement the k nearest neighbors algorithm in Python using the following code from sklearn.neighbors import K Neighbors Classifier knn K Neighbors Classifiern_neighbors3 knn.fitX,y prediction knn.predictX Here, we created the base model using the K Neighbors Classifier class of scikit learn and pass it the value of k, which in this case is 3  Then, we trained the model using the fit function The trained model can then be used to make predictions using the predict function X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables Now that we have an understanding of different types of classification, lets see how we can implement them Supervised Learning 137 Exercise 3 05 Text Classification Using the k nearest Neighbors Method In this exercise, we will classify reviews of musical instruments on Amazon with the help of the k nearest neighbors classification algorithm Follow these steps to implement this exercise Note To download the dataset for this exercise, visit 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  Read the data file in J SO N format using pandas Add the following code to implement this review_data pd.read_json datareviews_ Musical_ Instruments_5.json, linesTrue review_data.head138 Developing a Text Classifier The preceding code generates the following output Figure 3 21 Data stored in a Data Frame 4  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than alphabets, digits, and whitespaces with blank space Add the following code to implement this lemmatizer Word Net Lemmatizer review_data review_data .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx 5  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text  Add the following code to implement this review_datacleaned_review_text, review Text, overall.head Supervised Learning 139 The preceding code generates the following output Figure 3 22 Review texts before and after cleaning, along with their overall scores 6  Create a T FI DF matrix and transform it into a Data Frame Add the following code to do this tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform review_data.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 23 A T FI DF representation as a Data Frame 7  The following lines of code are used to create a new column target, which will have 0 if the overall parameter is less than 4 , and 1 otherwise Add the following code to implement this review_data review_data .applylambda x 0 if x4 else 1 review_data.value_counts 140 Developing a Text Classifier The preceding code generates the following output 1 6938 0 3323 Name target, dtype int64 8  Use sklearns K Neighbors Classifier function to fit a three nearest neighbor model on the T FI DF representation of these reviews after cleaning them Use the crosstab function of pandas to compare the results of our classification model with the actual classes target , in this case of the reviews from sklearn.neighbors import K Neighbors Classifier knn K Neighbors Classifiern_neighbors3 knn.fittfidf_df,review_data review_data knn.predicttfidf_df pd.crosstabreview_data, review_data The preceding code generates the following output Figure 3 24 Crosstab between actual target values and predicted labels by k nearest neighbors Here, we can see 2594 instances with the target label as 0 correctly classified and 729 such instances wrongly classified Furthermore, 6563 instances with the target label as 1 are correctly classified, whereas 375 such instances are wrongly classified You have just learned how to perform text classification with the help of various classification algorithms Note To access the source code for this specific section, please refer to  You can also run this example online at  Supervised Learning 141 In the next section, you will learn about regression, which is another type of supervised learning Regression To better understand regression, consider a practical example For example, say you have photos of several people, along with a list of their respective ages, and you need to predict the ages of some other people from their photos This is a use case for regression In the case of regression, the dependent variable age, in this example is continuous The independent variablesthat is, featuresconsist of the attributes of the images, such as the color intensity of each pixel Formally, regression analysis refers to the process of learning a mapping function, which relates features or predictors inputs to the dependent variable output There are various types of regression univariate , multivariate , simple , multiple , linear , non linear , polynomial regression , stepwise regression , ridge regression , lasso regression , and elastic net regression  If there is just one dependent variable, then it is referred to as univariate regression On the other hand, two or more dependent variables constitute multivariate regression Simple regression has only one predictor or target variable, while multivariate regression has more than one predictor variable Since linear regression in the base algorithm for all the different types of regression mentioned previously, in the next section, we will cover linear regression in detail Linear Regression The term linear refers to the linearity of parameters Parameters are the coefficients of predictor variables in the linear regression equation The following formula represents the linear regression equation Figure 3 25 Formula for linear regression Here, y is termed a dependent variable output it is continuous X is an independent variable or feature input 0 and 1 are parameters is the error component, which is the difference between the actual and predicted values of y Since linear regression requires the variable to be linear, it is not used much in the real world However, it is useful for high level predictions, such as the sales revenue of a product given the price and advertising cost 142 Developing a Text Classifier We can use the scikit learn library to implement the linear regression algorithm in Python with the following code from sklearn.linear_model import Linear Regression linreg Linear Regression linreg.fitX,y coefficient linreg.coef_ intercept linreg.intercept_ linreg.predictX Here, we created the base model using the Linear Regression class of scikit learn Then, we trained the model using the fit function Now that our linear regression model has been trained, we can use the coef_ and intercept_ parameters of the model to get the parameters and error components, as we discussed previously Here, X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables The trained model can then be used to make predictions using the predict function In the next section, we will solve an exercise to get a better understanding of regression analysis Exercise 3 06 Regression Analysis Using Textual Data In this exercise, we will make use of linear regression to predict the overall ratings from the reviews of musical instruments on Amazon Follow these steps to implement this exercise Note The dataset for this exercise can be downloaded from 9  Open a Jupyter Notebook Supervised Learning 143 10  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 11  Read the given data file in J SO N format using pandas  Add the following code to implement this review_data pd.read_json datareviews_ Musical_ Instruments_5.json, linesTrue review_data.head The preceding code generates the following output Figure 3 26 Reviews of musical instruments stored as a Data Frame 144 Developing a Text Classifier 12  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them, and concatenate them side by side Then, use the join function to concatenate a list of words into a single sentence In order to replace anything other than alphabets, digits, and whitespaces with blank space, use the regular expression method re Add the following code to implement this lemmatizer Word Net Lemmatizer review_data review_data .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx review_data.head The preceding code generates the following output Figure 3 27 Review texts before and after cleaning, along with their overall scores 13  Create a Data Frame from the T FI DF matrix representation of cleaned review Text  Add the following code to do this tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform review_data.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head Supervised Learning 145 The preceding code generates the following output Figure 3 28 T FI DF representation as a Data Frame 14  Use sklearns Linear Regression function to fit a linear regression model on this T FI DF Data Frame Add the following code to do this from sklearn.linear_model import Linear Regression linreg Linear Regression linreg.fittfidf_df,review_data linreg.coef_ The preceding code generates the following output Figure 3 29 Coefficients of the linear regression model 146 Developing a Text Classifier The preceding output shows the coefficients of the different features of the trained model Please note that Figure 3 29 is truncated 15  To check the intercept or the error term of the linear regression model, type the following code linreg.intercept_ The preceding code generates the following output 4 218882428983381 16  To check the prediction in a T FI DF Data Frame, write the following code linreg.predicttfidf_df The preceding code generates the following output array4 19200071 , 4 25771652 , 4 23084868 , , 4 40384767 , 4 49036403 , 4 14791976 17  Finally, use this model to predict the overall score and store it in a column called predicted_score_from_linear_regression  Add the following code to implement this review_data linreg.predicttfidf_df review_dataoverall, predicted_score_from_linear_regression.head10Supervised Learning 147 The preceding code generates the following output Figure 3 30 Actual scores and predictions of the linear regression model From the preceding table, we can see how the actual and predicted score varies for different instances We will use this table later to evaluate the performance of the model Note To access the source code for this specific section, please refer to  You can also run this example online at  You have just learned how to perform regression analysis on given data In the next section, you will learn about tree methods 148 Developing a Text Classifier Tree Methods There are several algorithms that have both classification and regression forms Tree based methods are instances of such cases In the context of machine learning, tree refers to a structure that aids decision makinghence, the term decision tree  Tree based methods have high accuracy and unlike linear methods, they model non linear relationships as well Additionally, decision trees handle categorical variables much better than linear regression Let us use the example of a hotel trying to identify if the reviews provided by their patrons have a positive sentiment or a negative one So, the reviews needed to be classified into two classes, namely, positive sentiments and negative sentiments A data scientist working for the hotel can create a dataset of all online reviews of their hotel and create a decision tree, as shown in the following diagram Figure 3 31 Decision tree In the preceding diagram, the first decision is made based on the length of the sentences He finds that the short length reviews generally have a positive sentiment, whereas a medium length review has a negative sentiment For reviews that were longer, he had to rely on keywords to determine the sentiment as longer length reviews are almost equally likely to be positive or negative If the excellent keyword is present, the review belongs to the positive sentiment otherwise, it belongs to the negative sentiment Supervised Learning 149 We can make use of the scikit learn library to implement the decision tree algorithm in Python using the following code from sklearn import tree dtc tree Decision Tree Classifier dtc dtc.fitX, y predicted_labels dtc.predictX Here, we created the base model using the Decision Tree Classifier class of scikit learn Then, we trained the model using the fit function The trained model can then be used to make predictions using the predict function Here, X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables Exercise 3 07 Tree Based Methods Decision Tree In this exercise, we will use the tree based method known as decision trees to predict the overall scores and labels of reviews of patio, lawn, and garden products on Amazon Follow these steps to implement this exercise Note To download the dataset for this exercise, visit 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import 150 Developing a Text Classifier import nltk nltk.downloadwordnet nltk.downloadpunkt import warnings warnings.filterwarningsignore 3  Now, read the given data file in J SO N format using pandas Add the following code to implement this data_patio_lawn_garden pd.read_json data reviews_ Patio_ Lawn_and_ Garden_5.json, lines True data_patio_lawn_garden.head The preceding code generates the following output Figure 3 32 Storing reviews as a Data Frame 4  Use the lambda function to extract tokens from each review Text of this Data Frame, lemmatize them using Worrd Net Lemmatizer , and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than letters, digits, and whitespaces with blank spaces Add the following code to do this lemmatizer Word Net Lemmatizer data_patio_lawn_garden data_patio_lawn_garden .applylambda x .joinlemmatizer.lemmatizeword.lower for word in word_tokenizere.subr_, , strx Supervised Learning 151 data_patio_lawn_gardencleaned_review_text, review Text, overall.head The preceding code generates the following output Figure 3 33 Review text before and after cleaning, along with overall scores 5  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text with the following code tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform data_patio_lawn_garden .todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 34 T FI DF representation as a Data Frame 6  The following lines of code are used to create a new column called target, which will have 0 if the overall parameter is less than 4 otherwise, it will have 1 data_patio_lawn_garden data_patio_lawn_garden .apply lambda x 0 if x4 else 1 data_patio_lawn_garden.value_counts 152 Developing a Text Classifier The preceding code generates the following output 1 7037 0 6235 Name target, dtype int64 7  Use sklearns tree function to fit a decision tree classification model on the T FI DF Data Frame we created earlier Add the following code to do this from sklearn import tree dtc tree Decision Tree Classifier dtc dtc.fittfidf_df, data_patio_lawn_garden data_patio_lawn_garden dtc.predict tfidf_df 8  Use pandas crosstab function to compare the results of the classification model with the actual classes target , in this case of the reviews Add the following code to do this pd.crosstabdata_patio_lawn_garden, data_patio_lawn_garden The preceding code generates the following output Figure 3 35 Crosstab between actual target values and predicted labels Here, we can see 6627 instances with a target label of 0 correctly classified, and 8 such instances wrongly classified Furthermore, 7036 instances with a target label of 1 are correctly classified, whereas 1 such instance is wrongly classified Supervised Learning 153 9  Use sklearns tree function to fit a decision tree regression model on the T FI DF representation of these reviews after cleaning To predict the overall scores using this model, add the following code from sklearn import tree dtr tree Decision Tree Regressor dtr dtr.fittfidf_df, data_patio_lawn_garden data_patio_lawn_garden dtr.predict tfidf_df data_patio_lawn_gardenpredicted_values_dtr, overall.head10 The preceding code generates the following output Figure 3 36 Overall scores with predicted values 154 Developing a Text Classifier From the preceding table, we can see how the actual and predicted scores vary for different instances We will use this table later to evaluate the performance of the model Note To access the source code for this specific section, please refer to  You can also run this example online at  Next, we will look at another tree based method, random forest Random Forest Imagine that you must decide whether to join a university In one scenario, you ask only one person about the quality of the education the university provides In another scenario, you ask several career counselors and academicians about this Which scenario do you think would help you make a better and the most stable decision The second one, right This is because, in the first case, the only person you are consulting may be biased Wisdom of the crowd tends to remove biases, thereby aiding better decision making In general terms, a forest is a collection of different types of trees The same definition holds true in the case of machine learning as well Instead of using a single decision tree for prediction, we use several of them In the scenario we described earlier, the first case is equivalent to using a single decision tree, whereas the second one is equivalent to using severalthat is, using a forest In a random forest, an individual trees vote impacts the final decision Just like decision trees, random forest is capable of carrying out both classification and regression tasks An advantage of the random forest algorithm is that it uses a sampling technique called bagging, which prevents overfitting  Bagging is the process of training meta- algorithms on a different subsample of the data and then combining these to create a better model Overfitting refers to cases where a model learns the training dataset so well that it is unable to generalize or perform well on another validationtest dataset Supervised Learning 155 Random forests also aid in understanding the importance of predictor variables and features However, building a random forest often takes a huge amount of time and memory We can make use of the scikit learn library to implement the random forest algorithm in Python using the following code from sklearn.ensemble import Random Forest Classifier rfc Random Forest Classifier rfc rfc.fitX, y predicted_labels rfc.predictX Here, we created the base model using the Random Forest Classifier class of scikit learn Then, we trained the model using the fit function The trained model can then be used to make predictions using the predict function X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables Gradient Boosting Machine and Extreme Gradient Boost There are various other tree based algorithms, such as gradient boosting machines G BM and extreme gradient boosting X GBoost  Boosting works by combining rough, less complex, or weak models into a single prediction that is more accurate than any single model Iteratively, a subset of the training dataset is ingested into a weak algorithm or simple algorithm such as a decision tree to generate a weak model These weak models are then combined to form the final prediction G BM makes use of classification trees as the weak algorithm The results are generated by improving estimations from these weak models using a differentiable loss function, which gives us the performance of the model by calculating how far the prediction is from the actual value The model fits consecutive trees by considering the net loss of the previous trees therefore, each tree is partially present in the final solution 156 Developing a Text Classifier X GBoost is an enhanced version of G BM that is portable and distributed, which means that it can easily be used in different architectures and can use multiple cores a single machine or multiple machines clusters As a bonus, the X GBoost library is written in C, which makes it fast It is also useful when working with a huge dataset as it allows you to store data on an external disk rather than load all the data into memory The main reasons for the popularity of X GBoost are as follows Ability to automatically deal with missing values High speed execution High accuracy, if properly trained Support for distributed frameworks such as Hadoop and Spark X GBoost uses a weighted combination of weak learners during the training phase We can make use of the xgboost library to implement the X GBoost algorithm in Python using the following code from xgboost import X GB Classifier xgb_clfX GB Classifier xgb_clf xgb_clf.fitX, y predicted_labels rfc.predictX Here, we created the base model using the X GB Classifier class of xgboost  Then, we trained the model using the fit function The trained model can then be used to make predictions using the predict function Here, X represents a Data Frame of independent variables, whereas y represents a Data Frame of dependent variables To get the important features for the trained model, we can use the following code pd Data FramewordX.columns,importancexgb_clf.feature_ importances_ Lets perform some exercises to get a better understanding of tree based methods Supervised Learning 157 Exercise 3 08 Tree Based Methods Random Forest In this exercise, we will use the tree based method random forest to predict the overall scores and labels of reviews of patio, lawn, and garden products on Amazon Follow these steps to implement this exercise Note To download the dataset for this exercise, visit 1  Open a Jupyter Notebook Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 2  Now, read the given data file in J SO N format using pandas  Add the following code to implement this data_patio_lawn_garden pd.read_json data reviews_ Patio_ Lawn_and_ Garden_5.json, lines True data_patio_lawn_garden.head 158 Developing a Text Classifier The preceding code generates the following output Figure 3 37 Storing reviews as a Data Frame 3  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them using Word Net Lemmatizer , and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use a regular expression re to replace anything other than letters, digits, and whitespaces with blank spaces Add the following code to do this lemmatizer Word Net Lemmatizer data_patio_lawn_garden data_patio_lawn_garden .applylambda x .joinlemmatizer.lemmatizeword.lower for word in word_tokenizere.subr_, , strx data_patio_lawn_gardencleaned_review_text, review Text, overall.head Supervised Learning 159 The preceding code generates the following output Figure 3 38 Review text before and after cleaning, along with overall scores 4  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text with the following code tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform data_patio_lawn_garden .todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 39 T FI DF representation as a Data Frame 5  Add the following lines of code to create a new column called target, which will have 0 if the overall parameter is less than 4 otherwise, it will have 1 data_patio_lawn_garden data_patio_lawn_garden .apply lambda x 0 if x4 else 1 data_patio_lawn_garden.value_counts 160 Developing a Text Classifier The preceding code generates the following output 1 7037 0 6235 Name target, dtype int64 6  Now, define a generic function for all the classifier models Add the following code to do this def clf_modelmodel_type, X_train, y model model_type.fitX_train,y predicted_labels model.predicttfidf_df return predicted_labels 7  Train three kinds of classifier modelsnamely, random forest, gradient boosting machines, and X GBoost For random forest, we predict the class labels of the given set of review texts and compare them with their actual classthat is, the target, using crosstabs Add the following code to do this from sklearn.ensemble import Random Forest Classifier rfc Random Forest Classifiern_estimators20,max_depth4, max_featuressqrt,random_state1 data_patio_lawn_garden clf_modelrfc, tfidf_df, data_patio_lawn_garden pd.crosstabdata_patio_lawn_garden, data_patio_lawn_garden The preceding code generates the following output Figure 3 40 Crosstab between actual target values and predicted labels Here, we can see 3302 instances with a target label of 0 correctly classified, and 2933 such instances wrongly classified Furthermore, 5480 instances with a target label of 1 are correctly classified, whereas 1557 such instances are wrongly classified Supervised Learning 161 8  Now, define a generic function for all regression models Add the following code to do this def reg_modelmodel_type, X_train, y model model_type.fitX_train,y predicted_values model.predicttfidf_df return predicted_values 9  Train three kinds of regression models random forest, gradient boosting machines, and X GBoost For random forest, we predict the overall score of the given set of review texts Add the following code to do this from sklearn.ensemble import Random Forest Regressor rfg Random Forest Regressorn_estimators20,max_depth4, max_featuressqrt,random_state1 data_patio_lawn_garden reg_modelrfg, tfidf_df, data_patio_lawn_garden data_patio_lawn_gardenoverall, predicted_values_rfg.head10 The preceding code generates the following output Figure 3 41 Actual overall score and predicted values using a random forest regressor 162 Developing a Text Classifier From the preceding table, we can see how the actual and predicted scores vary for different instances We will use this table later to evaluate the performance of the model Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, lets perform a similar task using the X GBoost method Exercise 3 09 Tree Based Methods X GBoost In this exercise, we will use the tree based method X GBoost to predict the overall scores and labels of reviews of patio, lawn, and garden products on Amazon Note To download the dataset for this exercise, visit  Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import Supervised Learning 163 import nltk nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  Now, read the given data file in J SO N format using pandas  Add the following code to implement this data_patio_lawn_garden pd.read_json data reviews_ Patio_ Lawn_and_ Garden_5.json, lines True data_patio_lawn_garden.head The preceding code generates the following output Figure 3 42 Storing reviews as a Data Frame 4  Use a lambda function to extract tokens from each review Text of this Data Frame, lemmatize them using Worrd Net Lemmatizer , and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than letters, digits, and whitespaces with blank spaces Add the following code to do this lemmatizer Word Net Lemmatizer data_patio_lawn_garden data_patio_lawn_garden.applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.sub 164 Developing a Text Classifier r_, , strx data_patio_lawn_gardencleaned_review_text, review Text, overall.head The preceding code generates the following output Figure 3 43 Review text before and after cleaning, along with overall scores 5  Create a Data Frame from the T FI DF matrix representation of the cleaned version of review Text with the following code tfidf_model Tfidf Vectorizermax_features500 tfidf_df pd Data Frametfidf_model.fit_transform data_patio_lawn_garden .todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 44 T FI DF representation as a Data Frame 6  The following lines of code are used to create a new column called target, which will have 0 if the overall parameter is less than 4 otherwise, it will have 1 data_patio_lawn_garden data_patio_lawn_garden .apply lambda x 0 if x4 else 1 data_patio_lawn_garden.value_counts Supervised Learning 165 The preceding code generates the following output 1 7037 0 6235 Name target, dtype int64 7  Now, define a generic function for all the classifier models Add the following code to do this def clf_modelmodel_type, X_train, y model model_type.fitX_train,y predicted_labels model.predicttfidf_df return predicted_labels 8  Predict the class labels of the given set of review Text and compare it with their actual class, that is, the target, using the crosstab function Add the following code to do this pip install xgboost from xgboost import X GB Classifier xgb_clfX GB Classifiern_estimators20,learning_rate0 03 , max_depth5,subsample0 6 , colsample_bytree 0 6 ,reg_alpha 10 , seed42 data_patio_lawn_garden clf_modelxgb_clf, tfidf_df, data_patio_lawn_garden pd.crosstabdata_patio_lawn_garden, data_patio_lawn_garden The preceding code generates the following output Figure 3 45 Crosstab between actual target values and predicted labels using X GBoost 166 Developing a Text Classifier Here, we can see 4300 instances with a target label of 0 correctly classified, and 1935 such instances wrongly classified Furthermore, 2164 instances with a target label of 1 are correctly classified, whereas 4873 such instances are wrongly classified 9  Now, define a generic function for all the regression models Add the following code to do this def reg_modelmodel_type, X_train, y model model_type.fitX_train,y predicted_values model.predicttfidf_df return predicted_values 10  Predict the overall score of the given set of review Text  Add the following code to do this from xgboost import X GB Regressor xgbr X GB Regressorn_estimators20,learning_rate0 03 , max_depth5,subsample0 6 , colsample_bytree 0 6 ,reg_alpha 10 ,seed42 data_patio_lawn_garden reg_modelxgbr, tfidf_df, data_patio_lawn_garden data_patio_lawn_gardenoverall, predicted_values_xgbr.head2 The preceding code generates the following output Figure 3 46 Actual overall score and predicted values using an X GBoost regressor Supervised Learning 167 From the preceding table, we can see how the actual and predicted scores vary for different instances We will use this table later to evaluate the performance of the model With that, you have learned how to use tree based methods to predict scores in data Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, you will learn about sampling Sampling Sampling is the process of creating a subset from a given set of instances If you have 1 , 000 sentences in an article, out of which you choose 100 sentences for analysis, the subset of 100 sentences will be called a sample of the original article This process is referred to as sampling Sampling is necessary when creating models for imbalanced datasets For example, consider that the number of bad comments on a review board for a company is 10 and the number of good comments is 1 , 000  If we input this data as it is into the model, it will not give us accurate results classifying all the comments as good will provide a near perfect accuracy, which is not really applicable to most real datasets Thus, we need to reduce the number of good reviews to a smaller number before using it as input for training There are different kinds of sampling methods, such as the following Simple random sampling In this process, each instance of the set has an equal probability of being selected For example, you have 10 balls of 10 different colors in a box You need to select 4 out of 10 balls without looking at their color In this case, each ball is equally likely to be selected This is an instance of simple random sampling 168 Developing a Text Classifier Stratified sampling In this type of sampling, the original set is divided into parts called strata, based on given criteria Random samples are chosen from each of these strata For example, you have 100 sentences, out of which 80 are non sarcastic and 20 are sarcastic To extract a stratified sample of 10 sentences, you need to select 8 from 80 non sarcastic sentences and 2 from 20 sarcastic sentences This will ensure that the ratio of non sarcastic to sarcastic sentences, that is, 8020 , remains unaltered in the sample thats selected Multi-Stage Sampling If you are analyzing the social media posts of all the people in a country related to the current weather, the text data will be huge as it will consist of the weather conditions of different cities Drawing a stratified sample would be difficult In this case, it is recommended to first extract a stratified sample by region and then further sample it within regions, that is, by cities This is basically performing stratified sampling at each and every stage To better understand these, lets perform a simple exercise Exercise 3 10 Sampling Simple Random, Stratified, and Multi-Stage In this exercise, we will extract samples from an online retail dataset that contains details about the transactions of an e-commerce website with the help of simple random sampling, stratified sampling, and multi-stage sampling Note To download the dataset for this exercise, visit  Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import pandas and read the dataset pip install xlrd import pandas as pd data pd.read_exceldataOnline Retail.xlsx data.shape Supervised Learning 169 The preceding code generates the following output 54190 , 8 3  We use pandas sample function to extract a sample from the Data Frame  Add the following code to do this selecting 10% of the data randomly data_sample_random data.samplefrac0 1 ,random_state42 data_sample_random.shape The preceding code generates the following output 54191 , 8 4  Use sklearns train_test_split function to create stratified samples Add the following code to do this from sklearn.model_selection import train_test_split X_train, X_valid, y_train, y_valid train_test_split data, data, test_size0 2 , random_state42, stratify data You can confirm the stratified split by checking the percentage of each category in the country column after the split To get the train set percentages, use the following code y_train.value_countsy_train.shape170 Developing a Text Classifier The following is part of the output of the preceding code Figure 3 47 The percentage of countries for the training set 5  Similarly, for the validation set, add the following code y_valid.value_countsy_valid.shape Supervised Learning 171 The following is part of the output of the preceding code Figure 3 48 The percentage of countries for the validation set As we can see, the percentages of all countries are similar in both the train and validation sets 172 Developing a Text Classifier 6  We filter out the data in various stages and extract random samples from it We will extract a random sample of 2% from those transactions by country that occurred in the United Kingdom, Germany, or France and where the corresponding quantity is greater than or equal to 2  Add the following code to implement this data_ugf data.isinUnited Kingdom, Germany, France data_ugf_q2 data_ugf2 data_ugf_q2_sample data_ugf_q2.samplefrac 02 , random_state42 data_ugf_q2.shape The preceding code generates the following output 356940 , 8 Now, add the following line of code data_ugf_q2_sample.shape This will generate the following output 7139 , 8 We can see the reduction in size of the data when the filtering criteria is applied and then the reduction in size when a sample of the filtered data is taken In this exercise, you learned about the three major sampling techniques that will help you create a good training dataset for the text classifier that you will learn how to build in the next section Note To access the source code for this specific section, please refer to  You can also run this example online at  Developing a Text Classifier 173 Developing a Text Classifier A text classifier is a machine learning model that is capable of labeling texts based on their content For instance, a text classifier will help you understand whether a random text statement is sarcastic or not Presently, text classifiers are gaining importance as manually classifying huge amounts of text data is impossible In the next few sections, we will learn about the different parts of text classifiers and implement them in Python Feature Extraction When dealing with text data, features denote its different attributes Generally, they are numeric representations of the text As we discussed in Chapter 2 , Feature Extraction Methods , T FI DF representations of texts are one of the most popular ways of extracting features from them Feature Engineering Feature engineering is the art of extracting new features from existing ones Extracting novel features, which tend to capture variation in data better, requires sound domain expertise Removing Correlated Features Correlation refers to the statistical relationship between two variables Two highly correlated variables provide the same kind of information For example, the remaining battery life of a laptop and its screen time are highly correlated The battery life will decrease as the screen time increases Regression models, including logistic regression, are unable to perform well when correlation between features exists Thus, features with correlation beyond a certain threshold need to be removed The most widely used correlation statistic is Pearson correlation, which can be calculated as follows Figure 3 49 Pearson correlation Here, cov is the covariance, is the standard deviation, and X and Y are two variables features of the training data that we are testing for correlation 174 Developing a Text Classifier Exercise 3 11 Removing Highly Correlated Features Tokens In this exercise, we will remove highly correlated words from a T FI DF matrix representation of sklearns fetch_20newgroups text dataset Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages from sklearn.datasets import fetch_20newsgroups import matplotlib as mpl import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadstopwords nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  We will be using stop words from the English language only Word Net is the lemmatizer we will be using Add the following code to implement this stop_words stopwords.wordsenglish stop_words stop_words liststring.printable lemmatizer Word Net Lemmatizer 4  To specify the categories of news articles you want to fetch, add the following code categories misc.forsale, sci.electronics, talk.religion.miscDeveloping a Text Classifier 175 5  To fetch sklearns 20newsgroups text dataset, corresponding to the categories mentioned earlier, use the following lines of code news_data fetch_20newsgroupssubsettrain, categoriescategories, shuffleTrue, random_state42, download_if_missingTrue news_data_df pd Data Frametext news_data, category news_data.target news_data_df.head The preceding code generates the following output Figure 3 50 Texts of news data as a Data Frame 6  Now, use the lambda function to extract tokens from each text of the news_ data_df Data Frame Check whether any of these tokens are stop words, lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than letters, digits, and whitespaces with blank spaces Add the following code to implement this news_data_df news_data_df .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx if word.lower not in stop_words 176 Developing a Text Classifier 7  Add the following lines of code used to create a T FI DF matrix and transform it into a Data Frame tfidf_model Tfidf Vectorizermax_features20 tfidf_df pd Data Frametfidf_model.fit_transform news_data_df.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 51 T FI DF representation as a Data Frame 8  Calculate the correlation matrix for this T FI DF representation Add the following code to implement this correlation_matrix tfidf_df.corr correlation_matrix.head The preceding code generates the following output Figure 3 52 Correlation matrix 9  Now, plot the correlation matrix using seaborns heatmap function Add the following code to implement this import seaborn as sns fig, ax plt.subplotsfigsize20, 20 sns.heatmapcorrelation_matrix,annotTrue, fmt.1g, vmin- 1 , vmax1, center 0 , cmap coolwarm Developing a Text Classifier 177 The preceding code generates the following output Figure 3 53 Heatmap representation of a correlation matrix 178 Developing a Text Classifier 10  To identify a pair of terms with high correlation, create an upper triangular matrix from the correlation matrix Create a stacked array out of it and traverse it Add the following code to do this import numpy as np correlation_matrix_ut correlation_matrix.wherenp.triu np.onescorrelation_matrix.shape .astypenp.bool correlation_matrix_melted correlation_matrix_ut.stack .reset_index correlation_matrix_melted.columns word1, word2, correlation correlation_matrix_melted correlation_matrix_melted correlation_matrix_melted 7 The preceding code generates the following output Figure 3 54 Highly correlated tokens You can see that the most highly correlated features are host , nntp , posting , organization , and subject  Next, we will remove nntp , posting , and organization since host and subject are highly correlated with them 11  Remove terms for which the coefficient of correlation is greater than 0 7 and create a separate Data Frame with the remaining terms Add the following code to do this tfidf_df_without_correlated_word tfidf_df.dropnntp, posting, organization, axis 1 tfidf_df_without_correlated_word.head Developing a Text Classifier 179 The preceding code generates the following output Figure 3 55 The Data Frame after removing correlated tokens After removing the highly correlated words from the T FI DF Data Frame, it appears like this We have cleaned the dataset to remove highly correlated features and are now one step closer to building our text classifier Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will learn how to reduce the size of the dataset and understand why this is necessary Dimensionality Reduction There are some optional steps that we can follow on a case to case basis For example, sometimes, the T FI DF matrix or Bag of Words representation of a text corpus is so big that it does not fit in memory In this case, it would be necessary to reduce its dimensionthat is, the number of columns in the feature matrix The most popular method for dimension reduction is Principal Component Analysis P CA  P CA is used to perform dimensionality reduction It converts a list of features which may be correlated into a list of variables that are linearly uncorrelated These linearly uncorrelated variables are known as principal components These principal components are arranged in descending order of the amount of variability they capture in the dataset For example, lets consider a Twitter tweet dataset where people misspell words such as good and instead write gud P CA will combine these two highly correlated features into a single feature and reduce the dimensionality In the next section, well look at an exercise to get a better understanding of this 180 Developing a Text Classifier Exercise 3 12 Performing Dimensionality Reduction Using Principal Component Analysis In this exercise, we will reduce the dimensionality of a T FI DF matrix representation of sklearns fetch_20newsgroups text dataset to two Then, well create a scatter plot of these documents Each category should be colored differently Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages from sklearn.datasets import fetch_20newsgroups import matplotlib as mpl import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import re import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from collections import Counter from pylab import import nltk nltk.downloadstopwords nltk.downloadpunkt nltk.downloadwordnet import warnings warnings.filterwarningsignore 3  Use stop words from the English language only Word Net states the lemmatizer we will be using Add the following code to implement this stop_words stopwords.wordsenglish stop_words stop_words liststring.printable lemmatizer Word Net LemmatizerDeveloping a Text Classifier 181 4  To specify the categories of news articles we want to fetch by, add the following code categories misc.forsale, sci.electronics, talk.religion.misc 5  To fetch sklearns dataset corresponding to the categories we mentioned earlier, use the following lines of code news_data fetch_20newsgroupssubsettrain, categoriescategories, shuffleTrue, random_state42, download_if_missingTrue news_data_df pd Data Frametext news_data, category news_data.target news_data_df.head The preceding code generates the following output Figure 3 56 News texts and their categories 182 Developing a Text Classifier 6  Use the lambda function to extract tokens from each text item of the news_ data_df Data Frame, check whether any of these tokens are stop words, lemmatize them, and concatenate them side by side Use the join function to concatenate a list of words into a single sentence Use the regular expression method re to replace anything other than letters, digits, and whitespaces with a blank space Add the following code to implement this news_data_df news_data_df .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx if word.lower not in stop_words 7  The following lines of code are used to create a T FI DF matrix and transform it into a Data Frame tfidf_model Tfidf Vectorizermax_features20 tfidf_df pd Data Frametfidf_model.fit_transform news_data_df.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.head The preceding code generates the following output Figure 3 57 T FI DF representation as a Data Frame Developing a Text Classifier 183 8  In this step, we are using sklearns P CA function to extract two principal components from the initial data Add the following code to do this from sklearn.decomposition import P CA pca P CA2 pca.fittfidf_df reduced_tfidf pca.transformtfidf_df reduced_tfidf The preceding code generates the following output Figure 3 58 Principal components In the preceding screenshot, you can see the two principal components that the P CA algorithm calculated 9  Now, well create a scatter plot using these principal components and represent each category with a separate color Add the following code to implement this scatter plt.scatterreduced_tfidf, reduced_tfidf, cnews_data_df, cmapgray plt.xlabeldimension_1 plt.ylabeldimension_2 plt.legendhandlesscatter.legend_elements, labelscategories, loclower left plt.titleRepresentation of N EW S documents in 2 D plt.show 184 Developing a Text Classifier The preceding code generates the following output Figure 3 59 Two dimensional representation of news documents Developing a Text Classifier 185 From the preceding plot, we can see that a scatter plot has been created in which each category of article is represented by a different color This plot shows another important use case of dimensionality reduction visualization We were able to plot this two dimensional image because we had two principal components With the earlier T FI DF matrix, we had 20 features, which is impossible to visualize In this section, you learned how to perform dimensionality reduction to save memory space and visualize datasets Note To access the source code for this specific section, please refer to  You can also run this example online at  Next, we will learn how to evaluate the machine learning models that we train Deciding on a Model Type Once the feature set is ready, its necessary to decide on the type of model that will be used to deal with the problem Usually, unsupervised models are chosen when data is not labeled If we have a predefined number of clusters in mind, we go for clustering algorithms such as k-means otherwise, we opt for hierarchical clustering For labeled data, we generally follow supervised learning methods such as regression and classification If the outcome is continuous and numeric, we use regression If it is discrete or categorical, we use classification The Naive Bayes algorithm comes in handy for the fast development of simple classification models More complex tree based methods such as decision trees, random forests, and so on are needed when we want to achieve higher accuracy In such cases, we sometimes compromise on model explainability and the time thats required to develop it When the outcome of a model has to be the probability of the occurrence of a certain class, we use logistic regression 186 Developing a Text Classifier Evaluating the Performance of a Model Once a model is ready, it is necessary to evaluate its performance This is because, without benchmarking it, we cannot be confident of how well or how badly it is functioning It is not advisable to put a model into production without evaluating its efficiency There are various ways to evaluate a models performance Lets work through them one by one Confusion Matrix This is a two dimensional matrix mainly used for evaluating the performance of classification models Its columns consist of predicted values, and its rows consist of actual values In other words, for a given confusion matrix, it is a crosstab between actual and predicted values The cell entries denote how many of the predicted values match with the actual values, and how many do not Consider the following image Figure 3 60 Confusion matrix In the preceding confusion matrix, the top left cell will have the count of all correctly classified 0 values by the classifier, whereas the top right cell will have the count of incorrectly classified 0 values, and so on To create a confusion matrix using Python, you can use the following code from sklearn.metrics import confusion_matrix confusion_matrixactual_values,predicted_values Developing a Text Classifier 187 Accuracy Accuracy is defined as the ratio of correctly classified instances to the total number of instances Whenever accuracy is used for model evaluation, we need to ensure that the data is balanced in terms of classes, meaning it should have an almost equal number of instances of each class Lets use an example of a dataset that has 90% positive labels and 10% negative labels A model that predicts all the data points as positive will receive 90% accuracy, but that will not be a good indicator of the performance of the model To get the accuracy of the predicted values using Python, you can use the following code from sklearn.metrics import accuracy_score accuracy_scoreactual_values,predicted_values Precision and Recall For a better understanding of precision and recall, lets consider a real life example If your mother tells you to explore the kitchen of your house, find items that need to be restocked, and bring them back from the market, you will bring P number of items from the market and show them to your mother Out of P items, she finds Q items to be relevant The QP ratio is called precision However, in this scenario, she was expecting you to bring R items relevant to her The ratio, QR, is referred to as recall Precision True Positive True Positive False Positive Recall True Positive True Positive False Negative F1 Score For a given classification model, the F1 score is the harmonic mean of precision and recall Harmonic mean is a way to find the average while giving equal weight to all numbers F1 score 2 Precision Recall Precision Recall To get the F1 score, precision, and recall values using Python, you can use the following code from sklearn.metrics import classification_report classification_reportactual_values,predicted_values188 Developing a Text Classifier Receiver Operating Characteristic R OC Curve To understand the R OC curve, we need to get acquainted with the True Positive Rate T PR and the False Positive Rate F PR T PR True Positive True Positive False Negative F PR False Positive False Positive True Negative The output of a classification model can be probabilities In that case, we need to set a threshold to obtain classes from those probabilities The R OC curve is a plot between the T PR and F PR for various values of the threshold The area under the R OC curve A UR OC represents the efficiency of the model The higher the A UR OC, the better the model is The maximum value of A UR OC is 1  To create the R OC curve using Python, use the following code fpr,tpr,thresholdroc_curveactual_values, predicted_probabilities print n Area under R OC curve for validation set, aucfpr,tpr fig, ax plt.subplotsfigsize6, 6 ax.plotfpr,tpr,labelValidation set A UC plt.xlabelFalse Positive Rate plt.ylabelTrue Positive Rate ax.legendlocbest plt.show Here, actual_values refers to the actual dependent variable values, whereas predicted_probabilities is the predicted probability of getting 1 from the trained predictor model Root Mean Square Error R MS E This is mainly used for evaluating the accuracy of regression models We define it as follows Figure 3 61 Formula for root mean square error Developing a Text Classifier 189 Here, n is the number of samples, Pi is the predicted value of the ith observation, and Oi is the observed value of the ith observation To find the R MS E using Python, use the following code from sklearn.metrics import mean_squared_error rmse sqrtmean_squared_errory_actual, y_predicted Maximum Absolute Percentage Error M AP E Just like R MS E, this is another way to evaluate a regression models performance It is described as follows Figure 3 62 Formula for maximum absolute percentage error Here, n is the number of samples, Pi is the predicted value that is, the forecast value of the ith observation, and Oi is the observed value that is, the actual value of the ith observation To find M AP E in Python, use the following code from sklearn.metrics import mean_absolute_error mape mean_absolute_errory_actual, y_predicted 100 Exercise 3 13 Calculating the R MS E and M AP E of a Dataset In this exercise, we will calculate the R MS E and M AP E of hypothetical predicted and actual values Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Use sklearns mean_squared_error to calculate the M SE and then use the sqrt function to calculate the R MS E Add the following code to implement this from sklearn.metrics import mean_squared_error from math import sqrt y_actual y_predicted rms sqrtmean_squared_errory_actual, y_predicted printRoot Mean Squared Error R MS E is, rms 190 Developing a Text Classifier The preceding code generates the following output Root Mean Squared Error R MS E is 0 21019038988498018 The preceding output shows the R MS E of the y_actual and y_predicted values that we created previously 3  Next, use sklearns mean_absolute_error to calculate the M AP E of a hypothetical model prediction Add the following code to implement this from sklearn.metrics import mean_absolute_error y_actual y_predicted mape mean_absolute_errory_actual, y_predicted 100 printMean Absolute Percentage Error M AP E is, roundmape, 2 , % The preceding code generates the following output Mean Absolute Percentage Error M AP E is 16 .6% The preceding output shows the M AP E of the y_actual and y_predicted values that we created previously You have now learned how to evaluate the machine learning models that we train and are equipped to create your very own text classifier Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will solve an activity based on classifying text Developing a Text Classifier 191 Activity 3 01 Developing End to End Text Classifiers For this activity, you will build an end to end classifier that figures out whether a news article is political or not Note The dataset for this activity can be found at  Follow these steps to implement this activity 1  Import the necessary packages 2  Read the dataset and clean it 3  Create a T FI DF matrix out of it 4  Divide the data into training and validation sets 5  Develop classifier models for the dataset 6  Evaluate the models that were developed using parameters such as confusion matrix, accuracy, precision, recall, F1 plot curve, and R OC curve Note The solution for this activity can be found via this link  We have seen how to build end to end classifiers Developing an end to end classifier was done in phases Firstly, the text corpus was cleaned and tokenized, features were extracted using T FI DF, and then the dataset was divided into training and validation sets The X GBoost algorithm was used to develop a classification model Finally, the performance was measured using parameters such as the confusion matrix, accuracy, precision, recall, F1 plot curve, and R OC curve In the next section, you will learn how to build pipelines for N LP projects 192 Developing a Text Classifier Building Pipelines for N LP Projects In general, a pipeline refers to a structure that allows a streamlined flow of air, water, or something similar In this context, pipeline has a similar meaning It helps to streamline various stages of an N LP project An N LP project is done in various stages, such as tokenization, stemming, feature extraction T FI DF matrix generation, and model building Instead of carrying out each stage separately, we create an ordered list of all these stages This list is known as a pipeline The Pipeline class of sklearn helps us combine these stages into one object that we can use to perform these stages one after the other in a sequence We will solve a text classification problem using a pipeline in the next section to understand the working of a pipeline better Exercise 3 14 Building the Pipeline for an N LP Project In this exercise, we will develop a pipeline that will allow us to create a T FI DF matrix representation from sklearns fetch_20newsgroups text dataset Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import Tfidf Transformer from sklearn import tree from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import Count Vectorizer import pandas as pd 3  Specify the categories of news articles you want to fetch Add the following code to do this categories misc.forsale, sci.electronics, talk.religion.miscBuilding Pipelines for N LP Projects 193 4  To fetch sklearns 20newsgroups dataset, corresponding to the categories mentioned earlier, we use the following lines of code news_data fetch_20newsgroupssubsettrain, categoriescategories, shuffleTrue, random_state42, download_if_missingTrue 5  Define a pipeline consisting of two stages Count Vectorizer and Tfidf Transformer  Fit it on the news_data we mentioned earlier and use it to transform that data Add the following code to implement this text_classifier_pipeline Pipelinevect, Count Vectorizer, tfidf, Tfidf Transformer text_classifier_pipeline.fitnews_data.data, news_data.target pd Data Frametext_classifier_pipeline.fit_transform news_data.data, news_data.target.todense.head The preceding code generates the following output Figure 3 63 T FI DF representation of the Data Frame created using a pipeline Here, we created a pipeline consisting of the count vectorizer and T FI DF transformer The outcome of this pipeline is the T FI DF representation of the text data that has been passed to it as an argument Note To access the source code for this specific section, please refer to  You can also run this example online at 194 Developing a Text Classifier Saving and Loading Models After a model has been built and its performance matches our expectations, we may want to save it for future use This eliminates the time needed for rebuilding it Models can be saved on the hard disk using the joblib and pickle libraries The pickle module makes use of binary protocols to save and load Python objects joblib makes use of the pickle library protocols, but it improves on them to provide an efficient replacement to save large Python objects Both libraries have two main functions that we will make use of to save and load our models dump This function is used to save a Python object to a file on the disk loads This function is used to load a saved Python object from a file on the disk To deploy saved models, we need to load them from the hard disk to the memory In the next section, we will perform an exercise based on this to get a better understanding of this process Exercise 3 15 Saving and Loading Models In this exercise, we will create a T FI DF representation of sentences Then, we will save this model on disk and later load it from the disk Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and the following code to import the necessary packages import pickle from joblib import dump, load from sklearn.feature_extraction.text import Tfidf Vectorizer 3  Define a corpus consisting of four sentences by adding the following code corpus Data Science is an overlap between Arts and Science, Generally, Arts graduates are right brained and Science graduates are left brained, Excelling in both Arts and Science at a time becomes difficult, Natural Language Processing is a part of Data ScienceSaving and Loading Models 195 4  Fit a T FI DF model to it Add the following code to do this tfidf_model Tfidf Vectorizer tfidf_vectors tfidf_model.fit_transformcorpus.todense printtfidf_vectors The preceding code generates the following output Figure 3 64 T FI DF representation as a matrix 5  Save this T FI DF model on disk using joblib  Add the following code to do this dumptfidf_model, tfidf_model.joblib 6  Finally, load this model from disk to memory and use it Add the following code to do this tfidf_model_loaded loadtfidf_model.joblib loaded_tfidf_vectors tfidf_model_loaded.transformcorpus.todense printloaded_tfidf_vectors 196 Developing a Text Classifier The preceding code generates the following output Figure 3 65 T FI DF representation as a matrix 7  Save this T FI DF model on disk using pickle  Add the following code to do this pickle.dumptfidf_model, opentfidf_model.pickle.dat, wb 8  Load this model from disk to memory and use it Add the following code to do this loaded_model pickle.loadopentfidf_model.pickle.dat, rb loaded_tfidf_vectors loaded_model.transformcorpus.todense printloaded_tfidf_vectors Saving and Loading Models 197 The preceding code generates the following output Figure 3 66 T FI DF representation as a matrix From the preceding screenshot, we can see that the saved model and the model that was loaded from the disk are identical You have now learned how to save and load models This section marks the end of this chapter, where you learned how to build a text classifier from scratch Note To access the source code for this specific section, please refer to  You can also run this example online at 198 Developing a Text Classifier Summary In this chapter, you learned about the different types of machine learning techniques, such as supervised and unsupervised learning We explored unsupervised algorithms such as hierarchical clustering and k means clustering, and supervised learning algorithms, such as k nearest neighbor, the Naive Bayes classifier, and tree based methods, such as random forest and X GBoost, that can perform both regression and classification We discussed the need for sampling and went over different kinds of sampling techniques for splitting a given dataset into training and validation sets Finally, we covered the process of saving a model on the hard disk and loading it back into memory for future use In the next chapter, you will learn about several techniques that you can use to collect data from various sources Overview This chapter introduces you to the concept of web scraping You will first learn how to extract data such as text, images, lists, and tables from pages that are written using H TM L You will then learn about the various types of semi structured data used to create web pages such as J SO N and X ML and extract data from them Finally, you will use A PIs for data extraction from Twitter, using the tweepy package Collecting Text Data with Web Scraping and A PIs4202 Collecting Text Data with Web Scraping and A PIs Introduction In the last chapter, we developed a simple classifier using feature extraction methods We also covered different algorithms that fall under supervised and unsupervised learning In this chapter, you will learn how to collect text data by scraping web pages, and then you will learn how to process that data Web scraping helps you extract useful data from online content, such as product prices and customer reviews, which can then be used for market research, price comparison for products, or data analysis You will also learn how to handle various kinds of semi structured data, such as J SO N and X ML We will cover different methods for extracting data using Application Programming Interfaces A PIs  Finally, we will explore different ways to extract data from different types of files Collecting Data by Scraping Web Pages The basic building block of any web page is H TM L Hypertext Markup Language a markup language that specifies the structure of your content H TM L is written using a series of tags, combined with optional content The content encompassed within H TM L tags defines the appearance of the web page It can be used to make words bold or italicize them, to add hyperlinks to the text, and even to add images Additional information can be added to the element using attributes within tags So, a web page can be considered to be a document written using H TM L Thus, we need to know the basics of H TM L to scrape web pages effectively The following figure depicts the contents that are included within an H TM L tag Figure 4 1 Tags and attributes of H TM L Collecting Data by Scraping Web Pages 203 As you can see in the preceding figure, we can easily identify different elements within an H TM L tag The basic H TM L structure and commonly used tags are shown and explained as follows Figure 4 2 Basic H TM L structure D OC TY PE This is a must have preamble for every H TM L page It informs the browser that the document is written in H TM L html tag This is considered the root of the page, encompassing all of the page content It is mainly divided into two tags head and body  head tag This tag provides meta information about the web page body tag This tag comprises content such as text, image, tables, and lists title tag This sets the title of your page, which is what you will see in the browsers tab headline tag As the name suggests, this represents six levels of section headings, from h1 to h6  p tag This is used to define the paragraph text content i tag We can use this tag to italicize the text strong tag This makes the text bold li tag We can use this tag to list the content in ordered the ol tag or unordered the ul tag list format 204 Collecting Text Data with Web Scraping and A PIs img tag This tag is used to add an image in the H TM L document h1 to h6 tags These represent the various levels of headings, with h1 having the biggest size and h6 having the smallest size span tag Although this tag provides no visual change by itself, it is useful for grouping inline elements in a document and adding a hook to a part of a text or a part of a document q tag Quotes are written within the q tag in H TM L table tag Tabular content is represented as a table tag, which contains th table header, tr table row, and td table data address tag In H TM L documents, addresses are enclosed within address tags In the next section, we will walk through an exercise in which well extract tag based information from H TM L files Exercise 4 01 Extraction of Tag Based Information from H TM L Files In this exercise, we will extract addresses, quotes, text written in bold, and a table present in an H TM L file Note The data for this sample H TM L file can be accessed from  Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the Beautiful Soup library from bs4 import Beautiful Soup Beautiful Soup is a Python library for pulling data out of H TM L and X ML files It provides a parser for H TM LX ML formats, allowing us to navigate, search, and modify the parsed tree Collecting Data by Scraping Web Pages 205 3  Create an object of the Beautiful Soup class and pass the location of the H TM L file to it soup Beautiful Soupopen datasample_doc.html, html.parser In the preceding line, html.parser is Pythons built in standard library parser Beautiful Soup also supports third party parsers such as html5lib , lxml , and others 4  Add the following code to check the text contents of the sample_doc.html file soup.text The preceding code generates the following output Figure 4 3 Text content of an H TM L file 5  Similarly, to see the contents, you can simply write the following code soup.contents Figure 4 4 Text content 6  To find the addresses from the document, insert a new cell and add the following code soup.findaddress The preceding code generates the following output address Mess on No 72 , Banamali Naskar Lane, Kolkata.address 206 Collecting Text Data with Web Scraping and A PIs 7  To locate all the address tags within the given content, write the following code soup.find_alladdress The preceding code generates the following output address Mess on No 72 , Banamali Naskar Lane, Kolkata.address, address221 B, Baker Street, London, U K.address 8  To find the quotes in the document, add the following code soup.find_allq The preceding code generates the following output q There are more things in heaven and earth, Horatio, br Than are dreamt of in your philosophy q 9  To check all the bold items, write the following command soup.find_allb The preceding code generates the following output 10  Write the following command to extract the tables in the document table soup.findtable table Collecting Data by Scraping Web Pages 207 The preceding code generates the following output Figure 4 5 Contents of the table tag 11  You can also view the contents of table by looping through it Insert a new cell and add the following code to implement this for row in table.find_alltr columns row.find_alltd printcolumns The preceding code generates the following output tdRamlaltd, tdB Musictd, tdN A td, tdDiploma in Musictd 208 Collecting Text Data with Web Scraping and A PIs 12  You can also locate specific content in the table To locate the value at the intersection of the third row and the second column, write the following command table.find_alltr.find_alltd The preceding code generates the following output tdM Techtd We have learned how to extract tag based information from an H TM L file Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will focus on fetching content from web pages Requesting Content from Web Pages Whenever you visit a web page from your web browser, you are actually sending a request to fetch its content This can be done using Python scripts The Python requests package is widely used to handle all forms of H TT P requests Lets walk through an exercise to get a better understanding of this concept To fetch content, you can use the get method, which, as the name suggests, sends a G ET request to the web page from which you want to fetch data Lets perform a simple exercise now to get a better idea of how we can implement this in Python Exercise 4 02 Collecting Online Text Data In this exercise, we will be fetching the web content with the help of requests  We will be pulling a text file from Project Gutenberg , the free e-book website, specifically, from the text file for Charles Dickens famous book, David Copperfield  Follow these steps to complete this exerciseCollecting Data by Scraping Web Pages 209 1  Use the requests library to request the content of a book available online with the following set of commands import requests Lets read the text version of david copper field available online r requests.get 0 .txt r.status_code The preceding code generates the following output 200 When the browser visits the website, it fetches the content of the specified U RL Similarly, using requests , we get the content from the specified U RL and all the information gets stored in the r object 200 indicates that we received the right response from the U RL 2  Locate the text content of the fetched file by using the requests object r and referring to the text attribute Write the following code for this r.text The preceding code generates the following output Figure 4 6 Text contents of the file 210 Collecting Text Data with Web Scraping and A PIs 3  Now, write the fetched content into a text file To do this, add the following code from pathlib import Path openPath dataDavid_ Copperfield.txt,w, encodingutf- 8 .writer.text The preceding code generates the following output 2033139 4  Similarly, we can do the same using Urllib3 First add the following code import urllib3 http urllib3 Pool Manager rr http.requestG ET, rr.status Again, we will get the output as 200 , similar to the previous method 5  Add the following code to locate the text content rr.data You will see that you get the same output as shown in Figure 4 6 6  Again, add the following code to write the fetched content into a text file openPath dataDavid_ Copperfield_new.txt, wb.writerr.data The preceding code will generate the following output 2033139 We have just learned how to collect data from online sources with the help of the requests library Note To access the source code for this specific section, please refer to  You can also run this example online at  Now, lets look at analyzing H TM L content from Jupyter Notebooks Collecting Data by Scraping Web Pages 211 Exercise 4 03 Analyzing the Content of Jupyter Notebooks in H TM L Format In this exercise, we will analyze the content of a Jupyter Notebook We will count the number of images, list the packages that have been imported, and check the models and their performance Note The H TM L file used for this exercise, can be accessed at  Follow these steps to complete this exercise 1  Import Beautiful Soup and pass the location of the given H TM L file using the following commands from bs4 import Beautiful Soup soup Beautiful Soupopen datatext_classifier.html, html.parser soup.text Here, we are loading H TM L using Beautiful Soup and printing parsed content The preceding code generates the following output nnn Ch3_ Activity7_ Developing_end_to_end_ Text_ Classifiersnnnn nn Twitter Bootstrapnnnn 2  Use the img tag to count the number of images lensoup.find_allimg The output shows that there are three img tags 3 3  If you open the H TM L file in the text editor or your web browsers console, you will see all import statements have the class attribute set to nn So, to list all the packages that are imported, add the following code, referring to finding the span element with an nn class attribute i.get_text for i in soup.find_all span,attrsclassnn212 Collecting Text Data with Web Scraping and A PIs The preceding code generates the following output Figure 4 7 List of libraries imported 4  To extract the models and their performance, look at the H TM L document and see which class attribute the models and their performance belong to You will see the h2 and div tags with the class attribute output_subarea output_stream output_stdout output_text  Add the following code to extract the models for md,i in zipsoup.find_allh2, soup.find_alldiv, attrsclassoutput_subarea output_stream output_stdout output_text printModel ,md.get_text printi.get_text print--------------------------------------------------------- nnn Collecting Data by Scraping Web Pages 213 The preceding code generates the following output Figure 4 8 Models and their performance So, in the preceding output, we have extracted a classification report from the H TM L file using Beautiful Soup by referring to the h2 and div tags Note To access the source code for this specific section, please refer to  You can also run this example online at 214 Collecting Text Data with Web Scraping and A PIs So far, we have seen how to get content from the web using the requests package, and in this exercise, we saw how to parse and extract the desired information Next time you come across an article and want to extract certain information from it, you will be able to put these skills to use, instead of manually going over all of the content Activity 4 01 Extracting Information from an Online H TM L Page In this activity, we will extract data about Rabindranath Tagore from the Wikipedia page about him Note Rabindranath Tagore was a poet and musician from South Asia whose art has had a profound influence on shaping the cultural landscape of the region He was also the first Indian to win the Nobel Prize for Literature, in 1913  After extracting the data, we will analyze information from the page This should include the list of headings in the Works section, the list of his works, and the list of universities named after him Follow these steps to implement this activity 1  Open a Jupyter Notebook 2  Import the requests and Beautiful Soup libraries 3  Fetch the Wikipedia page from using the get method of the requests library 4  Convert the fetched content into H TM L format using an H TM L parser 5  Print the list of headings in the Works section 6  Print the list of original works written by Tagore in Bengali Collecting Data by Scraping Web Pages 215 7  Print the list of universities named after Tagore Note The solution for this activity can be found via this link  We are now well versed in extracting generic data from H TM L pages Lets perform another activity now, where well be using regular expressions Activity 4 02 Extracting and Analyzing Data Using Regular Expressions To perform this activity, you will extract data from Packts website The data to be extracted includes frequently asked questions F AQs and their answers, phone numbers for customer care services, and the email addresses for customer care services Follow these steps to complete this activity 1  Import the necessary libraries and extract data from using the requests library 2  Fetch questions and answers from the data 3  Create a Data Frame consisting of questions and answers 4  Fetch email addresses with the help of regular expressions 5  Fetch the phone numbers, with the help of regular expressions Note The solution for this activity can be found via this link  In this activity, we were able to fetch data from online sources and analyze it in various ways Now that we are well versed in scraping web pages with the help of H TM L, in the next section, we will discuss how to scrape web pages with semi structured data 216 Collecting Text Data with Web Scraping and A PIs Dealing with Semi Structured Data We learned about various types of data in Chapter 2 , Feature Extraction Methods  Lets quickly recapitulate what semi structured data refers to A dataset is said to be semi structured if it is not in a row column format but, if required, can be converted into a structured format that has a definite number of rows and columns Often, we come across data that is stored as key value pairs or embedded between tags, as is the case with J SO N Java Script Object Notation and X ML Extensible Markup Language files These are the most popularly used instances of semi structured data J SO N J SO N files are used for storing and exchanging data J SO N is human readable and easy to interpret Just like text files and C SV files, J SO N files are language independent This means that different programming languages, such as Python, Java, and so on, can work with J SO N files effectively In Python, a built in data structure called a dictionary is capable of storing J SO N objects as is Generally, data in J SO N objects is present in the form of key value pairs The datatype of values of J SO N objects must be any of the following A string A number Another J SO N object An array A boolean Null Dealing with Semi Structured Data 217 No SQ L databases such as Mongo DB store data in the form of J SO N objects Most A PIs return J SO N objects The following figure depicts what a J SO N file looks like Figure 4 9 A sample J SO N file Often, the response we get when requesting a U RL is in the form of J SO N objects To deal with a J SO N file effectively, we need to know how to parse it The following exercise throws light on this 218 Collecting Text Data with Web Scraping and A PIs Exercise 4 04 Working with J SO N Files In this exercise, we will extract details such as the names of students, their qualifications, and additional qualifications from a J SO N file Note The sample J SO N file can be accessed at  Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and import json  Pass the location of the file mentioned using the following commands import json from pprint import pprint data json.loadopen datasample_json.json pprintdata In the preceding code, we are importing Pythons built in json module and loading the local J SO N file using the standard IO operation of Python This turns J SO N into the Python dict object The preceding code generates the following output Figure 4 10 Dictionary form of the fetched data Dealing with Semi Structured Data 219 3  To extract the names of the students, add the following code for dt in data The preceding code generates the following output 4  To extract their respective qualifications, enter the following code for dt in data The preceding code generates the following output 5  To extract their additional qualifications, enter the following code Remember, not every student will have additional qualifications Thus, we need to check this separately Add the following code to implement this if additional qualification in dt.keys else None for dt in data The preceding code generates the following output As J SO N objects are similar to the dictionary data structure of Python, they are widely used on the web to send and receive data across web applications Note To access the source code for this specific section, please refer to  You can also run this example online at  Now that we have learned how to load J SO N data, lets extract data using another format, called Extensible Markup Language X ML , which is also used by web apps and Word documents to store information 220 Collecting Text Data with Web Scraping and A PIs X ML Just like H TM L, X ML is another kind of markup language that stores data in between tags It is human readable and extensible that is, we have the liberty to define our own tags Attributes, elements, and tags in the case of X ML are similar to those of H TM L An X ML file may or may not have a declaration But, if it has a declaration, then that must be the first line of the X ML file This declaration statement has three parts Version , Encoding , and Standalone  Version states which version of the X ML standard is being used Encoding states the type of character encoding being used in this file Standalone tells the parser whether external information is needed for interpreting the content of the X ML file The following figure depicts what an X ML file looks like Figure 4 11 A sample X ML file Dealing with Semi Structured Data 221 An X ML file can be represented as a tree called an X ML tree This X ML tree begins with the root element the parent This root element further branches into child elements Each element of the X ML file is a node in the X ML tree Those elements that do not have any children are leaf nodes The following figure clearly differentiates between an original X ML file and a tree representation of an X ML file Figure 4 12 Comparison of an X ML structure X ML files are somewhat similar in structure to H TM L, with the main difference being that, in X ML, we have custom tags rather than the fixed tags vocabulary like H TM L As we learned how to parse H TM L using Beautiful Soup before, lets learn how to parse X ML files in the following exercise 222 Collecting Text Data with Web Scraping and A PIs Exercise 4 05 Working with an X ML File In this exercise, we will parse an X ML file and print the details from it, such as the names of employees, the organizations they work for, and the total salaries of all employees Note The sample X ML data file can be accessed here  Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell, import xml.etree Element Tree , and pass the location of the X ML file using the following code import xml.etree Element Tree as E T tree E T.parse datasample_xml_data.xml root tree.getroot root The preceding code generates the following output Element records at 0 112291710 3  To check the tag of the fetched element, type the following code root.tag The preceding code generates the following output records 4  Look for the name and company tags in the X ML and print the data enclosed within them for record in root.findallrecord printrecord.findname.text, ---, record.findcompany.textDealing with Semi Structured Data 223 The preceding code generates the following output Figure 4 13 Data of the name and company tags printed 5  To find the sum of the salaries, create a list consisting of the salaries of all employees by iterating over each record and finding the salary tag in it Next, remove the and , from the string of salary content, and finally, type cast into the integer to get the sum at the end Add the following code to do so sumintrecord.findsalary.text.replace, replace,, for record in root.findallrecord The preceding code generates the following output 745609 224 Collecting Text Data with Web Scraping and A PIs Thus, we can see that the sum of all the salaries is 745 , 609  We just learned how to extract data from a local X ML file When we request data, many U RLs return an X ML file Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will look at how A PIs can be used to retrieve real time data Using A PIs to Retrieve Real Time Data A PI stands for Application Programming Interface  To understand what an A PI is, lets consider a real life example Suppose you have a socket plug in the wall, and you need to charge your cellphone using it How will you do it You will have to use a chargeradapter, which will enable you to connect the cellphone to the socket Here, this adapter is acting as a mediator that connects the cellphone and the socket, thus enabling the smooth transfer of electricity between them Similarly, some websites do not provide their data directly Instead, they provide A PIs, which we can use to extract data from the websites Just like the cellphone charger, an A PI acts as a mediator, enabling the smooth transfer of data between those websites and us Lets perform a simple exercise to get hands on experience of collecting data using A PIs Exercise 4 06 Collecting Data Using A PIs In this exercise, we will use the Currency Exchange Rates A PI to convert U SD to another currency rate Follow these steps to implement this exercise 1  Open a Jupyter Notebook 2  Import the necessary packages Add the following code to do so import json import pprint import requests import pandas as pd Dealing with Semi Structured Data 225 3  Load the json data Add the following code to do this r requests.get api.com v4latestU SD data r.json pprint.pprintdata Note Watch out for the slashes in the string below Remember that the backslashes are used to split the code across multiple lines, while the forward slashes are part of the U RL The preceding code generates the following output Figure 4 14 Fetched data in the Python dict format 4  To create the Data Frame of the fetched data and print it, add the following code df pd Data Framedata df.head 226 Collecting Text Data with Web Scraping and A PIs The preceding code generates the following output Figure 4 15 Data Frame showing details of currency exchange rates Note that you will get a different output depending on the present currency exchange rates We just learned how to collect data using A PIs Note To access the source code for this specific section, please refer to  You can also run this example online at  In the next section, we will see how to create an A PI Extracting data from Twitter Using the O Auth A PI Many popular websites, such as Twitter, provide an A PI that allows access to parts of their services so that people can build software that integrates with the website Well be focusing mainly on Twitter in this section Twitters data and services such as tweets, advertisements, direct messages, and much more can be accessed via the Twitter A PI The Twitter A PI requires authentication and authorization to interact with its services using the O Auth method Authentication is required to prove identity, while authorization proves the right to access its services and data To access Twitter data and services using an A PI, you would need to register using a Twitter developer account Dealing with Semi Structured Data 227 You can collect data from Twitter using their Python module, named Tweepy  Tweepy is a Python library for accessing the Twitter A PI It is great for simple automation and creating Twitter bots It provides abstraction to communicate with Twitter and use its A PI to ease interactions, which makes this approach more efficient than using the requests library and Twitter A PI endpoints To use the Tweepy library, simply go to and fill in the form you will need to complete the necessary fields, such as App Name , Website U RL , Callback U RL , and App Usage  Once you would have done this, submit and receive the keys and tokens, which you can use for extracting tweets and more However, before you do any of this, you will first need to import the tweepy library Your Python code should look like this import tweepy consumer_key your consumer key here consumer_secret your consumer secret key here access_token your access token here access_token_secret your access token secret here auth tweepy O Auth Handlerconsumer_key, consumer_secret auth.set_access_tokenaccess_token, access_token_secret api tweepy A PIauth The preceding code uses auth instantiation from O Auth Handler , which takes in our consumer token and secret keys that were obtained during app registration O Auth Handler handles interaction with Twitters O Auth system To search for a query named randomquery using tweepy , you can use the Cursor object as follows tweepy Cursorapi.search, qrandomquery, langen Cursor handles all the iterating over pages work for us behind the scenes, whereas the api.search method provides tweets that match a specified query given with the q parameter Lets do an activity now, to put our knowledge into practice 228 Collecting Text Data with Web Scraping and A PIs Activity 4 03 Extracting Data from Twitter In this activity, you will extract 100 tweets containing the hashtag climatechange from Twitter, using the Twitter A PI via the tweepy library, and load them into a pandas Data Frame The following steps will help you implement this activity 1  Log in to your Twitter account with your credentials 2  Visit and fill in the form by completing the necessary fields, such as App Name , providing Website U RL , Callback U RL , and App Usage 3  Submit the form and receive the keys and tokens 4  Use these keys and tokens in your application when making an A PI call for climatechange 5  Import the necessary libraries 6  Fetch the data using the keys and tokens 7  Create a Data Frame consisting of tweets Note The solution for this activity can be found via this link  In this activity, we extracted data from Twitter and loaded it into a pandas Data Frame This data can also be used to analyze tweets and create a word cloud out of them, something that we will explore in detail in Chapter 8 , Sentiment Analysis  Publishers Note The preceding messages were extracted without bias from a given dataset and written by private individuals not affiliated with this company  The views expressed in these tweets do not necessarily reflect our companys official policies Summary 229 Summary In this chapter, we have learned various ways to collect data by scraping web pages We also successfully scraped data from semi structured formats such as J SO N and X ML and explored different methods of retrieving data in real time from a website without authentication In the next chapter, you will learn about topic modelingan unsupervised natural language processing technique that helps group documents according to the topics that it detects in them Overview This chapter introduces topic modeling, which means using unsupervised machine learning to find topics within a given set of documents You will explore the most common approaches to topic modeling, which are Latent Semantic Analysis L SA, Latent Dirichlet Allocation L DA, and the Hierachical Dirichlet Process H DP , and learn the differences between them You will then practice implementing these approaches in Python and review the common practical challenges in topic modeling By the end of this chapter, you will be able to create topic models from any given dataset Topic Modeling5232 Topic Modeling Introduction In the previous chapter, we learned about different ways to collect data from local files and online resources In this chapter, we will focus on topic modeling , which is an important area within natural language processing Topic modeling is a simple way to capture the sense of what a document or a collection of documents is about Note that in this case, documents are any coherent collection of words, which could be as short as a tweet or as long as an encyclopedia Topic modeling may be thought of as a way to automate the manual task of reading given documents to write an abstract, which you will then use to map the documents to a set of topics Topic modeling is mostly done using unsupervised learning algorithms that detect topics on their own Topic modeling algorithms operate by performing statistical analysis on words or tokens in documents and using those statistics to automatically assign each document to multiple topics A topic is represented by an arbitrary number and its keywords When the topics are not interpretable, then topic modeling may be thought of as an automated process of a manual task in which the semantic structure or meaning of the documents was neither understood nor abstracted before mapping the documents to a set of topics Topic modeling generally uses unsupervised learning algorithms, as opposed to supervised learning algorithms This means that, during training, we do not have to provide labels that is, topic names corresponding to each document in order to teach the model This not only helps us discover interesting topics that might exist, but also reduces the manual effort spent in labeling texts On the flip side, it can be a lot more challenging to evaluate the output of a topic model Topic modeling is often used as a first step to explore textual data in order to get a feel for the content of the text This is especially true when abstractssummaries are unavailable, and when the text is too large to be manually analyzed in the available timeframe Topic Discovery The main goal of topic modeling is to find a set of topics that can be used to classify a set of documents These topics are implicit because we do not know what they are beforehand, and they are unnamed Topic Discovery 233 The number of topics could vary from around 3 to, say, 400 or even more topics Since it is the algorithm that discovers the topics, the number is generally fixed as an input to the algorithm, except in the case of non parametric models in which the number of topics is inferred from the text These topics may not always directly correspond to topics that a human would find meaningful In practice, the number of topics should be much smaller than the number of documents In general, the number of topics specified in a parametric model ought to be greater than or equal to the expected number of topics in the text In other words, one should err on the side of a greater number of topics rather than fewer topics This is because fewer topics can cause a problem for the interpretability of topics Also, the more documents that we provide, the better the algorithm can map the documents to non mutually exclusive topics The number of topics chosen depends on the documents and the objectives of the project You may want to increase the number of topics if you have a large number of documents or if the documents are fairly diverse Conversely, if you are analyzing a narrow set of documents, you may want to decrease the number of topics This generally flows from your assumptions about the documents If you think that the document set might inherently contain a large number of topics, you should configure the algorithm to look for a similar number of topics Exploratory Data Analysis It is recommended to do exploratory data analysis prior to performing any machine learning project This helps you learn about the probability distribution of the items in the dataset We have seen this with word clouds in Chapter 2 , Feature Extraction Methods  Even better exploration is possible with topic modeling Doing this can give you a sense of the statistical properties of the text dataset and how the documents can be grouped For example, you might want to know whether the text dataset is skewed to any particular set of topics, or whether the sources are uniform or disparate This data further allows us to choose the appropriate algorithms for the actual project Transforming Unstructured Data to Structured Data Topic modeling clusters documents based on their topics Specifically, it is a soft clustering method, as each document gets mapped to multiple topics This is unlike hard clustering, which results in membership of an exemplar or a point of only one cluster Topic models typically give a weightprobability of the document being associated with a topic 234 Topic Modeling Thus, you can have a matrix of documents by topic, wherein the intersection of a document and a topic refers to the weightprobability that the document is associated with the topic This matrix is effectively a numeric representation of the text and can be considered a way to transform unstructured text into structured data Such a transformation is also an example of dimensionality reduction, as unstructured text can have many more dimensions each dimension corresponds to a unique word than the number of dimensions in structured data each dimension corresponds to a topic Bag of Words Before we explore modeling algorithms in depth, lets make a few simplifying assumptions Firstly, we treat a document as a bag of words , meaning we ignore the structure and grammar of the document and just use the count of each word in the document to infer patterns in the variation of word counts Ignoring the structure, sequences, and grammar allows us to use algorithms that rely on counts and probability to make the inferences As we have seen previously, a bag of words is a dictionary containing each unique word and the integer count of the occurrences of the word in the document Like all models, it is, at best, an approximation of reality All the topic modeling algorithms that we will discuss consider the text as a bag of words Note We will look at approaches that explicitly model sequences in later chapters The sequential structure of languages is different from the sequential structure in time series data Moreover, some aspects of the sequential structure may be specific to the natural language being considered This will be discussed in more detail in Chapter 6 , Vector Representation  Topic Modeling Algorithms 235 Topic Modeling Algorithms Topic modeling algorithms operate on the following assumptions Topics contain a set of words Documents are made up of a set of topics Topics can be considered to be a weighted collection of words After these common assumptions, different algorithms diverge in how they go about discovering topics In the upcoming sections, we will cover in detail three topic modeling algorithms namely L SA, L DA, and H DP Here, the term latent the L in these acronyms refers to the fact that the probability distribution of the topics is not directly observable We can observe the documents and the words but not the topics Note The L DA algorithm builds on the L SA algorithm In this case, similar acronyms are indicative of this association Latent Semantic Analysis L SA We will start by looking at L SA L SA actually predates the World Wide Web  It was first described in 1988  L SA is also known by an alternative name, Latent Semantic Indexing L SI, particularly when it is used for semantic searches of document indices The goal of L SA is to uncover the latent topics that underlie documents and words 236 Topic Modeling L SA How It Works Consider that we have a collection of documents, and these documents are made up of words Our goal is to discover the latent topics in the documents So, in the beginning, we have a collection of documents that we can represent as a term to document matrix This term to document matrix has terms as rows and documents as columns The following table gives a simplified illustration of a term to document matrix Figure 5 1 A simplified view of a term to document matrix Now, we break this matrix down into three separate matrix factors, namely a term to topics matrix, a topic importance matrix, and a topic to documents matrix Lets consider the matrix shown on the left hand side and the corresponding factor matrices on the right hand side Figure 5 2 Document matrix and its broken matrices As we can see in this diagram, the rectangular matrix is separated into the product of other matrices The process takes a matrix, M, and splits it, as shown in the following formula Figure 5 3 Splitting the matrix M Key Input Parameters for L SA Topic Modeling 237 The following are the broad definitions of the preceding equation M is an mm matrix U is an mn matrix is an nn diagonal matrix with non negative real numbers V is an mn matrix V T is an nm matrix, which is the transpose of V The matrices U and V T are not unique as matrix factorization does not give unique factors This is analogous to the fact that the number 108 can be factorized using three factors in more than one way 9x1x12, 27x1x4, 3x1x36, and so on In order to consistently get similar factors, a regularization parameter can be used Moreover, the multiplication of the factor matrices gives a matrix approximately and not exactly equal to the original matrix Collectively there are fewer elements in the factor matrices than in the original matrix and this is possible because the original matrix had many elements that were zero or close to zero The gensim library is a popular Python library for topic modeling It is easy to use and provides various topic modeling model classes, including Lda Model for L DA and Lsi Model for L SI The tomotopy library is also a powerful Python library for topic modeling It too is easy to use and includes popular topic modeling model classes, including H DP Model for H DP and L DA Model for L DA Other Python topic modeling libraries include scikit learn and lda for L DA Key Input Parameters for L SA Topic Modeling We will be using the gensim library to perform L SA topic modeling The key input parameters for gensim are corpus , the number of topics, and id2word  Here, the corpus is specified in the form of a list of documents in which each document is a list of tokens The id2word parameter refers to a dictionary that is used to convert the corpus from a textual representation to a numeric representation such that each word corresponds to a unique number Lets do an exercise to understand this concept better 238 Topic Modeling spa Cy is a popular natural language processing Library for Python In our exercises, we will be using spa Cy to tokenize the text, lemmatize the tokens, and check which part of speech that token is We will be using spa Cy v2 1 3  After installing spa Cy v2 1 3 we will need to download the English language model using the following code, so that we can load this model since there are models for many different languages python mspacy download en_core_web_sm Exercise 5 01 Analyzing Wikipedia World Cup Articles with Latent Semantic Analysis In this exercise, you will perform topic modeling using L SA on a Wikipedia World Cup dataset For this, you will make use of the Lsi Model class provided by the gensim library You will use the Wikipedia library to fetch articles, the spa Cy engine for the tokenization of the text, and the newline character to separate documents within an article Note The dataset used for this exercise can be found at  Follow these steps to complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import numpy as np import matplotlib.pyplot as plt %matplotlib inline import pandas as pd from gensim import corpora from gensim.models import Lsi Model from gensim.parsing.preprocessing import preprocess_string Key Input Parameters for L SA Topic Modeling 239 3  To clean the text, define a function to remove the non alphanumeric characters and replace numbers with the character Replace instances of multiple newline characters with a single newline character Use the newline character to separate out the documents in the corpus Insert a new cell and add the following code to implement this import re H AN DL E w L IN K httpst.cow S PE CI AL_ CH AR S ltltamp P AR An def cleantext text re.subL IN K, , text text re.subS PE CI AL_ CH AR S, , text text re.subP AR A, n, text return text 4  Insert a new cell and add the following code to find Wikipedia articles related to the World Cup import wikipedia wikipedia.searchCricket World Cup, wikipedia.searchF IF A World Cup240 Topic Modeling The code generates the following output Figure 5 4 Wikipedia articles related to the World Cup 5  Insert a new cell and add the following code fetch the Wikipedia articles about the 2018 F IF A World Cup and the 2019 Cricket World Cup, concatenate them, and show the result latest_soccer_cricket2018 F IF A World Cup, 2019 Cricket World Cup corpus for cup in latest_soccer_cricket corpuscorpuswikipedia.pagecup.content corpus Key Input Parameters for L SA Topic Modeling 241 The code generates the following output Figure 5 5 Result after concatenating articles from 2018 and 2019 6  Insert a new cell and add the following code to clean the text, using the spa Cy English language model to tokenize the corpus and exclude all tokens that are not detected as nouns textcleancorpus import spacy nlp spacy.loaden_core_web_sm docnlptext pos_list preproc_text preproc_sent for token in doc if token.textn if nottoken.is_stop and nottoken.is_punct and token.pos_ in pos_list preproc_sent.appendtoken.lemma_ else preproc_text.appendpreproc_sent preproc_sent last sentence preproc_text.appendpreproc_sent printpreproc_text 242 Topic Modeling The code generates the following output Figure 5 6 Output after tokenizing the corpus 7  Insert a new cell and add the following code to convert the corpus into a list in which each token corresponds to a number for more efficient representation, as gensim requires it in this form Then, find the topics in the corpus dictionary corpora Dictionarypreproc_text corpus N UM_ TO PI CS3 lsamodelLsi Modelcorpus, num_topicsN UM_ TO PI CS, id2word dictionary lsamodel.print_topics The code generates the following output Figure 5 7 Topics in the corpus To create our Lsi Model , we had to decide up front how many topics we wanted This would not necessarily match the number of topics that are actually in the corpus Note that, in the output, you can see that negative weights are associated with some words in a few topics Also, the sum of the weights does not add up to one The weights are not to be interpreted as probabilities This makes it difficult to even mechanically view the topic as a probability distribution over words Additionally, it may be observed that topic 0 is essentially about cricket even though the corpus includes both soccer and cricket Topic 1 seems to be related to a sports broadcast Topic 2 does not seem to be interpretable Key Input Parameters for L SA Topic Modeling 243 8  To determine which topics have the highest weight for a document, insert a new cell and add the following code model_arr np.argmaxlsamodel.get_topics,axis0 y, x np.histogrammodel_arr, binsnp.arangeN UM_ TO PI CS1 fig, ax plt.subplots plt.xticksticksnp.arangeN UM_ TO PI CS, labelsnp.arangeN UM_ TO PI CS1 ax.plotx, y fig.show The code generates the following output Figure 5 8 Graph representing weight of topics for the documents 244 Topic Modeling We can see that topic 1 and topic 0 have the highest weight in almost all the documents Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help in reproducibility, in general, the topics found using tomotopy are superior from the perspective of interpretability Generally, your output is expected to be different In order to have exactly the same topic model, we can save and load topic models, which well do in Exercise 5 04 , Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe  To access the source code for this specific section, please refer to  You can also run this example online at  We have now performed topic modeling with the help of L SA In the next section, we will learn about another topic modeling algorithm L DA Before we move onto its implementation, lets quickly try and build a basic intuition about a couple of concepts that will help us in the subsequent sections Dirichlet Process and Dirichlet Distribution A Dirichlet process is a distribution over a distribution It can be represented as D P,G where G is the base distribution and is the concentration parameter that defines how close D P,G is to the base distribution G It is for this reason that the Dirichlet process is a versatile way to represent various probability distributions It is used for the H DP topic modeling algorithm The Dirichlet distribution is a special case of the Dirichlet process, in which the number of topics needs to be specified explicitly It is used for the L DA topic modeling algorithm Key Input Parameters for L SA Topic Modeling 245 Latent Dirichlet Allocation L DA Instead of using matrix factorization, like we did for L SA, it is possible to consider a generative model called L DA L DA is considered an advancement over probabilistic L SA Probabilistic L SA is prone to overfitting as it does not probabilistically model the distribution of the documents L DA is a three level hierarchical generative statistical model that maps documents to topics, which in turn get mapped to wordsall in a probabilistic way In this case, we have two concentration parameters corresponding to the document level and the topic level L DA How It Works To understand how L DA works, lets look at a simple example We have four documents that contain only three unique words Cat, Dog, and Hippo  The following figure shows the documents and the number of times each word is found in each document Figure 5 9 Occurrence of words in different documents As we can see in the figure, the word Cat is found 10 times in Document 1 and Document 4 and 0 times in documents 2 and 3  Document 4 contains all three words 10 times each For its analysis, L DA maintains two probability tables The first table tracks the probability of selecting a specific word when sampling a specific topic The second table keeps track of the probability of selecting a specific topic when sampling a particular document Figure 5 10 Probability tables 246 Topic Modeling These probability tables reflect how likely it is to get a word if you sampled from each topic If you sampled a word from topic 3 , it would likely be Cat probability 99% If you sampled Document 4 , then there is a one third chance of getting each of the topics, since it contains all three t in equal proportions In this example, a word is exclusive to a topic In general, though, this is not the case The gensim and the scikit learn libraries use one way of implementing L DA called variational inference The tomotopy and lda libraries use another way called collapsed Gibbs sampling It is essentially because of these differing implementations when tomotopy is able to generate the topics in the available time, we usually prefer using tomotopy otherwise we use gensim The parameters that we use for tomotopy are as follows corpus This refers to text that we want to analyze Number of topics This is the number of topics that the corpus contains iter This refers to the number of iterations that the model considers the corpus This is associated with document generation This is associated with topic generation seed This helps with fixing the initial randomization Measuring the Predictive Power of a Generative Topic Model The predictive power of a generative topic model can be measured by analyzing the distribution of the generated corpus Perplexity is a measure of how close the distribution of the words in the generated corpus is to reality Log perplexity is a more convenient measure for this closeness The formula for log perplexity is as follows Figure 5 11 Formula for log perplexity Here, n is number of words and Pw is the probability associated with word w We can see that negative log likelihood is identical to log perplexity Key Input Parameters for L SA Topic Modeling 247 Usually, a lower log perplexity means better performance This is because the probability distribution of words is not uniform It is concentrated on a small subset of words And such a concentration a non uniform probability density function causes a lower negative likelihood In order to be sure that the model is generalizing well, the log likelihood should be computed on a hold out sample An extremely low negative log likelihood is indicative of an extremely low capacity of the model to learn If a topic model has an unacceptable log perplexity on the corpus used for training then it is unlikely to perform well on a hold out sample as it is indicative of the model having a low capacity to learn or it is indicative of the dataset not being generalizable The negative log likelihood is approximately estimated in topic modeling libraries as it is intractable to calculate Exercise 5 02 Finding Topics in Canadian Open Data Inventory Using the L DA Model In this exercise, we will use the tomotopy L DA model to analyze the Canadian Open Data Inventory For simplicity, we will consider that the corpus has twenty topics Note The dataset used for this exercise can be found at  The following steps will help you complete this exercise 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd pd.set_optiondisplay.max_colwidth, 800 import numpy as np import matplotlib.pyplot as plt %matplotlib inline248 Topic Modeling 3  Insert a new cell and add the following code to read from a download of the Canadian Open Data Inventory, and clean the text O PE N_ DA TA_ UR L datacanada open datainventory.csv import re H AN DL E w L IN K httpst.cow S PE CI AL_ CH AR S ltltamp P AR An def cleantext text re.subL IN K, , text text re.subS PE CI AL_ CH AR S, , text text re.subP AR A, n, text return text catalog.samplefrac0 25 ,replaceFalse, random_state0.to_c svO PE N_ DA TA_ UR L, encodingutf- 8 file datacanada open datacatalog.txt fopenfile,r,encodingutf- 8 textf.read f.close text cleantext 4  Insert a new cell and add the following code to clean the text, using the spa Cy English language model to tokenize the corpus and to exclude all tokens that are not detected as nouns import spacy nlp spacy.loaden_core_web_sm docnlptext pos_list preproc_text preproc_sent for token in doc if token.textn if nottoken.is_stop and nottoken.is_punct Key Input Parameters for L SA Topic Modeling 249 and token.pos_ in pos_list preproc_sent.appendtoken.lemma_ else preproc_text.appendpreproc_sent preproc_sent last sentence preproc_text.appendpreproc_sent printpreproc_text The code generates the following output Figure 5 12 Tokenized corpus after text preprocessing The pandas Data Frame was sampled 25% of the dataset has been considered so that the memory restrictions related to spa Cy can be addressed, since this is a fairly large sample 5  Insert a new cell and add the following code to see how the negative log likelihood varies by the number of iterations import tomotopy as tp N UM_ TO PI CS20 mdl tp L DA ModelkN UM_ TO PI CS,seed1234 for line in preproc_text mdl.add_docline for i in range0, 110 , 10 mdl.traini printIteration t Log likelihood  formati, mdl.ll_per_word 250 Topic Modeling The code generates the following output Figure 5 13 Variation of negative log likelihood with different iterations 6  Insert a new cell and add the following code to train a topic model with ten iterations and to show the inferred topics mdl.train10 for k in rangemdl.k printTop 10 words of topic .formatk printmdl.get_topic_wordsk, top_n7 The code generates the following output Top 10 words of topic 0 polygon, 0 36050185561180115 , dataset, 0 0334757782722234726 , information, 0 03004324994981289 , soil, 0 , 029185116291046143 , area, 0 , 026610717177391052 , surface, 0 025752583518624306 , map, 0 024036318063735962 7  Insert a new cell and add the following code to see the probability distribution of topics if you consider the entire dataset as a single document bag_of_words doc_inst mdl.make_docbag_of_words mdl.inferdoc_inst np.argsortnp.arraymdl.inferdoc_inst Key Input Parameters for L SA Topic Modeling 251 The code generates the following output array11, 17 , 14 , 19 , 12 , 7 , 4 , 13 , 10 , 2 , 3 , 15 , 1 , 18 , 16 , 9 , 0 , 6 , 8 , 5 , dtypeint64 8  Insert a new cell and add the following code to see the probability distribution of topic 11 printmdl.get_topic_words11, top_n7 The code generates the following output table, 0 24849626421928406 , census, 0 1265643984079361 , level, 0 06526772677898407 , series, 0 06306280940771103 , topic, 0 062401335686445236 , geography, 0 062401335686445236 , country, 0 06218084320425987 9  Insert a new cell and add the following code to see the probability distribution of topic 17 printmdl.get_topic_words17, top_n7 The code generates the following output datum, 0 0603327676653862 , information, 0 057247743010520935 , year, 0 03462424501776695 , dataset, 0 03291034325957298 , project, 0 017828006289734993 , website, 0 014057422056794167 , activity, 0 012000739574432373 10  Insert a new cell and add the following code to see the probability distribution of topic 5 printmdl.get_topic_words5, top_n7 The code generates the following output survey, 0 04966237023472786 , catch, 0 03862873837351799 , sponge, 0 0364220105111599 , sea, 0 0342152863740921 , datum, 0 028698472306132317 , fishing, 0 02759511023759842 , matter, 0 026491746306419373252 Topic Modeling Topic 11 , topic 17 , and topic 5 seem to be interpretable One could say that topic 11 , topic 17 , and topic 5 seem to be broadly about geographical data, internet data, and marine life data respectively Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help in reproducibility, in general, the topics found using tomotopy are superior from the perspective of interpretability Generally, your output is expected to be different In order to have exactly the same topic model, we can save and load topic models we do this in Exercise 5 04 , Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe  To access the source code for this specific section, please refer to  This section does not currently have an online interactive example and will need to be run locally Activity 5 01 Topic Modeling Jeopardy Questions Jeopardy is a popular T V show that covers a variety of topics In this show, participants are given answers and then asked to frame questions The purpose of this activity is to give a real world feel to some of the complexity associated with topic modeling In this activity, you will do topic modeling on a dataset of Jeopardy questions Note The dataset to be used for this activity can be found at  Follow these steps to complete this activity 1  Open a Jupyter Notebook 2  Insert a new cell and import pandas and other necessary libraries 3  Load the dataset into a pandas Data Frame Hierarchical Dirichlet Process H DP 253 4  Clean the data by dropping the Data Frame rows where the Question column has empty cells 5  Find the unique number of categories based on the Category column 6  Randomly select 4% of the questions Tokenize the text using spa Cy Select tokens that are nounsverbsadjectives or a combination 7  Train a tomotopy L DA model with 1 , 000 topics 8  Print the log perplexity 9  Find the probability distribution on the entire dataset 10  Sample a few topics and check for interpretability Note The solution for this activity can be found via this link  Hierarchical Dirichlet Process H DP H DP is a non parametric variant of L DA It is called non parametric since the number of topics is inferred from the data, and this parameter is not provided by us This means that this parameter is learned and can increase that is, it is theoretically unbounded The tomotopy H DP implementation can infer between 1 and 32 , 767 topics gensims H DP implementation seems to fix the number of topics at 150 topics For our purposes, we will be using the tomotopy H DP implementation The gensim and the scikit learn libraries use variational inference, while the tomotopy library uses collapsed Gibbs sampling When the time required by collapsed Gibbs sampling is not an issue, then it is preferable to use collapsed Gibbs sampling over variational inference In other cases, we may prefer to use variational inference For the tomotopy library, the following parameters are used iter This refers to the number of iterations that the model considers the corpus This concentration parameter is associated with document generation This concentration parameter is associated with topic generation 254 Topic Modeling seed This fixes the initial randomization min_cf This helps eliminate those words that occur fewer times than the frequency specified by us To get a better understanding of this, lets perform some simple exercises Exercise 5 03 Topics in Around the World in Eighty Days In this exercise, we will make use of the tomotopy H DP model to analyze the text file for Jules Vernes Around the World in Eighty Days , available from the Gutenberg Project We will use the min_cf hyperparameter that is used to ignore words that occur fewer times than the specified frequency and discuss its impact on the interpretability of topics Note The dataset used for this exercise can be found at 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd pd.set_optiondisplay.max_colwidth, 800 import numpy as np import matplotlib.pyplot as plt %matplotlib inline 3  Insert a new cell and add the following code to read from a download of the Gutenberg Projects Around the World in Eighty Days by Jules Verne, and clean the text O PE N_ DA TA_ UR L dataaroundtheworldpg103.txt fopenO PE N_ DA TA_ UR L,r,encodingutf- 8 textf.read f.close import re Hierarchical Dirichlet Process H DP 255 H AN DL E w L IN K httpst.cow S PE CI AL_ CH AR S ltltamp P AR An def cleantext text re.subL IN K, , text text re.subS PE CI AL_ CH AR S, , text text re.subP AR A, n, text return text text cleantext text The code generates the following output Figure 5 14 Text from Around the World in Eighty Days 4  Insert a new cell and add the following code to import the necessary libraries, clean the text using the spa Cy English language model to tokenize the corpus, and exclude all tokens that are not detected as nouns import spacy nlp spacy.loaden_core_web_sm docnlptext pos_list preproc_text preproc_sent for token in doc if token.textn if nottoken.is_stop and nottoken.is_punct and token.pos_ in pos_list preproc_sent.appendtoken.lemma_ else preproc_text.appendpreproc_sent 256 Topic Modeling preproc_sent preproc_text.appendpreproc_sent last sentence printpreproc_text The code generates the following output Figure 5 15 Tokenized corpus after the text is cleaned 5  Insert a new cell and add the following code to create H DP models in which tokens that occur fewer than five times are ignored, and then show how the negative log likelihood varies according to the number of iterations import tomotopy as tp mdl tp H DP Modelmin_cf5,seed0 for line in preproc_text mdl.add_docline for i in range0, 100 , 10 mdl.traini printIteration t Log likelihood  formati, mdl.ll_per_word for k in rangemdl.k printTop 10 words of topic .formatk printmdl.get_topic_wordsk, top_n7 Hierarchical Dirichlet Process H DP 257 The code generates the following output Figure 5 16 Variation of negative log likelihood with number of iterations 6  Insert a new cell and add the following code to see the probability distribution of topics if you consider the entire dataset as a single document bag_of_words doc_inst mdl.make_docbag_of_words np.argsortnp.arraymdl.inferdoc_inst The code generates the following output Figure 5 17 Probability distribution of topics if the entire dataset is considered 258 Topic Modeling 7  Insert a new cell and add the following code to see the probability distribution of topic 33 printmdl.get_topic_words33, top_n7 The code generates the following output danger, 0 1534954458475113 , hour, 0 0015197568573057652 , time, 0 0015197568573057652 , train, 0 0015197568573057652 , master, 0 0015197568573057652 , man, 0 0015197568573057652 , steamer, 0 0015197568573057652 8  Insert a new cell and add the following code to see the probability distribution of topic 21 printmdl.get_topic_words21, top_n7 The code generates the following output hour, 0 1344495415687561 , minute, 0 1232500821352005 , day, 0 08405196666717529 , quarter, 0 07285250723361969 , moment, 0 07285250723361969 , clock, 0 005605331063270569 , card, 0 039254117757081985 9  Insert a new cell and add the following code to see the probability distribution of topic 70 printmdl.get_topic_words70, top_n7 The code generates the following output event, 0 12901155650615692 , midnight, 0 12901155650615692 , detective, 0 06482669711112976 , bed, 0 06482669711112976 , traveller, 0 06482669711112976 , watch, 0 06482669711112976 , clown, 0 06482669711112976 10  Insert a new cell and add the following code to see the probability distribution of topic 4 printmdl.get_topic_words4, top_n7 The code generates the following output house, 0 20237493515014648 , opium, 0 10131379961967468 , town, 0 07604850828647614 , brick, 0 07604850828647614 , mansion, 0 07604850828647614 , glimpse, 0 50783220678567886 , ball, 0 .050783220678567886Hierarchical Dirichlet Process H DP 259 We can see that ignoring tokens that occur fewer than five times significantly improves the interpretability of the topic model Also, we have 378 topics in all, many of which are not likely to be interpretable So, what does this mean Lets analyze a corpus from another classic and then return to these questions Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help reproducibility, the topics found using tomotopy are superior from the perspective of interpretability Your output is expected to be different In order to have exactly the same topic model, we can save and load topic models, which well do now in Exercise 5 04 , Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe  To access the source code for this specific section, please refer to  You can also run this example online at  Exercise 5 04 Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe In this exercise, we will make use of the tomotopy H DP model to analyze a text corpus taken from the text file for Daniel Defoes The Life and Adventures of Robinson Crusoe , available on the Gutenberg Project website Here, we will take the value of as 0 8 and experiment with selecting tokens based on different combinations of parts of speech, before training the model Note The dataset used for this exercise can be found at 1  Open a Jupyter Notebook 260 Topic Modeling 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd pd.set_optiondisplay.max_colwidth, 800 import numpy as np import matplotlib.pyplot as plt %matplotlib inline 3  Insert a new cell and add the following code to read from a download of the Gutenberg Projects The Life and Adventures of Robinson Crusoe by Daniel Defoe, and clean the text O PE N_ DA TA_ UR L datarobinsoncrusoe521 0 .txt fopenO PE N_ DA TA_ UR L,r,encodingutf- 8 textf.read f.close import re H AN DL E w L IN K httpst.cow S PE CI AL_ CH AR S ltltamp P AR An def cleantext text re.subL IN K, , text text re.subS PE CI AL_ CH AR S, , text text re.subP AR A, n, text return text text cleantext text Hierarchical Dirichlet Process H DP 261 The code generates the following output Figure 5 18 Text from The Life and Adventures of Robinson Crusoe 4  Insert a new cell and add the following code to import the necessary libraries Clean the text using the spa Cy English language model to tokenize the corpus and to exclude all tokens that are not detected as nouns import spacy nlp spacy.loaden_core_web_sm docnlptext We can experiment with other or a combinations of parts of speech pos_list preproc_text preproc_sent for token in doc if token.textn if nottoken.is_stop and nottoken.is_punct and token.pos_ in pos_list preproc_sent.appendtoken.lemma_ else preproc_text.appendpreproc_sent preproc_sent preproc_text.appendpreproc_sent last sentence printpreproc_text 262 Topic Modeling The code generates the following output Figure 5 19 Tokenized corpus after preprocessing is done 5  Insert a new cell and add the following code to import the necessary libraries Create an H DP model with the concentration parameter as 0 8 and see how the negative log likelihood varies with the number of iterations import tomotopy as tp mdl tp H DP Modelalpha0 8 ,seed0 for line in preproc_text mdl.add_docline for i in range0, 110 , 10 mdl.traini printIteration t Log likelihood  formati, mdl.ll_per_word for k in rangemdl.k printTop 10 words of topic .formatk printmdl.get_topic_wordsk, top_n7 Hierarchical Dirichlet Process H DP 263 The code generates the following output Figure 5 20 Variation of negative log likelihood with the number of iterations 6  Insert a new cell and add the following code to save the topic model mdl.save datarobinsoncrusoehdp_model.bin 7  Insert a new cell and add the following code to load the topic model mdl tp H DP Model.load datarobinsoncrusoe hdp_model.bin 8  Insert a new cell and add the following code to see the probability distribution of topics if you consider the entire dataset as a single document bag_of_words doc_inst mdl.make_docbag_of_words mdl.inferdoc_inst np.argsortnp.arraymdl.inferdoc_inst 264 Topic Modeling The code generates the following output Figure 5 21 Probability distribution if the entire corpus is considered 9  Insert a new cell and add the following code to see the probability distribution of topic 163 printmdl.get_topic_words163, top_n7 The code generates the following output horse, 0 13098040223121643 , way, 0 026405228301882744 , mankind, 0 26405228301882744 , fire, 0 026405228301882744 , object, 0 026405228301882744 , bridle, 0 026405228301882744 , distress, 0 026405228301882744 10  Insert a new cell and add the following code to see the probability distribution of topic 103 printmdl.get_topic_words103, top_n7 The code generates the following output manor, 0 03706422075629234 , inheritance, 0 03706422075629234 , lord, 0 03706422075629234 , man, 0 0003669724682377309 , shore, 0 0003669724682377309 , ship, 0 0003669724682377309 Hierarchical Dirichlet Process H DP 265 11  Insert a new cell and add the following code to see the probability distribution of topic 28 printmdl.get_topic_words28, top_n7 The code generates the following output thought, 0 07716038823127747 , mind, 0 045609116554260254 , word, 0 038597721606492996 , face, 0 03509202599525452 , terror, 0 03509202599525452 , tear, 0 3158633038401604 , apprehension, 0 3158633038401604 We see that we have 195 topics in all, many of which are likely not interpretable In general, finding interpretable topics is difficult and connecting the words to interpret topics often requires familiarity with the domain We have seen that log perplexity has very limited utility In the case of prior knowledge of the corpus, the topic model has a much smaller role to play in the discovery of the thematic structure Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help reproducibility, in general, the topics found using tomotopy are superior from the perspective of interpretability Generally, your output is expected to be different In order to have exactly the same topic model, we can save and load topic models, and this was used in this exercise To access the source code for this specific section, please refer to  This section does not currently have an online interactive example and will need to be run locally We have explored three of the most popular approaches to topic modeling Lets now discuss the practical challenges in using topic modeling and the state of the art topic modeling technologies 266 Topic Modeling Practical Challenges The selection of the number of topics and topic modeling algorithms, the number of iterations, and the evaluation of the topic model are the main challenges faced by a practitioner Having prior knowledge about the domain can greatly help in choosing the number of topics In the absence of prior knowledge about the expected number of topics, we may need to rely on experimentation for the choice of the topic modeling algorithm The H DP model is an attractive choice when there is not much information about the number of topics In the case of a small corpus, the L SA model could be used One factor that makes interpreting topics difficult is that they contain a lot of very frequently occurring but indistinctive words To overcome this, we can iteratively identify these words and add them to a list of stop words At times, we may want to filter out words that are too rare andor too common The use of only nouns, only verbs, or a combination of various parts of speech can improve the interpretability of topics Qualitative evaluation of the topics is essential We may have to accept a mix of interpretable and non interpretable topics in the real world In the absence of human participants, we can use qualitative ways of considering word intrusion Unless there is a downstream use of the topic model being developed, a complete lack of interpretability will render the topic model useless When we have a downstream application, even non interpretable topics are useful as they offer a convenient means to carry out dimensionality reduction on the dataset State of the Art Topic Modeling There is no known benchmark for quantitively identifying the state of the art topic modeling algorithm It necessarily involves human participation whenever interpretable topics are required In cases where the interpretation of topics is not necessary, the topic model needs to be evaluated by downstream tasks A qualitative approach to interpreting topic models may be useful if there is prior knowledge or familiarity with the corpus While there have been attempts at using labeled topic modeling, there is no evidence of these models broadly outperforming unsupervised topic modeling algorithms Interestingly, given that much of the topic modeling literature was published prior to 2014 , this is not among the most active areas of research This suggests that complete automation is hard and human participation is here to stay as the state of- the art technique in the near future Hierarchical Dirichlet Process H DP 267 Activity 5 02 Comparing Different Topic Models The Consumer Financial Protection Bureau C FP B publishes consumer complaints made against organizations in the financial sector This original dataset is available at In this activity, you will qualitatively compare how H DP and L DA models perform on the interpretability of topics by analyzing student loan complaints Note The dataset to be used for this activity can be found at  Follow these steps to complete this activity 1  Open a Jupyter Notebook 2  Import the pandas library and load the dataset from a text file produced by partially processing the dataset from the C FP B website mentioned at the beginning of this section 3  Tokenize the text using spa Cy Select tokens that may be a part of speech noun verbadjective or a combination 4  Train an H DP model 5  Save and load the H DP model To save a topic model, use the following line of code mdl.save dataconsumercomplaintshdp_model.bin To load a topic model, use the following mdl tp H DP Model.load dataconsumercomplaintshdp_model.bin 6  Determine the topics in the entire set of complaints Sample a few topics and check for interpretability 7  Repeat steps 3 8 for an L DA model instead of an H DP model Consider the number of topics in the L DA model to around the number of topics found in the H DP model 268 Topic Modeling 8  Select the qualitatively better model from the H DP and L DA models trained in this activity Also, compare these two models quantitatively Note The solution for this activity can be found via this link  In this activity, we successfully compared two different models both qualitatively and quantitatively Summary In this chapter, we discussed topic modeling in detail Without delving into advanced statistics, we reviewed various topic modeling algorithms such as L SA, L DA, and H DP and how they can be used for topic modeling on a given dataset We explored the challenges involved in topic modeling, how experimentation can help address those challenges, and, finally, broadly discussed the current state of the art approaches to topic modeling In the next chapter, we will learn about vector representation of text, which helps us convert text into a numerical format to make it more easily understandable by machines Overview This chapter introduces you to the various ways in which text can be represented in the form of vectors You will start by learning why this is important, and the different types of vector representation You will then perform one hot encoding on words, using the preprocessing package provided by scikit learn, and character level encoding, both manually and using the powerful Keras library After covering learned word embeddings and pre trained embeddings, you will use Word2 Vec and Doc2 Vec for vector representation for Natural Language Processing N LP tasks, such as finding the level of similarity between multiple texts Vector Representation6272 Vector Representation Introduction The previous chapters laid a firm foundation for N LP But now we will go deeper into a key topicone that gives us surprising insights into how language processing works and how some of the key advances in human computer interaction are facilitated At the heart of N LP is the simple trick of representing text as numbers This helps software algorithms perform the sophisticated computations that are required to understand the meaning of the text As we have already discussed in previous chapters, most machine learning algorithms take numeric data as input and do not understand the text as such We need to represent our text in numeric form so that we can apply different machine learning algorithms and other N LP techniques to it These numeric representations are called vectors and are also sometimes called word embeddings or simply embeddings This chapter begins with a discussion of vectors, how text can be represented as vectors, and how vectors can be composed to represent complex speech We will walk through the various representations in both directionslearning how to encode text as vectors as well as how to retrieve text from vectors We will also look at some cutting edge techniques used in N LP that are based on the idea of representing text as vectors What Is a Vector The basic mathematical definition of a vector is an object that has both magnitude and direction In our definition, it is mostly compared with a scalar, which can be defined as an object that has only magnitude Vectors are also defined as an element in vector spacefor example, a point in space with the coordinates x4, y5, z6 is a vector Here, we can see the vector dimensions are the geometric coordinates of a point or element in space However, the vector dimensions can also represent any quantity or property of some element or object in addition to mere geometriccoordinates As an example, lets say that we are defining the weather at a given place using five features temperature, humidity, precipitation, wind speed, and air pressure The units that these would be measured in are Celsius, percentage, centimeters, kilometers per hour kmh, and millibar mbar, respectively The following are the values for two placesWhat Is a Vector 273 Figure 6 1 Weather indicators at two different places So, we can represent the weather of these places in vector form as follows Vector for place 1 Vector for place 2 In the preceding representation, the first dimension represents temperature, the second dimension represents humidity, and so on Note that the order of these dimensions should be consistent among all the vectors Similarly, we can also represent text as a vector in which each dimension can represent either the presence or absence of certain metrics Examples of these are bag of words and T FI DF vectors that we looked at in the previous chapters There are other techniques as well for vector representation of textlearned word embeddings, for instance We will discuss all these different techniques in the upcoming sections These techniques can be broadly classified into two categories Frequency based embeddings Learned word embeddings Frequency Based Embeddings Frequency based embedding is a technique in which the text is represented in vector form by considering the frequency of the word in a corpus The techniques that come under this category are the following Bag of words As we have already seen in Chapter 2 , Feature Extraction Methods , bag of words is the technique of converting text into vector or numeric form by representing each sentence or document in a list the length of which is equal to the total number of unique words in the corpus T FI DF As seen previously in Chapter 2 , Feature Extraction Methods , this technique considers the frequency of a term as well as the inverse of its occurrence in the corpus 274 Vector Representation Term frequency based technique This is a somewhat simpler version of T FI DF We represent each word in the vector by its number of occurrences in the document For example, lets say that a document contains the following sentences 1  The girl is pretty, and the boy is handsome 2  Do whatever your heart says 3  The boy has a bike 4  His bike was red in color Now lets build term frequency vectors of all these sentences We will first create a dictionary of unique words as follows Note that we are considering every word in lowercase only 1 the 2 girl 3 pretty 4 and 5 boy 5 is 7 handsome 8 do 9 whatever 10 your 11 heart 12 says 13 was 14 has 15 bike 16 his 17 red What Is a Vector 275 18 in 19 color Now every document will be represented by a vector with 19 dimensions, where every dimension represents the frequency of a word in that document So, for sentence 1 , the vector will be  Similarly, for sentence 2 , the vector representation will be 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , and so on Note that the order needs to be consistent here, too Note It is recommended that you use preprocessing techniques such as stemming, stop word removal, and conversion to lowercase before converting a text into the aforementioned vector format Term frequency is a simple and quick technique for converting text into vector form However , the T FI DF technique is a more effective technique than term frequency as it not only considers the frequency of a word in the current document but also in the background corpus One hot encoding In all techniques described previously, we have represented a word with a single number Using one hot encoding, we can represent a word with an array To understand this concept better, lets take the following sentences 5  I love cats and dogs 6  Cats are light in weight We will use a dictionary to assign a numeric label or index to each unique word after converting to lowercase as follows 1 i 2 love 3 cats 4 and 5 dogs276 Vector Representation 6 are 7 light 8 in 9 weight Now we will represent each word in these sentences as follows i love cats and dogs are light in weight We can see that each vector consists of 9 elements that is, the number of elements equals the total number of words in the dictionary  For each word, the value of an element will be 1 , only if the word is present at the corresponding position in the dictionary  When one hot encoding words, you also need to consider the vocabulary  The meaning of vocabulary here is the total number of unique words in the text sources for your project So, if you have a large source, then you will end up with a huge vocabulary and large one hot vector sizes, which will eventually consume a lot of memory The next exercise on word level one hot encoding will help us understand this better Label encoding is a technique used to convert categorical data in numerical data, where each category is represented by a unique number In order to perform label encoding and one hot encoding, we will be using the Label Encoder and One Hot Encoder classes from the preprocessing package provided by the scikit learn library The following exercise will help us get a better understanding of this What Is a Vector 277 Exercise 6 01 Word Level One Hot Encoding In this exercise, we will one hot encode words with the help of the preprocessing package provided by the scikit learn library For this, we shall make use of a file containing lines from Jane Austens Pride and Prejudice  Note The text file used for this exercise can be found at  Follow these steps to implement this exercise 1  Open a Jupyter notebook 2  First, load the file containing the lines from the novel using the Path class provided by the pathlib library to specify the location of the file Insert a new cell and add the following code from pathlib import Path data Path data novel_lines_file data novel_lines.txt 3  Now that you have the file, open it and read its contents Use the open and read functions to perform these actions Store the results in the novel_lines file variable Insert a new cell and add the following code to implement this with novel_lines_file.open as f novel_lines_raw f.read 4  After reading the contents of the file, load it by inserting a new cell and adding the following code novel_lines_raw278 Vector Representation The code generates the following output Figure 6 2 Raw text from the file In the output, you will see a lot of newline characters This is because we loaded the entire content at once into a single variable instead of separate lines You will also see a lot of non alphanumeric characters 5  The main objective is to create one hot vectors for each word in the file To do this, construct a vocabulary, which is the entire list of unique words in the file, by tokenizing the string into words and removing newlines and non alphanumeric characters Define a function named clean_tokenize to do this Store the vocabulary created using clean_tokenize inside a variable named novel_lines  Add the following code import string import re alpha_characters str.maketrans, , string.punctuation def clean_tokenizetext text text.lower text re.subrn, , text text text.translatealpha_characters What Is a Vector 279 text re.subr , , text return text.split novel_lines clean_tokenizenovel_lines_raw 6  Take a look at the content inside novel_lines now It should look like a list Insert a new cell and add the following code to view it novel_lines The code generates the following output Figure 6 3 Text after preprocessing is done 7  Insert a new cell and add the following code to convert the list to a Num Py array and print the shape of the array import numpy as np novel_lines_array np.array novel_lines_array novel_lines_array.reshape- 1 , 1 novel_lines_array.shape 280 Vector Representation The code generates the following output 459 , 1 As you can see, the novel_lines_array array consists of 459 rows and 1 column Each row is a word in the original novel_lines file Note Num Py arrays are more specific to N LP algorithms than Python lists It is the format that is required for the scikit learn library, which we will be using to one hot encode words 8  Now use encoders, such as the Label Encoder and One Hot Encoder classes from scikit learns preprocessing package, to convert novel_ lines_array to one hot encoded format Insert a new cell and add the following lines of code to implement this from sklearn import preprocessing label Encoder preprocessing Label Encoder novel_lines_labels label Encoder.fit_transform novel_lines_array import warnings warnings.filterwarningsignore word One Hot Encoder preprocessing One Hot Encoder line_onehot word One Hot Encoder.fit_transform novel_lines_labels.reshape- 1 , 1 In the code, the Label Encoder class encodes the labels, and the fit_ transform method fits the label encoder and returns the encoded labels What Is a Vector 281 9  To check the list of encoded labels, insert a new cell and add the following code novel_lines_labels The preceding code generates output that looks as follows Figure 6 4 List of encoded labels The One Hot Encoder class encodes the categorical integer features as a one hot numeric array The fit_transform method of this class takes the novel_lines_labels array as input This is a numeric array, and each feature included in this array is encoded using the one hot encoding scheme 282 Vector Representation 10  Create a binary column for each category A sparse matrix is returned as output To view the matrix, insert a new cell and type the following code line_onehot The code generates the following output 459x199 sparse matrix of type class numpy.float64 With 459 stored elements in Compressed Sparse Row format 11  To convert the sparse matrix into a dense array , use the toarray function Insert a new cell and add the following code to implement this line_onehot.toarray The code generates the following output Figure 6 5 Dense array Note To access the source code for this specific section, please refer to  You can also run this example online at  The preceding output shows that we have achieved our objective of one hot encoding words One hot encoding is mostly used in techniques such as language generation models, where a model is trained to predict the next word in the sequence given the words that precede it think about your phone recommending words while you are chatting with your friends Language models are used in many important natural language tasks nowadays, including machine translation, spell correction, text summarization, and in tools like Amazon Echo, Alexa, and more What Is a Vector 283 In addition to word level language models, we can also build character level language models, which can be trained to predict the next character in a sequence of characters For character level language models, we need character level one hot encoding Lets explore this in the next section Character Level One Hot Encoding In character level one hot encoding, we assign a numeric value to all the possible characters We can use alpha numeric characters and punctuation as well Then, we represent each character by an array of size equal to all the characters in the document This array contains zero at all the indices, other than the index assigned with the character Lets explain this with an example Consider the word hello Lets say our vocabulary contains only twenty six characters, so our dictionary will look like this a 0 b 1 c 2 d 3 e 4 f 5 g 6 h 7 i 8 j 9 k 10 .z 25 Now, h will be represented as  Similarly, e can be represented as  Lets see how we can implement this in the next exercise 284 Vector Representation Exercise 6 02 Character One Hot Encoding Manual In this exercise, we will create our own function that can one hot encode the characters of the word data Follow these steps to complete this exercise 1  Open a Jupyter notebook 2  To one hot encode the characters of a given word, create a function named onehot_word  Within this function, create a lookup table for each of the characters in the given word Then, map each character to an index Add the following code to implement this def onehot_wordword lookup v v for v in enumeratesetword word_vector 3  Next, loop through the characters in the word and create a vector named one_ hot_vector of the same size as the number of characters in the lookup This vector is filled with zeros Then, use the lookup table to find the position of the character and set that characters value to 1  Note Execute the code for step 1 and step 2 together Add the following code for c in word one_hot_vector lenlookup one_hot_vector 1 word_vector.appendone_hot_vector return word_vector The function created earlier will return a word vector 4  Once the onehot_word function has been created, test it by adding some input as a parameter Add the word data as an input to the function To implement this, add a new cell and write the following code onehot_vector onehot_worddata printonehot_vectorWhat Is a Vector 285 The code generates the following output , , , Since there are four characters in the input data , there will be four one hot vectors To determine the size of each one hot vector for data , we enumerate the total number of characters in it It is important to note that only one index gets assigned for repeated characters After enumerating through the characters, the character d will be assigned index 0 , the character a will be assigned index 1 , and the character t will be assigned index 2  Based on each characters index position, the elements in each one hot vector will be marked as 1 , leaving other elements marked 0  In this way, we can manually one hot encode any given text Note that, in most practical applications, the size of one hot encoded vector is equal to the size of all the characters, and sometimes, non alphabetical characters are also considered Note To access the source code for this specific section, please refer to  You can also run this example online at  We have learned how character level one hot encoding can be performed manually by developing our own function We will focus on performing character level one hot encoding using Keras in the next exercise Keras is a machine learning library that works along with Tensor Flow to create deep learning models We will be using the Tokenizer class from Keras to create vectors from the text Tokenizer can work on both characters and words, depending on the char_ level argument If char_level is set to true , then it will work on the character level otherwise, it will work on the word level The Tokenizer class comes with the following functions fit_on_text This method reads all the text and creates an internal dictionary , either word wise or character wise We should always call it for the entire text, so that no word or character is left out of the dictionary All the methodsvariables listed after this should be called or used only after calling this method 286 Vector Representation word_index This is a dictionary that contains all the possible words or characters in the vocabulary Each word or character is assigned a unique numberindex index_word This is the reverse dictionary of word_index it contains key value pairs with the index as the key and the word or character as its value texts_to_sequences This function converts each word or character sequence into its corresponding index value texts_to_matrix This converts each word or character in a given text into one hot vector using a built in dictionary It takes the text as input, processes it, and returns a Num Py array of one hot encoded vectors Exercise 6 03 Character Level One Hot Encoding with Keras In this exercise, we will perform one hot encoding on a given word using the Keras library Follow these steps to implement this exercise 1  Open a Jupyter notebook 2  Insert a new cell and the following code to import the necessary libraries from keras.preprocessing.text import Tokenizer import numpy as np 3  Once you have imported the Tokenizer class, create an instance of it by inserting a new cell and adding the following code char_tokenizer Tokenizerchar_levelTrue Since you are encoding at the character level, in the constructor, char_level is set to True  Note By default, char_level is set to False if we are encoding words 4  To test the Tokenizer instance, you will require some text to work on Insert a new cell and add the following code to assign a string to the text variable text The quick brown fox jumped over the lazy dogWhat Is a Vector 287 5  After getting the text, use the fit_on_texts method provided by the Tokenizer class Insert a new cell and add the following code to implement this char_tokenizer.fit_on_textstext In this code, char_tokenizer will break text into characters and internally keep track of the tokens, the indices, and everything else needed to perform one hot encoding 6  Now, look at the possible output One type of output is the sequence of the charactersthat is, the integers assigned with each character in the text The texts_to_sequences method of the Tokenizer class helps assign integers to each character in the text Insert a new cell and add the following code to implement this seq char_tokenizer.texts_to_sequencestext seq The code generates the following output Figure 6 6 List of integers assigned to each character 288 Vector Representation As you can see, there were 44 characters in the text variable From the output, we can see that for every unique character in text , an integer is assigned 7  Use sequences_to_texts to get text from the sequence with the following code char_tokenizer.sequences_to_textsseq The snippet of the preceding output follows Figure 6 7 Text generated from the sequence What Is a Vector 289 8  Now look at the actual one hot encoded values For this, use the texts_to_matrix function Insert a new cell and add the following code to implement this char_vectors char_tokenizer.texts_to_matrixtext Here, the results of the array are stored in the char_vectors variable 9  In order to view the vector values, just insert a new cell and add the following line char_vectors On execution, the code displays the array of one hot encoded vectors Figure 6 8 Actual one hot encoded values for the given text 10  In order to investigate the dimensions of the Num Py array, make use of the shape attribute Insert a new cell and add the following code to execute it char_vectors.shape The following output is generated 44 , 27 So, char_vectors is a Num Py array with 44 rows and 27 columns This is because we are considering 26 characters and an additional character for space 11  To access the first row of char_vectors Num Py array, insert a new cell and add the following code char_vectors This returns a one hot vector, which can be seen in the following figure array0 ., 0 ., 0 ., 0 ., 1 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 ., 0 290 Vector Representation 12  To access the index of this one hot vector, use the argmax function provided by Num Py Insert a new cell and write the following code to implement this np.argmaxchar_vectors The code generates the following output 4 13  The Tokenizer class provides two dictionaries, index_word and word_index , which you can use to view the contents of Tokenizer in key value form Insert a new cell and add the following code to view the index_word dictionary char_tokenizer.index_word The code generates the following output Figure 6 9 The index_word dictionary What Is a Vector 291 As you can see in this figure, the indices act as keys, and the characters act as values Now insert a new cell and the following code to view the word_index dictionary char_tokenizer.word_index The code generates the following output Figure 6 10 The word_index dictionary In this figure, the characters act as keys, and the indices act as values 292 Vector Representation 14  In the preceding steps, you saw how to access the index of a given one hot vector by using the argmax function provided by Num Py Using this index as a key, you can access its value in the index_word dictionary To implement this, we insert a new cell and write the following code char_tokenizer.index_word The preceding code generates the following output t In this code, np.argmaxchar_vectors produces an output of 4  This will act as a key in finding the value in the index_word dictionary So, when char_tokenizer.index_word is executed, it will scan through the dictionary and find that, for key 4 , the value is t, and finally, it will print t Note To access the source code for this specific section, please refer to  You can also run this example online at  In the preceding section, we learned how to convert text into one hot vectors at either the character level or the word level One hot encoding is a simple representation of a word, but it has a disadvantage Whenever the corpus is large that is, when the number of unique characters or words increases, the size of the one hot encoded vector also increases Thus, it becomes very memory intensive and is sometimes not feasible speed and simplicity here lead to the curse of dimensionality by creating a new dimension for each categoryword To tackle this problem, learned embeddings can be used, as explained in the following sections What Is a Vector 293 Learned Word Embeddings The vector representations discussed in the preceding section have some serious disadvantages, as discussed here Sparsity and large size The sizes of one hot encoded or other frequency- based vectors depend upon the number of unique words in the corpus This means that when the size of the corpus increases, the number of unique words increases, thereby increasing the size of the vectors in turn Context None of these vector representations consider the words with respect to its context while representing it as a vector However, the meaning of a word in any language depends upon the context it is used in Not taking the context into account can often lead to inaccurate results Prediction based word embeddings or learned word embeddings try to address both problems For starters, these methods represent words with a fixed number of dimensions Moreover, these representations are actually learned from the different contexts in which the word has been used at different places Learned word embeddings is actually a collective name given to a set of language models that represent words in such a way that words with similar meanings have somewhat similar representations There are different techniques for creating learned word embeddings, such as Word2 Vec and Glo Ve Lets discuss them one by one Word2 Vec Word2 Vec is a prediction based algorithm that represents a word by a vector of a fixed size This is a form of unsupervised learning algorithm, which means that we need not to provide manually annotated data we just feed the raw text It will train a model in such a way that each word is represented in terms of its context throughout the training data This algorithm has two variations, as follows Continuous Bag of Words C Bo W This model tends to predict the probability of a word given the context The learning problem here is to predict the word given a fixed window contextthat is, a fixed set of continuous words in text Skip Gram model This model is the reverse of the C Bo W model, as it tends to predict the context of a word 294 Vector Representation These vectors find application in a lot of N LP tasks including text generation, machine translation, speech to text, text to speech, text classification, and text similarity Lets explore how they can be used for text similarity Suppose we generated 300 dimensional vectors from words such as love, adorable, and hate If we find the cosine similarity between the vectors for love and adorable, and love and hate, we will find a higher similarity between the former pair of words than the latter In the next exercise, we will train word vectors using the gensim library Specifically, well be using the Word2 Vec class The Word2 Vec class has parameters such as documents , size , window , min_count , and workers  Here, documents refers to the sentences that we have to provide to the class, size represents the length of the dense vector to represent each token, min_count represents the minimum count of words that can be taken into consideration when training a particular model, and workers represents the number of threads that are required when training a model For training a model, we use the model.train method This method takes arguments such as documents , total_examples , and epochs  Here, documents represents the sentences, and total_examples represents the count of sentences, while epochs represents the total number of iterations over the given data Finally, the trained word vectors get stored in model.wv , which is an instance of Keyed Vectors  In order to perform basic text cleaning, before its processed, we will make use of the textcleaner class from gensim Some of the most useful functions available in textcleaner that we will be using are as follows split_sentences As the name suggests, this function splits the text and gets a list of sentences from the text simple_preprocess This function converts a document into a list consisting of lowercase tokens Lets see how we can use these functions to create word vectors Exercise 6 04 Training Word Vectors What Is a Vector 295 In this exercise, we will train word vectors We will be using books freely available on Project Gutenberg for this We will also see the vector representation using Matplotlibs pyplot framework Note The file we are using for this exercise can be found at  Follow these steps to implement this exercise 1  Open a Jupyter notebook 2  Use the requests library to load books from the Project Gutenberg website, the json library to load a book catalog, and the regex package to clean the text by removing newline characters Insert a new cell and add the following code to implement this import requests import json import re 3  After importing all the necessary libraries, load the json file, which contains details of 10 books, including the title, the author, and the I D Insert a new cell and add the following steps to implement this with open dataProject Gutenberg Books.json, r as catalog_file catalog json.loadcatalog_file296 Vector Representation 4  To print the details of all the books, insert a new cell and add the following code catalog The preceding code generates the following output Figure 6 11 Book details in the catalog 5  Create a function named load_book , which will take book_id as a parameter and, based on that book_id , fetch the book and load it It should also clean the text by removing the newline characters Insert a new cell and add the following code to implement this G UT EN BE RG_ UR L 0 .txt def load_bookbook_id url G UT EN BE RG_ UR L.formatbook_id, book_id contents requests.geturl.text cleaned_contents re.subrrn, , contents return cleaned_contents 6  Once you have defined our load_book function, you will loop through the catalog, fetch all the id instances of the books, and store them in the book_ids list The id instances stored in the book_ids list will act as parameters for our load_book function The book information fetched for each book I D will be loaded in the books variable Insert a new cell and add the following code to implement this book_ids for book in catalog books To view the information of the books variable, add the following code in a new cell books What Is a Vector 297 A snippet of the output generated by the preceding code is as follows Figure 6 12 Information of various books 7  Before you can train the word vectors, you need to split the books into a list of documents In this case, you want to teach the Word2 Vec algorithm about words in the context of the sentences that they are in So here, a document is actually a sentence Thus, you need to create a list of sentences from all 10 books Insert a new cell and add the following code to implement this from gensim.summarization import textcleaner from gensim.utils import simple_preprocess def to_sentencesbook sentences textcleaner.split_sentencesbook sentence_tokens simple_preprocesssentence for sentence in sentences return sentence_tokens In the preceding code, all the text preprocessing takes place inside the to_sentences function that you have defined 298 Vector Representation 8  Now, loop through each book in books and pass each book as a parameter to the to_sentences function The results should be stored in the book_ sentences variable Also, split books into sentences and sentences into documents The result should be stored in the documents variable Insert a new cell and add the following code to implement this books_sentences documents sentence for book_sent in books_sentences for sentence in book_sent 9  To check the length of the documents, use the len function as follows lendocuments The code generates the following output 32922 10  Now that you have your documents, train the model by making use of the Word2 Vec class provided by the gensim package Insert a new cell and add the following code to implement this from gensim.models import Word2 Vec build vocabulary and train model model Word2 Vec documents, size100, window10, min_count2, workers10 model.traindocuments, total_exampleslendocuments, epochs50 The code generates the following output 27809439 , 37551450 Now make use of the most_similar function of the model.wv instance to find the similar words The most_similar function takes positive as a parameter and returns a list of strings that contribute positively Insert a new cell and add the following code to implement this model.wv.most_similarpositiveworseWhat Is a Vector 299 The code generates the following output Figure 6 13 Most similar words Note You may get a slightly different output as the output depends on the model training process, so you may have a different model than the one we have trained here 11  Create a show_vector function that will display the vector using pyplot , a plotting framework in Matplotlib Insert a new cell and add the following code to implement this %matplotlib inline import matplotlib.pyplot as plt def show_vectorword vector model.wv fig, ax plt.subplots1, 1 , figsize10, 2 ax.tick_paramsaxisboth, whichboth, leftFalse, bottomFalse, topFalse, labelleftFalse, labelbottomFalse 300 Vector Representation ax.gridFalse printword ax.barrangelenvector, vector, 0 5 show_vectorsad The code generates the following output Figure 6 14 Graph of the vector when the input is sad Note To access the source code for this specific section, please refer to  You can also run this example online at  In the preceding figure, we can see the vector representation when the word provided to the show_vector function is sad We have learned about training word vectors and representing them using pyplot  In the next section, we will focus more on using pre trained word vectors , which are required for N LP projects What Is a Vector 301 Using Pre Trained Word Vectors For a machine learning model, the more data you have, the better the model you get But training the model on large amounts of data is intensively resource consuming in terms of both time and memory So, we usually train a Word2 Vec model on a large amount of data and retain the model for future use There are also a lot of pre trained models publicly available have been trained on huge datasets such as Wikipedia articles These models include gensim by fast Text research group by Facebook, and Word2 Vec has recently proved to be state of the art for tasks including checking for word analogies and word similarities, as follows vectorParis vectorFrance vectorItaly results in a vector that is very close to vectorRome  vectorking-vectorman vectorwoman is close to vectorqueen  Googles publicly available glove model is similar to the Word2 Vec model and has produced incredible results In some applications, we may need to train a Word2 Vec model on our own specific dataset rather than train a new model from scratch that is, we can train a pre trained model on more data This process is called transfer learning Transfer learning is based on the concept of transferring knowledge from one domain into another Note Pre trained word vectors can get pretty large For example, vectors trained on Google News contain 3 million words, and on disk, its compressed size is 1 5 G B To better understand how we can use pre trained word vectors in Python, lets walk through a simple exercise 302 Vector Representation Exercise 6 05 Using Pre Trained Word Vectors In this exercise, we will load and use pre trained word embeddings We will also show the image representation of a few word vectors using the pyplot framework of the Matplotlib library We will be using glove6 B50d.txt , which is a pre trained model Note The pre trained model being used for this file can be found at  Download this file and place it in the data folder of Chapter 6 , Vector Representation  Follow these steps to complete this exercise 1  Open a Jupyter notebook 2  Add the following statement to import the numpy library import numpy as np import zipfile 3  Move the downloaded model from the preceding link to the location given in the following code snippet In order to extract data from a Z IP file, use the zipfile Python package Add the following code to unzip the embeddings from the Z IP file G LO VE_ DI R data G LO VE_ ZI P G LO VE_ DI R glove6 B50d.txt.zip printG LO VE_ ZI P zip_ref zipfile Zip FileG LO VE_ ZI P, r zip_ref.extractallG LO VE_ DI R zip_ref.close 4  Define a function named load_glove_vectors to return a model Python dictionary Insert a new cell and add the following code to implement this def load_glove_vectorsfn printLoading Glove Model with open fn,r, encodingutf8 as glove_vector_file model for line in glove_vector_fileWhat Is a Vector 303 parts line.split word parts embedding np.arrayfloatval for val in parts model embedding printLoaded words.formatlenmodel return model glove_vectors load_glove_vectorsG LO VE_ DI R glove6 B50d.txt Here, glove_vector_file is a text file containing a dictionary In this, words act as keys and vectors act as values So, we need to read the file line by line, split it, and then map it to a Python dictionary The preceding code generates the following output Loading Glove Model Loaded 400000 words If we want to view the values of glove_vectors , then we insert a new cell and add the following code glove_vectors You will get the following output Figure 6 15 Dictionary of glove_vectors The order of the result dictionary can vary as it is a Python dict 304 Vector Representation 5  The glove_vectors object is basically a dictionary containing the mappings of the words to the vectors, so you can access the vector for a word, which will return a 50 dimensional vector Insert a new cell and add the code to check the vector for the word dog glove_vectors Figure 6 16 Array of glove vectors with an input of dog In order to see the vector for the word cat , add the following code glove_vectors Figure 6 17 Array of glove vectors with an input of cat What Is a Vector 305 6  Now that you have the vectors, represent them as an image using the pyplot framework of the Matplotlib library Insert a new cell and add the following code to implement this %matplotlib inline import matplotlib.pyplot as plt def to_vectorglove_vectors, word vector glove_vectors.getword.lower if vector is None vector 50 return vector def to_imagevector, word fig, ax plt.subplots1, 1 ax.tick_paramsaxisboth, whichboth, leftFalse, bottomFalse, topFalse, labelleftFalse, labelbottomFalse ax.gridFalse ax.barrangelenvector, vector, 0 5 ax.textsword, x1, yvector.max0 5 return vector In the preceding code, you defined two functions The to_vector function accepts glove_vectors and word as parameters Here, the get function of glove_vectors will find the word and convert it into lowercase The result will be stored in the vector variable 7  The to_image function takes vector and word as input and shows the image representation of vector  To find the image representation of the word man , type the following code man to_imageto_vectorglove_vectors, man306 Vector Representation The code generates the following output Figure 6 18 Graph generated with an input of man 8  To find the image representation of the word woman , type the following code woman to_imageto_vectorglove_vectors, woman This will generate the following output Figure 6 19 Graph generated with an input of woman What Is a Vector 307 9  To find the image representation of the word king , type the following code king to_imageto_vectorglove_vectors, king This will generate the following output Figure 6 20 Graph generated with an input of king 10  To find the image representation of the word queen , type the following code queen to_imageto_vectorglove_vectors, queen This will generate the following output Figure 6 21 Graph generated with an input of queen 308 Vector Representation 11  To find the image representation of the vector for king man woman queen , type the following code diff to_imageking man woman queen This will generate the following output Figure 6 22 Graph generated with king manwoman queen as input 12  To find the image representation of the vector for king man woman , type the following code nd to_imageking man woman This will generate the following output Figure 6 23 Graph generated with king manwoman as input What Is a Vector 309 Note To access the source code for this specific section, please refer to  This section does not currently have an online interactive example, and will need to be run locally The preceding results are the visual proof of the example we already discussed we have learned how to load and use pre trained word vectors and view their image representations In the next section, we will focus on document vectors and their uses Document Vectors Word vectors and word embeddings represent words But if we wanted to represent a whole document, wed need to use document vectors Note that when we refer to a document, we are referring to a collection of words that have some meaning to a user A document can be a single sentence or a group of sentences A document can consist of product reviews, tweets, or lines of movie dialogue, and can be from a few words to thousands of words A document can be used in a machine learning project as an instance of something that the algorithm can learn from We can represent a document with different techniques Calculating the mean value We calculate the mean of all the constituent word vectors of a document and represent the document by the mean vector Doc2 Vec Doc2 Vec is a technique by which we represent documents by a fixed length vector It is trained quite similarly to the way we train the Word2 Vec model Here, we also add the unique I D of the document to which the word belongs Then, we can get the vector of the document from the trained model using the document I D Similar to Word2 Vec , the Doc2 Vec class contains parameters such as min_count , window , vector_size , sample , negative , and workers  The min_count parameter ignores all the words with a frequency less than that specified The window parameter sets the maximum distance between the current and predicted words in the given sentence The vector_size parameter sets the dimensions of each vector 310 Vector Representation The sample parameter defines the threshold that allows us to configure the higher frequency words that are regularly down sampled, while negative specifies the total amount of noise words that should be drawn and workers specifies the total number of threads required to train the model To build the vocabulary from the sequence of sentences, Doc2 Vec provides the build_vocab method Well be using all of these in the upcoming exercise Uses of Document Vectors Some of the uses of document vectors are as follows Similarity We can use document vectors to compare texts for similarity For example, legal A I software can use document vectors to find similar legal cases Recommendations For example, online magazines can recommend similar articles based on those that users have already read Predictions Document vectors can be used as input into machine learning algorithms to build predictive models In the next section, we will perform an exercise based on document vectors Exercise 6 06 Converting News Headlines to Document Vectors In this exercise, we will convert some news headlines into document vectors Also, we will look at the image representation of the vector Again, for image representation, we will be using the pyplot framework of the Matplotlib library Follow these steps to complete this exercise Note The file which we are going to use in this exercise is in zipped format and can be found at  It should be unzipped once downloaded 1  Open a Jupyter notebook 2  Import all the necessary libraries for this exercise You will be using the gensim library Insert a new cell and add the following code import pandas as pd from gensim import utils from gensim.models.doc2vec import Tagged Document What Is a Vector 311 from gensim.models import Doc2 Vec from gensim.parsing.preprocessing import preprocess_string, remove_stopwords import random import warnings warnings.filterwarningsignore In the preceding code snippet, other than other imports, you imported Tagged Document from gensim, which prepares the document formats used in Doc2 Vec  It represents the document along with the tag This will be clearer from the following code lines Doc2 Vec requires each instance to be a Tagged Document instance 3  Move the downloaded file to the following location and create a variable of the path as follows sample_news_data datasample_news_data.txt 4  Now load the file with opensample_news_data, encodingutf8, errorsignore as f news_lines 5  Now create a Data Frame out of the headlines as follows lines_df pd Data Frame indices listrangelennews_lines lines_df news_lines lines_df indices 6  View the head of the Data Frame using the following code lines_df.head312 Vector Representation This will create the following output Figure 6 24 Head of the Data Frame 7  Create a class, the object of which will create the training instances for the Doc2 Vec model Insert a new cell and add the following code to implement this class Document Datasetobject def __init__self, datapd Data Frame, column document data.applyself.preprocess self.documents for index, text in document.iteritems def preprocessself, document return preprocess_string remove_stopwordsdocument def __iter__self for document in self.documents yield documents def tagged_documentsself, shuffleFalse if shuffle random.shuffleself.documents return self.documents What Is a Vector 313 In the code, the preprocess_string function applies the given filters to the input As its name suggests, the remove_stopwords function is used to remove stopwords from the given document Since Doc2 Vec requires each instance to be a Tagged Document instance, we create a list of Tagged Document instances for each headline in the file 8  Create an object of the Document Dataset class It takes two parameters One is the lines_df_small Data Frame and the other is the Line column name Insert a new cell and add the following code to implement this documents_dataset Document Datasetlines_df, news 9  Create a Doc2 Vec model using the Doc2 Vec class Insert a new cell and add the following code to implement this doc Vec Model Doc2 Vecmin_count1, window5, vector_size100, sample1e 4 , negative5, workers8 doc Vec Model.build_vocabdocuments_dataset.tagged_documents 10  Now you need to train the model using the train function of the Doc2 Vec class This could take a while, depending on how many records we train Here, epochs represents the total number of records required to train the document Insert a new cell and add the following code to implement this doc Vec Model.traindocuments_dataset tagged_documentsshuffleTrue, total_examples doc Vec Model.corpus_count, epochs10 11  Save this model for future use as follows doc Vec Model.save datadoc Vec Model.d2v 12  The model has been trained To verify this, access one of the vectors with its index To do this, insert a new cell and add the following code to find the doc vector of index 657 doc Vec Model314 Vector Representation You should get an output similar to the one below Figure 6 25 Lines represented as vectors 13  To check the image representation of any given vector, make use of the pyplot framework of the Matplotlib library The show_news_lines function takes a line number as a parameter Based on this line number, find the vector and store it in the doc_vector variable The show_image function takes two parameters, vector and line , and displays an image representation of the vector Insert a new cell and add the following code to implement this import matplotlib.pyplot as plt def show_imagevector, line fig, ax plt.subplots1, 1 , figsize10, 2 ax.tick_paramsaxisboth, whichboth, leftFalse, bottomFalse, topFalse, labelleftFalse, labelbottomFalse ax.gridFalse printline What Is a Vector 315 ax.barrangelenvector, vector, 0 5 def show_news_linesline_number line lines_df.news doc_vector doc Vec Model show_imagedoc_vector, line 14  Now that you have defined the functions, implement the show_news_lines function to view the image representation of the vector Insert a new cell and add the following code to implement this show_news_lines872 The code generates the following output Figure 6 26 Image representation of a given vector Note To access the source code for this specific section, please refer to  You can also run this example online at  We have learned how to represent a document as a vector We have also seen a visual representation of this In the next section, we will complete an activity to find similar news headlines using the document vector 316 Vector Representation Activity 6 01 Finding Similar News Article Using Document Vectors To complete this activity, you need to build a news search engine that finds similar news articles like the one provided as input using the Doc2 Vec model You will find headlines similar to U S raise T V indecency U S politicians are proposing a tough new law aimed at cracking down on indecency Follow these steps to complete this activity 1  Open a Jupyter notebook and import the necessary libraries 2  Load the new article lines file 3  Iterate over each headline and split the columns and create a Data Frame 4  Load the Doc2 Vec model that you created in the previous exercise 5  Create a function that converts the sentences into vectors and another that does the similarity checks 6  Test both the functions Note The solution for this activity can be found via this link  So, in this activity, we were able to find similar news headlines with the help of document vectors A common use case of inferring text similarity from document vectors is in text paraphrasing, which well explore in detail in the next chapter Summary In this chapter, we learned about the motivations behind converting human language in the form of text into vectors This helps machine learning algorithms to execute mathematical functions on the text, detect patterns in language, and gain an understanding of the meaning of the text We also saw different types of vector representation techniques, such as character level encoding and one hot encoding In the next chapter, we will look at the areas of text paraphrasing, summarization, and generation We will see how we can automate the process of text summarization using the N LP techniques we have learned so far Overview This chapter begins with the concept of text generation using Markov chains, before moving on to two types of text summarizationnamely , abstractive and extractive summarization You will then explore the Text Rank algorithm and use it with different datasets By the end of this chapter, you will understand the applications and challenges of text generation and summarization using Natural Language Processing N LP approaches Text Generation and Summarization7320 Text Generation and Summarization Introduction The ability to express thoughts in words sentence generation, the ability to replace a piece of text with different but equivalent text paraphrasing, and the ability to find the most important parts of a piece of text summarization are all key elements of using language Although sentence generation, paraphrasing, and summarization are challenging tasks in N LP, there have been great strides recently that have made them considerably more accessible In this chapter, we explore them in detail and see how we can implement them in Python Generating Text with Markov Chains An idea is expressed using the words of a language As ideas are not tangible, it is useful to look at text generation in order to gauge whether a machine can think on its own The utility of text generation is currently limited to an auto complete functionality, besides a few negative use cases that we will discuss later in this section Text can be generated in many different ways, which we will explore using Markov chains Whether this generated text can correspond to a coherent line of thought is something that we will address later in this section Markov Chains A state space defines all possible states that can exist A Markov chain consists of a state space and a specific type of successor function For example, in the case of the simplified state space to describe the weather, the states could be Sunny, Cloudy, or Rainy The successor function describes how a system in its current state can move to a different state or even continue in the same state To better understand this, consider the following diagramGenerating Text with Markov Chains 321 Figure 7 1 Markov chain for weather The successor function of a Markov chain is a random selection of a successor state based on probabilities For instance, consider that the initial state is randomly selected as Rainy The next state could be Rainy there is a 0 8 probability that the state stays Rainy Then, the next state could be Sunny there is a 0 05 probability associated with this transition It could be Rainy again, and then it could be Cloudy, and so on Our sequence of states is Rainy Rainy Sunny Rainy Cloudy  For each state, the successor state is found by a random selection this is called a random walk on the Markov chain 322 Text Generation and Summarization Similarly, if we have a state space in which the states correspond to a vocabulary, then a random walk on such a Markov chain will generate text Now, the vocabulary could have around 20 , 000 words In this case, the Markov chain will have 20 , 000 states The probabilities in this case will correspond to the likelihood of a word succeeding a given word We can begin with any state randomly drawn from among the words that could be used for the first word of a sentence, for example, common words such as the, a, I, he, she, if, this, why, and where We then find its successor state in a random way, followed by the next successor state found in a random way, and continue in the same manner until we have generated a sequence of words of the required length In the next section, we will do an exercise related to Markov chains to get a better understanding of them Exercise 7 01 Text Generation Using a Random Walk over a Markov Chain In this exercise, we will generate text with the help of Markov chains We will use Robert Frosts collection of poems, North of Boston , available from Project Gutenberg, to specify the successor states for each state using a dictionary Well use a list to specify the successor states for any state so that the number of times a successor state occurs in that list is directly proportional to the probability of transitioning to that successor state Then, we will generate 10 phrases with three words in addition to an initial word, and then generate another 10 phrases with four words in addition to an initial word The initial state or initial word will be randomly selected from among these words the, a, I, he, she, if, this, why, and where Note that since we are generating text using a random walk over a Markov chain, in general, the output you get will be different from the output shown in this exercise Each different output corresponds to new text generation Note You can find the text file thats been used for this exercise at  Generating Text with Markov Chains 323 Follow these steps to complete this exercise 1  Open a Jupyter notebook 2  Insert a new cell and add the following code to import the necessary libraries and read the dataset import re import random O PE N_ DA TA_ UR L datarobertfrostpg3026.txt fopenO PE N_ DA TA_ UR L,r,encodingutf- 8 textf.read f.close 3  Insert a new cell and add the following code to preprocess the text using regularexpressions H AN DL E wn L IN K httpst.cow S PE CI AL_ CH AR S ltltamp P AR An def cleantext text re.subH AN DL E, , text text re.subL IN K, , text text re.subS PE CI AL_ CH AR S, , text text re.subP AR A, n, text return text text cleantext 4  Split the corpus into a list of words Show the number of words in the corpus corpustext.split corpus_lengthlencorpus corpus_length The preceding code generates the following output 19985324 Text Generation and Summarization 5  Insert a new cell and add the following code to define the successor states for each state Use a dictionary for this succ_func corpus_counter0 for token in corpus corpus_countercorpus_counter1 if corpus_countercorpus_length if token not in succ_func.keys succ_func else succ_func.appendcorpus succ_func The preceding code generates an output as follows Note that we are only displaying a part of the output here Figure 7 2 Dictionary of successor states We find that he is shown as a successor of who more than once This is because this occurs more than once in the dataset In effect, the number of times the successors occur in the list is proportional to their respective probabilities Though it is not the only method, this is a convenient way to represent the successor function 6  Define the list of initial states Then, define a function to select a random initial state from these and concatenate it with successor states These successor states are randomly selected from the list containing successor states for a specific current state Add the following code to do this initial_statesThe,A,I,He,She,If, This,Why,Where def generate_wordsk5 Generating Text with Markov Chains 325 initial_staterandom.choiceinitial_states current_stateinitial_state textcurrent_state for i in rangek succ_staterandom.choicesucc_func texttextsucc_state current_statesucc_state printtext.split 7  Insert a new cell and add the following code to generate text containing 10 phrases of four words including the initial word and 10 phrases of five words including the initial word for k in range3, 5 for j in range10 generate_wordsk The preceding code generates the following output Figure 7 3 Phrases generated, consisting of four and five words 326 Text Generation and Summarization Note To access the source code for this specific section, please refer to  You can also run this example online at  Its quite interesting that we are able to generate text using a random walk over a Markov chain If we look more closely, we will see that only a few of the phrases make sense Broadly speaking, we are generating text that has an element of Robert Frosts style However, it can hardly be said to correspond to a thought of any kind The practical utility of generating text using a Markov chain is somewhat limited to generating spam spam generators could use a Markov chain and generating something that is a little amusing Nevertheless, this exercise demonstrates the surprising results we can get by using a simple approach in which nothing about the structure of a language is explicitly taught to the machine In general, auto complete is one positive use case and arguably the sole positive use case for text generation given that other use cases besides spam tend to include the generation of misinformation Paraphrasing involves replacing some text with different text that has the same meaning Now, intuitively, a machine will be able to tell whether one piece of text is a paraphrase of another, but only if that machine understands the meaning So, one way of checking whether a machine understands the meaning of a piece of text is to check if it can tell if another different piece of text is a paraphrase of that first text Benchmark datasets provide a standard touchstone for evaluating approaches to solve a problem The approaches are typically ranked in a publicly available leaderboard Even in the case of such benchmark datasets, as of February 21 , 2020 , the Super GL UE leaderboard sets human baselines at the top when considered across a variety of tasks This means that humans are superior at paraphrasing than the most sophisticated approaches even on the specified datasets Paraphrasing is even tougher outside of benchmark datasets because it is tougher to teach models in a more general way so that the model is as effective for other datasets Thus, compared to machines, humans can paraphrase even better on other datasets than machines can In short, paraphrasing using N LP is challenging and is currently of limited practical utility to the practitioner In the next section, we will learn about summarization Text Summarization 327 Text Summarization Automated text summarization is the process of using N LP tools to produce concise versions of text that preserve the key information present in the original content Good summaries can communicate the content with less text by retaining the key information while filtering out other information and noise or useless text, if any A shorter text may often take less time to read, and thus summarization facilitates more efficient use of time The type of summarization that we are typically taught in school is abstractive summarization One way to think of this is to consider abstractive summarization as a combination of understanding the meaning and expressing it in fewer sentences It is usually considered as a supervised learning problem as the original text and the summary are both required However, a piece of text can be summarized in more than one way This makes it hard to teach the machine in a general way While abstractive summarization is an active area of research, it is, for the time being, not at a stage that will be of interest to the practitioner There is another form of summarization, called extractive summarization, in which parts of the text are extracted to form a summary There is no paraphrasing in this form of summarization This second type will be the focus of the remainder of this section We will look at the Text Rank algorithm, which is an unsupervised machine learning method For simplicity, we will focus on single document summarization in this chapter To implement this, we will be using the gensim library Text Rank Text Rank is a graph based algorithm developed by Rada Mihalcea and Paul Tarau used to find the key sentences in a piece of text As we already know, in graph theory, a graph has nodes and edges In the Text Rank algorithm, we estimate the importance of each sentence and create a summary with the sentences that have the highest importance 328 Text Generation and Summarization The Text Rank algorithm works as follows 1  Represent a unit of text say, a sentence as a node 2  Each node is given an arbitrary importance score 3  Each edge has a weight that corresponds to the similarity between two nodes for instance, the sentences Sx and Sy The weight could be the number of common words say, wk in the two sentences divided by the sum of the number of words in the two sentences This can be represented as follows Figure 7 4 Formula for similarity between two sentences 4  For each node, we compute a new importance score, which is a function of the importance score of the neighboring nodes and the edge weights wji between them Specifically, the function f could be the edge weighted average score of all the neighboring nodes that are directed toward that node that is adjusted by all the outward edge weights wjk and the damping factor d This can be represented as follows Figure 7 5 Formula for importance score d0 85 is typically used as the damping factor While we have used a directed graph here, an undirected graph could also be used with a Text Rank algorithm 5  We repeat the preceding step until the importance score varies by less than a pre defined tolerance level in two consecutive iterations 6  Sort the nodes in decreasing order of the importance scores 7  The top n nodes give us a summary Key Input Parameters for Text Rank 329 The number of iterations required for convergence depends on the number of nodes and the connectedness among the nodes The number of iterations required for an undirected graph is expected to be higher than the number of iterations required for a directed graph since the edges do not have a direction in the case of the former We typically use a directed graph in the Text Rank algorithm In general, around 20 40 iterations may be required for convergence We can drop edges that have less than a certain threshold weight for faster convergence since they will not have much of an impact on the result anyway The basic concept underpinning the Text Rank algorithm is that key parts of a document are connected to form a coherent summary Key Input Parameters for Text Rank Well be using the gensim library to implement Text Rank The following are the parameters required for this text This is the input text ratio This is the required ratio of the number of sentences in the summary to the number of sentences in the input text The gensim implementation of the Text Rank algorithm uses B M25a probabilistic variation of T F I DFfor similarity computation in place of the similarity measure described in step 3 of the algorithm This will be clearer in the following exercise, in which you will summarize text using Text Rank Exercise 7 02 Performing Summarization Using Text Rank In this exercise, we will use the classic short story, After Twenty Years by O Henry, which is available on Project Gutenberg, and the first section of the Wikipedia article on Oscar Wilde We will summarize each text separately so that we have 20% of the sentences in the original text and then have 25% of the sentences in the original text using the gensim implementation of the Text Rank algorithm In all, we shall extract and print four summaries In addition to these libraries, you will need to import the following from gensim.summarization import summarize summarizetext,ratio0 20330 Text Generation and Summarization In the preceding code snippet, ratio0 20 means that 20% of the sentences from the original text will be used to create the summary Note The text corpus for O Henrys short story, After Twenty Years, being used in this exercise can be found at  The Oscar Wilde section from the Wikipedia article can be found at  Complete the following steps to implement this exercise 1  Open a Jupyter notebook 2  Insert a new cell and add the following code to import the necessary libraries and extract the required text from After Twenty Years from gensim.summarization import summarize import wikipedia import re file_url_after_twentyr dataohenrypg2776.txt with openfile_url_after_twenty, r as f contents f.read start_stringA FT ER T WE NT Y Y EA RSnnn end_stringnnnnnn LO ST O N D RE SS P AR AD E text_after_twentycontentscontents.findstart_string contents.findend_string text_after_twentytext_after_twenty.replacen, text_after_twentyre.subrs, ,text_after_twenty text_after_twenty Key Input Parameters for Text Rank 331 The preceding code generates the following output Figure 7 6 Text from After Twenty Years 3  Add the following code to extract the required text and print the summarized text, with the ratio parameter set to 0 2 summary_text_after_twentysummarizetext_after_twenty, ratio0 2 printsummary_text_after_twenty The preceding code generates the following output Figure 7 7 Summarized text when the ratio parameter is 0 2 4  Insert a new cell and add the following code to summarize the text and print the summarized text, with the ratio parameter set to 0 25 summary_text_after_twentysummarizetext_after_twenty, ratio0 25 printsummary_text_after_twenty The preceding code generates the following output Figure 7 8 Summarized text when the ratio parameter is 0 25 332 Text Generation and Summarization 5  Insert a new cell and add the following code to extract the required text from the Wikipedia page for Oscar Wilde text_wiki_oscarwildewikipedia.summaryOscar Wilde file_url_wiki_oscarwilder dataoscarwilde ow_wikipedia_sum.txt with openfile_url_wiki_oscarwilde, r, encodinglatin 1 as f text_wiki_oscarwilde f.read text_wiki_oscarwildetext_wiki_oscarwilde.replacen, text_wiki_oscarwildere.subrs, ,text_wiki_oscarwilde text_wiki_oscarwilde The preceding code generates the following output Figure 7 9 Text from the Wikipedia page for Oscar Wilde 6  Insert a new cell and add the following code to summarize the text and print the summarized text using ratio0 2 summary_wiki_oscarwildesummarizetext_wiki_oscarwilde, ratio0 2 printsummary_wiki_oscarwilde The preceding code generates the following output Figure 7 10 Summarized text when the ratio parameter is 0 2 7  Add the following code to summarize the text and print the summarized text using ratio0 25 summary_wiki_oscarwildesummarizetext_wiki_oscarwilde, ratio0 25 printsummary_wiki_oscarwilde Key Input Parameters for Text Rank 333 The preceding code generates the following output Figure 7 11 Summarized text when the ratio is 0 25 Note To access the source code for this specific section, please refer to  You can also run this example online at  We find that the summary for the Wikipedia article is much more coherent than the short story We can also see that the summary with a ratio of 0 20 is a subset of a summary with a ratio of 0 25  Would extractive summarization work better for a childrens fairytale than it does for an O Henry short story Lets explore this in the next exercise Exercise 7 03 Summarizing a Childrens Fairy Tale Using Text Rank In this exercise, we consider the fairy tale Little Red Riding Hood in two variations for the input texts The first variation is from Childrens Hour with Red Riding Hood and Other Stories , edited by Watty Piper, while the second variation is from The Fairy Tales of Charles Perrault , both of which are available on Project Gutenbergs website The aim of this exercise is to explore how Text Rank gensim performs on this summarization Note You can find the text from the Watty Piper variation at  The text from the Charles Perrault version can be found at 334 Text Generation and Summarization Complete the following steps to implement this exercise 1  Open a Jupyter notebook 2  Insert a new cell and add the following code to import the required libraries from gensim.summarization import summarize import re 3  Insert a new cell and add the following code to fetch Watty Pipers version of Little Red Riding Hood file_url_grimmsr datalittleredrhpg11592.txt with openfile_url_grimms, r as f contents_grimms f.read start_string_grimmsL IT TL E R ED R ID IN G H OO Dnnn end_string_grimmsnnnnn TH E G OO SE G IR L text_grimmscontents_grimmscontents_grimms.find start_string_grimms contents_grimms.find end_string_grimms text_grimmstext_grimms.replacen, text_grimmsre.subrs, ,text_grimms text_grimms The preceding code generates the following output Figure 7 12 Text from the Watty Piper variation of Little Red Riding Hood 4  Insert a new cell, add the following code, and fetch the Perrault fairy tale version of Little Red Riding Hood file_url_perraultr datalittleredrhpg29021.txt with openfile_url_perrault, r as f contents_perrault f.read start_string_perraultLittle Red Riding Hoodnn end_string_perraultnn_ The Moral_ text_perraultcontents_perraultcontents_perrault.find start_string_perrault Key Input Parameters for Text Rank 335 contents_perrault.find end_string_perrault text_perraulttext_perrault.replacen, text_perraultre.subrs, ,text_perrault text_perrault The preceding code generates the following output Figure 7 13 Tales from the Perrault version of Little Red Riding Hood 5  Insert a new cell and add the following code to generate the two summaries with a ratio of 0 20 llrh_grimms_textranksummarizetext_grimms,ratio0 20 llrh_perrault_textranksummarizetext_perrault,ratio0 20 6  Insert a new cell and add the following code to print the Text Rank summary ratio of 0 20 of Grimms version of Little Red Riding Hood printllrh_grimms_textrank The preceding code generates the following output Figure 7 14 Output after implementing Text Rank on the Watty Piper variation 7  Insert a new cell and add the following code to print the Text Rank summary ratio of 0 20 of Perraults version of Little Red Riding Hood printllrh_perrault_textrank The preceding code generates the following output Figure 7 15 Output after implementing Text Rank on the Perrault version 336 Text Generation and Summarization 8  Add the following code to generate two summaries with a ratio of 0 5 llrh_grimms_textranksummarizetext_grimms,ratio0 5 llrh_perrault_textranksummarizetext_perrault,ratio0 5 9  Add the following code to print a Text Rank summary ratio of 0 5 of Pipers version of Little Red Riding Hood printllrh_grimms_textrank The preceding code generates the following output Figure 7 16 Output after implementing Text Rank on the Watty Piper variation 10  Add the following code to print a Text Rank summary ratio of 0 5 of Perraults version of Little Red Riding Hood printllrh_perrault_textrank The preceding code generates the following output Figure 7 17 Output after implementing Text Rank on the Perrault version Note To access the source code for this specific section, please refer to  You can also run this example online at  With this, we have found that the four summaries lack coherency and are also incomplete This is also true of the two summaries with a ratio of 0 5 that is, even when half of the sentences are extracted for the summary This might be because the conversations in the fairytale are contextual in nature, as a sentence often refers to the preceding sentences This contextual aspect of language makes N LP complex for machines Key Input Parameters for Text Rank 337 Interestingly, extractive summarization works much better for an O Henry short story such as After Twenty Years than it does for a childrens fairytale such as Little Red Riding Hood  Furthermore, this is not specific to the language used by a specific author, as we have explored with two different variations of this fairytale It seems a fairytale is unsuitable for extractive summarization Lets now do an activity in which well use the Text Rank algorithm to summarize complaints that customers have written against some organizations Activity 7 01 Summarizing Complaints in the Consumer Financial Protection Bureau Dataset The Consumer Financial Protection Bureau publishes consumer complaints made against organizations in the financial sector This original dataset is available at https To complete this activity, you will summarize a few complaints using Text Rank Note You can find the dataset to be used for this activity at E6f0 I5vda B4 WVm R6 TCgadl0previewConsumer_ Complaints.csv  To complete the activity, you will need to place the .csv file into the data folder for this chapter in your local directory Follow these steps to implement this activity 1  Import the summarization libraries and instantiate the summarization model 2  Load the dataset from a .csv file into a pandas Data Frame Drop all columns other than Product , Sub-product , Issue , Sub-issue , and Consumer complaint narrative 3  Select 12 complaints corresponding to the rows 242830 , 1086741 , 536367 , 957355 , 975181 , 483530 , 950006 , 865088 , 681842 , 536367 , 132345 , and 285894 from the 300 , 000 odd complaints with a narrative Note that since the dataset is an evolving dataset, the use of a version thats different from the one in the data folder could give different results because the input texts could be different 338 Text Generation and Summarization 4  Add a column with the Text Rank summary Each element of this column corresponds to a summary, using Text Rank, of the complaint narrative in the corresponding column Use a ratio of 0 20  Also, use a try except clause since the gensim implementation of the Text Rank algorithm throws exceptions with summaries that have very few sentences 5  Show the Data Frame You should get an output similar to the following figure Figure 7 18 Data Frame showing the summarized complaints Note The solution for this activity can be found via this link  Recent Developments in Text Generation and Summarization Alan Turing for whom the equivalent of the Nobel Prize in Computer Science is named proposed a test for artificial intelligence in 1950  This test, known as the Turing Test, says that if humans ask questions and cannot distinguish between text responses generated by a machine and a human, then that machine can be deemed to be intelligent Recent Developments in Text Generation and Summarization 339 Text generation using very large models, such as the G PT-2with around 1 5 billion parameters and B ER T Bidirectional Encoder Representation from Transformers with around 340 million parameters, can aid in auto completion tasks Auto- completion presents unique ethical challenges While it can offer convenience, it can also reinforce biases in the data This is accentuated by the fact that most user experience layouts can show only a limited number of options Furthermore, auto completion can controversially suggest responses that are different from what the sender originally wants to type Unfortunately, most use cases for text generation are negative use cases for generating spam and misinformation Given that the Turing Test may not be passed any time soon, we are clearly nowhere near considering text generation as a proxy for thought within a machine and there is no widely accepted benchmark for text generation Since late 2018 , with the invention of self attention, transformers, and B ER T, these approaches are generally considered the best way to teach a machine about some of the most challenging N LP tasks Self attention is a technique in which a word is combined with other words in its neighborhood by matrix multiplications Such multiplications are possible because of vector representations of words Using such combined representations for all the words in a sentence allows us to represent a sentence in a way that captures context This allows us to build much larger models that have a significantly higher capacity to learn A transformer is a combination of attention units and includes position information for each word, that is, multiple self attention layers and position information are used to capture the context better B ER T is a transformer that learns the sequential structure of a text in both directions, that is, from left to right and from right to left This is achieved by randomly masking the words while the model is trained, much like how children are often taught a language by using fill in the blanks exercises Such is the generalized learning of B ER T that it can be used even for translation related tasks, even though it has not been specifically taught translation as a task B ER T and other large models, such as G PT- 2 , require a huge computing infrastructure, which is generally not available to most people outside of leading universities and the biggest technology corporations Pre trained models fill the void in such cases The Text Rank algorithm considers each sentence to be a bag of words With the advent of B ER T, it is possible for us to have a superior sentence representation that captures meaning much better than the bag of words model 340 Text Generation and Summarization In the case of summarization, even though there is a benchmark called Recall- Oriented Understudy for Gisting Evaluation R OU GE , summarization is best evaluated qualitatively given that there is not only one correct way to summarize text In February 2020 , Microsofts Turing N LG model, which has 17 billion parameters, generated abstractive summaries for three examples, which were shared publicly However, the model is not publicly available currently and so the results cannot be reproduced Furthermore, we do not know how the Microsoft N LG model does with a nave test such as the Little Red Riding Hood test In general, extractive summarization of the kind discussed earlier in this chapter is by far the most useful for practitioners compared with the utility of the state of the art technology in text generation and paraphrasing Due to this, in the next section, well largely focus on practical challenges in extractive summarization Practical Challenges in Extractive Summarization Given the rapid pace of development in N LP, it is even more important to use compatible versions of the libraries that we use Evaluation of a documents suitability for extractive summarization can be undertaken manually Often, we would like to summarize multiple pieces of text, all of which could be short in length The Text Rank algorithm will not work well in such cases All unverified claims reported in this field ought to be taken with a grain of salt until the claim has been verified Such claims ought to be subjected by practitioners to nave tests such as the Little Red Riding test We can only use a model if it works and if the limitations related to scope and any biases are considered Summary In this chapter, we learned about text generation using Markov chains and extractive summarization using the Text Rank algorithm We also explored both the power and limitations of various advanced approaches In the next chapter, we will learn about sentiment analysis Overview This chapter introduces you to one of the most exciting applications of natural language processingthat is, sentiment analysis You will explore the various tools used to perform sentiment analysis, such as popular N LP libraries and deep learning frameworks You will then perform sentiment analysis on given text data using the powerful textblob library You will load textual data and perform preprocessing on it to fine tune the results of your sentiment analysis program By the end of the chapter, you will be able to train a sentiment analysis model Sentiment Analysis8344 Sentiment Analysis Introduction In the previous chapter, we looked at text generation, paraphrasing, and summarization, all of which can be immensely useful in helping us focus on only the essential and meaningful parts of the text corpus This, in turn, helps us to further refine the results of our N LP project In this chapter, we will look at sentiment analysis , which, as the name suggests, is the area of N LP that involves teaching computers how to identify the sentiment behind written content or parsed audiothat is, audio converted to text Adding this ability to automatically detect sentiment in large volumes of text and speech opens new possibilities for us to write useful software In sentiment analysis, we try to build models that detect how people feel This starts with determining what kind of feeling we want to detect Our application may attempt to determine the level of human emotion most often, whether a person is sad or happy satisfied or dissatisfied or interested or disinterested and so on The common thread here is that we measure how sentiments vary in different directions This is also called polarity Polarity signifies the emotions present in a sentence, such as joy or anger For example, I love oranges implies an emotionally positive statement, whereas I hate politics is a strong negative emotion Why Is Sentiment Analysis Required In machine learning projects, we try to build applications that work similarly to a human being We measure success in part by seeing how close our application is to matching human level performance Generally, machine learning programs cannot exceed human level performance by a significant marginespecially if our training data source is human generated Lets say that we want to carry out a sentiment analysis of product reviews The sentiment analysis program should detect how reviewers feel Obviously, it is impractical for a person to read thousands of movie reviews This is where automated sentiment analysis enters the picture Artificial intelligence is useful when it is impractical for people to perform the task In this case, the task is reading thousands of reviews Introduction 345 The Growth of Sentiment Analysis The field of sentiment analysis is driven by a few main factors Firstly, its driven by the rapid growth in online content thats used by companies to understand and respond to how people feel Secondly, since sentiment drives human decisions, businesses that understand their customers sentiments have a major advantage in predicting and shaping purchasing decisions Finally, N LP technology has improved significantly, allowing the much wider application of sentiment analysis The Monetization of Emotion The growth of the internet and internet services has enabled new business models to work with human connection, communication, and sentiment In January 2020 , Facebook had about 61 .3% of the social media traffic and has been one of the most successful social media platforms at connecting people across the world and providing features that enable users to express their thoughts and post memorable moments from their life online Similarly, although Twitter had just 14 .51% of the traffic, it has still proved to be an influential way to display sentiment online There are now large amounts of information on social media about what people like or dislike This data is of significant value not only in business but also in political campaigns This means that sentiment has significant business value and can be monetized Types of Sentiments There are various sentiments that we can try to detect in language sources Lets discuss a few of them in detail Emotion Sentiment analysis is often used to detect the emotional state of a person It checks whether the person is happy or sad, or content or discontent Businesses often use it to improve customer satisfaction For example, lets look at the following statement I thought I would have enjoyed the movie, but it left me feeling that it could have been better In this statement, it seems as though the person who has just watched a movie is unhappy about it A sentiment detector, in this case, would be able to classify the review as negative and allow the business the movie studio, for instance to adjust how they make movies in the future 346 Sentiment Analysis Action Orientation versus Passivity This is about whether a person is prone to action or not This is often used to determine how close a person is to making a choice For example, using a travel reservation chatbot, you can detect whether a person needs to make a reservation urgently or is simply making passive queries and is therefore less likely to book a ticket right now The level of action orientation or passivity provides additional clues to detect intention This can be used to make smart business decisions Tone Speech and text are often meant to convey certain impressions that are not necessarily factual and not entirely emotional Examples of this are sarcasm, irony, and humor This may provide useful additional information about how a person thinks Although tone is tricky to detect, there might be certain words or phrases that are often used in certain contexts We can use N LP algorithms to extract statistical patterns from document sources For example, we can use sentiment analysis to detect whether a news article is sarcastic Subjectivity versus Objectivity You may want to detect whether the given text source is subjective or objective For example, you might want to detect whether a person has issued and expressed an opinion, or whether their statement reads more like a fact and can only be true or false Lets look at the following two statements to get a better understanding Statement 1 The duck was overcooked, and I could hardly taste the flavor Statement 2 Ducks are aquatic birds In these two statements, statement 1 should be recognized as a subjective opinion and statement 2 as an objective fact Determining the objectivity of a statement helps us decide on the appropriate response to the statement Introduction 347 Key Ideas and Terms Lets look at some of the key ideas and terms that are used in sentiment analysis Classification As we learned in Chapter 3 , Developing a Text Classifier , classification is the N LP technique of assigning one or more classes to text documents This helps in separating and sorting the documents If you use classification for sentiment analysis, you assign different sentiment classes such as positive, negative, or neutral Sentiment analysis is a type of text classification that aims to create a classifier trained on a set of labeled pairs text and its corresponding sentiment label Upon training such a classifier on a large labeled dataset, the sentiment analysis model generalizes well and can classify unseen text into appropriate sentiment categories Supervised Learning As we have already seen, in supervised learning , we create a model by supplying data and labeled targets to the training algorithms The algorithms learn using this supply When it comes to sentiment analysis, we provide the training dataset with the labels that represent the sentiment For example, for each text in a dataset, we would assign a value of 1 if the sentiment is positive, and a value of 0 if the statement is negative Polarity Polarity is a measure of how negative or positive the sentiment is in a given language Polarity is used because it is simple and easy to measure and can be easily translated to a simple numeric scale It usually ranges between -1and 1  Values close to 1 reflect documents that have positive sentiments, whereas values close to 1reflect documents that have negative sentiments Values around 0 reflect documents that are neutral in sentiment Its worth noting that the polarity detected by a model depends on how it has been trained On political Reddit threads, the opinions tend to be highly polarized On the other hand, if you use the same model on business documents to measure sentiments, the scores tend to be neutral So, you need to choose models that are trained in similar domains Intensity In contrast to polarity, which is measured from negative to positive, intensity is measured in terms of arousal, which ranges from low to high Most often, the level of intensity is included in the sentiment score It is measured by looking at the closeness of the score to 0 or 1 348 Sentiment Analysis Applications of Sentiment Analysis There are various applications of sentiment analysis Financial Market Sentiment Financial markets operate partially on economic fundamentals but are also heavily influenced by human sentiment Stock market prices, which tend to rise and fall, are influenced by the opinions of news articles regarding the overall market or any specific securities Financial market sentiment helps measure the overall attitude of investors toward securities Market sentiment can be detected using news or social media articles We can use N LP algorithms to build models that detect market sentiment and use those models to predict future market prices Product Satisfaction Sentiment analysis is commonly used to determine how customers feel about products and services For example, Amazon makes use of its extensive product reviews dataset This not only helps to improve its products and services but also acts as a source of training data for its sentiment analysis services Social Media Sentiment A really useful area of focus for sentiment analysis is social media monitoring Social media has become a key communication medium with which most people around the world interact every day, and so there is a large and growing source of human language data available there More importantly, the need for businesses and organizations to be able to process and understand what people are saying on social media has only increased This has led to an exponential growth in demand for sentiment analysis services Brand Monitoring A companys brand is a significant asset and companies spend a lot of time, effort, and money maintaining their brand value With the growth of social media, companies are now exposed to considerable potential brand risks from negative social media conversations On the other hand, there is also the potential for positive brand growth from positive interactions and messages on social media For this reason, businesses deploy people to monitor what is said about them and their brands on social media Automated sentiment analysis makes this significantly easier and also more efficient Tools Used for Sentiment Analysis 349 Customer Interaction Organizations often want to know how their customers feel during an interaction in an online chat or a phone conversation In such cases, the objective is to detect the level of satisfaction with the service or the products Sentiment analysis tools help companies handle large volumes of text and voice data that are generated during customer interaction Every company, irrespective of the domain, wants to utilize the data at their disposal to glean valuable insights, as there is potential revenue to be had if companies can gain insights into customer satisfaction Tools Used for Sentiment Analysis There are a lot of tools capable of analyzing sentiment Each tool has its advantages and disadvantages We will look at each of them in detail N LP Services from Major Cloud Providers Online sentiment analysis is carried out by all major cloud services providers, such as Amazon, Microsoft, Google, and I BM You can usually find sentiment analysis as a part of their text analysis services or general machine learning services Online services offer the convenience of packaging all the necessary algorithms behind the providers A PI These algorithms are capable of performing sentiment analysis To use such services, you need to provide the text or audio sources, and in return, the services will provide you with a measure of the sentiment These services usually return a standard, simple score, such as positive, negative, or neutral The score usually ranges between 0 and 1  The following are the advantages and disadvantages of N LP services from major cloud providers Advantages You require almost no knowledge of N LP algorithms or sentiment analysis This results in fewer staffing needs Sentiment analysis services provide their own computation, reducing your own computational infrastructure needs Online services can scale well beyond what regular companies can do on their own You gain the benefits of automatic improvements and updates to sentiment analysis algorithms and data 350 Sentiment Analysis Disadvantages Online services requireat least temporarilya reduction in privacy since you must provide the documents to be analyzed by the service Depending on your projects privacy needs, this may or may not be acceptable There might also be laws that restrict data crossing into another national jurisdiction The service provided by cloud providers is like one solution fits all and is considered very generic, so it will not necessarily apply to niche use cases Online Marketplaces Recently, A I marketplaces have emerged that offer different algorithms from third parties Online marketplaces differ from cloud providers An online marketplace allows third party developers to deploy sentiment analysis services on their platform Here are the advantages and disadvantages of online marketplaces Advantages A I marketplaces provide the flexibility of choosing between different sentiment analysis algorithms instead of just one algorithm This enables users to try out different techniques and see which one fits their business needs the best Using algorithms from an A I marketplace reduces the need for dedicated data scientists for your project Disadvantages Algorithms from third parties are of varying quality Since the algorithms are provided by smaller companies, there is no guarantee that they will not disappear And for businesses, this is a big risk since their solution has a direct dependency on a third party that is outside their control Python N LP Libraries There are a few N LP libraries that need to be integrated into your project instead of being called upon as services These are called dedicated N LP libraries and they usually include many N LP algorithms from academic research Sophisticated N LP libraries used across the industry are spa Cy, gensim, and Allen NL P Here are the advantages and disadvantages of Python N LP librariesTools Used for Sentiment Analysis 351 Advantages Its usually state of the art research that goes into these libraries, and they usually have well chosen datasets They provide a framework that makes it much easier to build projects and do rapid experiments They offer out of the box abstractions that are required for all N LP projects, such as Token and Span They are easy to scale to real world deployment Disadvantages This will not be considered a true disadvantage since libraries are meant to be general purpose, but for complex use cases, developers would have to write their own implementations as required Deep Learning Frameworks Deep learning libraries such as Py Torch and Tensor Flow are meant to be used to build complex models for a wide range of applications, not limited to just N LP These libraries provide you with more advanced algorithms and mathematical functions, helping you develop powerful and complex models The advantages and disadvantages of these frameworks are explained here Advantages You have the flexibility to develop your sentiment analysis model to meet complex business needs You can integrate the latest and the most advanced algorithms when they are available in general purpose libraries You can make use of transfer learning, which takes a model trained on a large text source, to fine tune the training as per your projects needs This allows you to create a sentiment analysis model that is more suitable for your needs 352 Sentiment Analysis Disadvantages This approach requires you to have in depth knowledge of machine learning and complex topics such as deep learning Deep learning libraries require a large volume of rich annotated datasets along with an intense computational infrastructure to train and experiment with different modeling techniques to get a generalized model thats fit to be deployed in production So, there is a requirement for training on non C PU hardware such as G PUsT PUs Now that we have learned about the various tools available for sentiment analysis, lets explore the most popular Python libraries The textblob library textblob is a Python library used for N LP, as we have seen in the previous chapters It has a simple A PI and is probably the easiest way to begin with sentiment analysis textblob is built on top of the N LT K library but is much easier to use In the following sections, we will do an exercise and an activity to get a better understanding of how we can use textblob for sentiment analysis Exercise 8 01 Basic Sentiment Analysis Using the textblob Library In this exercise, we will perform sentiment analysis on a given text For this, we will be using the Text Blob class of the textblob library Follow these steps to complete this exercise 1  Open a Jupyter notebook 2  Insert a new cell and add the following code to implement to import the Text Blob class from the textblob library from textblob import Text Blob 3  Create a variable named sentence and assign it a string Insert a new cell and add the following code to implement this sentence but you are Late Flight again Again and again Where are the crew The textblob library 353 4  Create an object of the Text Blob class Add sentence as a parameter to the Text Blob container Insert a new cell and add the following code to implement this blob Text Blobsentence 5  In order to view the details of the blob object, insert a new cell and add the following code printblob The code generates the following output but you are Late Flight again Again and again Where are the crew 6  To use the sentiment property of the Text Blob class which returns a tuple, insert a new cell and add the following code blob.sentiment The code generates the following output Sentimentpolarity0 5859375 , subjectivity0 6 Note To access the source code for this specific section, please refer to  You can also run this example online at  In the code, we can see the polarity and subjectivity scores for a given text The output indicates a polarity score of - 0 5859375 , which means that negative sentiment has been detected in the text The subjectivity score means that the text is somewhat on the subjective side, though not entirely subjective We have performed sentiment analysis on a given text using the textblob library In the next section, we will perform sentiment analysis on tweets about airlines 354 Sentiment Analysis Activity 8 01 Tweet Sentiment Analysis Using the textblob library In this activity, you will perform sentiment analysis on tweets related to airlines You will also be providing condition for determining positive, negative, and neutral tweets, using the textblob library Note You can find the data to be used for this activity here  Follow these steps to implement this activity 1  Import the necessary libraries 2  Load the C SV file 3  Fetch the text column from the Data Frame 4  Extract and remove the handles from the fetched data 5  Perform sentiment analysis and get the new Data Frame 6  Join both the Data Frames 7  Apply the appropriate conditions and view positive, negative, and neutral tweets After executing those steps, the output for positive tweets should be as follows Figure 8 1 Positive tweets As you can see from the preceding output, the Polarity column shows a positive integer This implies that the tweet displays positive sentiment The Subjectivity column indicates that most tweets are found to be of a subjective nature The textblob library 355 The output for negative tweets is as follows Figure 8 2 Negative tweets The preceding output shows a Polarity column with a negative integer, implying that the tweet displays negative sentiment, while the Subjectivity column shows a positive integer, which implies the same as beforepersonal opinion or feeling The output for neutral tweets should be as follows Figure 8 3 Neutral tweets 356 Sentiment Analysis The preceding output has a Polarity column and a Subjectivity column with a zero or almost zero value This implies the tweet has neither positive nor negative sentiment, but neutral moreover, no subjectivity is detected for these tweets Note The solution for this activity can be found via this link  In the next section, we will explore more about performing sentiment analysis using online web services Understanding Data for Sentiment Analysis Sentiment analysis is a type of text classification  Sentiment analysis models are usually trained using supervised datasets  Supervised datasets are a kind of dataset that is labeled with the target variable, usually a column that specifies the sentiment value in the text This is the value we want to predict in the unseen text Exercise 8 02 Loading Data for Sentiment Analysis In this exercise, we will load data that could be used to train a sentiment analysis model For this exercise, we will be using three datasetsnamely Amazon, Yelp, and I MDb Note You can find the data being used in this exercise here  Follow these steps to implement this exercise 1  Open a Jupyter notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd pd.set_optiondisplay.max_colwidth, 200 This imports the pandas library It also sets the display width to 200 characters so that more of the review text is displayed on the screen Understanding Data for Sentiment Analysis 357 3  To specify where the sentiment data is located, first load three different datasets from Yelp, I MDb, and Amazon Insert a new cell and add the following code to implement this D AT A_ DI R datasentiment_labelled_sentences I MD B_ DA TA_ FI LE D AT A_ DI R imdb_labelled.txt Y EL P_ DA TA_ FI LE D AT A_ DI R yelp_labelled.txt A MA ZO N_ DA TA_ FI LE D AT A_ DI R amazon_cells_labelled.txt C OL UM N_ NA ME S Each of the data files has two columns one for the review text and a numeric column for the sentiment 4  To load the I MDb reviews, insert a new cell and add the following code imdb_reviews pd.read_tableI MD B_ DA TA_ FI LE, namesC OL UM N_ NA ME S In this code, the read_table method loads the file into a Data Frame 5  Display the top 10 records in the Data Frame Add the following code in the new cell imdb_reviews.head10 The code generates the following output Figure 8 4 The first few records in the I MDb movie review file In the preceding figure, you can see that the negative reviews have sentiment scores of 0 and positive reviews have sentiment scores of 1 358 Sentiment Analysis 6  To check the total number of records of the I MDb review file, use the value_ counts function Add the following code in a new cell to implement this imdb_reviews Sentiment.value_counts The expected output with total reviews should be as follows 1 386 0 362 Name Sentiment, dtype int64 In the preceding figure, you can see that the data file contains a total of 748 reviews, out of which 362 are negative and 386 are positive 7  Format the data by adding the following code in a new cell imdb_counts imdb_reviews Sentiment.value_counts.to_frame imdb_counts.index pd Series imdb_counts The code generates the following output Figure 8 5 Counts of positive and negative sentiments in the I MDb review file We called value_counts , created a Data Frame, and assigned Positive and Negative as index labels 8  To load the Amazon reviews, insert a new cell and add the following code amazon_reviews pd.read_tableA MA ZO N_ DA TA_ FI LE, namesC OL UM N_ NA ME S amazon_reviews.head10 Understanding Data for Sentiment Analysis 359 The code generates the following output Figure 8 6 Reviews from the Amazon dataset 9  To load the Yelp reviews, insert a new cell and add the following code yelp_reviews pd.read_tableY EL P_ DA TA_ FI LE, namesC OL UM N_ NA ME S yelp_reviews.head10 The code generates the following output Figure 8 7 Reviews from the Yelp dataset 360 Sentiment Analysis Note To access the source code for this specific section, please refer to  You can also run this example online at  We have learned how to load data that could be used to train a sentiment analysis model The review files mentioned in this exercise are an example Each file contains review text, plus a sentiment label for each This is the minimum requirement of a supervised machine learning project to build a model that is capable of predicting sentiments However, the review text cannot be used as is it needs to be preprocessed so that we can extract feature vectors out of it and eventually provide it as input to the model Now that we have learned about loading the data, in the next section, we will focus on training sentiment models Training Sentiment Models The end product of any sentiment analysis project is a sentiment model  This is an object containing a stored representation of the data on which it was trained Such a model has the ability to predict sentiment values for text that it has not seen before To develop a sentiment analysis model, the following steps should be taken 1  The document dataset must be split into train and test datasets The test dataset is normally a fraction of the overall dataset It is usually between 5% and 40% of the overall dataset, depending on the total number of examples available If the amount of data is too large, then a smaller test dataset can be used 2  Next, the text should be preprocessed by stripping unwanted characters, removing stop words, and performing other common preprocessing steps 3  The text should be converted to numeric vector representations in order to extract the features These representations are used for training machine learning models 4  Once we have the vector representations, we can train the model This will be specific to the type of algorithm being used During the training, our model will use the test dataset as a guide to learn about the text 5  We can then use the model to predict the sentiment of documents that it has not seen before This is the step that will be performed in production Training Sentiment Models 361 In the next section, we will train a sentiment model Well make use of the Tfidf Vectorizer and Logistic Regression classes, which we explored in one of the previous chapters Activity 8 02 Training a Sentiment Model Using T FI DF and Logistic Regression To complete this activity, you will build a sentiment analysis model using the Amazon, Yelp, and I MDb datasets that you used in the previous exercise Use the T FI DF method to extract features from the text and use logistic regression for the learning algorithm The following steps will help you complete this activity 1  Open a Jupyter notebook 2  Import the necessary libraries 3  Load the Amazon, Yelp, and I MDb datasets 4  Concatenate the datasets and take out a random sample of 10 items 5  Create a function for preprocessing the text, that is, convert the words into lowercase and normalize them 6  Apply the function created in the previous step on the dataset 7  Use Tfidf Vectorizer to convert the review text into T FI DF vectors and use the Logistic Regression class to create a model that uses logistic regression for the model These should be combined into a Pipeline object 8  Now split the data into train and test sets, using 70% to train the data and 30% to test the data 9  Use the fit function to fit the training data on the pipeline 10  Print the accuracy score 11  Test the model on these sentences I loved this place and I hated this place  Note The solution for this activity can be found via this link 362 Sentiment Analysis Summary We started our journey into N LP with basic text analytics and text preprocessing techniques, such as tokenization, stemming, lemmatization, and lowercase conversion, to name a few We then explored ways in which we can represent our text data in numerical form so that it can be understood by machines in order to implement various algorithms After getting some practical knowledge of topic modeling, we moved on to text vectorization, and finally, in this chapter, we explored various applications of sentiment analysis This included different tools that use sentiment analysis, from technologies available from online marketplaces to deep learning frameworks More importantly, we learned how to load data and train our model to use it to predict sentiment Appendix366 Appendix Chapter 1 Introduction to Natural Language Processing Activity 1 01 Preprocessing of Raw Text Solution Lets perform preprocessing on a text corpus To complete this activity, follow these steps 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries from nltk import download downloadstopwords downloadwordnet nltk.downloadpunkt downloadaveraged_perceptron_tagger from nltk import word_tokenize from nltk.stem.wordnet import Word Net Lemmatizer from nltk.corpus import stopwords from autocorrect import Speller from nltk.wsd import lesk from nltk.tokenize import sent_tokenize from nltk import stem, pos_tag import string 3  Read the content of file.txt and store it in a variable named sentence  Insert a new cell and add the following code to implement this load the text file into variable called sentence sentence open datafile.txt, r.read 4  Apply tokenization on the given text corpus Insert a new cell and add the following code to implement this words word_tokenizesentenceChapter 1 Introduction to Natural Language Processing 367 5  To print the list of tokens, insert a new cell and add the following code printwords This code generates the following output The, reader, of, this, course, should, have, a, basic, knowledge, of, the, Python, programming, lenguage, ., Heshe, must, have, knowldge In the preceding figure, we can see the initial 20 tokens of our text corpus 6  To perform spelling correction in our given text corpus, loop through each token and correct the tokens that are wrongly spelled Insert a new cell and add the following code to implement this spell Spellerlangen def correct_sentencewords corrected_sentence corrected_word_list for wd in words if wd not in string.punctuation wd_c spellwd if wd_c wd printwd has been corrected to wd_c corrected_sentence corrected_sentence wd_c corrected_word_list.appendwd_c else corrected_sentence corrected_sentence wd corrected_word_list.appendwd else corrected_sentence corrected_sentence wd corrected_word_list.appendwd return corrected_sentence, corrected_word_list corrected_sentence, corrected_word_list correct_sentencewords368 Appendix This code generates the following output lenguage has been corrected to language knowldge has been corrected to knowledge Familiarity has been corrected familiarity 7  To print the corrected text corpus, add a new cell and type the following code corrected_sentence This code generates the following output The reader of this course should have a basic knowledge of the Python programming language Heshe must have knowledge of data types in Python He should be able to write functions, and also have the ability to import and use libraries and packages in Python familiarity with basic linguistics and probability is assumed although not required to fully complete this course 8  To print a list of the initial 20 tokens of the corrected words, insert a new cell and add the following code printcorrected_word_list This code generates the following output The, reader, of, this, course, should, have, a, basic, knowledge, of, the, Python, programming, language,  , Heshe, must, have, knowledge 9  To add a Po S tag to all the corrected words in the list, insert a new cell and add the following code printpos_tagcorrected_word_list This code generates the following output Figure 1 5 List of corrected words tagged with appropriate Po S Chapter 1 Introduction to Natural Language Processing 369 10  To remove the stop words, insert a new cell and add the following code stop_words stopwords.wordsenglish def remove_stop_wordsword_list corrected_word_list_without_stopwords for wd in word_list if wd not in stop_words corrected_word_list_without_stopwords.appendwd return corrected_word_list_without_stopwords corrected_word_list_without_stopwords remove_stop_words corrected_word_list corrected_word_list_without_stopwords This code generates the following output Figure 1 6 List excluding the stop words In the preceding figure, we can see that the stop words have been removed and a new list has been returned 370 Appendix 11  Apply the stemming process, and then insert a new cell and add the following code stemmer stem Porter Stemmer def get_stemsword_list corrected_word_list_without_stopwords_stemmed for wd in word_list corrected_word_list_without_stopwords_stemmed .appendstemmer.stemwd return corrected_word_list_without_stopwords_stemmed corrected_word_list_without_stopwords_stemmed get_stemscorrected_word_list_without_stopwords corrected_word_list_without_stopwords_stemmed This code generates the following output Figure 1 7 List of stemmed words In the preceding code, we looped through each of the words in the corrected_word_list_without_stopwords list and applied stemming to them The preceding figure shows the list of the initial 20 stemmed words Chapter 1 Introduction to Natural Language Processing 371 12  To apply the lemmatization process to the corrected word list, insert a new cell and add the following code lemmatizer Word Net Lemmatizer def get_lemmaword_list corrected_word_list_without_stopwords_lemmatized for wd in word_list corrected_word_list_without_stopwords_lemmatized .appendlemmatizer.lemmatizewd return corrected_word_list_without_stopwords_lemmatized corrected_word_list_without_stopwords_lemmatized get_lemmacorrected_word_list_without_stopwords_stemmed corrected_word_list_without_stopwords_lemmatized This code generates the following output Figure 1 8 List of lemmatized words 372 Appendix In the preceding code, we looped through each of the words in the corrected_word_list_without_stopwords list and applied lemmatization to them The preceding figure shows a list of the initial 20 lemmatized words 13  To detect the sentence boundary in the given text corpus, use the sent_ tokenize method Insert a new cell and add the following code to implement this printsent_tokenizecorrected_sentence This code generates the following output The reader of this course should have a basic knowledge of the Python programming language., Heshe must have knowledge of data types in Python., He should be able to write functions and also have the ability to import and use libraries and packages in Python., familiarity with basic linguistics and probability is assumed although not required to fully complete this course Note To access the source code for this specific section, please refer to  You can also run this example online at  Chapter 2 Feature Extraction Methods 373 Chapter 2 Feature Extraction Methods Activity 2 01 Extracting Top Keywords from the News Article Solution The following steps will help you complete this Activity 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries and download the data import operator from nltk.tokenize import Whitespace Tokenizer from nltk import download, stem The below statement will download the stop word list nltk_datacorporastopwords at home directory of your computer downloadstopwords from nltk.corpus import stopwords The download statement will download the stop word list at nltk_data corporastopwords into your systems home directory 3  Create the different types of methods to perform various N LP tasks Activity 2 01 .ipynb def load_filefile_path news .join return news This method will take string as input and return the string converted into lowercase def to_lower_casetext return text.lower This will take a text string as input and return the token wht Whitespace Tokenizer def tokenize_texttext return wht.tokenizetexttext The full code snippet can be found at Appendix The load_file function will take the file path as input and return the content of the file as a string The lower_case function will take a string as an argument and convert it into lowercase Next, the tokenize_text function will tokenize the string into its constituent tokens The get_stem method will perform stemming on the tokens, while get_freq will calculate the frequency of the tokens Finally, get_top_n_words will return the n tokens with the highest frequency 4  Load a text file into a string using the load_file method path datanews_article.txt news_article load_filepath 5  Convert the text into lowercase using the to_lower_case method lower_case_news_art to_lower_casetextnews_article 6  Tokenize the text with the tokenize_text method using the following line of code tokens tokenize_textlower_case_news_art 7  Remove the stop words from the list add the following code to do this removed_tokens remove_stop_wordstokens 8  Perform stemming on the words using the get_stems method stems get_stemsremoved_tokens 9  Now, calculate the frequency of stemmed tokens with the get_freq method freq_dict get_freqstems 10  To get the top six most frequently used words in the news article, use the following code top_keywords get_top_n_wordsfreq_dict, 6 top_keywords The preceding line of code will generate the following output Chapter 2 Feature Extraction Methods 375 Thus, we have extracted the top six keywords from the news article, which can give us an idea of what the article is about However, in this example, we have extracted only unigrams For a more comprehensive output, bigrams and trigrams are often more useful So, for even better results, you can perform the preceding activity on bigrams and trigrams Note To access the source code for this specific section, please refer to  You can also run this example online at  Activity 2 02 Text Visualization Solution 1  Open a Jupyter Notebook Insert a new cell and add the following code to import the necessary libraries from wordcloud import Word Cloud, S TO PW OR DS import matplotlib.pyplot as plt %matplotlib inline from nltk import word_tokenize from nltk.stem import Word Net Lemmatizer import nltk nltk.downloadpunkt from collections import Counter import re import matplotlib as mpl mpl.rc Params 300 2  To fetch the dataset and read its content, add the following code text open datatext_corpus.txt, r, encodingutf- 8 .read text376 Appendix The preceding code generates the following output Figure 2 31 Text corpus 3  The text in the fetched data is not clean In order to clean it, we need to make use of various preprocessing steps, such as tokenization and lemmatization Add the following code to implement this def lemmatize_and_cleantext nltk.downloadwordnet lemmatizer Word Net Lemmatizer cleaned_lemmatized_tokens lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , text return cleaned_lemmatized_tokens 4  To check the set of unique words, along with their frequencies, as well as to find the 50 most frequently occurring words, add the following code Counterlemmatize_and_cleantext.most_common50 Chapter 2 Feature Extraction Methods 377 The preceding code generates the following output Figure 2 32 The 50 most frequent words 378 Appendix 5  Once you get the set of unique words along with their frequencies, remove the stop words Then, generate the word cloud for the top 50 most frequent words Add the following code to implement this stopwords setS TO PW OR DS cleaned_text .joinlemmatize_and_cleantext wordcloud Word Cloudwidth 800 , height 800 , background_color white, max_words50, stopwords stopwords, min_font_size 10 .generatecleaned_text plt.imshowwordcloud, interpolationbilinear plt.axisoff plt.show The preceding code generates the following output Figure 2 33 Word cloud representation of the 50 most frequent words Chapter 2 Feature Extraction Methods 379 As shown in the preceding image, words that occur more frequently, such as unbeaten, final, and wicket, appear in larger sizes in the word cloud Note To access the source code for this specific section, please refer to  You can also run this example online at 380 Appendix Chapter 3 Developing a Text Classifier Activity 3 01 Developing End to End Text Classifiers Solution The following steps will help you implement this activity 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary packages import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem import Word Net Lemmatizer from sklearn.feature_extraction.text import Tfidf Vectorizer from sklearn.model_selection import train_test_split import nltk nltk.downloadstopwords nltk.downloadpunkt nltk.downloadwordnet import warnings import string import re warnings.filterwarningsignore from sklearn.metrics import accuracy_score, roc_curve, classification_report, confusion_matrix, precision_recall_curve, auc 3  Read a data file It has three columns is_political , headline , and short_description  The headline column contains various news headlines, the short_description column contains an abstract of the article, and the is_political column indicates whether the article is about politics or not Here, label 0 denotes that a headline is not political and label 1 denotes that the headline is political Here, we will only use the short_ description column to train our model Add the following code to do this data pd.read_csvdatanews_political_dataset.csv data.sample5Chapter 3 Developing a Text Classifier 381 The preceding code generates the following output Figure 3 67 Text data and labels stored as a Data Frame 4  Create a generic function for all the classifiers called clf_model  It takes four inputs the type of model, the features of the training dataset, the labels of the training dataset, and the features of the validation dataset It returns predicted labels, predicted probabilities, and the model it has been trained on Add the following code to do this def clf_modelmodel_type, X_train, y_train, X_valid model model_type.fitX_train,y_train predicted_labels model.predictX_valid predicted_probab model.predict_probaX_valid return 5  Furthermore, another function is defined, called model_evaluation  It takes three inputs actual values, predicted values, and predicted probabilities It prints a confusion matrix, accuracy, f1 score, precision, recall scores, and the A UR OC curve It also plots the R OC curve def model_evaluationactual_values, predicted_values, predicted_probabilities cfn_mat confusion_matrixactual_values,predicted_values printconfusion matrix n,cfn_mat printnaccuracy ,accuracy_score actual_values,predicted_values printnclassification report n, classification_reportactual_values,predicted_values fpr,tpr,thresholdroc_curveactual_values, predicted_probabilities printn Area under R OC curve for validation set, aucfpr,tpr fig, ax plt.subplotsfigsize6, 6 382 Appendix ax.plotfpr,tpr,labelValidation set A UC plt.xlabelFalse Positive Rate plt.ylabelTrue Positive Rate ax.legendlocbest plt.show 6  Use a lambda function to extract tokens from each text in this Data Frame called data, check whether any of these tokens are stop words, lemmatize them, and then concatenate them side by side Use the join function to concatenate a list of words into a single sentence After that, use the regular expression method re to replace anything other than letters, digits, and whitespaces with blank spaces Add the following code to implement this lemmatizer Word Net Lemmatizer stop_words stopwords.wordsenglish stop_words stop_words liststring.printable data data .applylambda x .join lemmatizer.lemmatize word.lower for word in word_tokenize re.subr_, , strx if word.lower not in stop_words 7  Create a T FI DF matrix representation of these cleaned texts Add the following code to do this M AX_ FE AT UR ES 200 tfidf_model Tfidf Vectorizermax_featuresM AX_ FE AT UR ES tfidf_df pd Data Frametfidf_model.fit_transform data.todense tfidf_df.columns sortedtfidf_model.vocabulary_ tfidf_df.headChapter 3 Developing a Text Classifier 383 The preceding code generates the following output Figure 3 68 T FI DF representation of the Data Frame 8  Use sklearns train_test_split function to divide the dataset into training and validation sets Add the following code to do this X_train, X_valid, y_train, y_valid train_test_splittfidf_df, data, test_size0 2 , random_state42,stratify data 9  Train an X GBoost model using the X GB Classifier function and evaluate it for the validation set Add the following code to do this pip install xgboost from xgboost import X GB Classifier xgb_clfX GB Classifiern_estimators10,learning_rate0 05 , max_depth18,subsample0 6 , colsample_bytree 0 6 , reg_alpha 10 ,seed42 results clf_modelxgb_clf, X_train, y_train, X_valid model_evaluationy_valid, results, results model_xgb results 384 Appendix The preceding code generates the following output Figure 3 69 Performance of the X GBoost model Chapter 3 Developing a Text Classifier 385 10  Extract the importance of features, that is, tokens or words that play a vital role in determining the type of content Add the following code to do this word_importances pd Data Frame wordX_train.columns, importancemodel_xgb.feature_importances_ word_importances.sort_valuesimportance, ascending False.head4 The preceding code generates the following output Figure 3 70 Words and their importance Note To access the source code for this specific section, please refer to  You can also run this example online at 386 Appendix Chapter 4 Collecting Text Data with Web Scraping and A PIs Activity 4 01 Extracting Information from an Online H TM L Page Solution Lets extract the data from an online source and analyze it Follow these steps to implement this activity 1  Open a Jupyter Notebook 2  Import the requests and Beautiful Soup libraries Pass the U RL to requests with the following command Convert the fetched content into H TM L format using the Beautiful Soup H TM L parser Add the following code to do this import requests from bs4 import Beautiful Soup r requests .get r.status_code soup Beautiful Soupr.text, html.parser 3  To extract the list of headings, see which H TM L elements belong to each bold headline in the Works section You can see that they belong to the h3 tag We only need the first six headings here Look for a span tag that has a class attribute with the following set of commands for element in soup.find_allh3, limit6 spans element.findspan, attrsclassmw headline printspans The preceding code generates the following output Drama Short_stories Novels Poetry Songs_Rabindra_ Sangeet Art_works Chapter 4 Collecting Text Data with Web Scraping and A PIs 387 4  To extract information regarding the original list of works written in Bengali by Tagore, look for the table tag Traverse through the table and use select to pick table rows tr from following table data td associated with it Add the following code to extract the text table soup.findtable, attrsclasswikitable for row in table.selecttr td printrow.text The preceding code generates the following output Figure 4 16 List of Tagores work 388 Appendix 5  To extract the list of universities named after Tagore, look for the ol tag Add the following code to do this The preceding code generates the following output Figure 4 17 List of universities named after Rabindranath Tagore Note To access the source code for this specific section, please refer to  You can also run this example online at  Activity 4 02 Extracting and Analyzing Data Using Regular Expressions Solution Follow these steps to complete this activity 1  Collect the data using the requests package with the following code import requests from bs4 import Beautiful Soup r requests.get r.status_code The preceding code generates the following output 200 Chapter 4 Collecting Text Data with Web Scraping and A PIs 389 2  Convert the fetched content into H TM L format using Beautiful Soup s H TM L parser soup Beautiful Soupr.text, html.parser 3  Inspect the H TM L tag of the Packt website F AQs page You can extract the question text by first searching for the div tag with the classtab attribute and inside that element, find the label tag to get the question text Similarly, to get the answer text, find the div tag with classtab content , as shown here qas for each in soup.find_alldiv, attrsclasstab question each.findlabel answer each.finddiv, attrsclasstab content qas.appendquestion.text, answer.text printqas The preceding code generates the following output What format are Packt e Books, n Packt e Books can be downloaded as a P DF, E PU B or M OB I file They can also be viewed online using your subscription.n 4  Create a Data Frame consisting of these questions and answers import pandas as pd pd Data Frameqas, columns.head The preceding code generates the following output Figure 4 18 Data Frame of the question and answers 390 Appendix 5  To extract email addresses, make use of a regular expression Insert a new cell and add the following code to implement this tc_page_r requests .get packtterms and conditions tc_page_r.status_code soup2 Beautiful Souptc_page_r.text, html.parser import re setre.findall r 2 , 4 , soup2.text Here, the regular expression pattern will be looking for an alphanumeric blob, followed by the sign, followed by an alphanumeric blob Next, it will look for a dot  followed by a 2 to 4 character suffix for domains cominorg  The preceding code generates the following output customercarepackt.com, subscription.supportpackt.com 6  To extract phone numbers using a regular expression, insert a new cell and add the following code re.findallrd2s10sd3sd3sd3,soup2.text The preceding code generates the following output Note To access the source code for this specific section, please refer to  You can also run this example online at  Chapter 4 Collecting Text Data with Web Scraping and A PIs 391 Activity 4 03 Extracting Data from Twitter Solution Lets extract tweets using the tweepy library Follow these steps to implement this activity 1  Log in to your Twitter account with your credentials 2  Visit , fill in the necessary details, and submit the form 3  Once the form is submitted, go to the Keys and tokens tab copy consumer_key , consumer_secret , access_token , and access_token_secret from there 4  Open a Jupyter Notebook 5  Import the relevant packages and follow the authentication steps by writing the following code consumer_key your_consumer_key consumer_secret your_consumer_secret access_token your_access_token access_token_secret your_access_token_secret import pandas as pd import json from pprint import pprint import tweepy auth tweepy O Auth Handlerconsumer_key, consumer_secret auth.set_access_tokenaccess_token, access_token_secret api tweepy A PIauth392 Appendix 6  Call the Twitter A PI with the climatechange search query Insert a new cell and add the following code to implement this tweet_list for tweet in tweepy Cursorapi.search, qclimatechange, langen.items100 tweet_list.appendtweet lentweet_list tweet_list The preceding code generates an output that should look similar to the following screenshot The content will vary since the tweets will be different according to when you are running the program Figure 4 19 The Twitter A PI called with the climatechange search query 7  Each tweepy Status object will have a json object associated with it, which will have tweet content and meta info Lets see what information is present status tweet_list status_json status._json pprintstatus_json Chapter 4 Collecting Text Data with Web Scraping and A PIs 393 The preceding code generates the following output with different tweet content fetched at the time of running the program Figure 4 20 Twitter status objects converted to J SO N objects 8  To check the tweet text, use the following code status_json Again, though the content may vary, the preceding code generates output similar to the following The latest The Passivhaus Daily Thanks to The Markof Polo Peter Gleick boris_kapkov passivehouse climatechange 9  To create a Data Frame consisting of the text of tweets, add a new cell and write the following code tweets for twt in tweet_list tweets.appendtwt._json tweet_text_df pd Data Frametweet_text tweets tweet_text_df.head 394 Appendix The preceding code generates the following output Again, the content may vary depending on the current tweets Figure 4 21 Data Frame with the text of tweets Note To access the source code for this specific section, please refer to  This section does not currently have an online interactive example and will need to be run locally Chapter 5 Topic Modeling 395 Chapter 5 Topic Modeling Activity 5 01 Topic Modeling Jeopardy Questions Solution Lets perform topic modeling on the dataset of Jeopardy questions 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import pandas and other libraries import numpy as np import spacy nlp spacy.loaden_core_web_sm import pandas as pd pd.set_optiondisplay.max_colwidth, 800 3  After downloading the data, you can extract it and place at the location below Then load the Jeopardy C SV file into a pandas Data Frame Insert a new cell and add the following code J EO PA RD Y_ CS V datajeopardyJeopardy.csv questions pd.read_csvJ EO PA RD Y_ CS V questions.columns 4  The data in the Data Frame is not clean In order to clean it, remove records that have missing values in the Question column Add the following code to do this questions questions.dropnasubset 5  Find the number of unique categories Add the following code to do this questions.nunique The code generates the following output 27995396 Appendix 6  Sample 4% of the questions and tokenize the corpus where the tokens are classified as N OU N by spa Cy file dataJ Questions.txt questions.samplefrac0 04 ,replaceFalse, random_state0.to_csvfile fopenfile,r,encodingutf- 8 textf.read f.close docnlptext pos_list preproc_text preproc_sent for token in doc if token.textn if nottoken.is_stop and nottoken.is_punct and token.pos_ in pos_list preproc_sent.appendtoken.lemma_ else preproc_text.appendpreproc_sent preproc_sent preproc_text.appendpreproc_sent last sentence printpreproc_text The code generates output like the following Figure 5 22 Tokenized corpus after selecting 4% of the sample Chapter 5 Topic Modeling 397 7  Train a tomotopy L DA model with 1 , 000 topics Print a few topics Add the following code to do this import tomotopy as tp N UM_ TO PI CS1000 mdl tp L DA ModelkN UM_ TO PI CS,seed1234 for line in preproc_text mdl.add_docline mdl.train10 for k in rangemdl.k printTop 7 words of topic .formatk printmdl.get_topic_wordsk, top_n7 The code generates the following output Figure 5 23 Topics inferred after training the L DA model 8  Now print the log perplexity Add the following code to do this printLog perplexity,mdl.ll_per_word The code generates output like so Log perplexity 14 396450040387437 9  Insert a new cell and add the following code to see the probability distribution of topics if we consider the entire dataset as a single document bag_of_words doc_inst mdl.make_docbag_of_words np.argsortnp.arraymdl.inferdoc_inst 398 Appendix The code generates output like so Figure 5 24 Probability distribution of topics if the entire dataset is considered 10  Insert a new cell and add the following code to see the probability distribution of topic 461 printmdl.get_topic_words461, top_n7 The code generates output like so city, 0 15946216881275177 , device, 0 02001992054283619 , force, 0 02001992054283619 , character, 0 2001992054283619 , death, 0 010059761814773083 , person, 0 010059761814773083 , language, 0 010059761814773083 11  Insert a new cell and add the following code to see the probability distribution of topic 234 printmdl.get_topic_words234, top_n7 The code generates output like so year, 0 09871795773506165 , group, 0 02968442067503929 , child, 0 019822485744953156 , murder, 0 019822485744953156 , field, 0 019822485744953156 , writing, 0 009960552677512169 , memorial, 0 009960552677512169 Chapter 5 Topic Modeling 399 12  Insert a new cell and add the following code to see the probability distribution of topic 186 printmdl.get_topic_words186, top_n7 The code generates output like so dragon, 0 027016131207346916 , power, 0 01357526984065711 , flying, 0 013575269840657711 , line, 0 013575269840657711 , process, 0 013575269840657711 , crystal, 0 013575269840657711 , freestyle, 0 013575269840657711 We find that the log perplexity is around 14 , but the topics are not interpretable and the number of categories is an order of magnitude greater than the number of topics The topic model could still be used for dimensionality reduction Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help with reproducibility, in general, the topics found using tomotopy are superior from the perspective of interpretability Generally, your output is expected to be different In order to have exactly the same topic model, we can save and load topic models, and we do this in Exercise 5 04 , Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe  To access the source code for this specific section, please refer to  This section does not currently have an online interactive example, and will need to be run locally 400 Appendix Activity 5 02 Comparing Different Topic Models Solution Lets perform topic modeling on the C FP B dataset Follow these steps to complete this activity 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the pandas library import numpy as np import spacy nlp spacy.loaden_core_web_sm file_student dataconsumercomplaints student_comp_narrative.txt fopenfile_student,r,encodingutf- 8 student_textf.read f.close 3  Tokenize and include only nouns doc_studentnlpstudent_text student_pos_list student_preproc_text student_preproc_sent for token in doc_student if token.textn if nottoken.is_stop and nottoken.is_punct and token.pos_ in student_pos_list student_preproc_sent.appendtoken.lemma_ else student_preproc_text.appendstudent_preproc_sent student_preproc_sent student_preproc_text.appendstudent_preproc_sent last sentence printstudent_preproc_textChapter 5 Topic Modeling 401 The code generates the following output Figure 5 25 Tokenized corpus containing only nouns 4  Train an H DP model and print the log perplexity and topics import tomotopy as tp mdl tp H DP Modelalpha0 1 ,seed0 for line in student_preproc_text mdl.add_docline mdl.train50 printLog Perplexity, mdl.ll_per_word for k in rangemdl.k printTop 10 words of topic .formatk printmdl.get_topic_wordsk, top_n10 The code generates the following output Figure 5 26 Log perplexity and the topics inferred from the H DP model 5  Insert a new cell and add the following code to save the topic model mdl.save dataconsumercomplaintshdp_model.bin 6  Insert a new cell and add the following code to load the topic model mdl tp H DP Model.load dataconsumercomplaintshdp_model.bin 402 Appendix 7  Insert a new cell and add the following code to see the probability distribution of topics if we consider the entire dataset as a single document bag_of_wordsword for sent in student_preproc_text for word in sent doc_inst mdl.make_docbag_of_words np.argsortnp.arraymdl.inferdoc_inst The code generates the following output array5, 7 , 4 , 6 , 0 , 1 , 11 , 9 , 14 , 2 , 18 , 17 , 10 , 8 , 12 , 13 , 15 , 3 , 16 , dtypeint64 8  Insert a new cell and add the following code to see the probability distribution of topic 5 printmdl.get_topic_words5, top_n7 The code generates the following output school, 0 05379803851246834 , aid, 0 05379803851246834 , password, 0 003592493385076523 , username, 0 03592493385076523 , information, 0 03592493385076523 , direction, 0 03592493385076523 , bus, 0 03592493385076523 9  Insert a new cell and add the following code to see the probability distribution of topic 7 printmdl.get_topic_words7, top_n7 The code generates the following output graduate, 0 061739806085824966 , program, 0 061739806085824966 , assistance, 0 04634334146976471 , loan, 0 03094688430428505 , school, 0 03094688430428505 , world, 0 03094688430428505 10  Insert a new cell and add the following code to see the probability distribution of topic 4 printmdl.get_topic_words4, top_n7Chapter 5 Topic Modeling 403 The code generates the following output employer, 0 03343059867620468 , graduation, 0 03343059867620468 , book, 0 03343059867620468 , diploma, 0 025093790143728256 , debt, 0 025093790143728256 , education, 0 025093790143728256 , college, 0 025093790143728256 11  Now, train the L DA model Add the following code for this N UM_ TO PI CS20 mdl tp L DA ModelkN UM_ TO PI CS,alpha0 1 ,seed0 for line in student_preproc_text mdl.add_docline mdl.train50 printLog Perplexity, mdl.ll_per_word for k in rangemdl.k printTop 10 words of topic .formatk printmdl.get_topic_wordsk, top_n10 The code generates the following output Figure 5 27 Log perplexity and topics inferred from the L DA model 12  Insert a new cell and add the following code to save the topic model mdl.save dataconsumercomplaintslda_model.bin 404 Appendix 13  Insert a new cell and add the following code to load the topic model mdl tp L DA Model.load dataconsumercomplaintslda_model.bin 14  Insert a new cell and add the following code to see the probability distribution of topics if we consider the entire dataset as a single document bag_of_words doc_inst mdl.make_docbag_of_words np.argsortnp.arraymdl.inferdoc_inst The code generates the following output array17, 7 , 6 , 8 , 12 , 0 , 2 , 4 , 10 , 5 , 18 , 14 , 13 , 11 , 16 , 15 , 9 , 3 , 1 , 19 , dtypeint64 15  Insert a new cell and add the following code to see the probability distribution of topic 17 printmdl.get_topic_words17, top_n7 The code generates the following output interest, 0 20065094530582428 , loan, 0 16345429420471191 , payment, 0 152724489569664 , rate, 0 07046262919902802 , balance, 0 04184982180595398 , year, 0 0314776748418808 , principal, 0 25755111128091812 16  Insert a new cell and add the following code to see the probability distribution of topic 7 printmdl.get_topic_words7, top_n7 The code generates the following output loan, 0 14698922634124756 , year, 0 09230735898017883 , repayment, 0 08487062156200409 , payment, 0 08312080055475235 , plan, 0 07349679619073868 , income, 0 05074914172291756 , month, 0 .03981276974081993Chapter 5 Topic Modeling 405 17  Insert a new cell and add the following code to see the probability distribution of topic 6 printmdl.get_topic_words6, top_n7 The code generates the following output loan, 0 24387744069099426 , time, 0 06379450112581253 , student, 0 051527272909879684 , m, 0 05103658139705659 , money, 0 04514831304550171 , payment, 0 03239039331674576 , collection, 0 02794190190434456 For our dataset and with the experimentation undertaken, the L DA topics were much more interpretable than the H DP topics As seen from the preceding outputs, the log perplexity of the L DA model is also better than the log perplexity of the H DP model We did, of course, benefit from using the number of topics found by the H DP model when training the L DP model, and so this is not an entirely fair comparison Rather, this illustrates that there could be benefits to using an H DP model first even if we later select the L DA model for better interpretability or better log perplexity Note In general, the topics found are extremely sensitive to randomization in both gensim and tomotopy While setting a random_state in gensim could help reproducibility, in general, the topics found using tomotopy are superior from the perspective of interpretability Generally, your output is expected to be different In order to have exactly the same topic model, we can save and load topic models, and this was used in Exercise 5 04 , Topics in The Life and Adventures of Robinson Crusoe by Daniel Defoe  To access the source code for this specific section, please refer to 406 Appendix Chapter 6 Vector Representation Activity 6 01 Finding Similar News Article Using Document Vectors Solution Follow these steps to complete this activity 1  Open a Jupyter Notebook Insert a new cell and add the following code to import all necessary libraries import warnings warnings.filterwarningsignore from gensim.models import Doc2 Vec import pandas as pd from gensim.parsing.preprocessing import preprocess_string, remove_stopwords 2  Now load the news_lines file news_file datasample_news_data.txt 3  After that, you need to iterate over each headline in the file and split the columns, then create a Data Frame containing the headlines Insert a new cell and add the following code to implement this with opennews_file, encodingutf8, errorsignore as f news_lines lines_df pd Data Frame indices listrangelennews_lines lines_df news_lines lines_df indices lines_df.headChapter 6 Vector Representation 407 The code produces the following output Figure 6 27 Head of the Data Frame 4  You already have a trained document model named doc Vec Model.d2v in the previous exercise Now you can simply load and use it Insert a new cell and add the following code to implement this doc Vec Model Doc2 Vec.load datadoc Vec Model.d2v 5  Now, since you have loaded the document model, create two functions, namely, to_vector and similar_news_articles  The to_ vector function converts the sentences into vectors The second function, similar_news_articles , implements the similarity check It uses the doc Vec Model.docvecs.most_similar function, which compares the vector against all the other lines it was built with To implement this, insert a new cell and add the following code from gensim.parsing.preprocessing import preprocess_string, remove_stopwords def to_vectorsentence cleaned preprocess_stringsentence doc Vector doc Vec Model.infer_vectorcleaned return doc Vector 408 Appendix def similar_news_articlessentence vector to_vectorsentence similar_vectors doc Vec Model.docvecs.most_similar positive printsimilar_vectors similar_lines lines_df .news return similar_lines 6  Now that you have created the functions, it is time to test them Insert a new cell and add the following code to implement this similar_news_articlesU S raise T V indecency U S politicians are proposing a tough new law aimed at cracking down on indecency The code will generate the following output 1958 Clarke to unveil immigration plan New controls Name news, dtype object Note To access the source code for this specific section, please refer to  You can also run this example online at  Chapter 7 Text Generation and Summarization 409 Chapter 7 Text Generation and Summarization Activity 7 01 Summarizing Complaints in the Consumer Financial Protection Bureau Dataset Solution Follow these steps to complete this activity 1  Open a Jupyter Notebook and insert a new cell Add the following code to import the required libraries import warnings warnings.filterwarningsignore import os import csv import pandas as pd from gensim.summarization import summarize 2  Insert a new cell and add the following code to fetch the Consumer Complaints dataset and consider the rows that have a complaint narrative Drop all the columns other than Product , Sub-product , Issue , Sub-issue , and Consumer complaint narrative complaints_pathname dataconsumercomplaints Consumer_ Complaints.csv df_all_complaints pd.read_csvcomplaints_pathname df_all_narr df_all_complaints.dropna subset df_all_narr df_all_narrProduct,Sub product,Issue, Sub issue, Consumer complaint narrative410 Appendix 3  Insert a new cell and add the following code to select 12 complaints df_part_narr df_all_narrdf_all_narr.index.isin 242830 , 1086741 , 536367 , 957355 , 975181 , 483530 , 950006 , 865088 , 681842 , 536367 , 132345 , 285894 df_part_narr The preceding code generates the following output Figure 7 19 Data Frame showing the 12 selected complaints 4  Insert a new cell and add the following code to add a new column, named Text Rank Summary , that includes a Text Rank summary for each of the 12 complaints def try_summarizex,ratio try returnsummarizex,ratioratio except return df_part_narrdf_part_narr .applylambda x try_summarize x,ratio0 20 Chapter 7 Text Generation and Summarization 411 5  Insert a new cell and add the following code to show the Data Frame df_part_narr The preceding code generates the following output Figure 7 20 Data Frame showing the summarized complaints Note To access the source code for this specific section, please refer to  This section does not currently have an online interactive example, and will need to be run locally 412 Appendix Chapter 8 Sentiment Analysis Activity 8 01 Tweet Sentiment Analysis Using the textblob library Solution To perform sentiment analysis on the given set of tweets related to airlines, follow these steps 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd from textblob import Text Blob import re 3  Since we are displaying the text in the notebook, we want to increase the display width for our Data Frame Insert a new cell and add the following code to implement this pd.set_optiondisplay.max_colwidth, 240 4  Now, load the airline tweets.csv dataset We will read this C SV file using pandas read_csv function Insert a new cell and add the following code to implement this tweets pd.read_csvdataairline tweets.csv 5  Insert a new cell and add the following code to view the first 10 records of the Data Frame tweets.headChapter 8 Sentiment Analysis 413 The code generates the following output Figure 8 8 The first few tweets 6  If we look at the preceding figure, we can see that the tweets contain Twitter handles, which start with the symbol It might be useful to extract those handles The string column included in the Data Frame has an extract function, which uses a regex to get parts of a string Insert a new cell and add the following code to implement this tweets tweets.str.extractrS This code declares a new column called At and sets the value to what the extract function returns The extract function uses a regex, S , to return strings that start with  To view the initial 10 records of the tweets Data Frame, we insert a new cell and write the following code tweets.head10 The output should look something like this only top four tweets are shown here Figure 8 9 The first 10 tweets along with the Twitter handles 414 Appendix 7  Now, we want to remove the Twitter handles inside the tweets since they are irrelevant for sentiment analysis First, create a function named remove_ handles , which accepts a Data Frame as a parameter After passing the Data Frame, the re.sub function will remove the handles in the Data Frame Insert a new cell and add the following code to implement this def remove_handlestweet return re.subrS, , tweet 8  To remove the handles, insert a column in the Data Frame called tweets_ preprocessed and add the following code tweets tweets .applyremove_handles tweets.head10 The expected output for the tweets after removing the Twitter handles should look like this only the top four are shown in this figure Figure 8 10 The first 10 tweets after removing the Twitter handles From the preceding figure, we can see that the Twitter handles have been separated from the tweets Chapter 8 Sentiment Analysis 415 9  Now we can apply sentiment analysis on the tweets First, create a get_ sentiment function, which accepts a Data Frame and a column as parameters Using this function, we create two new columns, Polarity and Subjectivity , which will show the sentiment scores of each tweet Insert a new cell and add the following code to implement this def get_sentimentdataframe, column text_column dataframe textblob_sentiment text_column.applyText Blob sentiment_values Polarity v.sentiment.polarity, Subjectivity v.sentiment.subjectivity for v in textblob_sentiment.values return pd Data Framesentiment_values This function takes a Data Frame and applies the Text Blob constructor to each value of text_column  Then it extracts and creates a new Data Frame with the Polarity and Objectivity columns 10  Since the function has been created, we test it and pass the necessary parameters The result of this will be stored in a new Data Frame, sentiment_ frame  Insert a new cell and add the following code to implement this sentiment_frame get_sentimenttweets, tweet_preprocessed 11  To view the initial four values of the new Data Frame, type the following code sentiment_frame.head4 The code generates the following output Figure 8 11 Polarity and subjectivity scores 416 Appendix 12  To join the original tweet Data Frame to the sentiment_frame Data Frame, use the concat function Insert a new cell and add the following code to implement this tweets pd.concat, axis1 13  To view the initial 10 rows of the new Data Frame, we add the following code tweets.head10 The expected output with sentiment scores added should be as follows Figure 8 12 Tweets Data Frame with sentiment scores added From the preceding figure, we can see that for each tweet , Polarity , and Subjectivity scores have been calculated 14  To distinguish between the positive, negative, and neutral tweets, we need to add certain conditions Consider tweets with polarity scores greater than 0 5 as positive, and tweets with polarity scores less than or equal to - 0 5 as negative For neutral tweets, consider only those tweets that fall in the range of - 0 1 and 0 1  Insert a new cell and add the following code to implement this positive_tweets tweets negative_tweets tweets neutral_tweets tweets tweets Polarity - 0 1 tweets Polarity 0 1 Chapter 8 Sentiment Analysis 417 15  To view positive, negative, and neutral tweets, add the following code positive_tweets.head15 negative_tweets.head15 neutral_tweets This displays the result of positive, negative, and neutral tweets We have seen how to perform sentiment analysis using the textblob library The following image shows the top four neutral tweets Figure 8 13 Neutral tweets Note To access the source code for this specific section, please refer to  You can also run this example online at 418 Appendix Activity 8 02 Training a Sentiment Model Using T FI DF and Logistic Regression Solution 1  Open a Jupyter Notebook 2  Insert a new cell and add the following code to import the necessary libraries import pandas as pd pd.set_optiondisplay.max_colwidth, 200 3  To load all three datasets, insert a new cell and add the following code D AT A_ DI R datasentiment_labelled_sentences I MD B_ DA TA_ FI LE D AT A_ DI R imdb_labelled.txt Y EL P_ DA TA_ FI LE D AT A_ DI R yelp_labelled.txt A MA ZO N_ DA TA_ FI LE D AT A_ DI R amazon_cells_labelled.txt C OL UM N_ NA ME S yelp_reviews pd.read_tableY EL P_ DA TA_ FI LE, namesC OL UM N_ NA ME S amazon_reviews pd.read_tableA MA ZO N_ DA TA_ FI LE, namesC OL UM N_ NA ME S imdb_reviews pd.read_tableY EL P_ DA TA_ FI LE, namesC OL UM N_ NA ME S If we look at the code, even though the data comes from three different business domains, they are labeled and stored in the same format, which can help us to concatenate them together This is the reason we can combine them to train our sentiment analysis model 4  Now we concatenate the different datasets into one dataset using the concat function Insert a new cell and add the following code to implement this review_data pd.concatamazon_reviews, imdb_reviews, yelp_reviews Since we combined the data from three separate files, lets make use of the sample function, which returns a random selection from the dataset This will allow us to see the reviews from different files Insert a new cell and add the following code to implement this review_data.sample10Chapter 8 Sentiment Analysis 419 The code generates the following output only the top four reviews are displayed here Figure 8 14 Output from calling the sample function 5  To view the number of counts, add the following code review_data Sentiment.value_counts 6  Create a function named clean and do some preprocessing Basically, we need to remove unnecessary characters Insert a new cell and add the following code to do this import re def cleantext text re.subr, , text.lower text text.replacehadn t , had not .replacewasn t, was not .replacedidn t, did not return text In the preceding code snippet, first, the text is converted to lowercase and cleaned, and then keywords with apostrophes are converted into their original form 7  Once the function is defined, we can clean and tokenize the text It is a good practice to apply transformation functions on copies of our data unless you are really constrained with memory Insert a new cell and add the following code to implement this review_model_data review_data.copy review_model_data Review review_data Review.applyclean 420 Appendix 8  Now sample the data again to see what the processed text looks like Add the following code in a new cell to implement this review_model_data.sample10 The code generates the following output only the top four reviews are displayed here Figure 8 15 Sample of 10 after cleaning the Review column In the preceding figure, we can see that the text is converted to lowercase and only alphanumeric characters remain 9  Now it is time to develop our model We will use Tfidf Vectorizer to convert each review into a T FI DF vector We will then use Logistic Regression to build a model Insert a new cell and add the following code to import the necessary libraries from sklearn.feature_extraction.text import Tfidf Vectorizer from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.linear_model import Logistic Regression Next, combine Tfidf Vectorizer and Logistic Regression in a Pipeline object In order to do this, insert a new cell and add the following code tfidf Tfidf Vectorizer log_reg Logistic Regression log_tfidf Pipelinevect, tfidf, clf, log_reg Chapter 8 Sentiment Analysis 421 10  Once the data is ready, split it into train and test sets Split it into 70% for training and 30% for testing This can be achieved with the help of the train_test_split function Insert a new cell and add the following code to implement this X_train, X_test, y_train, y_test train_test_split review_model_data Review, review_model_data Sentiment, test_size0 3 , random_state42 11  Fit the training data to the training pipeline with the help of the fit function Insert a new cell and add the following code to implement this log_tfidf.fitX_train.values, y_train.values The code generates the following output Figure 8 16 Output from calling the fit function on the training model 12  In order to check our models accuracy , use the score function Insert a new cell and add the following code to implement this test_accuracy log_tfidf.scoreX_test.values, y_test.values The model has a test accuracy of .0%.formattest_accuracy You should an output as follows The model has a test accuracy of 81% As you can see from the preceding figure, our model has an accuracy of 81% , which is pretty good for such a simple model 422 Appendix 13  The model is ready with an accuracy of 81%  Now we can use it to predict the sentiment of sentences Insert a new cell and add the following code to implement this log_tfidf.predict You should see an output like the following array, dtypeint64 In the preceding figure, we can see how our model predicts sentiment For a positive test sentence, it returns a score of 1  For a negative test sentence, it returns a score of 0  Note To access the source code for this specific section, please refer to  You can also run this example online at  Index A airshow 42 , 47 , 51 Allen NL P 350 Amazon 126 , 131 , 137 , 142 , 149 , 157 , 162 , 282 , 348 349 , 356 359 , 361 arrays 280 B banamali 205 206 bigrams 45 46 D dendrogram 110 111 , 115 116 doctype 203 Dropbox 337 G Gaussian 134 Gutenberg 208 210 , 254 , 259 260 , 295 296 , 322 , 329 , 333 H Hadoop 156 histogram 243 J Jaccard 92 94 , 96 Java Script 216 joblib 194 195 K key value 216 , 286 , 290 kmean Model 122 k nearest 124 , 135 137 , 140 , 198 L lsamodel 242 243 M matplotlib 84 , 87 , 99 101 , 111 , 119 , 126 , 132 , 137 , 143 , 149 , 157 , 162 , 174 , 180 , 238 , 247 , 254 , 260 , 295 , 299 , 302 , 305 , 310 , 314 Mongo DB 217 N n grams 44 , 47 O one hot 32 , 271 , 275 278 , 280 290 , 292 293 , 316 P perplexity 246 247 , 253 , 265 pprint 218 , 224 225 pyplot 84 , 99 , 111 , 119 , 126 , 132 , 137 , 143 , 149 , 157 , 162 , 174 , 180 , 238 , 247 , 254 , 260 , 295 , 299 300 , 302 , 305 , 310 , 314 Py Torch 351 S seaborn 119 , 176 stemmer 20 22 , 59 61 stemming 1 , 8 , 15 , 19 20 , 22 , 30 , 32 33 , 36 , 41 , 58 , 60 61 , 67 68 , 105 , 108 , 192 , 275 , 362 stop word 36 , 65 66 , 105 , 108 T Text Blob 41 , 44 , 46 47 , 49 , 63 66 , 69 71 , 343 , 352 354 Text Rank 319 , 327 329 , 333 , 335 340 tf idf 33 , 329 toarray 282todense 81 82 , 89 , 94 , 96 , 115 , 120 , 128 , 133 , 139 , 144 , 151 , 159 , 164 , 176 , 182 , 193 , 195 196 tokenizer 50 57 , 67 , 285 288 , 290 292 tomotopy 237 , 244 , 246 247 , 249 , 252 254 , 256 , 259 , 262 , 265 Tweepy 201 , 227 228 U unigrams 8 urllib 210 X X GBoost 107 , 155 156 , 160 162 , 165 166 , 191 , 198\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 S RH University Heidelberg Curriculum Applied Data Science and Analytics Master of Science M Sc Valid for Group April 23 Batch Content Required courses 1 Scope Big Data Science Practice and Research 2 A DS A First Steps into Case Studies 3 A DS A Case Study I 5 A DS A Master Thesis Project 7 Scope Data Engineering and Programming 9 A DS A Data Engineering 1 Big Data Databases 10 A DS A Data Engineering 2 Big Data Architectures 12 A DS A Big Data Programming Python 14 Scope Data Management 16 A DS A Data Management 1 Data Acquisition and Data Cleaning 17 A DS A Data Management 2 Data Curation and Data Management 19 Scope Data Analytics 21 A DS A Data Analytics 1 Statistics and Machine Learning 22 A DS A Data Analytics 2 Text Mining and Natural Language Processing 24 A DS A Data Analytics 3 Deep Learning 26 Scope Data Visualization and Storytelling 28 A DS A Data Visualization and Storytelling 12 Design Basics and Designing Interactive Dashboards 29 A DS A Data Visualization and Storytelling 3 Advanced Data Visualization 31 Scope Data Privacy 33 A DS A Privacy, Ethics and International Law 34 Elective Module 36 A DS A Case Study 2 37 A DS A Internship 39 S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 1 Required courses SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 2 Scope Big Data Science Practice and Research SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 3 AD SA First Steps into Case Studies Module responsibility Prof Dr Swati Chandna Level Master Module Nr 3627 2 Credits 5 benotetDuration 1 Semester Frequency 2x each year Workload Total 125 hours presence time 75 hours self study 50 hours Teaching and learning methods Casework Group Work Problem oriented learning Exercise Language English Participation requirements none Examination Project work Course work NA Constructive Alignment Using the examination types project work, the students are allowed to reect practically on their learning progress and scientic abilities Through the analysis of practical examples and the performance of a complete case study using the business perspective as well as the multiple forms of Data handling Choice, evaluation, cleansing, providing, analysis, and communication, the students get a rst glimpse of the technical, organizational and methodological principals of Big Data They can also interconnect them directly with the dierent aspects and phases of a big data project Another element for the examination mentioned above types is that they enable a step by step improvement in skills and t optimally to the practice oriented character of this module Virtual teams are formed to allow for the studiablity parallel to the students main occupation These teams are provided the main course material data, software, scripts, literature via a cloud platform, and they may also store and share their progress Additional coaching of the teams during the module is provided via live chats and S AS e-learning Qualication goals learning outcomes and competencies Professional competence The students know the fundamental aspects of Big Data science.-Theyare able to dene the ve Vs of Big Data Volume, Velocity, Variety, Value and Veracity as well as the dierent phases of a Big Data project They may explain them in well dened practical sessions in an application oriented manner Theyknow diverse practical examples of Big Data projects and are able to explain their approach in business context as well as to compare with each other Theyunderstand the dierent phases of a Big Data project and may explain them in context with Big Data projects.-Theyare able to structure their scientic work as well as their results They gain results based on appropriate scientic criteria, e.g objectivity, validity and reliability Methodological competence The students are able to understand various types of project management and may apply as well as validate their knowledge in teams Moreover, they understand the connection between business oers and the Big Data Life Cycle Self competence The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks i.e occupation, private life and studies simultaneously Social competence The main function of this module is to lay the essential knowledge foundation for all later modules Students analyze various practical examples of Big Data projects As teams, they perform well dened Big Data case studies which involve the whole process of a Big Data project Denition of a concrete problem within a business Data acquisition and cleansing Data saving Data analysis and interpretation Data visualization and communication of the results provided by Data analytics Recommendation of further actions This setup enables the Big Data project to be embedded into a business context Thus, Students are encouraged to interconnect commercial necessities and decision making with ethical issues during the Big Data project, which, vice versa, avoids an approach exclusively catering to technical and analytical requirements The datasets used in the case studies are prepared by S RH University and distributed to the students using a cloud platform The concepts, methods, and tools learned in this module will be repeated and intensied in the following modules this especially aects the modules Case Studies I and I I Parallel to the Case Studies the students are taught basic scientic competencies They understand the essential aspects of scientic work and can plan and structure a scientic process Furthermore, they show profound knowledge in applying tools and methods during the scientic writing process S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 4 The students know the fundamental tools to organize working processes in virtual teams and approaches to dealing with team intern conicts They can use both tools to accomplish results mutually Module content -Introductionto Articial Intelligence, Big Data, Data Science, Data Analysis, and Data Engineering -Five Vs Volume, Velocity, Variety, Value, and Veracity Big Data Life Cycle Generation and collection of data, Data -Applicationsand Examples Job Roles K DDand C RI SP D M -Business Understanding -Data Understanding -Data Preparation -Modeling -Evaluation -Deployment -Data Preparation -Relational Databases S QL -Introductionto the S AS Analytics Platform -Self study S AS Visual Analytics 1 for S AS Viya Basic -Self-study S AS Visual Analytics 2 for S AS Viya Advanced 2e-Learning Badges Literature recommendations Dorschel J Praxishandbuch Big Data Wirtschaft Recht Technik, Springer Gabler, Heidelberg, 2015 Davenport, Thomas Big data at work dispelling the myths, uncovering the opportunities Harvard Business Review Press, 2014 .-Fan, W., Bifet, A 2013  Mining big data current status, and forecast to the future A CM S IG KD D explorations newsletter, 142 , 1 5  Aggarwal, C C 2015  Data mining the textbook Vol 1  New York Springer.-Gupta, G K 2014  Introduction to data mining with case studies P HI Learning Pvt Ltd.-Kantardzic, M 2011  Data mining concepts, models, methods, and algorithms John Wiley Sons.-Fayyad, U., Piatetsky Shapiro, G., Smyth, P 1996  The K DD process for extracting useful knowledge from volumes of data Communications of the A CM, 3911 , 27 34  Azevedo, A., Santos, M F 2008  K DD, S EM MA and C RI SP D M a parallel overview I AD S D M.-Schrer, C., Kruse, F., Gmez, J M 2021  A systematic literature review on applying C RI SP D M process model Procedia Computer Science, 181 , 526 534  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 5 AD SA Case Study I Module responsibility Prof Dr Swati Chandna Level Master Module Nr 3628 Credits 8 benotetDuration 1 Semester Frequency 2x each year Workload Total 200 hours presence time 60 hours self study 140 hours Teaching and learning methods Casework Case Work and reection Group Work Project development Exercise Language English Participation requirements First Steps into Case Studies Examination Project work Course work Case Study, Practical work, Presentation, Praxis Situation, Project work Constructive Alignment The examination forms project work and presentation evaluates the students abilities to application specically document their learning progress Through the application of the skills and competencies acquired in the previous modules during case studies 1 the students are given the opportunity to intensify their knowledge in a holistic manner Furthermore, the students learn to evaluate the applicability and cooperation of methods, techniques and tools in a context of a certain project The project work is an adequate examination for project module because of its emphasis on the visualization and communication of the resultsrecommendations Qualication goals learning outcomes and competencies Professional competence The students are able to transfer a company problem into a Big Data question as well as planning and performing it afterwards Theyidentify the data being necessary for this question and are able to estimate properly the value of the data in context of the problem.-Theymay prepare data for Data Mining.-Theyexecute a data mining analysis with the help of established tools and software.-Theyare capable to adequately visualize and communicate the results in context of the developed problem Methodological competence The students are able to specically apply creativity techniques to develop a problem and to identify required data.-Theyevaluate properly the applicability of methods and tools for the dierent phases of the Big Data project in context of a certain project and are able to select and execute the adequate methods.-Theyinterpret and evaluate the results of the analytics process with regard to the developed Big Data problem Self competence The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks i.e occupation, private life and studies simultaneously Social competence The students know the fundamental tools to organize working processes in virtual teams and approaches to dealing with team intern conicts They can use both tools to accomplish results mutually Module content Project management Agile Data Science S CR UM Organization and management Creativity techniques and formulations of problems Big Data architectures Data Mining Text Mining Storage and Retrieval Tools Data Mining Tools, methods, and techniques SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 6 Literature recommendations Freiknecht J Big Data in der Praxis Lsungen mit Hadoop, Hbase und Hive Daten speichern, aufbereiten, visualisieren, 2014 .-Han J et al Data Mining Concepts and Techniques, ElsevierMorgan Kaufmann, Amsterdam, 2006 .-Hand D et al Principles of Data Mining, M IT Press, Cambridge Mass London, 2001 .-Kantardzic M Data Mining, Wiley, 2011 .-Schwaber, K 1997  Scrum development process In Business Object Design and Implementation O OP SL A95 Workshop Proceedings 16 October 1995 , Austin, Texas pp 117 134  Springer London.-Larson, D., Chang, V 2016  A review and future direction of agile, business intelligence, analytics and data science International Journal of Information Management, 365 , 700 710  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 7 AD SA Master Thesis Project Module responsibility Prof Dr Swati Chandna Level Master Module Nr A 1003 2 Credits 27 benotetDuration 1 Semester Frequency 2x each year Workload Total 675 hours presence time 67 hours self study 608 hours Teaching and learning methods Laboratory Problem oriented learning Language English Participation requirements Scientic work and ethics Examination Colloquium and Thesis Course work NA Constructive Alignment The thesis projects main focus is the works scientic content, which will be submitted in written form This way, the student can also prove their ability to apply scientic methods, which are not restricted to a written text only Initially, a research question and the structure of the thesis have to be done and conrmed by the supervisor The master thesis can also be carried out in a company The student demonstrates the capability to apply logical thinking to gather information and draw valid results from it to earn the title of Master of Science Elements such as experiments or modeling can be included The students understand the fundamental aspects of scientic work They can structure and exert the cognitive process from the original problem to systematically answering a well dened scientic problem individually They know the essential methods and tools for developing scientic work and may critically reect on the results They are capable of a qualitative and quantitative evaluation of the method used The students may work independently on a scientically applied data science problem using common scientic methods and gain new insights Finally, the student is required to present their ndings to the supervisors In the presentation, the candidate proves their ability to summarize the most important content of hisher thesis coherently and comprehensively During this examination, the student needs to justify hisher choices and conclusions Qualication goals learning outcomes and competencies Professional competence Learning outcomes professional skills and methodology The students are able to structure their scientic cognitive process They yield results according to the criteria of good scientic work i.e objectivity, validity and reliability-Theyare capable of giving proper qualitative as well as quantitative judgments regarding the adequate use of scientic methods They may critically evaluate and reect on the gained results and method Theyintensify functional and scientic methodological competences learned during the master program over the dened problem of the master thesis project Theyare able to transfer the knowledge of Scientic work and ethics on the master thesis project.-Theyare competent to lead and moderate a functional scientic discussion to analytically critically reect scientic results and use of methods Methodological competence The student are able to integrate the knowledge and abilities which they accumulated during the Master course into the thesis.-Theyare able to do independent research under the guidance of a supervisor so that the thesis extends existing knowledge with new professional insights.-Theydemonstrate the ability to investigate, discuss, evaluate, and verify information on the scientic level.-Thestudent demonstrates the ability to apply research methods to their own project, select an appropriate research question and give a suitable, logical structure to the thesis project Self competence The students are able to perform research work systematically and independently as well as to reect insights using iterative thinking processes Theyare competent to structure the scientic cognitive process of the master thesis project regarding scheduling, systematic structuring and gaining of insights Social competence The students are able to evaluate results, gain in insights on a functional basis and may verbalize constructive feedback S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 8 They are capable of leading a functional discussion to gain insights Module content Master Thesis Guidelines The masters thesis is a carefully argued scholarly paper of approximately 20 , 000 words roughly 80 pages It should present an original argument that is carefully documented from primary andor secondary sources The thesis must have a substantial research component and a focus that falls within arts and science, and it must be written under the guidance of an advisor As the nal element in the masters degree, the masters thesis allows students to demonstrate expertise in the chosen research area After doing the initial research on their topic, students prepare a 1 2 paragraph abstract, a preliminary bibliography approximately ten to fteen books or journal articles, and a brief outline before approaching a possible advisor These will help students to convince their future advisors of the value and interest of their project Once a faculty member has agreed to be the advisor, students need to discuss the anticipated graduation date and agree on a timetable for meetings and submission of drafts It is each students responsibility to keep hisher advisor apprised of the works progress After a student has rened hisher topic and hisher advisor has approved it, the student needs to complete the Application for Approval of Masters Thesis Topic, have the advisor sign it, and submit it to the oce In most cases, students and advisors need to meet three or four times initially to nalize a topic and to review the rst or second draft Remember that the advisor must have enough time to read and evaluate the work before returning it to the student with comments and that the student must have time to incorporate those comments Dont expect the advisor to return the thesis in a day or two, whether it is an early draft or the nal copy Students should also be prepared for the possibility that their advisor will request substantial changes in the thesis Do not expect that the draft thesis will require only minor corrections or that the proposed nal version will necessarily be approved without further changes It is each students responsibility to see that the nal copy is free from spelling and grammatical errors the advisor is not responsible for line by line editing Literature recommendations Links Google Scholar D BL P I EE E Computer Society I EE E T VC G camera ready document guidelines Literature management Citavi SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 9 Scope Data Engineering and Programming SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 10 AD SA Data Engineering 1 Big Data Databases Module responsibility Prof Dr Binh Vu Level Master Module Nr 4087 Credits 5 benotetDuration 1 Semester Frequency 2x each year Workload Total 125 hours presence time 75 hours self study 50 hours Teaching and learning methods Group Work Problem oriented learning Seminar Exercise Language English Participation requirements none Examination Project work Course work NA Constructive Alignment This introductory course focuses on the fundamental concepts and technologies of data engineering The objective is to provide students with a comprehensive understanding of the engineering principles behind Big Data projects, including the handling and processing of large amounts of data The course covers various technological foundations of Big Data processing, including scale out architectures, the Map Reduce paradigm, and popular technologies like Hadoop and Spark The students will learn about the design of Wide Column databases and the advantages they oer The course also introduces the concepts of distributed systems and their impact on No SQ L databases, including eventual consistency The students will also be introduced to the basics of Dev Ops and Data Ops, demonstrated through Docker, Kubernetes, and Terraform As part of the course project, students will apply the concepts they have learned by creating a data pipeline, covering the main steps of data collection, storage, and retrieval through a practical example Students are tasked with building a big data architecture for a specic application scenario To accomplish this goal, they work to develop the necessary knowledge and skills for planning and constructing eective architecture This process begins by gathering the knowledge and abilities of all team members and identifying any gaps in knowledge or expertise The module also includes studying practical examples and hands on exercises using the most commonly used big data technologies Throughout the module, students document their learning progress using a practical journal and presentations and ultimately demonstrate their understanding and mastery of the subject through a nal project presentation Qualication goals learning outcomes and competencies Professional competence Upon completion of this course, the students have a comprehensive understanding of the common Big Data architectures and are able to distinguish between them Theyhave the ability to plan and construct a complete Big Data pipeline for data storage and retrieval and have gained practical experience with a variety of data engineering tools.-Thestudents are equipped with the knowledge to evaluate and make informed decisions on selecting the appropriate Big Data technologies to fulll specic requirements.-Thestudents have a solid foundation in data engineering and are able to apply their skills and knowledge to real world challenges Methodological competence The students problem solving abilities have been built and sharpened, positioning them as valuable assets to their future employers and clients.-Theyare well equipped to tackle the complex challenges faced by data engineers in the industry, increasing their chances of success in their careers Self competence The students develop a strong sense of competence and condence in their ability to detect and close gaps in their knowledge independently Social competence The students develop the necessary skills to work eectively in virtual teams and have the condence to utilize the collective knowledge and abilities of their team members to achieve their objectives Module content Introduction to Data Engineering SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 11 -Data Engineering Lifecycle C AP Theorem, B AS E Principle Relational Databases A CI D model, My SQ L No SQ L Databases Mongo DB Big Column Cassandra H Base Key Value Store Graph Database Redis, Neo4j Data Acquisition, Data Crawling Dev Ops Data Ops Containerization Docker Kubernetes Infrastructure as Code Terraform Continuous Integration Continuous Delivery Hadoop Ecosystem Cloud Introduction Cache and Memory based Storage Systems Indexing, Partitioning, and Clustering H DF S Literature recommendations Bengtfort B Kim J Data Analytics with Hadoop An Introduction for Data Scientists Freiknecht J Big Data in der Praxis Lsungen mit Hadoop, Hbase und Hive Daten speichern, aufbereiten, visualisieren Grus J Data Science from Scratch Redmond E Wilson J R Seven Databases in Seven Weeks A Guide to Modern Databases and the No SQ L Movement White T Hadoop The Denite Guide Recent research literature from peer reviewed journals SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 12 AD SA Data Engineering 2 Big Data Architectures Module responsibility Prof Dr Swati Chandna Level Master Module Nr 4088 Credits 6 benotetDuration 1 Semester Frequency 2x each year Workload Total 150 hours presence time 75 hours self study 75 hours Teaching and learning methods Group Work Seminar Tutorium Exercise Language English Participation requirements none Examination Project work Course work NA Constructive Alignment Based on the rst module on data engineering, the students continue the exploration of data engineering tasks and gain experience with additional tools Beginning with the problem to guarantee the quality of the provided raw data in an application context, the students develop the necessary know how in the eld of data management Starting with collecting the knowledge distributed amongst the team members, the students recognized and closed knowledge gaps by researching and exercising in their respective groups The students prove their gain in competencies in project work as well as a nal presentation Qualication goals learning outcomes and competencies Professional competence The students have a comprehensive knowledge of Big Data architectures.-Theyare able to evaluate and select Big Data technologies adequately for fullling given requirements.-Theyare capable of planning and building a complete Big Data pipeline for various purposes Thestudents have gained extended hands on experience with various data engineering tools.-Thestudents are able to evaluate and select Big Data technologies from a large range of potential options Methodological competence The students increase their competencies in problem solving Self competence The students improve their competence to detect and close gaps in knowledge independently Social competence The students intensify their ability to work in virtual teams and are also capable to use the knowledge and abilities distributed amongst the team to solve a problem in a target oriented manner Module content Data Architecture What is data architecture Principles of good architecture Types of Data Architecture Data Warehouse Data Lake Lambda Architecture Kappa Architecture Dataow model Data Mesh Distributed Processing Infrastructure Spark Cluster D E Streaming Queue Kafka, PubSub SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 13 Processing Spark Streaming Data Pipeline with Google Cloud Services Storage components on G CP G CS Dataproc H DF S Loading Data into a Data warehousing tool on G CP Big Query HandlingWriting Data Orchestration and dependencies using Apache Airow Google Composer Batch Data ingestion using Cloud Sql or Apache Airow Real Time data streaming and analytics using the latest A PI, Spark Structured Streaming Micro batching using Py Spark streaming Hive on Dataproc Deployment Backend Engineering Flask A PI Literature recommendations Recent research literature from peer reviewed journals Cielen D Meysman A Introducing Data Science, Manning Verlag, 2016 Garofalakis M Gehrke J Data Stream Management Processing High Speed Data Streams Data Centric Systems and Applications, Springer Verlag, 2016 Komball R Caserta J The Data Warehouse E TL Toolkit Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data, Kimball Group, 2004 Lindstrom M Small Data Was Kunden wirklich wollen wie man aus Hinweisen geniale Schlsse zieht, Plassen Verlag, 2016 Mitchell M N Data Management Using Stata A Practical Handbook, Stata Press, 2010 Rossak I Hanser C Datenintegration Integrationsanstze, Beispielszenarien, Problemlsungen, Talend Open Studio, 2013 Thome G Solbach W Grundlagen und Modelle des Information Lifecycle Management, Xpert.press, 2007 SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 14 AD SA Big Data Programming Python Module responsibility Dr Ing Kamellia Reshadi Level Master Module Nr 3642 Credits 6 unbenotetDuration 1 Semester Frequency 2x each year Workload Total 150 hours presence time 75 hours self study 75 hours Teaching and learning methods Laboratory Exercise Lecture Language English Participation requirements none Examination Project work The basis for calculation is generally 1 E CT S 25 hrs Deviations are covered exclusively by Appendix 2 Bachelor and 2a Master of the S PO Course work Project work Constructive Alignment The project work is designed to provide participants with hands on experience in project management by simulating a real world project scenario By working in teams, participants not only gain practical knowledge in project management methods but also develop their social and self competencies They learn to communicate and collaborate eectively with their team members and work towards a common goal Additionally, the project work allows participants to enhance their software development skills by utilizing modern software engineering concepts and tools Through the examination form of project work, students are evaluated on their ability to write clean code and apply programming methodologies Furthermore, the project work provides an opportunity for students to demonstrate their advanced scientic abilities by conducting research and presenting their ndings to their peers The presentation also helps students develop their public speaking skills and teaches them to defend their ideas in front of an audience Overall, the project work serves as a comprehensive learning experience that prepares students for their future careers in the eld of software development and project management Qualication goals learning outcomes and competencies Professional competence The students know the fundamentals of the Python programming languages and can develop and implement more complex programs independently.-Theycan work with code repository management.-Theycan develop clean code in python programming.-Theycan independently develop simple machine learning procedures Methodological competence The students are equipped with fundamental knowledge in programming and are provided with an understanding of the latest developments in cloud based programming By learning the basics of programming and exploring programming in cloud environments, students are well prepared to tackle real world challenges and are equipped with the skills required to build innovative solutions Theprogram places a strong emphasis on developing the problem solving and logic building competencies of the students Throughout the curriculum, students are presented with various problem solving challenges that require them to think critically and logically As a result, they become adept at breaking down complex problems into manageable components, identifying patterns, and developing eective solutions The development of these competencies not only helps students succeed academically but also prepares them for success in their future careers They will be able to approach any problem systematically and develop logical and innovative solutions that drive business outcomes.-Thestudents increase their competencies in problem solving and logic building Self competence The students are provided with a comprehensive understanding of programming fundamentals, allowing them to approach coding systematically and with condence They are equipped with the skills necessary to analyze problems and identify the underlying principles and concepts required to develop eective solutions With a strong foundation in coding, students are well prepared to create complex applications and software systems that meet the demands of modern businesses and organizations Theprogram emphasizes the development of teamwork and problem solving skills Through hands on projects and collaborative assignments, students learn to work eectively in teams and develop solutions to complex problems They learn how to communicate eectively, delegate tasks, and utilize each others strengths to achieve a common goal Additionally, students are encouraged to solve problems individually, developing their critical thinking skills and independent problem solving abilities With a focus on teamwork and individual skill development, students graduate with a well rounded set of competencies that prepare them for success in any environment S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 15 In summary, this program provides students with the skills and knowledge required to approach programming systematically and solve complex problems eectively With a focus on teamwork and individual skill development, students graduate with a comprehensive set of competencies that prepare them for success in the rapidly evolving eld of technology They are well equipped to analyze and understand problems, develop innovative solutions, and work eectively in teams to achieve common goals Social competence The students improve their ability to analyse problems, to break large problems down into digestible portions.-Bypresenting their own and others work, they also improve their communication skills The student learns to work with a team comprising of dierent skilled people and are also capable of using the knowledge and abilities among the team to solve a problem in a target oriented manner Module content Introduction to the Python and R programming language Source code management, revision and branch and version management, refactoring Software documentation and tools Test driven development and architecture Coding design principles, clean coding, safe coding Introduction to the Python programming language Object oriented python programming, Module and Package management, Iterators and decorators, Context managers Introduction to programming in Cloud Introduction to Iaa S, Paa S, and Saa S, Container management and container orchestrations Fundamentals of using public python packages Num Py, Pandas Advanced Python Programming Multi-threading, Multi-processing, Asynchronous Python Programming Web development in Python using Flask Python code packaging and deployment Cloud deployment and Dev Ops, Understanding continuous integration, continuous delivery and development on a cloud platform, Setup.py management, Deployment of python code in a productive environment Docker compose, Cloud Foundary, Heroku, Kubernetes cluster management Literature recommendations Martin R C The Clean Coder A Code of Conduct for Professional Programmers, 1st edition, Prentice Hall, 2011 .-Martin RC Clean Architecture A Craftsmans Guide to Software Structure and Design, 1st edition, Prentice Hall, 2017 .-Ramalho, L Fluent Python, 1st edition, O Reilly, 2015 Wickham H Grolemund G, R for Data Science, 1st edition, O Reilly, 2017 Mc Kinney, Wes Python for Data Analysis Data Wrangling with Pandas, Num Py, and I Python OReilly Media, 2017 .-Grus, Joel Data Science from Scratch First Principles with Python OReilly Media, 2015 .-Fandango, Armando Big Data Analytics with Python Packt Publishing, 2016 .-Albon, Chris Machine Learning with Python Cookbook Practical Solutions from Preprocessing to Deep Learning OReilly Media, 2018 .-Raschka, Sebastian Python Machine Learning Packt Publishing, 2015  Witten, Ian H., Eibe Frank, and Mark A Hall Data Mining Practical Machine Learning Tools and Techniques Morgan Kaufmann, 2016  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 16 Scope Data Management SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 17 AD SA Data Management 1 Data Acquisition and Data Cleaning Module responsibility Prof Dr Theodoros Soldatos Level Master Module Nr 4089 Credits 4 benotetDuration 1 Semester Frequency 2x each year Workload Total 100 hours presence time 50 hours self study 50 hours Teaching and learning methods Group Work Exercise Lecture Language English Participation requirements none Examination Project work Course work NA Constructive Alignment The module aims at preparing students to address the challenge of ensuring the quality of raw data in an application context, and develop the necessary expertise in data management The module also includes classes that introduce publically available data prolingcleaning tools The students demonstrate their competence via a project work and a nal presentation Qualication goals learning outcomes and competencies Professional competence Students are able to understand the importance of data quality in the context of data analysis.-Theycan apply data acquisition and cleaning techniques to improve the quality of data Thestudents are able to identify areas for improvement Thestudents are able to apply data proling and cleansing techniques Thestudents are able to demonstrate competence in main project work Thestudents should be able not only to apply data quality assessment and cleaning techniques to real world data management scenarios but also comprehensively communicate their ndings This will enhance the value and reliability of data driven decisions and will contribute to a culture of data quality in their future workplaces Methodological competence The students know methods and tools for cleaning data and can use a particular spectrum of them Thestudents can apply their theoretical competency practically, using selected software systems Theyknow the essential methods of data cleansing and apply the introduced concepts correctly by solving specic problems and interpreting the results Self competence The students are competent to structure and recognize the ambiguity in the raw data and systematically clean them Thestudents are capable of applying quality control techniques on raw datasets and of creating a technical reusable solution for cleaning the raw data Social competence The students are capable of analyzing and understanding the problems in the data individually and may develop user centered solutions based on proling results Thestudents can compare, evaluate and discuss the methods applied for cleansing the data Module content Main content includes Five Vs of Big Data Data identication, verication, cleansing Data quality Data proling Data formatting, cleansing Metadata SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 18 Literature recommendations Barton R D Talend Open Studio Cookbook, Packt Publishing, 2013 Blokdyk G Data transformation A Clear and Concise Reference, Create Space Independent Publishing Platform, 2018 Cielen D Meysman A Introducing Data Science, Manning Verlag, 2016 Halevy A et al Principles of Data Integration, Elsevier L TD, 2012  Mitchell M N Data Management Using Stata A Practical Handbook, Stata Press, 2010 Verborgh R De Wilde M Using Open Rene, Packt Publishing, 2013 Recent research literature from peer reviewed journals SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 19 AD SA Data Management 2 Data Curation and Data Management Module responsibility Prof Dr Binh Vu Level Master Module Nr 4090 Credits 4 benotetDuration 1 Semester Frequency 2x each year Workload Total 100 hours presence time 50 hours self study 50 hours Teaching and learning methods Group Work Exercise Lecture Language English Participation requirements none Examination Project work Course work NA Constructive Alignment The rst course in Data Management focuses on ensuring the quality of raw data in an application context This is achieved by covering the relevant issues and challenges that arise in this area In the second course, students delve deeper into the eld of data management, exploring advanced concepts and techniques The module starts with a thorough investigation of the raw data, including proling, in order to gain a deeper understanding of it Each student is then tasked with designing a S MA RT data pipeline to transform the data and answer a specic business question This module is supported by classes that impart a comprehensive understanding of various data management methods To demonstrate their mastery of these concepts, students are required to present their ndings in a nal presentation, showcasing the methods and techniques they have learned throughout the course Qualication goals learning outcomes and competencies Professional competence Students will have a deep understanding of the methods and technologies involved in managing and curating massive amounts of data in a data warehouse with an automated E TL pipeline.-Theywill be able to assess the quantity of data in terms of volume and velocity and its potential value in a well dened scenario.-Studentswill be able to integrate data from various sources and formats, consolidate their attributes, and design a solution to address a specic hypothesis Theywill be equipped to cleanse, homogenize, aggregate, transform, and prepare collected data in accordance with a dened application context.-Studentswill be aware of the role of open source and commercial tools and programming libraries in designing eective E TL pipelines for a data warehouse Methodological competence Upon completion of this module, the students will have a comprehensive understanding of programming libraries, methods, and tools for automating the management and curation of data through E TL pipelines.-Theywill be familiar with the process of transforming raw data into actionable information through the use of either O LA P techniques or machine learning models, enabling them to make informed decisions.-Studentswill have a broad understanding of the various applications and methods utilized in industry for transforming data required for business decision making Self competence The students will have the necessary competencies to structure and comprehend raw data and to design a robust E TL pipeline that addresses the specic business needs of the use case.-Todemonstrate their understanding, they will be tasked with creating an E TL pipeline that integrates O LA P or M L based techniques that they have learned in the course This pipeline will serve as evidence of their mastery of the concepts covered in the module and will be showcased through a nal presentation and technical demonstration Social competence Students possess the skills to identify and comprehend the requirements of a business problem and apply the knowledge and techniques learned in class to develop eective solutions to address the business use case.-Studentshave the competence to make informed decisions, choose the right technology, and use it eectively to tackle the business challenge S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 20 Module content Data Extraction, Transformation, and Integration Businesses Smart Data Data Modelling Data Lifecycle Management Star Snowake Schema Big Data Processing Batch Processing Stream Processing Map Reduce Spark E TL E LT Data Warehouse Big Query On Premises Cloud Data Warehouse Data Cubes O LA P Operations Literature recommendations Barton R D Talend Open Studio Cookbook Blokdyk G Data transformation A Clear and Concise Reference Ramez Elmasri, Shamkant B Navathe Fundamentals of Database Systems, Addison Wesley, 2015 Chambers B Zaharu M The Denitive Guide Big Data Processing Made Simple Huy Nguyen, Ha Pham, Cedric Chin The Analytics Setup Guidebook, Holistics, 2020 Garofalakis M Gehrke J Data Stream Management Processing High Speed Data Streams Data Centric Systems and Applications Bernard Marr Big Data In Practice How 45 successful companies used big data analytics to deliver extraordinary results, Wiley, 2016 Halevy A et al Principles of Data Integration Komball R Caserta J The Data Warehouse E TL Toolkit Practical Techniques for Extracting, Cleaning, Conforming, and Delivering Data, Wiley, 2004 Bernard Marr Big Data Using S MA RT Big Data, Analytics and Metrics To Make Better Decisions and Improve Performance, Wiley, 2015 Paulraj Ponniah Data Warehousing Fundamentals for I T Professionals, Wiley, 2010 SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 21 Scope Data Analytics SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 22 AD SA Data Analytics 1 Statistics and Machine Learning Module responsibility Prof Dr Theodoros Soldatos Level Master Module Nr 4091 Credits 8 benotetDuration 1 Semester Frequency 2x each year Workload Total 200 hours presence time 100 hours self study 100 hours Teaching and learning methods Group Work Exercise Lecture Language English Participation requirements none Examination Practical work Course work NA Constructive Alignment During the module, students learn essential tools and methods of inductive and descriptive statistics as well as machine learning fundamentals Students learn also how to become independent analysts, by using common public tools and software currently, main focus is on R language In this way, students develop an understanding of the applicability, prerequisites and interpretation purposes of the various statistical methods taught in the module The course consists of theoretical lectures as well as of practical sessions where the concepts and methods presented in the theoretical lectures are implemented and practiced on smaller and larger problems using statistical software currently, the R language In this setting, students learn theory alongside with practical implementation Similarly, the exam consists of a practical test which tests both theory and practical concepts Qualication goals learning outcomes and competencies Professional competence Students are able to demonstrate prociency in applying relevant statistical concepts and methods adequately Studentsunderstand the purpose of each method in context and can utilize them eectively to solve problems.-Studentsare able to perform complex analyses and to evaluate results analytically.-Studentsare able to adapt their knowledge and skills to tackle new and unfamiliar projects with a goal oriented approach Methodological competence Learning outcomes include Theability to analyze and understand data problems.-Theability to apply theoretical competency practically, by using taught software tools.-Theability to evaluate results in an analytical manner Self competence Learning outcomes include Theability to learn new methods independently Theability to compare and discuss applied methods and own solutions Social competence Learning outcomes include Theability to tackle problems independently, both individually and as a team member.-Theability to communicate implemented choices and solutions Module content Main content includes-Introductionof key concepts of logic and statistics Introduction of data mining methods Data mining as a process Descriptive Statistics Measure of central tendency, Dispersion parameters, Variable distribution Inferential Statistics Corelation and Co-variance, Hypothesis testing, Analysis of count data SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 23 Analysis of Variance A NO VA, M AN OV A-Time Series Analysis Machine Learning Supervised Machine Learning Regression, Classsication Unsupervised Machine Learning Clustering, Association, Survival Analysis For more comprehensive training, theorotical and practical sessions are co ordinated and delivered in sync with each other, as much as possible Literature recommendations Schmueli G et al Data Mining for Business Analytics, Concepts, Techniques and Applications in R, Wiley 2018 James G, Witten D, Hastie T, Tibshirani R An Introduction to Statistical Learning with Applications in R, Springer 2017 8th edition Marsland S Machine Learning An Algorithmic Perspective C RC Press, 2nd Edition Bruce P, Bruce A Practical Statistics for Data Scientists, 50 Essential Concepts, O Reilly, 2017 Reinhart A Statistics done wrong, No Starch Press Backhaus K, Erichson B, Plinke W, Weiber R Multivariate Analysemethoden, Springer Gabler 15th edition Bamberg G Baur F Statistik, 12  Auage, Oldenbourg, MnchenWien, 2006  Fahrmeir L et al Statistik Der Weg zur Datenanalyse, 7  Auage, Springer, Berlin, 2010 Handl A Multivariate Analysemethoden Theorie und Praxis multivariater Verfahren unter besonderer Bercksichtigung von S Plus, 2  Auage, Springer, Berlin, 2010  Hartung J Statistik Lehr-und Handbuch der angewandten Statistik, 14  Auage, Oldenbourg Verlag, Mnchen, 2005  Mosler K Schmid F Beschreibende Statistik und Wirtschaftsstatistik, 4  Auage, Springer Verlag, Heidelberg, 2009 Schlittgen R Multivariate Statistik, Oldenbourg, MnchenWien, 2009  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 24 AD SA Data Analytics 2 Text Mining and Natural Language Processing Module responsibility Prof Dr Swati Chandna Level Master Module Nr 4092 Credits 7 benotetDuration 1 Semester Frequency 2x each year Workload Total 174 hours presence time 87 hours self study 87 hours Teaching and learning methods Group Work Seminar Exercise Lecture Language English Participation requirements none Examination Practical work Course work NA Constructive Alignment The students know the challenges posed by Big data on structured data acquisition and their processing of the information, helping make business relevant decisions They are capable of structuring complex problems and performing systematic research work They may analyze and prepare reports based on huge data amounts to generate user centered knowledge They can select adequate text mining and natural language processing techniques to solve specic business relevant problems and to visualize the gained results appropriately Finally, the results are evaluated critically regarding their validity The students prove their application oriented knowledge and competencies by solving well dened problems and exercises during a practical examination This form of examination also evaluates the students interpretation capabilities Qualication goals learning outcomes and competencies Professional competence The students know the essential methods for the procedural steps of the preparation of text mining methods preparation of raw data, structuring and rening.-Theyare capable to perform and apply the most important methods of context analyses They are able to perform more complex analyses and to evaluate the results in a functional way.-Theymay critically reect the validity of the results regarding qualitative as well as quantitative aspects Theyknow the most important web mining tools and are capable to apply them in relevant practical exercises.-Theyidentify state of the art concept to visualize data mining results Methodological competence The students may apply the introduced methods correctly by solving specic problems and interpreting the results adequately Theyknow the essential methods in text analysis.-Theycan apply the methods learned by using specic software solutions and may critically reect the results validity Social competence The students are capable to analyze the methods used as well as the results gained in their entity and evaluate them benet oriented during a business specic decision making process Module content Refresher Machine Learning from Analytics 1 Python for Data Science Text Mining Text Preprocessing, Feature Creation, Feature Selection, Pattern Discovery Natural Language Processing Text Processing Information Extraction String Similarity Information Retrieval Ranked Retrieval Model Selection SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 25 Feature Engineering Feature Reduction, Feature Scaling, Feature Encoding, Feature Selection Literature recommendations Dorschel J Praxishandbuch Big Data Wirtschaft Recht Technik, Springer Gabler, Heidelberg, 2015  Ester, M Sander J Knowledge Discovery in Databases Techniken und Anwendungen Springer, Berlin 2000  Ferber R Information Retrieval Suchmodelle und Data Mining Verfahren fr Textsammlungen und das Web, dpunkt.verlag, Heidelberg, 2003  Fischer P Algorithmisches Lernen, B G Teubner, Stuttgart, 1999 Han J et al Data Mining Concepts and Techniques, ElsevierMorgan Kaufmann, Amsterdam, 2006  Hand D et l Principles of Data Mining, M IT Press, Cambridge Mass London, 2001  Kantardzic M Data Mining, Wiley, 2011  Liu B Web Data Mining Exploring Hyperlinks, Contents and Usage Data Data Centric Systems and Applications, 2  Auage, Springer, 2011  Marsland S Machine Learning An Algorithmic Perspective, C RC Press, 2009  Mitchell T M Machine Learning, Mc Graw Hill, 1997  Sutton Reinforcement Learning An Introduction, second edition, 2018 Runkler T A Data Mining Methoden und Algorithmen intelligenter Datenanalyse, Springer Vieweg, 2010  Schwarz T Big Data im Marketing Chancen und Mglichkeiten fr eine eektive Kundenansprache, Haufe Lexware, 2015  Witten I H et al Data Mining Practical Machine Learning, Tools and Techniques, 3rd edition, Elsevier, 2011  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 26 AD SA Data Analytics 3 Deep Learning Module responsibility Prof Dr Binh Vu Level Master Module Nr 4093 Credits 8 benotetDuration 1 Semester Frequency 2x each year Workload Total 200 hours presence time 100 hours self study 100 hours Teaching and learning methods Problem oriented learning Seminar Exercise Language English Participation requirements Data Analytics 1 Examination Practical work Course work NA Constructive Alignment The students gain a comprehensive understanding of essential predictive analytics techniques and develop prociency in using common standard tools, such as, e.g., Python, Tensorow, Keras, and Scikit learn, to perform independent analyses They learn to critically evaluate the validity and relevance of results and apply their newfound skills to tackle new and complex scenarios objectively By the end of the module, students will have developed a strong foundation in predictive analytics, allowing them to make informed decisions and solve real world problems using data driven insights The students prove their application oriented knowledge and competences by solving well- dened problems and exercises during a practical examination This form of examination also evaluates the students interpretation capabilities Qualication goals learning outcomes and competencies Professional competence The students are taught to use functional terms eectively and apply predictive analytics methods in a practical setting.-Theydevelop the ability to perform complex analyses and functionally evaluate results, considering both qualitative and quantitative factors Thestudents are encouraged to critically reect on the validity of their results, fostering a deeper understanding of the concepts and methods learned Methodological competence The students become procient in applying the methods learned through the use of specic software solutions.-Theydevelop the ability to use these tools to analyze data and draw meaningful insights, gaining hands on experience in a professional setting Thestudents are encouraged to critically reect on the validity of their results, fostering their analytical skills and ability to make informed decisions based on data driven insights Social competence The students possess the skills to both individually and collaboratively analyze and categorize problems, allowing them to develop eective, user centered solutions Throughhands on experience, they learn to apply their analytical and problem solving skills to real world scenarios and create innovative solutions that meet the needs of the intended users.-Thisdevelopment of critical thinking and collaboration skills will serve them well in their future careers Module content Introduction to Deep Learning Linear Neural Networks Multilayer Neural Networks Backward Propagation Improving Deep Neural Networks Convolutional Neural Networks Conv Nets in Practice Transfer Learning Word Embedding Recurrent Neural Networks SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 27 Long Short Term Memory Gated Recurrent Unit Generative Adversarial Networks Autoencoders Attention and Transformers Graph Neural Networks Diusion Models Deep Reinforcement Learning Literature recommendations Andrew Glassner Deep Learning A Visual Approach, No Starch Press, 2021 Charu C Aggarwal Neural Networks and Deep Learning A Textbook, Springer, 2018 Aston Zhang, Zachary C Lipton, Mu Li, and Alexander J Smola Dive into Deep Learning, ar Xiv preprint, 2021 Hisham El Amir, Mahmoud Hamdy Deep Learning Pipeline Building a Deep Learning Model with Tensor Flow, Apress, 2020 Ian Goodfellow, Yoshua Bengio, Aaron Courville Deep Learning, M IT Press, 2016 Liangqu Long, Xiangming Zeng Beginning Deep Learning with Tensor Flow Work with Keras, M NI ST Data Sets, and Advanced Neural Networks, Apress, 2022 Nikhil Ketkar, Jojo Moolayil Deep Learning with Python Learn Best Practices of Deep Learning Models with Py Torch, Apress, 2021 Nithin Buduma, Nikhil Buduma, and Joe Papa Fundamentals of Deep Learning Designing Next Generation Machine Intelligence Algorithms, O Reilly, 2022 David Paper State of the Art Deep Learning Models in Tensor Flow Modern Machine Learning in the Google Colab Ecosystem, Apress, 2021 Wei Di, Anurag Bhardwaj, Jianing Wei Deep Learning Essentials, Packt Publishing, 2018 SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 28 Scope Data Visualization and Storytelling SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 29 AD SA Data Visualization and Storytelling 12 Design Basics and Designing Interactive Dashboards Module responsibility Prof Dr Swati Chandna Level Master Module Nr 4095 Credits 7 benotetDuration 1 Semester Frequency 2x each year Workload Total 175 hours presence time 100 hours self study 75 hours Teaching and learning methods Group Work Seminar Exercise Lecture Language English Participation requirements none Examination Learning Diary and Project work Course work NA Constructive Alignment The module Data Visualization and Storytelling 1 provides students with the necessary knowledge and practical skills to develop a strong foundation in data visualization, and to design and develop advanced applications for visual data analysis for eective communication of insights regarding the original problem These insights are often described and presented using interactive dashboards, infographics, etc This module introduces the skills required to create professional business dashboards and infographics, and is covered during the rst and second semester as following First Semester Course Data Visualization and Storytelling 1 Design Basics 2 C P-Typeof exam Learning Diary Second Semester -Course Data Storytelling and Visualization 2 Creating Interactive Dashboards 5 C P-Typeof exam Project Work This module is meant to prepare students to work on complex data science projects that require the development of interactive visual interfaces for data analysis Various projects in this module allow students to decode, critique, and redesign interactive dashboards using real world datasets that will help students to identify the story within data and discover how to use data story points to create a powerful story to leave a long and lasting impression on the target audience The students analyze dierent story examples and develop own stories based on role plays The methodological spectrum reaches from data selection and visualization to interpretations for dierent target groups This work is performed individually as well as in teams Finally, the students or their respective groups present their developed stories to each other and subsequently critically reect the results The examination form consisting of a learning diary for the rst semester and project work in the second semester suit the module intention adequately because there is a continuous documentation and evaluation of the students improvements in competences as well as of the status of the data story Qualication goals learning outcomes and competencies Professional competence The students will gain knowledge and understanding of design principles and elements The students are able to use data visualizations for interactive storytelling, enabling and supporting the exploration of analysis results as well as the derivation of new problems.-Thestudents will learn about basic and advanced visualization techniques Theymay visualize and communicate analysis results in a target group oriented way.-Theywill understand how people work in data visualization projects Methodological competence Students are capable to choose eective visualization techniques for a particular problem.-Aftercompletion of this module the students know tools supporting interactive data storytelling e.g Tableau Public, Tableau Desktop, Tableau Prep and are able to use them in a target oriented manner.-Theyare capable to prepare insights according to their target group and help them to make better decisions regarding the original problem SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 30 Social competence They improve their communication abilities They are capable of recognizing the needs of a target audience so that they prepare information and communicate insights properly Module content Working knowledge of design principles and elements Design vocabulary based on principles of design Theories of data visualization and data storytelling User persona, storyboarding, paper prototyping Assess the quality of the data and perform explorative data analysis Eectively choosing visualization techniques Design principles Forms, colors, etc Preattentiveattributes Storytelling with data Interplay between narrative and visual communication Designing Infographics, interactive dashboards for the target audience Combine and the data and follow the best practices to present your data Examine, navigate, learn to use various features of Tableau Tools Tableau Desktop, Tableau Public, Tableau Prep Basics of Data preparation and Tableau Time series, aggregation, and lters Creating maps, working with hierarchies Working with interactive action Filter and Highlighting Joining and Blending, Dual Axis charts Table Calculations, Level of Detail L OD Calculations Groups and Sets Advanced Dashboards, Data Storytelling Literature recommendations Alexander B The New Digital Storytelling Creating Narratives with New Media, A BC Clio, 2011  Berinato S Good Charts The H BR Guide to Making Smarter, More Persuasive Data Visualizations, Harvard Business Review Press, 2016  De Barros A Practical S QL A Beginners Guide to Storytelling with Data, No Starch Press, 2018  Evergreen S DH Eective Data Visualization The Right Chart for the Right Data, Sage Pubn, 2016  Foreman J W Data Smart Using Data Science to Transform Information into Insight, 1st edition, Wiley, 2013  Marr B Big Data Using S MA RT Big Data, Analytics and Metrics to Make Better Decisions and Improve Performance, 1st edition, Wiley, 2015  Nussbaumer Knaic C Storytelling mit Daten Die Grundlagen der eektiven Komunikation und Visualisierung mit Daten, 1  Auage, Vahlen, 2017  Provost F Fawcett T Data Science for Business What you need to know about data mining and data analytic thinking, 1st edition, OReilly, 2013  Wong D M The Wall Street Journal Guide to Information Graphics The Dos and Donts of Presenting Data, Facts, and Figures, Reprint- Auage, Ww Norton Co, 2014  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 31 AD SA Data Visualization and Storytelling 3 Advanced Data Visualization Module responsibility Prof Dr Swati Chandna Level Master Module Nr 4097 Credits 5 benotetDuration 1 Semester Frequency 2x each year Workload Total 125 hours presence time 75 hours self study 50 hours Teaching and learning methods Group Work Exercise Lecture Language English Participation requirements Knowledge of design principles and the process of data storytelling Examination Project work Course work NA Constructive Alignment The module Data Visualization and Storytelling 3 provides students with the advanced knowledge and practical skills to develop dashboards for visual data analysis for eective communication of insights regarding a specic business problem using Power BI and D3.js Also, one of the most important concepts students will learn in this module is to conduct a heuristic evaluation for usability in data visualization This module introduces the skills required to create professional business dashboards using Power BI, and will enable students to recruit users and conduct user interviews for evaluating the dashboards This module is meant to prepare students to apply advanced data visualization concepts in data science projects that require the creation of interactive visual interfaces for descriptive data analysis The students analyze dierent story examples and subsequently develop their own stories based on role plays The methodological spectrum reaches from data selection and visualization to interpretations for dierent target groups This work is performed individually as well as in teams Finally, the students or their respective groups present their developed stories to each other and subsequently critically reect on the results The examination form consisting of project work suits the module intention adequately because there is a continuous documentation and evaluation of the students improvements in competencies as well as of the status of the data story Qualication goals learning outcomes and competencies Professional competence The students are able to use advanced data visualizations for interactive storytelling to support exploration results The students will learn other advanced visualization techniques Thestudents will understand complete business intelligence workow from end to end The students will be to able to blend and create beautiful and advanced interactive dashboards The students will learn what is user experience in data visualization Methodological competence After completion of this module the students know other visualization technologies such as Power B I, and D3.js and are able to use them in a target oriented manner.-Theyare able to conduct cognitive walkthrough, recruit user for usability studies, and conduct user observations Social competence They improve their communication abilities They are capable of recognizing the needs of a target audience so that they prepare information and communicate insights properly.-Theyare capable of conducting user interviews Module content Working knowledge of design principles and data storytelling process Advanced data visualization techniques Power B I Connecting and Shaping Data, Creating Table Relationships and Data Models, Analyzing Data D AX Calculations, Visualizing Data with Power BI Reports, Articial Intelligence Visuals SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 32 Literature recommendations Few S Information Dashboard Design, Analytics Press, 2015 Iliinsky N, Steele J Beautiful Visualization, O Reilly Media, 2010 Cairo A The functional art An introduction to information graphics and visualization, New Riders Publishing, 2012 Berinato S Good Charts The H BR Guide to Making Smarter, More Persuasive Data Visualizations, Harvard Business Review Press, 2016  Evergreen S DH Eective Data Visualization The Right Chart for the Right Data, Sage Pubn, 2016  Foreman J W Data Smart Using Data Science to Transform Information into Insight, 1st edition, Wiley, 2013  Marr B Big Data Using S MA RT Big Data, Analytics and Metrics to Make Better Decisions and Improve Performance, 1st edition, Wiley, 2015  Nussbaumer Knaic C Storytelling mit Daten Die Grundlagen der eektiven Komunikation und Visualisierung mit Daten, 1  Auage, Vahlen, 2017  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 33 Scope Data Privacy SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 34 AD SA Privacy, Ethics and International Law Module responsibility Prof Dr Swati Chandna Level Master Module Nr 3639 5 Credits 6 benotetDuration 1 Semester Frequency 2x each year Workload Total 150 hours presence time 75 hours self study 75 hours Teaching and learning methods Problem oriented learning Language English Participation requirements none Examination Written Examination Course work NA Constructive Alignment During this module, the students develop fundamental knowledge about privacy, ethics, and the judicial aspects in the context of data analysis They generate awareness of ethically relevant problems, and they are able to evaluate individual, social, and institutional actions in socio technical situations e.g., based on privacy law Additionally, they learn to impose privacy requirements through organizational- technical measures During the module, the students learn, analyze and discuss ethical and judicial aspects in the context of big data and data analysis through well dened practical examples as well as presentations The gained theoretical competencies in the existing privacy laws and regimentations are evaluated through a written examination This form of examination enables the students to reproduce, apply and discuss judicial aspects of privacy law on well dened examples and scenarios This module enables the students to develop technical and organizational measures to enforce privacy and personality laws in big data projects and data analyses The evaluation of the students competence is performed via casework Qualication goals learning outcomes and competencies Professional competence M US S N OC H V ER TE IL T W ER DE N Abstract ------------------Thestudents are able to examine contexts of origin and eects from an ethical perspective and may apply ethical and privacy concepts on dened examples of socio technical scenarios Theyknow the prerequisites of a transparent, informed approval as well as the prerequisites of data transfer and may derive consequences for big data projects.-Theyare capable of reproducing and applying the principles of data curation and utilization according to national and international law They know and exert the relevant privacy laws, regulations and stragies Methodological competence The students know and target orientedly apply organizational as well as technical measures to impose privacy and personal rights Social competence The students may analyze and evaluate well dened problems independently Theya are able discuss in a functional and scientic way Module content Ethics and international law Terminology of ethics, business ethics Ethics within the technical civilizationoccupations Individual and institutional ethics Ethical codices for computer scientists Ethics within an interconnected world Lawful actions and conict of interests Rights of the persons aected International data processing and jurisdiction SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 35 Principles of appropriation and approval requirements Regimentation in big data inquirys Contracts regarding data and data analyses German privacy, internet and communication laws Bundesdatenschutzgesetz, Telemediengesetz, Telekommunikationsgesetz-Datatransfer within a business and places outside the E U Privacy and its enforcement Principles of privacy law Data separation Technologies to enforce privacy requirements Organizational measures Anonymization and pseudonymization Application scenarios Risks caused by data aggregation Misuse of data Literature recommendations Bachmann R et al Big Data Fluch oder Segen Unternehmen im Spiegel gesellschaftlichen Wandels, mitp Press, 2014  Dorschel J Praxishandbuch Big Data Wirtschaft Recht Technik, Springer Gabler, Heidelberg, 2015 Gola P Reif Y Praxisflle Datenschutzrecht, 1  A., Heidelberg, 2013 Grunwald A Technikfolgenabschtzung, 2  Auage, Berlin, 2010  Hausmanninger T Capurro R Netzethik Grundlegungsfragen der Internetethik, Mnchen, 2002  Kuhlen R Informationsethik, Konstanz, 2004  Lenk H Ropohl G Technik und Ethik, Stuttgart, 1993  Richter P Privatheit, entlichkeit und demokratische Willensbildung in Zeichen von Big Data, Nomos, Baden Baden, 2015 Stamatellos, G Computer Ethics A global perpective, Sudbury, 2007  Stoecker R et al Handbuch Angewandte Ethik, Stuttgart, 2011  Taeger J Einfhrung in das Datenschutzrecht, 1  A., Heidelberg, 2013  Worms N Informationsethik und Online Netzwerke Im Spannungsfeld zwischen struktureller Bedingtheit und Privatsphre, 1  A., Berlin, 2010  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 36 Elective Module SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 37 AD SA Case Study 2 Module responsibility Prof Dr Swati Chandna Level Master Module Nr 3629 Credits 14 benotetDuration 1 Semester Frequency 2x each year Workload Total 350 hours presence time 105 hours self study 245 hours Teaching and learning methods Casework Group Work Project work Language English Participation requirements First steps into case studies Data Engineering Analytics 2 Examination Project work Course work Case Study, Presentation, Project work Constructive Alignment The examination forms project work and presentation evaluates the students abilities to document their learning progress application- specically Through the application of the skills and competencies acquired in the previous modules during case studies 2 the students are given the opportunity to intensify their knowledge in a holistic manner Furthermore, the students learn to evaluate the applicability and cooperation of methods, techniques and tools in a context of a certain project The project work is an adequate examination for project module because of its emphasis on the visualization and communication of the resultsrecommendations Moreover, the students proof their ability to communicate the recommendations based on the results of the case studies via the nal presentations By documenting their progresses within the module using a learn journal the students are given the opportunity to solve problems in a self reecting manner Qualication goals learning outcomes and competencies Professional competence The students are able to identify a real world research problem, translate it into a complex Data Science research question, and address it using adequate methods from the eld of Data Science The module results are to be communicated as business oriented advice and, if possible Thestudents are able to identify a research question and transfer it into a Big Data question They are able to plan and pursue a Data Science Project according to the 6 step process of Data Science.-Theyare able to identify the data being necessary for this question and are able to properly estimate the value of the data in the context of the problem Theyare able to prepare their data as required for the analysis or machine learning algorithms Theycan execute a predictive analysis with the help of established tools and software.-Theyare capable to adequately visualizing and communicate the results in the context of the developed problem Methodological competence The students are able to specically apply creativity techniques to develop a problem and to identify required data.-Theyevaluate properly the applicability of methods and tools for the dierent phases of the Big Data project in context of a certain project and are able to select and execute the adequate methods.-Theyinterpret and evaluate the results of the analytics process with regard to the developed Big Data problem Self competence The students are able to take their role within the virtual team parallel to their occupational activities and organize multiple tasks i.e occupation, private life and studies simultaneously Social competence The students know the fundamental tools to organize working processes in virtual teams as well approaches dealing with team intern conicts They are able to use both tools to accomplish results mutually Module content Project management Organization and management Creativity techniques, formulation of questions Data management Big Data architectures SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 38 Predictive analytics tools Data mining tools Visualization tools Predictive Analytics Ethics Predictive Customer Insight Literature recommendations Chamoni P Gluchowski P Analytische Informationssysteme Business Intelligence Technologien und -Anwendungen, akt Auage.-Dorschel J Praxishandbuch Big Data Wirtschaft Recht Technik, Springer Gabler, Heidelberg, 2015 .-Freiknecht J Big Data in der Praxis Lsungen mit Hadoop, Hbase und Hive Daten speichern, aufbereiten, visualisieren, 2014 .-Kemperet al Business Intelligence Grundlagen und praktische Anwendungen, 3  Auage, Vieweg, Wiesbaden, 2010 .-Koster K International Project Management, Sage Publications Ltd., 2009  S RH University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 39 AD SA Internship Module responsibility Prof Dr Theodoros Soldatos Level Master Module Nr 2532 Credits 14 benotetDuration 1 Semester Frequency 2x each year Workload Total 350 hours presence time 10 hours self study 340 hours Teaching and learning methods Internship Language English Participation requirements none Examination Scientic Poster Presentation and Internship Report Course work NA Constructive Alignment The Internship phase is an essential element of the Applied Data Science and Analytics A DS A master program, oering students hands on experience applying the knowledge and skills acquired in a real world industry setting Its primary objectives are to provide professional expertise, familiarize students with relevant industries and companies, and prepare them for successful careers The internships main focus is to apply the knowledge and skill gained in the study course in practical work The students will be informed about the intention, content, and possibilities for an internship compare internship regulations They search and apply for internship by themselves and develop their social skills in interviews The lecturers have a consulting role Interim meetings with the supervisor and, optionally, the mentoring professor ensure that the internship has an optimum outcome To integrate practical and theoretical aspects of the program, students must submit a written report on their internship experience, allowing them to reect on it critically, apply theoretical knowledge to real life problems, compare and contextualize theory with practice, and make informed decisions about future specializations It aims to allow students to document and reect upon their personal learning process and skill acquisition during the internship In addition, they should present examples of how the material and methodology skills they acquired throughout their studies were applied in the eld At the end of the internship, students are expected to have developed a comprehensive understanding of the latest technologies used in various business environments and their applications The program also fosters critical thinking skills and the ability to compare and evaluate the benets of modern technologies in applied data analysis and computer science A DS A constructive alignment ensures that the student cannot but achieve the desired learning goals Qualication goals learning outcomes and competencies Professional competence Upon completing the working internship, a student should have acquired practical skills, knowledge, and professional competencies necessary for success in their future career.-Thestudents are able to show deeper knowledge, understanding, and attitudes in the context of the practical work in the eld of data science.-Thestudents are able to participate in a structured and supervised work experience, where theoretical knowledge gained in the classroom can be applied to real world situations Thestudents are able to ask questions, seek clarication, and engage in critical thinking to deepen their own understanding of the workplace and industry.-Thestudent are able to will receive constructive feedback on their nal presentation, with suggestions for improvement and areas of strength Methodological competence The students can apply the knowledge and skills learned during the masters program in a real world industry setting Thestudents are able to compare and contrast the industrys work practices with the theoretical expectations learned during the program Thestudents are able to reect on personal learning processes and skill acquisition during the internship, including how the skills and materials learned in the program were applied in the eld Thestudents are able to acquire state of the art knowledge regarding the methods and tools used for applied data analysis and analytics tasks within various organizations, including industry, administration, and research organizations Thestudents are able to present examples of how the material and methodology skills they acquired throughout their studies were applied in the eld Self competence SR H University Heidelberg Master of Science M Sc Applied Data Science and Analytics Curriculum last edit 01 04 2023 025304 40 -The students are able to eectively communicate and present the results of their own work in a company, as well as during scientic poster presentations and subsequent discussions in front of an academic audience.-Thestudents are able to demonstrate appropriate professional behavior in various workplace environments and adapt accordingly Thestudents can analyze and discuss both orally and in written form various concepts for solving applied data science problems and break down large problems into manageable parts for eective project planning and execution Social competence The students can work eectively in professional teams, Thestudents are able to manage communication within teams and in meetings with colleagues and coaches and contribute to discussions in a valuable way These experiences help students develop strong communication and collaboration skills essential in the workplace Module content The Internship should focus on as many aspects of data science as possible i.e., from data collection to data management, analysis, and storytelling Literature recommendations Links Google Scholar D BL P I EE E Computer Society I EE E T VC G camera ready document guidelines Literature management Citavi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Applied Data Science and Analytics Applied Data Science and Analytics Master of Science Big Data -Challenges 20 October2021 SR HHochschule Heidelberg BD BA2 Discovery Data Quality Storage Security Analytics Difficulty in finding patterns and insights Problem managing large amount of data sets Keeping data secure is a challenge Performing right analysis Messy, Inconsistent,Incomplete S RH Hochschule Heidelberg Prof Dr Swati Chandna Learning Outcomes In depth knowledge and understanding of Applied Data Science Application of analytical techniques and machine Learningdeep Learning algorithms to solve complex data problems in business Skills you need to leverage data to reveal valuable insights and help make customers valuable decisions Formulate technical problem solutions and represent them in discourse 12322 3 SR H Hochschule Heidelberg Prof Dr Swati Chandna Masters in Applied Data Science and Analytics 4 Understand Collect Clean Visualize Resolve Analyze Case studies data engineering data management analytics visualisation Data storytellingund communication S RH Hochschule Heidelberg Prof Dr Swati Chandna Course Curriculum S RH Hochschule Heidelberg Prof Dr Swati Chandna5 Study accordingtothe CO RE principle Competence Oriented Research and Education Our activated teaching learning method Group project,Flipped classroom, case studies, project pitchesfrom local companies Practical teaching and exams5 week blocks no end of the semester exam stressFocus only on 1 2 subjects in 5 weekblocks Direct contact hours with professors 6 SR H Hochschule Heidelberg Prof Dr Swati Chandna ThankyouProf Dr Swati Chandna Study Programme Director Applied Data Science and Analytics, M Sc Swati.chandnasrh.de Admissions Office Studyinheidelbergsrh.de Connect withyourstudentson Unibuddy7 hsheidelbergS RH Hochschule HDS RH Hochschule HDS RH Hochschule Heidelberg\n",
      "\n",
      "\n",
      "erweiterung Special Activities 2024comfortable comfortable picturesque picturesque exciting exciting moving moving culinary culinary sporting sporting cultural cultural informative informative Heidelberg Marketing Gmb H Neuenheimer Landstrae 5 69120 Heidelberg, Germany Phone 49 6221 5840 223 - 225 Fax 49 6221 5840 222 guideheidelberg-marketing.de marketing.com The service team is pleased to help you Monday Thursday 900 am 500 pm Friday 900 am 300 pm subject to change Welcome to Heidelberg As early as in the 19th century, Heidelberg cast its spell on poets such as Joseph von Eichen dorff and Johann Wolfgang von Goethe with its romantic charisma The city was immortalized in many publications Heidelberg has managed to preserve this irresistible magic to this day Heidelberg has many facets its romantic castle ruin towering over the roofs of the picturesque Old Town, idyllic nature around the Neckar River, which is winding its way through the green valley between the Knigstuhl Kings Seat and Heiligenberg Holy Mountain, the inspiring university that has drawn those looking for knowledge from around the world into the city for more than 600 years all of this enchants millions of guests in Heidelberg every year Let the city cast its spell on you as well We look forward to seeing you Your Heidelberg Marketing team heidelberg4you Special Activities 2024 On the following pages, we have compiled some diverse programs for you Let us spoil you with culinary delights and experience Heidelberg in all its diversity whether local wine, beer or other delicacies, there are no limits for your enjoyment Choose between varied tours through the Old Town or through the world famous castle and its castle garden, which offer a wonderful view of our picturesque city Hike to the idyllically located Benedictine abbey Stift Neuburg or get to know Heidelberg from a different perspective by bike or by boat Numerous guided tours in this fascinating city with its lively history will enrich your stay with interesting and exciting stories Immerse yourself in the special flair of Heidelberg and ecome part of our city We look forward to welcoming you as a guest in Heidelberg and offer you free assistance in planning your stay to make your Heidelberg experience as pleasant as possible Contact us to take advantage of our one stop full service for your perfect vacation Your Heidelberg Marketing team Note Please note that the statutory provisions do not include any statutory revoca tion right after conclusion of the contract for contracts on services for domestic tourism, in particular for contracts on accommodation, guest tours and cultural events Only a withdrawal usually subject to fees according to the agreed terms and conditions or the statutory provisions is possible in most cases Our service your benefit free and professional consulting mediation of qualified tour guides booking of boat trips and bus transfers and much more around your stay Special Activities 5 Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de 6 Tours of the Old Town and the castle Tours of the Old Town and the castle Prices for all Old Town Castle Tours Prices in Hours German Foreign languages 1 1 , 5 100 110 2 110 120 2 , 5 3 135 145 4 145 155 5 165 175 6 185 195 7 205 215 8 255 265 Recommended group size max 20 persons per tour guide Languages German, English, Arabic, Chinese, Czech, Danish, Dutch, French, Italian, Japanese, Polish, Portuguese, Romanian, Russian, Swedish, Spanish, Turkish Note For Castle Tours, the admission fee to the castle courtyard is not included in the rates For more information on castle courtyard admissions, see pages 32 33 castle ticket incl funicular railway Tip If you arrive with your own bus, we recommend the Neckarmnzplatz or, if you arrive individually, the Lwenbrunnen Lions Fountain at the Universittsplatz University Square to meet with your tour guide For other meeting points, see the booking form on page 38  Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Tours of the Old Town and the castle 7 1 Guided Tour of the Old Town The oldest part of Heidelberg has a lot more to offer than just the Alte Brcke Old Bridge, the picture perfect alleys and the unique view to the most famous ruin in the world The Old Town is vibrant with its charming squares and a mixture of people of all ages and countries They meet in many small cafs and pubs and not only the unique cultu ral programs of our museums and theaters attract our visi tors Discover Heidelberg with your guide and really get to know the city Recommended duration 1 5 or 2 hours 3 Old Town and Castle Tour The combined Old Town and Castle Tour brings history to life Use a guided tour through the winding alleys of the Old Town to learn about the many facets of Heidelberg sleepy squares, Germanys oldest university, the Heiliggeistkirche Church of the Holy Spirit and the Jesuit quarter After wards, the funicular railway will take you up to the castle Explore the electoral ruin on a tour through the inner court- yard and the castle garden and visit the famous Groes Fass Great Barrel Recommended duration 3 hours 5 Old Town Tour with Boat Trip Discover the beauty of Heidelberg in an Old Town Tour followed by experiencing the citys special flair during a 50 minute boat trip along the banks of the Neckar River This combined guided tour on foot and by boat will intro duce Heidelbergs trio of river, city and castle to you Dates April October, between 1130 am 600 pm Recommended duration 2 hours Languages German, English, Danish, Dutch, French, Spanish Price plus ticket for the boat trip Meeting points Neckarmnzplatz, Kornmarkt, Universi ttsplatz University Square, Marktplatz Market Square or the boat landing stage2 Castle Tour Heidelberg Castle majestically towers above the roofs of the Old Town, inspiring millions of people every year A gui ded tour of the castle courtyard and the castle garden will allow you to immerse yourself in the eventful history of the world renowned ruin A visit to the Groes Fass Great Bar rel, the largest wine barrel in the world that has ever been filled, is part of the program as well Recommended duration 1 5 hours 4 City and Castle Sightseeing Tour Get to know the city with a City Sightseeing Tour approx 1 5 hours in your own bus Immerse yourself in the fascinating history of Heidelberg and admire the worldfamous sights comfortably from the bus You will continue on to the castle Explore the electoral ruin by foot in a guided tour no inner rooms Recommended duration 2 5 hours Note For groups with more than 20 persons, a second tour guide will be needed for the Castle Tour surcharge German 100 , other languages 110  Note Tour busses travelling from Heidelberg Old Town to the castle must take the detour via Gaiberg The Neue Schlossstrae is closed to vehicles with a total weight of 3 5 tons or more until further notice The detour route of approx 15 km leads via Leimen, Lingental and Gaiberg Please note that the journey to the castle and therefore the total duration of the City and Castle Sightseeing Tour will be extended by approx 30 minutes due to this detour If you have any questions, please do not hesitate to contact us at any time Note Please note that the Ziegelhuser Brcke Ziegel huser Bridge may only be used by vehicles up to 3 5 tons Tip Are you looking for a digital guided tour for your event or would you like to explore Heidelberg comfortably from home Then book our Digital guided tour A different kind of city tour digital and yet personal Duration 30 minutes, price 35  Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de 8 Costume Tours Costume Tours Booking information for all Costume Tours Languages German, English Group size max 20 persons per tour guide 6 Murder Mystery Tour The death in sight Murder at the university A death at the university was it murder A terrible secret awaits in the middle of the cozy Heidelberg Old Town A murder mystery tour will take you back to the time around the turn of the century Are the professors and their wives truly as venerable and righteous as they seem Are the stu-dents as reputable and the bourgeois daughters as deco rous as the better circles of society desire Uncover the truth during your walk through the alleys of the Old Town, where evidence and witness statements wait to be disco vered Experience this exciting, interactive murder mystery tour together with various historically dressed actors Dis cover the time around 1900 up close in an entertaining manner Duration approx 2 hours Price German 248 , English 2587 Sweet Temptations and Naked Truths Heidelberg Love Stories Let us follow some lovers from Heidelbergs history, liste ning to the citys love stories This walk through the Old Town will tell you of sweet temptations and naked truths, of divorce and strife, romantic yearning and electoral be droom stories Let delicate passions, sensuous poetry and sinful love letters fascinate you, while recipes for love spells and impotence treatments offer practical advice for life This amorous view of historical morals offers a frivolously amusing view of the customs of love and marriage through the centuries Love is always a fitting subject Duration approx 1 5 hours Price German 150 , English 160 Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Costume Tours 9 8 Out in old Heidelberg with the Brgersfrau citizens wife Up close to the action A citizens wife in historical dress tells the stories behind history, presenting everything first hand Past eras come back to life as she chats about every day excitement in the Old Town, bad student habits, court intrigues and much more All of this is served with a good dash of humor and a pinch of scandal Let us surprise you Duration approx 1 5 hours Price German 150 , English 160 10 Through the dark alleys of Heidelberg with the night watchman Since the Middle Ages, the night watchman has ensured the safety of the citizens within the town walls When the romantics began to celebrate Heidelberg in songs and im mortalize the enormous castle ruin in copper engravings at the start of the 19th century, guests from all over the world came to the Neckar River The night watchman pro tected them as well from robbers, fraudulent antiques dea lers and other immoral offers Walk through the alleys of the Old Town by the side of the night watchman or his daughter while you are told long forgotten stories from the past Duration approx 1 or 1 5 hours Price German 150 , English 160 approx 1 hour German 180 , English 190 approx 1 5 hours9 Witches, oppressors, poor sinners from Heidelbergs legal history Executions used to be true spectacles for everyone In 1762 , the university was closed to allow the students to at tend an execution a revolting example In this guided tour, a real witch will take you through the centuries On the way through the city, she reports of amusing regulati ons and gruesome punishments City history not fit for children or weak nerves Duration approx 1 5 hours Price German 150 , English 160 11 With washerwoman Babett through Heidelberg The big wash was a tough business back in the day grin ding, rubbing, crushing, beating, rinsing, bleaching, dry ing Best done in company Listen to the stories of the 18th century washerwoman Babett Not one to mince words, the good woman chats about hard work, soft water, every day life, juicy details and Heidelbergs history Duration approx 1 5 hours Price German 150 , English 160 Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de 10 Thematic Tours Thematic Tours Booking information for all Thematic Tours Duration approx 1 5 2 hours Languages German, English, other languages available Price German 130 , other languages 140 student groups German 105 , other languages 115 Group size max 20 persons per tour guide Note Possible admission fees not included, costume surcharge 50 , costumed appearances on request 12 Heidelbergs Nobel Prize Laureates of Medicine The excellent reputation of the Heidelberg University is not least based on its Nobel Prize laureates from the areas of physiology or medicine Adolf Kossel, Otto Meyerhof, Bert Sakmann, Harald zur Hausen Learn about the scientific achievements, the special circumstances of the discoveries and the scientists own personalities on a tour of the laure ates worksites Discover the excitement in research Meeting point In front of the main entrance of the Kopfklinik north side, Im Neuenheimer Feld 40013 Architectural exploration through Heidelbergs Old Town The winding alleys of Heidelberg date back to the Middle Ages The harmoniously looking Old Town is made up of buildings from many different styles and eras This ex-pert walk will take you from the origins to the current building projects From the icons of tourist interest such as the university library, you will move on to the hidden beauties, such as the Bluntschli house This is an exciting search for tracks and traces, not only for architecture en-thusiastsInformation and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Thematic Tours 11 14 Heidelberg at the time of Romanti cism Down there in verdant meadows, a mill wheel turns around Famous Heidelberg Romanticism the young poets Joseph von Eichendorff, Clemens Brentano, Achim von Arnim and their friends turned Heidelberg into a center of late Ro-manticism in the early 19th century Their traces can still be found throughout the city and will be brought to light in this guided tour of the Old Town 16 The famous Philosophenweg Philosophers Walk Heidelberg at its most beautiful Once, scholars were walking here in their stiff frock coats, relaxing their thoughts during strolls today, the Philo sophenweg beckons with unique views of the city, the ri ver, the Alte Brcke Old Bridge, the castle and the near ly 600 meters high Knigstuhl Kings Seat Walking up the Philosophenweg, you will be rewarded with a specta cular view The path will also take you to the Philosophen grtchen Philosophers Garden, on the sunspoiled na tural balcony on which an abundance of exotic plants is thriving 15 In the beginning, there was thirst Pub and restaurant culture in Heidelberg Enjoyment is at the focus of this entertaining guided tour, full of anecdotes Heidelberg is famous for its pub and inn culture Rustic pubs, student drinking ha-bits and specialties this tour will spirit you away into the enjoyable history of Heidelberg, from the Homo heidelbergensis to this day Note No food and beverages are served during this tour 17 The University in the Old Town Founded by Prince Elector Ruprecht I in 1386 , the Ruper to Carola is Germanys oldest university and one of the most venerable education facilities in Europe The tour conveys not only the universitys history but also provides an insight into student life It takes you from the university library and the Peterskirche St Peters Church, the oldest church in Heidelberg, to the Alte Aula Old Auditorium and the historical Studentenkarzer Student Prison Visit the place that once made people suffer From 1778 to 1914 , students were punished there for gentlemanly offences Price The entrance fee of 6 per person for the Alte Aula Old Auditorium and the Studentenkarzer Student Prison will be added Interior visits are only possible on request and accor ding to availability on certain days Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de12 Thematic Tours 18 Bergfriedhof Mountain Graveyard Fire, Earth and Immortality Concealed behind high walls on a former vineyard and deli berately designed as a park, Heidelbergs Bergfriedhof sur prises with its new Garten der Kulturen Garden of Cultures Poetry readings are held in a pavilion Botanical treasures and monumental graves ornamentally line the paths Fami liar names of famous Heidelberg citizens bring to mind the relevance of this city in past centuries Meeting point In front of the main entrance of the Berg friedhof bus tram stop Bergfriedhof Note The tour must begin before twilight Male partici pants must cover their heads while in the Jewish graveyard Tours on Saturdays exclude the Jewish part of the grave yard 20 Queer Tour Heidelberg Heidelberg and homosexual history Join us for a walking tour in the Old Town through the Rainbow City Heidelberg since September 2020 Heidel berg is a member of the Rainbow Cities with the focus on Heidelberg and homosexual history Get to know the most important sights of Heidelberg from the point of view of queer life, but also visit lesser known places in the city that have a special connection to queer culture The tour meant to remind one to contemplate and to remem ber those, who have been forgotten Additionally, it shows you how colorful, vigorous and encouraging queer life in Heidelberg is Duration approx 2 5 hours Meeting point Karlsplatz Carls Square, Sebastian- Mnster Brunnen Sebastian Mnster Fountain Note The tour is bookable Friday Sunday, currently only in German 19 Christmas Market Walking Tour Take a walk through the Old Town and immediately get in the Christmas spirit with the scent of roasted almonds and mulled wine Lovingly arranged booths spread over various historical squares The unique backdrop with Heidelberg Castle towering above the Old Town creates an outstanding atmosphere and makes the Heidelberg Christmas Market one of the most fairy tale like events in Germany The Advent themed guided tour provides interesting information on the regions Christmas and pre Christmas tradi tions The walk starts at the Marktplatz Market Square and passes the most beautiful corners of the Old Town ending at the Universittsplatz University Square 21 Heidelberg History criss cross Take this entertaining walk through the historic Old Town and find out what kind of cross the people of the Palati- nate had and why beer was helpful when changing religi on Learn how the medieval navigator looked like and why it is good for your knees to have the right elector Learn about lousy times, fun student life and unexpected haute couture problems The tour is a criss crossing mix of city history and anecdotes of everyday life Note Currently the tour is only available in German Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Thematic Tours 13 22 The Bahnstadt Research, Living, Work A new district comes into being The extent to which the university and the science, re-search and education sectors have helped shape the citys distinctive complexion over the centuries can be seen in Heidelbergs history, townscape, architecture and also in numerous new projects Today, the structure and image of the city are inextricably linked with scientific de-velopments These developments are particularly visible in Heidelbergs newest district, the Bahnstadt It is one of the worlds largest passive housing areas and relies on innovative solutions to combine environmental protec tion, attractive living space and optimal infrastructure Meeting point Sky Labs, Bahnstadt district 24 B OD Y W OR LD S The Anatomy of Happiness Heidelberg is the location of Germanys oldest university and has been a center of medical innovation for centuries Walk through the history in the footsteps of medical action and learn about Heidelbergs diverse history of anatomy When and where was the first department of anatomy set up What happened in the early anatomic theater Hu man health is directly connected to the development of medicine The tour therefore ends at the entrance of the K RP ER WE LT EN Museum B OD Y W OR LD S museum in the Altes Hallenbad, where you can subsequently explore the exhibition The Anatomy of Happiness on your own Opening hours Monday Sunday 1000 am 600 pm last admission at 500 pm Closed on December 24  Price A museum admission fee of 19 per person Monday to Friday, 21 per person Saturday, Sunday and public holidays, incl audio guide GermanEnglish via smartphone, will be charged additionally Meeting point Anatomiegarten at the Bunsendenkmal Bunsen Monument23 Coffeehouse culture in Heidelberg On the way through the scenic alleys of Heidelbergs Old Town, you follow the traces of Heidelbergs cof feehouse culture once and today Where did crowned heads, corporate students and coffeehouse poets meet What was the favourite snack and how much did a cup of coffee cost Get to know which specialties can be discovered in Heidelbergs cafs and where small, family run coffee roasters and a real coffee sommelier are located 25 Lets get down to brass tacks The Proverbs Tour My name is Hare, I dont know a thing But dont worry, you dont have to feel that way Find out what you had on the ball auf der Pfanne haben in Heidelberg, what went beyond the pale nicht auf eine Kuhhaut gehen and how you were fobbed off abgespeist Dont make tracks Kratzen Sie nicht die Kurve, because well untie the ety mological knots for you and certainly wont lead you down the garden path ein X fr und U vormachen after all we majored in the subject After that, youre allowed to pull our leg, too einen Bren aufbinden For real You may be lead but not on a merry chase by Dr Kauder Welsch or Dr Gobble de Gook The special city sightseeing tour with aha effect Experience Heidelberg in entertaining word stories, idioms and plays on words Note Also recommended as a guided tour for school classes from grade 10 14 Active Tours Active Tours 26 Guided Hike to the Benedictine Abbey Stift Neuburg The hike will take you along the Philosophenweg Philo sophers Walk, Heidelbergs famous natural balcony, and on through the forest into Neuenheimer Schweiz Neuenheims Switzerland, an impressive nature preser ve with beautiful views of the Neckar valley Finally, you will reach the old Benedictine abbey Stift Neuburg, where you can visit the monastery church after your hike Duration approx 3 hours Languages German, English Price German 130 , English 140 Meeting point Bus stop Bergstrae Neuenheim Group size max 20 persons per tour guide27 Heidelberg City Rally Exploration tours as communicative experiences The play ing field is Heidelbergs romantic Old Town with its winding alleys and beautiful squares The participating teams must solve tricky riddles and tasks All participants will receive a certificate and the winners will get an additional gift Duration 2 hours Languages German, English, French Price German 130 , other languages 140 Group size max 28 persons per tour guide Note If preferred, the rally can end and be evaluated in a restaurant in the Old Town The costs for food and beve rages are not included and must be paid separately on site Please provide a list of participants for the certificates Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Active Tours 15 28 Guided Bike Tour through Heidelberg Explore Heidelbergs Old Town on a guided bicycle tour and visit the Neuenheim quarter, the Neuenheimer Feld with its Botanical Garden and the internationally renowned institutes and clinics as well as the research campus of the Ruprecht Karls Universitt Ruprecht Karl University The tour continues through the Handschuhsheimer Feld to Handschuhsheim, Heidelbergs oldest quarter with the fa mous Tiefburg and back to the Old Town, where the unique view of the Heidelberg Castle and the Old Town from the Alte Brcke Old Bridge offers a grand finale Duration approx 3 hours Languages German, English, Dutch Price German 130 , other languages 140 Group size max 10 persons per tour guide29 Guided Bike Tour on the Lower Neckar River On a guided bike tour along the lower Neckar River you will discover both flora and fauna of the Altneckar as well as as pects of life and work along the river You will cycle along hidden architectural treasures and get to know Roman tra ces in Ladenburg and rural Neubotzheim After having passed Heidelbergs vegetable garden the tour continues to the centre of scientific research and finally reaches the favo urite recreation area of Heidelbergs residents Duration approx 5 hours Languages German, English Price German 170 , English 180 Group size max 10 persons per tour guide Note Please carry appropriate protective clothing e.g rain jacket, rain cape depending on the weather condi tions At the beginning of the tour, you will receive a brie fing from the tour guide The route length is about 30 km, profile no incline, possibility of picnic and stop Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Note for Guided Bike Tours For bike rental, please contact our partners directly Ifa Gmb H Radolino Fahrradverleih Radhof Bergheim Lahrer Strae 24 Bergheimer Strae 101 69126 Heidelberg 69115 Heidelberg Phone 49 163 6213898 Phone 49 6221 6599452 inforadolino.de martin.rachfahlifa-heidelberg.de and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de16 Active Tours 30 Segway Tour Highly Philosophical The Segway Tour reveals an entirely new perspective Glide along the Neckar River to the historical and trendy quarter Neuenheim accompanied by a tour guide After wards, you move on to the zoo, the Neuenheimer Feld with its impressive university and hospital buildings and up to the Philosophenweg Philosophers Walk This Hei delberg sight is not easily seen at first glance The former vintners path was once taken by scholars who desired the inspiration of the Heidelberg Trio The harmony of the baroque city, the Heidelberg Castle and the Neckar River It is an extraordinary view Dates February November Duration approx 1 hours Languages German, English Price 59 per person including helmet for rent and segway license Meeting point Neckarmnzplatz Group size 10 20 persons, smaller and larger groups on request Minimum requirements 14 years old, size 1 40 m, weight 45 115 kg 31 Segway Tour All in 360 Discover Heidelberg and the Neckar valley on a city safari that provides a mixture of everything Your tour starts at the Neckarmnzplatz and continues to the Philosophen weg Philosophers Walk with its fantastic view of the city The second place steeped in history is the Neuburg mo nastery, which was founded in 1130 and looks back on a fascinating past Head towards the Kpfel and Ziegelhau sen to the other side of the Neckar River and right up to the Wolfsbrunnen Proceed in the direction of Heidelberg Castle Enjoy the great view of the castle garden above the castle before continuing to the Klingenteich Passing the Jewish cemetery, the tour takes you back to the Old Town and your starting point at the Neckarmnzplatz Dates February November Duration approx 2 5 hours Languages German, English Price 69 per person including helmet for rent and segway license Meeting point Neckarmnzplatz Group size 10 20 persons, smaller and larger groups on request Minimum requirements 14 years old, size 1 40 m, weight 45 115 kg18 Culinary Offers Culinary Offers 32 Wine tasting in Heidelbergs Old Town Instructed by a wine expert, you will be able to get to know six different wines in the restaurant Sudpfanne The special tasting experience is accompanied by expla nations on the growing areas and characters of the wines The tour through the world of wine is completed by a hearty 2 course menu Languages German, English Price 57 per person Group size 10 50 persons Note Further drinks are charged separately on site Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Included Services Wine tasting with six different regional red, ros and white wines Hearty, seasonal meal of your choice April October Menu A Small mixed salad Kurpflzer Saumagen stuffed pigs stomach, served with winesauerkraut, fried potatoes and fried eggs Menu B Tomato soup Mediterranean vegetable dish with gratinated sheeps cheese November March Menu A Small mixed salad roast pork with dark beer sauce, served with winesauerkraut and fried potatoes Men B French onion soup with cheese crouton Homemade bread dumplings with mushroom cream Note Menu is subject to change Culinary Offers 19 33 Guided Tour Delectable Heidelberg Fall in love with the culinary delights of romantic Heidelberg Take a tour through the picturesque Old Town with an ape ritif matured under the sun of Baden and a regional 3 course menu in two traditional restaurants The time and journeys between the courses and locations are entertainingly spiced up with history and stories, magical elixirs, and are topped off with a sweet Heidelberg treat Duration approx 3 hours Languages German, English Price 98 per person Group size 10 25 persons Note Drinks are charged separately in each location Please note that this tour is not suitable for children Included services Guided tour Aperitif, appetizer, main course, dessert 2 surprise treats Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de34 Brewery Tour with beer tasting and Old Town Tour Have you always wondered how beer is brewed After an one hour guided tour through the historical Old Town, a master brewer will teach you everything from the world of beers Try three types of beer in the brewery cellar with a warm pretzel After this, a table buffet will be waiting for you in the brewery Languages German, English Price 61 per person Group size 15 80 persons Tip Groups with tight schedule can book the brewery tour without the preceding Old Town Tour price reduction 3 50 per person Note Further drinks are not included and charged separately on site Menu is subject to change Included services 1 hour Old Town Tour Brewery tour Beer tasting with pretzels Followed by a rich buffet in the brewery Crusted roast of Swabian pork on sauerkraut, bacon and bread dumplings, braised veal boiled fillet and grilled corn poularde on seasonal vegetables, homemade spaetzle and buttered potatoes, Mediterranean vegetable casse role with peppers, eggplant, zucchini baked with feta cheese and crme fraiche vegetarian Additional tip Abtei Stift Neuburg Benedictine Abbey Stift Neuburg Embedded in the Neckar valley, surrounded by a lush green and with a wonderful view of the meandering river and the slopes of the Odenwald, the monastery attracts locals and guests alike The Benedictine abbey was found ed by the monastery Lorsch in 1130  It was struggling with its bad economic situation until the monastery was dissolved in 1562  For a long time, the buil ding was given to different uses thereafter, among others as the Jungfern- Stift, where women from noble families were to live together as a communi ty of virtuous women therefore, it is called the Stift Neuburg to this day Since 1927 , monks have been living here again according to the rules of Saint Benedict The monastery includes a small organic brewery Following the rules of their Benedictine order, the monks focus on pastoral work, supporting and meeting with guests through diverse cultural events such as concerts or lectures on contemporary subjects Travel to the monastery By foot along the Neckar River or the Philosophenweg Philosophers Walk By bus with line 34 and 37 , towards Ziegelhausen, Stift Neuburg stop, By ship with the Weisse Flotte Heidelberg White Fleet, landing stage Kloster Neuburg, Information on monastery tours Monastery tours can be booked upon request and as available during the following times Monday Saturday 1000 am 400 pm, Sundays and public holidays 1130 am 300 pm Information and booking 49 6221 895143 , fuehrungenstift neuburg.de The monastery does not receive any governmental financial support The monks would be grateful for any donation on site Further information can be found here  Wi Fi Heidelberg4 You 1  Choose network Heidelberg4you 2  Connect Accept terms and conditions 3  Lets go Join more than 200 hotspots 22 Guided Tours for students, young people and families Guided Tours for students, young people and families 35 Game of Stones A different kind of history lessons Fantasy novels, movies and T V shows determine a great part of our current image of the past Is it all made up, or have there been only few changes How much is based on what actually happened Magic, sword fights, elves, dangerous intrigues and princesses in distress are true parts of Palatinate history Search for traces of J R R Tolkien, Joanne K Rowling, George R R Martin and Co in Heidelberg Duration approx 1 5 hours Languages German, English Price for student groups German 105 , English 115 Meeting point Universittsplatz University Square at Lwenbrunnen Lions Fountain Group size max 20 persons per tour guide, larger groups on request Note Age specifically adjusted guided tour for children, young adults and student groups in the age brackets from 9 13 years and 14 19 years Can be booked as outside Castle or Old Town Tour 36 The secret of the lost Carbuncle Stone A Guided Tour full of mysteries A mysterious box, which was found by a castle guide con tains an unsolved riddle from bygone times Deeper and deeper it goes into the history of Heidelberg Castle Cle ver minds are needed to uncover the secret of an ancient curse and to find the Carbuncle stone that was thought lost Who says that castle tours have to be boring Duration approx 1 5 hours Languages German Price for student groups German 105 Meeting point Heidelberg Castle, visitor center Group size max 20 persons per tour guide, larger groups on request Note Suitable for children aged 8 14 years, well suited for families with children Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de37 Exciting Exploration Tour through Heidelberg Popular for years the children and students guided tour An exciting journey of exploration through the Old Town or at the castle Robbers, avid readers and much more exciting and funny stories are waiting for you in Heidelbergs Old Town Have you ever been to a real cast le with immense walls, thick towers, drawbridges and exciting legends Duration approx 1 5 2 hours Sprachen German, English, Danish, Finnish, French, Italian, Spanish Price for student groups German 105 , other languages 115 Group size max 20 persons per tour guide, larger groups on request Note Suitable for children and young people aged 5 18 years 39 Animal Tracking in Heidelbergs Old Town Why is a lion with a sword, orb and crown enthroned on the fountain at the Universittsplatz University Square Who is hiding on the wall of the Marstall What other creatures can be found in the streets and squares of Heidelbergs Old Town While strolling through the Old Town, you discover a number of animal figures, some are clearly visible in famous squares and buildings, some are almost hidden so that you have to search for them very precisely All of them have a story of their own, telling us about life in the city and exciting former times Come along on the animals journey through the Old Town and find out how they have conquered their place over the years and are now waiting for visitors to tell them their stories Duration approx 1 14 1 5 hours Languages German Price for student groups German 105 Group size max 20 persons per tour guide, larger groups on request Note Suitable for children aged 6 12 years and also for families with children The tour can be booked onwards from 11 00 a.m The presence of an accompanying adult is required 38 Heidelberg City Rally for students Exploration tours as communication experiences With open eyes, imagination and team spirit, the city and its history are revealed to the students in an entertaining way In small teams, various tasks and riddles are solved between Universittsplatz University Square and Markt platz Market Square Each team receives a folder with tasks and a city map from the tour guide On demand, all participants will receive certificates and the winners will get an additional gift Duration approx 2 hours Languages German, English, French Price for student groups German 105 , other languages 115 Group size max 28 persons per tour guide Note There are four different levels of difficulty There- fore, it is necessary to indicate the type of school and the grade level 40 Family Tour What can be discovered about the old electoral coat of arms Which figures and symbols can be found on the his torical facades and what did and do they mean When dra wing Neckar water from the citys symbol, the Alte Brcke Old Bridge, you can physically experience just how high that bridge is Playful exploration for the little ones and interesting stories for the older family members are inclu ded The tour starts at the Marktplatz Market Square and leads along the most exciting and important sights such as the Heiliggeistkirche Church of the Holy Spirit, the Alte Brcke Old Bridge and the Friedrich Ebert Memorial to the Universittsplatz University Square Duration approx 1 5 hours Languages German Price 105 Group size max 20 persons max 10 children per tour guide Tip The tower of the Heiliggeistkirche Church of the Holy Spirit can be visited by asking admission via email heiliggeistekihd.de Further information can be found on the homepage heidelberg.de Donations for this service are welcome Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de Guided Tours for students, young people and families 23 N EW Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223 - 225 , guideheidelberg-marketing.de 24 Boat Trips Boat Trips 41 Boat trips with the Weisse Flotte White Fleet Heidelberg From Easter until the end of October, the Weisse Flotte Heidelberg offers daily trips from Heidelberg to Neckar steinach approx 1 5 hours one way and back Food and drinks are available on board All rides are moderated in English and German Boat landing stage Stadthalle Heidelberg, Neckarstaden 25 Times March 29 October 31 , 2024 Monday Sunday 1030 am, 1130 am, 230 pm and 330 pm During summer vacation July 25 September 7 , 2024 Monday Sunday 1000 am, 1100 am, 1200 am, 200 pm, 300 pm and 400 pm Price Round trip Heidelberg Neckarsteinach Heidelberg 25 per person One way trip Heidelberg Neckarsteinach or reverse 16 per person Boarding at Stift Neuburg Benedictine Abbey Stift Neuburg, Neckargemnd and Neckarsteinach Group discount as of 15 persons 5%, as of 25 persons 10%, as of 100 persons 15%, as of 250 persons 20% Note The boats can be rented for special trips and trips with exclusive groups Prices on request Boat capacity approx 50 400 persons Tip The Weisse Flotte Heidelberg offers diverse and entertaining themed tours for any age year round Tip For groups of 15 persons or more, the Weisse Flotte Heidelberg offers coffee and cake during the Four Castles- Tour Advance orders required Tickets and further information can be found here Information and booking 49 6221 5840 223 - 225 , guideheidelberg marketing.de42 Solar powered boat Neckarsonne Enjoy a fantastic all round view of Heidelbergs sights from the solar powered boat Neckarsonne Glazed on all sides, the solar powered catamaran runs on sunlight only as it glides gently and silently across the water The 50 minute round trip on the Neckar River along the banks of Heidelberg comes with interesting information about the Neckarsonne, shipping in general and, of course, the history and peculiarities of the city and its surroundings Gastronomy on board Boat landing stage Karl Theodor Bridge Alte Brcke Old Bridge, Old Town side Times March 29 October 31 , 2024 Tuesday Sunday 1130 am, 100 pm, 300 pm and 430 pm booking of at least 20 persons additionally 1000 am and 600 pm Price adults 13 50 per person, school and university students 11 per person Group size as of 10 adults Note Special rides and rides for exclusive groups are available upon request Capacity approx 250 persons inside 150 , outside 100 44 The sloop cruise with a guide on board Would you like to take a unique boat trip and enjoy the view of Heidelberg Castle and other sights in a relaxed way from the water and privately with your loved ones Then book our 1 5 hour cruise on the extraordinary and traditional 100 year old wooden sloops Discover the Neckar River and Heidelberg from a new perspective with up to 11 people exclusively During the trip you will be accompanied by a Heidelberg guide 43 Heidelberg Sightseeing Tour with the Weisse Flotte White Fleet Heidelberg No matter if this is your first time visiting Heidelberg or if you want to rediscover your home region round trips by boat on the Neckar River offer many impressions that will inspire you anew every time, in particular with the option of experienc ing the city and its diverse river valley from a very unique angle Enjoy a special view of Heidelberg and its sights, such as the Alte Brcke Old Bridge or the Heidelberg Castle, from the water in a 50 minute round trip Boat landing stage Stadthalle Heidelberg, pier 5 , Neckarstaden 25 Times March, 29 October 31 , 2024 Monday Sunday at 1200 am, 130 pm, 300 pm, 430 pm and 600 pm Price adults 17 per person including 1 soft drink or beer, children 8 50 per person including 1 soft drink Group size as of 10 adults Boat landing stage Stadthalle Heidelberg, Neckarstaden 25 Times April October Duration approx 1 5 hours Languages German, English Price 495 including 2 bottles of high quality Palatinate ros wine, 2 bottles of water, a guide and a captain Group size 1 11 persons, larger groups up to 22 persons two sloops on request for an additional charge Tip Longer trips as well as themed tours can be booked on request Boat Trips 25 Information und Buchung 49 6221 5840 223 - 225 , guideheidelberg-marketing.de Information und Buchung 49 6221 5840 223 - 225 , guideheidelberg-marketing.de 26 Accessible Tours Accessible Tours 45 Discover Heidelberg by Sensing, Smelling, Feeling A guided tour that brings the citys history to life by sensing the environment, feeling monuments and smelling the scents of the city A tour for the blind and people with im paired vision Duration approx 2 hours Languages German, English Price German 100 , English 110 Group size max 15 persons per tour guide, larger groups on request Meeting point Neckarmnzplatz Note We ask the accompanying person to contact the guide in advance of the tour Heidelberg hrdenlos barrier free Guided tours for deaf and hearing impaired guests The company Gebrden Verstehen from Heidelberg pro vides sign language interpreters to translate guided tours of the city into Deutsche Gebrdensprache D GS German Sign Language for you They can be booked from Gebrden Verstehen directly upon request Phone 49 6221 7287478 , Fax 49 6221 3541477 , dolmetschengebaerdenverstehen.de Please book the interpreters several weeks in advance, so that your preferred date can be guar anteed City guide, quarter maps, regional excursion destinations, gastronomy, culture Heidelberg can be experienced accessibly as well The Beirat von Men schen mit Behinderungen der Stadt Heidelberg coun cil of people with disabilities of the city of Heidelberg and the Verein zur beruflichen Integration und Qualifi zierung e V have collected interesting and valuable information about an accessible stay in Heidelberg Further information about the company Gebrden Verstehen Information about an accessible stay in Heidelberg und Buchung 49 6221 5840 223 - 225 , guideheidelberg marketing.de Guided Tours of the surroundings 46 Schwetzingen A castle garden based on the image of Versailles From Heidelberg, this trip will take you to Schwetzingen, twelve kilometers away Based on the image of Versailles, the Schwetzingen Castle with its baroque castle garden was built as the Prince Electors summer residence, that made even French philosopher Voltaire eloquently en-thuse about it as a truly magical garden Duration approx 5 hours Languages German, English, French, other languages on request Price German 165 , other languages 17547 Speyer In the footsteps of famous emperors The cathedral and imperial city at the Rhein River has long been among the excursion destinations that have a very special attraction to those interested in art and cultural his tory You will learn a lot about the cathedral built by the Salian emperors 950 years ago and its important history A tour through the city will teach you more about the history of Speyer On request, the famous Mikveh, the Jewish ritual bath, in the former Jewish quarter can also be visited for an additional admission fee Duration approx 5 hours Languages German, English, French, other languages on request Price German 165 , other languages 175 Guided Tours of the surroundings 27 Information und Buchung 49 6221 5840 223 - 225 , guideheidelberg marketing.de28 Guided Tours of the surroundings 48 Along the German Wine Road The all day tour will take you on a trip through the beautiful wine landscape along the wine road after a side trip to the famous Speyer Cathedral Get to know idyllic Neustadt on a tour with your guide and have a look at the Riesenfass Giant Barrel in Bad Drkheim Duration approx 8 hours Languages German, English, other languages on request Price German 255 , other languages 265 50 Odenwald trip Enjoy a trip through the romantic Neckar valley via Neckar steinach, the four castle corner, all the way to Hirschhorn Then you will continue on to Erbach, where you can visit the unique ivory museum if you like for an admission fee In Michelstadt, you can see the famous oldest city hall in Ger many The return trip will take you through the Odenwald via the Siegfriedstrae and along the Bergstrae Duration approx 8 hours Languages German, English, other languages on request Price German 255 , other languages 26549 The Bergstrae Ladenburg Weinheim Heppenheim Embedded in the picturesque landscape between the gently rolling hills of the Odenwald and the sprawling planes of the Rhein River, there is the Bergstrae with its many castles, picturesque medieval towns Weinheim, Heppenheim and Ladenburg, the Roman city and the place where Carl Benz worked Duration approx 8 hours Languages German, English, other languages on request Price German 255 , other languages 265 Heidelberg Neckar meadow From the first warm spring days to the golden fall, the northern banks of the Neckar River between the Theodor- Heuss Bridge and the Ernst Walz Bridge are a point of ab solute attraction This is where all generations meet strollers, joggers, sun lovers, beach volleyballers or picnic enthusiasts Particularly popular among children the water playground with kiosk Zoo Heidelberg Find fun and inspiration in the Heidelberg Zoo Stand eye to eye with a lion or watch the spirited elephant bulls at their baths you will love the diversity of species in the zoo All children also love the gigantic playing ship Noahs Ark, which invites them to play and climb Naturally Heidelberg Get to know Heidelbergs fascinating nature From an ori-entation course in the Odenwald to wild herb tours and bat exploration trips to the forest theater holiday week and further training offers for teachers The hike Naturally Heidelberg comprises extraordinary places, views, beauties of nature and monuments strung together like pearls on a string Your personal experience and visit in combination with exciting information on the natural, botanical, geological and historical special fea tures will give you a particularly impressive idea of the na ture and culture around Heidelberg Information on the exciting group offers are available in the booking office Natrlich Heidelberg, where you can also book them directly Bergstrae The Bergstrae connects Heidelberg to Darmstadt It is an ideal starting point for restorative hikes and nature focused bicycle tours Strung in one sequence along the foot of the Odenwald, little towns will invite you to linger and eat Our special advice One of the first and most beautiful almond blossoms in Germany in spring The Burgenstrae Castle Road Take the Burgenstrae into the Neckar valley by bike, ship or car Along the Neckar River, the Burgenstrae from Mann heim to Bayreuth leads past old monasteries, fascinating castles and picturesque old towns true to the motto of experience a travel through time Sinsheim Auto- Technikmuseum Car Technology Museum and Thermen und Badewelt The excursion destination for your entire family Vintage cars, trains and airplanes in the technology museum or ba thing fun, wellness and sauna enjoyment in the bathing world Sinsheim is only 30 minutes away from Heidelberg and offers exciting activities for young and old Leisure tips 29 Leisure tips Bus parking Kornmarkt Cabriob us City tour Neckarm nzplatz Molken kur Knigstuhl Bergbah nfunicular railwayKarlstorAl tstadt Bussem ergass e Kl Mantelgasse Groe Ma ntelgasse Haspelgasse Wehrsteg Floring Kr mergasse Mit tel bad gasse Apotheke rgas se Fisc he rg Se mme lsg Steingasse Leyerg Obere Neckarstrae Mnchgasse Dreiknigsstr  Kettengasse Schulgasse Grabengasse Sandgasse Theaterstr ae Friedrichstr ae Mrzgasse Akademiestr ae Neugasse Rohrbacher Strae Bismar ckstr ae Nadlerstr ae St Anna Gasse Fahrtgasse Thibautstr ae Pfaffengasse Am Brckentor Zoo Heiliggeiststr  Marstallstr ae Schiffgasse Bauamtsgasse Ziegelgasse Brunnengasse Bienenstr ae Kar pfen gas se Unter e Str ae Fischmarkt Marsiliusplatz Richar d- Hauser- Platz Stadthalle Friedrich- Ebert Platz Bismar ck- Platz Rathaus Kornmarkt Schlangenweg Unter e Neckarstr ae Landfriedstr ae Neuenheimer Landstr ae Ziegelhuser Landstr ae Neckarstaden Friedrich Ebert Anlage Gaisbergtunnel Schlossberg tunnel Friedrich Ebert Anlage Kurfrsten Anlage Neckarstaden Schurmanstr ae Uferstr ae Neuenheimer Landstr ae Plck Plck Hauptstr ae Bergheimer Strae Bahnhofstrae Hauptstr ae Merianstr ae Ingrimstrae Zwingerstrae Unt Fauler Pelz Oberer Fauler Pelz Neue Schlossstrae The odo r H eu ss Br c ke Karl Theodor Brcke Brckenstrae Neue Schlossstrae Mrchenparadies Fairy Tale ParadiseSchlierbach Rohrba ch Neuen heim Neckarwiese Hirschgasse Alte Brcke Old BridgeFugngerbergan g pedest rian crossing Karlsplatz Neckarmnz- platz Marktplatz Universitts- platz Schloss Solar po wered boat Heidelberger Schl oss CastleNeuenheimer Feld Kliniken Hospital Hauptbahnhof main station Kloster Benedictine abbey Stift Neubu rg Philosoph enweg Philoso phers Walk Kirchheim Weststad t Ziegelhause n Weisse Flotte Footpath Bus parking Parking gar age Meeting point Funicular r ailway Pier Bus tours Public toilets Tourist Information H DCar d sale Only entry and e xit Bus parking Neckarmnzplatz Boarding and unboarding point for tour buses Neckarmnzplatz with its Tourist Information is the ideal starting point to explore the Old Town on foot or to reach the funicular railway and the castle Attention This square is subject to an absolute stop ping prohibition except for the purpose of passenger change not to exceed 10 minutes However, the following bus parking places are available nearby Karlstor Altstadt max 15 parking spaces For Old Town Tours, we recommend using the bus parking place Karlstor Altstadt This parking space is located at the end of the Old Town at the end of the famous pedes trian zone right behind the Karlstor at the S Bahn station If you have booked an Old Town Tour, the bus can ideally stop briefly at the Neckarmnzplatz, only 300 m away, to let passengers board or unboard, before parking at Karlstor Altstadt Bus parking 31 Kornmarkt Cabriob us City tour Neckarm nzplatz Molken kur Knigstuhl Bergbah nfunicular railwayKarlstorAl tstadt Bussem ergass e Kl Mantelgasse Groe Ma ntelgasse Haspelgasse Wehrsteg Floring Kr mergasse Mit tel bad gasse Apotheke rgas se Fisc he rg Se mme lsg Steingasse Leyerg Obere Neckarstrae Mnchgasse Dreiknigsstr  Kettengasse Schulgasse Grabengasse Sandgasse Theaterstr ae Friedrichstr ae Mrzgasse Akademiestr ae Neugasse Rohrbacher Strae Bismar ckstr ae Nadlerstr ae St Anna Gasse Fahrtgasse Thibautstr ae Pfaffengasse Am Brckentor Zoo Heiliggeiststr  Marstallstr ae Schiffgasse Bauamtsgasse Ziegelgasse Brunnengasse Bienenstr ae Kar pfen gas se Unter e Str ae Fischmarkt Marsiliusplatz Richar d- Hauser- Platz Stadthalle Friedrich- Ebert Platz Bismar ck- Platz Rathaus Kornmarkt Schlangenweg Unter e Neckarstr ae Landfriedstr ae Neuenheimer Landstr ae Ziegelhuser Landstr ae Neckarstaden Friedrich Ebert Anlage Gaisbergtunnel Schlossberg tunnel Friedrich Ebert Anlage Kurfrsten Anlage Neckarstaden Schurmanstr ae Uferstr ae Neuenheimer Landstr ae Plck Plck Hauptstr ae Bergheimer Strae Bahnhofstrae Hauptstr ae Merianstr ae Ingrimstrae Zwingerstrae Unt Fauler Pelz Oberer Fauler Pelz Neue Schlossstrae The odo r H eu ss Br c ke Karl Theodor Brcke Brckenstrae Neue Schlossstrae Mrchenparadies Fairy Tale ParadiseSchlierbach Rohrba ch Neuen heim Neckarwiese Hirschgasse Alte Brcke Old BridgeFugngerbergan g pedest rian crossing Karlsplatz Neckarmnz- platz Marktplatz Universitts- platz Schloss Solar po wered boat Heidelberger Schl oss CastleNeuenheimer Feld Kliniken Hospital Hauptbahnhof main station Kloster Benedictine abbey Stift Neubu rg Philosoph enweg Philoso phers Walk Kirchheim Weststad t Ziegelhause n Weisse Flotte Footpath Bus parking Parking gar age Meeting point Funicular r ailway Pier Bus tours Public toilets Tourist Information H DCar d sale Only entry and e xit Stand December 2024  Subject to change Castle Tour busses can only drive up to the castle with a parking reservation special permit requiring a fee You can obtain these if parking spaces are available at heidelberg.de or by email to parkreiseshop heidelberg.de Immediate reservations can be made on the day before, or on the same day, by phone 49 172 6200063  Between 700 pm and 800 am, driving up to the castle is prohibited Individual permits can be obtained from the Traffic Management Office by phone 49 6221 58 30500  Note All information regarding the availability of bus parking and access to the castle can be found at by sending an email to parkreiseshop heidelberg.de Attention The Neue Schlossstrasse is closed for vehicles weighing 3 5 tons or more A detour of approx 15 km leads via Gaiberg Please use this detour Castle ticket including funicular railway Two Heidelberg attractions go hand in hand with the castle ticket you have the option of reaching the romantic Heidelberg Castle comfortably, eventful and in an environmental lyfriendly way by taking the funicular railway The funicular railway also combines two special features the lower funicular railway to the Molkenkur station is the most modern, while the upper one leading to the Knigstuhl Kings Seat is the oldest cableway in Germany 9 per person Castle ticket including funicular railway 33 Your benefit The castle ticket includes admission to the castle cour tyard, the barrel cellar as well as the German Pharmacy Museum and the fare for the funicular railway trip to the castle with a continuation of the trip to the Molkenkur station return trip including one stop Price per person valid for using the lower railway 9 adult, 4 50 discount price pupils and university students up to 28 years of age and severely disabled people with appropriate I D You can get the castle ticket including the funicular railway at the following places in Heidelberg Tourist Information at the Hauptbahnhof main station Tourist Information at the Neckarmnzplatz Old Town Tourist Information in the Rathaus town hall at the Marktplatz Market Square, Old Town Checkout counter at the Kornmarkt and castle funicular railway station Heidelberg Castle checkout counter Group order For groups of 15 people or more, you can order castle tickets via our reservation department dispatch by post within Germany Please let us know the final number of participants two weeks before traveling refund for excess ordered tickets is not possible Phone 49 6221 58 40 228 gruppenheidelberg-marketing.de Opening hours Heidelberg Castle Castle courtyard, Great Barrel All year 900 am 600 pm last admission 530 pm German Pharmacy Museum April October 1000 am 600 pm, last admission 540 pm November March 1000 am 530 pm, last admission 510 pm Note Information are subject to change Heidelberg CA RD The Heidelberg CA RD makes your journey even easier Benefit from the following included services castle ticket including funicular railway free use of public transport in Heidelberg combo ticket one time free entrance to the University Museum, the Student Prison and the special exhibition numerous discounts on tours, museums, leisure activities, restaurants and shops 1 day 26 Valid from midnight to midnight on the day of validity 2 days 28 Valid all day on the first day until midnight the following day 4 days 30 Valid all day on the first day until midnight of the fourth day Family 2 days 60 Valid all day on the first day until midnight the following day for a family 2 adults up to 3 children or 1 adult up to 4 children under the age of 16  Funicular railway departure times Annual funicular railway maintenance expected March 4 March 17 , 2024 Summer timetable valid from March 25 November 1 , 2024 Kornmarkt Castle 900 am every 10 minutes, last trip at 800 pm Castle Kornmarkt 903 am every 10 minutes, last trip at 803 pm Winter timetable valid from November 2 , 2024 March 31 , 2025 Kornmarkt Castle 900 am every 10 minutes, last trip at 510 pm Castle Kornmarkt 903 am every 10 minutes, last trip at 543 pm Note The lower funicular railway from the Kornmarkt station via the castle to the Molkenkur station is equipped to facilitate disabled access Strollers must be carried on the steps The conductors are happy to assist you We recommend that families with small children use a buggy Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from Family Gltig ab Valid from34 Events 2024 Heidelberger Frhling Heidelberg Spring March 15 April 13 , 2024 One of the largest music festivals in Germany that belongs to the top league of international festivals for classical music and once again invites visitors to well over 100 events this year fruehling.deen Heidelberger Schlossfestspiele Heidelberg Castle Festival June 9 July 28 , 2024 The one of a kind Renaissance buildings, the sleepy nooks and crannies, as well as the sprawling gardens and parks offer fascinating opportunities for the artistic work of the Heidelberg Theater Squares and surfaces become stages, walls and corners backdrops Heidelberg Castle Illuminations with Fireworks June 1 and September 7 , 2024 Bengali lights blaze two times every summer on the walls of the ruin in a picturesque and eternal manner The illumi-nation of the beautiful facade of the castle is complemen ted by festive fireworks which bathe the Old Town in an im pressive brilliance Summer at the River August 24 and 25 , 2024 Stroll, relax, enjoy the river The city is moving closer to the water and invites you to linger and stroll along the banks of the Neckar River, with musical entertainment and a varied range of information and gastronomic offerings marketing.com Heidelberger Herbst Heidelberg Autumn September 28 and 29 , 2024 One of the biggest Old Town festivals in the region features an artisan market, a giant flea market, numerous regional specialties and many bands providing entertainment in various different squares in the Old Town marketing.com Heidelberg Wine Village October 2 13 , 2024 Enjoy delicious local and regional wines in the heart of the Old Town marketing.com Enjoy Jazz Beginning of October Mid November 2024 The festival offers diverse events for about seven weeks, with an emphasis not only on jazz, but also on other genres in exclusive venues, e.g in Heidelberg Heidelberg Christmas Market November 25 December 22 , 2024 Nestled in the Old Town, illuminated by the world famous castle above, the Heidelberg Christmas Market invites you to take some time out and visit one of its six historic locations Winterwldchen Winter Forest and Heidelberg Ice Rink November 25 , 2024 Beginning of January 2025 At the Kornmarkt, the Winter Forest beckons visitors with its magical atmosphere, while one of Germanys prettiest ice rinks offers ice skating pleasure on the Karlsplatz marketing.com Excerpt of the events 2024 A great program all year round Further events can be found on our website  Dates and information are subject to change Further events can be found on our website  Dates and information are subject to change 36 Good to know Good to know The central location of Heidelberg makes traveling easy with all modes of transport Rail Heidelberg is well connected to the European long distance network, as well as to the German Rail Network I CE and I C E C, Within the region and across its bor ders, the Rhine Neckar Transport Association ensures the best connections with the S Bahn Car Bus The A5 A6 highways Autobahn access large sections of the entire Rhine Neckar region with inter connected exits and connecting federal roads The A5 A656 highways have direct exits to Heidelberg Airports City Airport Mannheim approx 18 km Frankfurt Airport approx 80 km Baden Airpark Flughafen Karlsruhe Baden Baden approx 90 km Flughafen Stuttgart approx 120 km Frankfurt Hahn Airport approx 150 km airport.deen TL STransfer Limousine Service Simply and conveniently book your T LS transfer that will pick you up directly at your terminal at Frankfurt Airport and take you to your hotel in Heidelberg Prices on request H LS Heidelberg Limousine Service Book with us an exclusive limousine service, V IP first class service, cab service, airport transfer or shuttle ser vice Our trained chauffeurs will bring you safely to your destination Price on request R V Camping R V site Harbigweg 1 3 , 69124 Heidelberg 48 parking spaces open all year round Camping Heidelberg Schlierbacher Landstrae 151 , 69118 Heidelberg quiet location, right beside the Neckar River open April October Camping Haide Ziegelhuser Landstrae 91 , 69151 Neckargemnd 200 parking spaces, right beside the Neckar River open April November haide.de Nette Toilette Nice restrooms You can use a total of 30 nice restrooms in the Old Town area without being forced to purchase or consume anything Some restaurants and stores bear the Nette Toilette symbol on the entrance door here Good to know 37 Public passenger transport You can get information on the Heidelberg public trans portation lines and fares around the clock from the service hotline 49 621 1077077 , online at or at Rhein- Neckar Verkehr Gmb Hs R NV Customer Service Center at the Hauptbahnhof main station, Kurfrsten Anlage 62  Bicycle Because Heidelberg is a very bicycle friendly town, bike rental is a good move Radolino bike rental radhof B ER GH EI M D B bike rental system in front of the main station Pedelec rental E-bike, several locations hd.org Joyrides E bike rental V RNnextbike V RNnextbike has numerous bike stations in the metropo litan area Rhine Neckar and operates around the clock The stations are located at busy transport hubs near bus and tram stops For further information Phone 49 30 69205046 , Stadtmobil Rhein Neckar Phone 49 621 12855585 e Carsharing Rhein Neckar Phone 49 6221 3574974 Any Move Phone 493083795645 Taxi Taxizentrale Phone 49 6221 302030 Taxi HDirekt Phone 49 6221 739090 e-Scooter At many locations throughout the city of Heidelberg, nu merous e-scooters from various providers are available for rent To rent an e-scooter, register via the app of the respec tive rental company of transportation in Heidelberg Mobile in Heidelberg Booking form Special Activities Please complete this form and send it to Note It is possible to book a tour guide for a Heidelberg Marketing Gmb H, Neuenheimer Landstrae 5 , maximum of 20 people in a group except for tours in 69120 Heidelberg Germany, your own coach For larger groups it is necessary to book Fax 49 6221 5840 222 , guideheidelberg-marketing.de more guides Contact for further questions 49 6221 5840 223 225 Only for urgent matters on weekends 49 6221 58 44444 I would like to make a binding booking mandatory fields, You will receive the invoice a few days after the tour Date and weekday Total number of persons Number of guides Language Duration hours, from to am pm Special Activities Guided Tours Old Town Tour City and Castle Sightseeing Tour Castle Tour Other tours Please fill in the name of the tour Old Town and Castle Tour combined Name of the tour Meeting point University Square Lions Fountain recommended for arrival by foot, with public transportation or car Neckarmnzplatz recommended for arrival with own bus Bus parking place at Heidelberg Castle recommended for Castle Tour and arrival with own bus Castle visitor center recommended for Castle Tour and if you are traveling individually Special requests Payment The tour and any possible entry fees will be paid by invoice The tour will be paid by invoice, any possible entry fees will be paid cash on site Note Cash payment for guided tours is not possible Contact First and last name title Company institute Street zip code city Phone fax email to receive the booking confirmation Contact person on site name and mobile number Share some additional information to help the tour guide to get prepared for your tour Type of group Average age constitution, physical impairments Will the guests stay in Heidelberg for the first time Yes No Other tour agenda items in Heidelberg Special requests interests requirements Please send me the Heidelberg Marketing Gmb H newsletter by email Note I hereby confirm that I have read, understood and accepted the General Terms and Conditions Note Please note that you will find the privacy policy in detail on our website With your signature, you agree to our privacy policy Date signature stamp 38 Booking form Special Activities Heidelberg Marketing Gmb H heidelberg4you marketing.comHeidelberg Triospecial price 22 90 instead of 27 90 1 bottle of Riesling Winery Clauer 1 bottle of Pinot Noir Winery Hans Winter 1 bottle of Pinot Noir Ros Winery Bauer Available at the Tourist Information Neckarmnzplatz Heidelberg Marketing Gmb H heidelberg4you marketing.de Der Heidelberger Dreiklang Heidelberg Marketing Gmb H heidelberg4you marketing.de Der Heidelberger Dreiklang General Terms and Conditions Heidelberg Marketing Gmb H, Special Activities Dear guests, As far as they are effectively agreed, the following provisions shall become the terms of the service contract concluded between the custom er and Heidelberg Marketing Gmb H hereinafter H DM for provision of Special A ctivities  They shall supplement the statutory provisions of sections 611 et seq German Civil Code and detail these  Therefore, please read these Terms and Conditions with car e before booking 1  Position of H DM Area of Application of these Terms and Conditions Applicable Legal Provisions 1 1 These terms and conditions for Special A ctivities shall apply to guided tours for visitors, tours and boat rides that are offered in the catalog Special Activities of H DM and that take less than 24 hours according to section 651a para 5 no 2 German Civil Code, and do not include any overnight stay day trips and the travel price of which does not exceed 500 Euro These day trips are hereinafter referred to as Special Activities 1 2 H DM shall render the offered Special Activity services as a service provider and direct contracting partner of the customer or the client 1 3 The legal relationship between H DM and the customer or the client shall be primarily subject to the agreements reached with H DM , and these terms and conditions as a supplement, with the statutory provisions on contracts for services section 611 et seqq German Civil Code applying alternatively 1 4 As far as mand atory provisions under international or European law that are to apply to the contractual relationship with H DM do not stipulate anything else to the customers or clients benefit, the entire legal and contractual relationship with H DM shall be subject to German law exclusively 1 5 The following provisions shall only apply to Special Activities of H DM  Travel agreements and multi -daytrips that include accommodation services are subject to the travel conditions of H DM 2  Conclusion of the Contract Groups Provision of a Group Client 2 1 The following shall apply to all bookings of Special Activities a Bookings are accepted as bookings in person, by phone, by fax or by email b The basis of the offer from H DM and the customers booking shall be the d escription of the special activity and the supplementary information in the booking basis, as far as these are available to the customer when booking c If the content of the booking confirmation deviates from the content of the booking, this constitutes a new offer by H DM  The contract shall be concluded based on this new offer when the customer declares acceptance by express declaration, deposit or payment of a remaining amount or by using the services d The customer who places the booking shall be lia ble for the contractual obligations of other participants for whom he places the booking as if fo r his own, as far as he has assumed the corresponding obligation by express and separate declaration The same shall apply respectively to group clients or per sons responsible for the group with regard to the participants of special activities as registered by the group client or person responsible for the group 2 2 The following provisions shall apply as supplements for Special Activities to closed groups Special Activities to closed groups within the meaning of these provisions shall only be group trips that are organized by H DM as the responsible provider and booked and or processed via a p erson responsible for the group or a group client who acts as autho rized person for a certain group of participants and takes over the role as only customer vis - vis H DM 2 3 H DM and the respective group client can agree in respect of such a group trip that the group client as authorized representative of the group trip participants is granted special rights 2 4 H DM shall not be liable for any services or service aspects of any kind that with or without knowledge of H DM are offered, organized, performed and or provided to the customers by the group client or person responsible for the group in addition to the services of H DM  This shall specifically include the travel to and from departure and return locations contractually agreed with H DM that is organized by the group client or the person responsible for the group , any events not contained in the service scope of H DM before and after the Special Activity and along the way transport, excursions, meetings, etc and any tour guides deployed by the group client or person responsible for the group who are not contract ually owed by H DM 2 5 H DM shall not be liable for any measures and omissions of the group client or person responsible for the group or any tour guide deployed by the group client or person responsible for the group before, during and after the tour, in p articular not for any changes to contractual services that are not coordinated with H DM , instructions to local guides, special agreements with the different service providers, information and representations towards the customers 2 6 As far as this is not expressly agreed, the group client or the person responsible for the group or any tour guides deployed by him shall not have the right or the authorization to receive any reports on defects from the group tour participants They also shall not have the ri ght to accept any customer complaints or payment claims in the name of H DM for H DM during or after the Special Activity 2 7 Bookings of Special Activities shall be directly binding upon the customer and shall lead to conclusion of the binding contract on the Special Activity by H DM s confirmation by phone or orally The contract shall therefore be concluded by receipt of the booking confirmation acceptance declaration by H DM , which shall not require any specific form, with the consequence that oral confi rmations and confirmations by phone shall be legally binding upon the customer 2 8 H DM notes that there is no right to withdraw according to the statutory provisions section 312g paragraph 2 sentence 1 no 9 German Civil Code, even if the service contr act was concluded by way of distance selling The other statutory rescission and termination rights of the customer shall not be affected by this 2 9 For bookings made through the website of H DM, the following shall apply to conclusion of the contract a By clicking the button Book subject to payment , the customer bindingly offers conclusion of the contract for the Special Activity to H DM  Receipt of the customers booking will be confirmed to him without delay electronically b The submission of the c ontract offer by clicking the button Book subject to payment shall not found any claim of the customer or client to conclusion of a contract with H DM according to his booking information Instead, H DM shall be free in its decision to accept or reject the customers or clients contract offer c The contract shall be concluded by receipt of the booking confirmation of H DM by the customer or client 3  Services, Reservation of Replacement Deviating Agreements Changes to Essential Services Duration of Services Weather 3 1 The service owed by H DM shall comprise rendering of the respective service according to the service description and the additionally concluded agreements 3 2 If a certain group size must not be undercut or exceeded for a service, th is must be indicated in the service description 3 3 Insofar as nothing else was explicitly agreed, special activities do not need to be offered by a specific person f ex one specific tour guide In the event that a specific person has been identified, we reserve the right to replace that person with another person in case of compelling reasons in particular due to illness If H DM is not able to find a replacement in particular for solo self employedpersons in the event of compelling reasons not at tributable to H DM, H DM shall have the right to rescind the contract andor shall have the right to an extraordinary termination of the contract with good cause In that case the customer shall not be obligated to pay any remuneration Any further rights of the customer, in particular reimbursements for outward or inward journeys, shall be excluded 3 4 Modifications of or supplements to the contractually offered services shall require an express agreement with H DM , for which text form is urgently recommende d to serve as evidence 3 5 Modifications of essential services that deviate from the agreed terms of the contract and that become necessary after conclusion of the contract in particular also changes to the schedule of the respective rendering of service s and that were not initiated by H DM in bad faith shall be permitted as far as the changes are not considerable and do not impair the overall scope of the service Any warranty claims of the customer or client in case of such modifications of essential se rvices shall not be affected 3 6 Information on the duration of services shall be approximates 3 7 The following shall apply to weather conditions and their effects on agreed services a Where not expressly agreed on differently from case to case, the agreed services shall take place in any weather  b Weather shall therefore not entitle the customer or client to free of charge rescission or termination concerning the contract with H DM  This shall not apply only if the weather impairs the body, health o r property of the customer or the participants of the 40 General Terms and Conditions client in the service so considerably that performance is objectively unreasonable for the customer or client and his participants c If such situations are present at commencement of the service or if they are objectively expected for the agreed time of the service before its commencement, both the customer or client, and H DM shall have the right to terminate the contract for the service by way of proper or extraordinary termination d In the event of such termination by H DM, the customer or client shall not have any claims for reimbursement of costs, in particular any travel and accommodation fees, except if contractual or statutory claims of the customer or client to damages or reimbursement of expen ses are justified regarding this 4  Rendering of Services and Payment Terms 4 1 The agreed services shall include rendering of services and additionally offered and agreed services 4 2 The agreed price shall be paid 14 days after the invoice date or at commencement of the Special Activity , depending on the agreement by H DM 4 3 The following shall apply where the customer has no contractual or statutory rescission right and H DM is willing and able to render the contractual services a If the customer d oes not pay the service fee when due, or if the payment is incomplete, H DM shall have the right to withdraw from the contract after sending a reminder stipulating an appropriate grace period, and after expiration of this period, and to claim damages from t he customer in accordance with sections 280 1 , 241 2 German Civil Code, in accordance with the proviso of the following item 8 , unless the customer has a right to set off or retention at the due date or is not at fault for the default of payment b Without complete payment of the service price, the customer shall not have any claim to using the services 5  Booking Changes A claim of the customer or the client to modifications concerning the date of the service, time, departure and destination sites of the services booking change after conclusion of the contract shall not apply  Upon the customers or clients wish, it can be reviewed whether a booking change is possible anyway The booking change request will only be accepted in text form 6  Non -Utilizationof Services 6 1 If the customer or client does not use the agreed services wholly or in part without any fault of H DM, in particular due to not appearing for rendering of the respective service without termination of the contract , although H DM is willing and able to render the services, there shall be no claim to reimbursement of payments already made 6 2 The agreed remuneration shall be according to the statutory provisions section 615 s 1 and 2 German Civil Code a The agreed remuneratio n shall be paid without there being any claim to subsequent performance b However, H DM shall accept set -offof expenses saved and any remuneration that H DM acquires by other use of the agreed services or neglects to acquire in bad faith against the remun eration 7  Rescission by H DM because the minimum number of participants is not reached 7 1 H DM may rescind the contract if a minimum number of participants according to the following provisions is not reached a The minimum number of participants and t he latest date of rescission by H DM must be clearly stated in the specific service offer or, in the case of consistent provisions for specific types of Special Activities , in a general notice or general service description b H DM must clearly state the mi nimum number of participants and the latest rescission deadline in the booking confirmation or refer to the corresponding information in the service description c H DM shall be obligated to declare cancelation of the Special Activity towards the customer without undue delay once it is certain that the Special Activity will not take place due to the minimum number of participants not being reached d If a minimum number of participants is agreed, a deposit specified in the offer shall be due upon booking, and the remaining payment shall be due upon confirmation of the event 7 2 If the Special Activity does not take place for this reason, the customer shall be reimbursed for the Special Activity without undue delay 8  Termination and Rescission by the Customer or Client 8 1 The customer or client may terminate the contract with H DM after conclusion of the contract Termination shall not require any specific form Terminations shall be submitted in written form Section 126b of the German Civil Code, f ex e -mailor fax 8 2  If the customer or client declares termination or does not use services without declaring termination in particular by not appearing , H DM may demand reimbursement for the travel plans made and the connected expenses Calculati on of the reimbursement shall generally consider possible other uses of the service and usually saved expenses The following cancel ation fees shall apply from day 9 to day 5 before the first day of rendering the service 50%, from day 4 onwards and if not showing up to the event 90% of the total price agreed 8 3 The customer shall in any case have the right to prove to H DM that H DM has incurred no damage or a much lesser damage than the flat rate claimed by it in compensation 8 4 H DM reserves the rig ht to demand a higher specific compensation instead of the above amounts as far as H DM proves that H DM has incurred considerably higher expenses, in particular where individual service components of the Special Activity are not reimbursed by the service pr ovider If H DM asserts such a claim, H DM shall be obligated to specify the amount and document the required compensation under consideration of any saved expenses and any other use of the travel service 8 5 The above termination rules shall not affect the statutory or contractual termination rights of the customer in case of defects of the services of H DM and any other statutory warranty claims 9  Liability of H DM Insurances 9 1 H DM shall be liable without restriction as far as the damage results from violation of an essential obligation of H DM, the performance of which was required for proper execution of the contract or the violation of which endangers achievement of the purpose of the contract, or the damage results from violation of life, body or health of the customer Apart from this, liability of H DM shall be limited to damage caused by H DM or its servants willfully or grossly negligently 9 2 H DM shall not be liable for services, measures or omissi ons of accommodation and meal operations or any other providers that are visited in connection with the service, except if the damage was caused or contributed to by culpable violation of obligations of H DM 9 3 The agreed contractual services contain insu rances to the benefit of the customer or client only if this is expressly agreed The customer or client is expressly recommended to take out travel rescission insurance 10  Termination due to behavior related Reasons 10 1 H DM may terminate the servic e contract without observing any period of notice if the customer causes sustained disturbance in spite of a warning by H DM or acts in violation of the contract at a scope that justi fies immediate cance lation of the contract 10 2 If H DM declares terminati on, H DM shall retain the claim to the service price however, H DM must accept set -offof the value of saved expenses and the benefit that H DM acquires from other use of the service not utilized 11  Special Obligations of Customers Concerning Special Acti vities with Physical Activities e.g by Bicycle or Segway 11 1 The customer shall be responsible for finding out before booking and before using the Special Activities whether the respective activities are suitable for them under consideration of their personal health 11 2 H DM shall not owe any special medical information or instruction in this respect unless explicitly agreed, in particular concerning the respective customers specific situation 11 3 H DM or its vicarious agents guides, etc may excl ude the customer wholly or in part if there are any reasonable indications that the Special Activities may overtax the guest, provided that the customer threatens to endanger themselves or others due to this Item 6 et seq shall apply accordingly 11 4 If the customer withdraws or discontinues due to injury or illness for which the provider is not at fault, or at their own request, the provisions of item 6 et seq shall apply as well 11 5 Although the Special Activities are accompanied by a guide, they re quire a high degree of personal responsibility on the side of the customer 11 6 Customers are advised to wear clothing suitable for Special Activities that protects them from strong sunlight, rain, or wind It is also recommended that they carry a change of clothes The provider of the Special Activities reserves the right to exclude any customer from the Special Activity for safety reasons wholly or in part if they join the Special Activities in unsuitable clothing or footwear General Terms and Conditions 41 11 7 Instructions of the g uides before and during the Special Activities must be observed Traffic rules must be observed and consideration for other road users must be shown at all times 11 8 Non -swimmersare not permitted to participate in physically active Special Activities on water 12  Special Arrangements in Connection wit h Pandemics in particular the Corona Virus 12 1 The parties agree that the agreed services shall always be provided by H DM in compliance and in accordance with the official requirements and conditions applicable at the time at which the service is rendered 12 2 The customer agrees to observe appropriate rules of use or restrictions imposed by H DM when using services and to inform H DM without undue delay if they experience any typical symptoms of illnes s 12 3 The provisions above shall not affect any possible warranty claims of the customer 13  Choice of Law Place of Jurisdiction Information on Consumer Dispute Resolution 13 1 The entire legal and contractual relationship between the customer and H DM shall be subject to German law exclusively The customer may raise a claim against H DM only at the registered office of H DM 13 2 For actions of H DM against the customer, the customers place of residence shall be relevant For claims against customers who are merchants, legal entities under public or private law or persons who have their residence or common abode abroad or whose place of residence or common abode is not known at the time at which the claim is raised, the place of jurisdiction shall be t he office of H DM 13 3 The above provisions shall not apply a if and as far as anything other results to the benefit of the customer from any terms that cannot be contractually waived from conventions applicable to the contract for services between the cu stomer and H DM or for any contracts concluded in electronic legal transactions b if and as far as any provisions applicable to the contract for services that cannot be waived in the member state of the E U to which the customer belongs are more beneficial for the customer than the above provisions or the correspondi ng German provisions 13 4 In light of the law on consumer dispute resolution, H DM notes that H DM will not participate in any voluntary consumer dispute resolution If consumer dispute resolution was to become binding upon H DM after printing of these trave l conditions, H DM shall inform the consumers about this in a suitable form H DM informs about the European online dispute solution platform protection Tour Law Noll Htten Dukic Rechtsanwlte, Stuttgart Mnchen 2023 Tourism agency Heidelberg Marketing Gmb H Managing director Mathias Schiemer Neuenheimer Landstrae 5 69120 Heidelberg, Germany Phone 49 6221 5840 200 Telefax 49 6221 5840 222 infoheidelberg -marketing.de Commercial register number H RB 337405 Register court A G Mannheim V AT I D D E226325597 42 General Terms and Conditions Imprint Heidelberg Marketing Gmb H Neuenheimer Landstrae 5 69120 Heidelberg Germany Phone 49 6221 5840 200 Fax 49 6221 5840 222 infoheidelberg-marketing.de marketing.com The Heidelberg Marketing Gmb H is a subsidiary of the City of Heidelberg Content Heidelberg Marketing Gmb H Layout a B Grafik Artem Bathauer Photos Cover page, pages 3 , 4 , 6 , 9 , 10 , 11 , 17 , 18 , 19 , 24 , 26 , 32 , 35 , 36 , 37 Tobias Schwerdt Page 8 Christian Buck Pages 14 , 22 T MB W Stefan Kuhn Page 20 Heidelberg Marketing Gmb H Page 25 Riverboat Gmb H Page 27 City of Schwetzingen Tobias Schwerdt Page 28 Frank Jger 2024  All contents, in particular texts, photographs and graphics, are protected by co-pyright Unless expressly stated otherwise, Hei delberg Marketing Gmb H owns the  Heidelberg Marketing Gmb H Neuenheimer Landstrae 5 69120 Heidelberg Germany Phone 49 6221 58 44444 Fax 49 6221 58 40222 infoheidelberg marketing.de\n",
      "S RH University of Applied Sciences Library E ZProxy F AQs 1 2 What is E ZProxy E ZProxy gives university members the ability to access library e -books, e -journals and databases outside of the campus network, on the road or from home E ZProxy is a convenient alternative to V PN and C IT RI X No installation of additional software or plugins is required login is simply done with the university I D What are the technic al requirements E ZProxy works independently of the browser and operating system used To use E ZProxy, Javascript, pop upsand cookies must be allowed in the browser The default settings of browsers usually meet these requirements Does E ZProxy access wo rk for all databases E ZProxy access is available for almost all databases licensed by the library However, E ZProxy access does N OT work for Beck online remote access only possible after personal registration on campus with 2 factorauthentication Instructions  How can I log in University members can select the desired research source on the Digital Library page and acces s it by clicking on the link via E ZProxy and using their university I D username and password How can I see if the login worked You can see if you are logged in correctly in the address bar of your browser the actual U RL is followed by srh hochschule -heidelberg.idm.oclc.org e.g springer com srh hochschule heidelberg.idm.oclc.org In addition, most databases display the university logo or name in the header Please note U RLs of articles or e -booksaccessed with E Z -Proxyare not citable Use the D OI or U RN of the article or the eI SB N of the e -bookwhere indicated S RH University of Applied Sciences Library E ZProxy F AQs 2 2 Important If you have left the database web pages or call a database manually, e.g via a bookmark, you will leave E ZProxy and may have to log in again Tip You can revise existing bookmarks on library databases by placing https login.srh hochschule heidelberg.idm.oclc.orgloginurl in front of the original bookmark U RL Error s when accessing with E ZProxy The most common cause of errors are old browser cookies Therefore, in case of E ZProxy problems, first delete your browser cookies, close the browser, restart it and then try again Switching browsers e.g Google Chrome instead of Firefox can also help Sometimes there are difficulties if the security options are set too strict in the browser settings, e.g if cookies are rejected on principle You must then explicitly allow cookies of E ZProxy in the settings In some networks firewalls or other proxies prevent access to E ZProxy This is often the case in corporate networks In this case please contact S RH -I T Sometimes images or navigation elements of web pages accessed via E ZProxy are not displayed or are displayed incorrectly In most cases this can b e fixed by reloading the page Possibly E ZProxy access to a newly licensed database has not yet been configured by us or the database provider has changed its web address We are happy to receive any hints in this regard .\n",
      "\n",
      "Fundamentals of Data Engineering Plan and Build Robust Data Systems With Early Release ebooks, you get books in their earliest formthe authors raw and unedited content as they writeso you can take advantage of these technologies long before the of ficial release of these titles Joe Reis and Matt Housley Fundamentals of Data Engineering by Joe Reis and Matt Housley 2022 Joseph Reis and Matthew Housley   Printed in the United States of America Published by O Reilly Media, Inc , 1005 Gravenstein Highway North, Sebastopol, C A 95472  O Reilly books may be purchased for educational, business, or sales promotional use Online editions are also available for most titles .com  For more information, contact our corporateinstitutional sales department 800 998 9938 or corporateor eilly.com  Acquisitions Editor Jessica Haberman Development Editor Nicole T ache Production Editor Gregory Hyman Interior Designer David Futato Cover Designer Karen Montgomery Illustrator Kate Dullea September 2022 First Edition Revision History for the Early Release 2021 10 08 First Release 2021 1 1 12 Second Release 2022 03 02 Third Release See .comcatalogerrata.cspisbn9781098108304 for release details The O Reilly logo is a registered trademark of O Reilly Media, Inc Fundamentals of Data Engineering , the cover image, and related trade dress are trademarks of O Reilly Media, Inc The views expressed in this work are those of the authors, and do not represent the publisher s views While the publisher and the authors have used good faith ef forts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work Use of the information and instructions contained in this work is at your own risk If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses andor rights 978 1 098 10830 4 Chapter 1  Data Engineering Described A N OT E F OR E AR LY R EL EA SE R EA DE RS With Early Release ebooks, you get books in their earliest formthe authors raw and unedited content as they writeso you can take advantage of these technologies long before the of ficial release of these titles This will be the first chapter of the final book If you have comments about how we might improve the content andor examples in this book, or if you notice missing material within this chapter , please reach out to the authors at book_feedbackternarydata.com  Since this book is called Fundamentals of Data Engineering , its critical that we clearly define what we mean by data engineering and data engineer  Terms matter , and there s a lot of confusion about what data engineering means, and what data engineers do Let s look at the landscape of how data engineering is described, and develop some nomenclature we can use throughout this book What Is Data Engineering So what is data engineering This is a great question Though data engineering has existed in some form since companies have done things with datasuch as data analysis and reportsit came into sharp focus alongside the rise of data science in the 2010s But what is data engineering, exactly Let s start with some real talkthere are endless definitions ofdata engineering A Google exact match search for what is data engineering returns over 91 , 000 unique results Lets look at a few examples to illustrate the dif ferent ways data engineering is defined Data engineering is a set of operations aimed at cr eating interfaces and mechanisms for the flow and access of information It takes dedicated specialistsdata engineersto maintain data so that it r emains available and usable by others In short, data engineers set up and operate the or ganization s data infrastructur e preparing it for further analysis by data analysts and scientists The first type of data engineering is S QL focused The work and primary storage of the data is in r elational databases All of the data pr ocessing is done with S QL or a S QL based language Sometimes, this data processing is done with an E TL tool The second type of data engineering is Big Data focused The work and primary storage of the data is in Big Data technologies like Hadoop, Cassandra, and H Base All of the data processing is done in Big Data frameworks like Map Reduce, Spark, and Flink While S QL is used, the primary pr ocessing is done with programming languages like Java, Scala, and Python In relation to pr eviously existing r oles, the data engineering field could be thought of as a superset of business intelligence and data war ehousing that brings mor e elements fr om softwar e engineering This discipline also integrates specialization ar ound the operation of so called big data distributed systems, along with concepts ar ound the extended Hadoop ecosystem, str eam pr ocessing, and in computation at scale Data engineering is all about the movement, manipulation, and management of data Wow That s only a handful of definitions, and you can see there s a lot of variety  Clearly , there is not yet a consensus around what data engineering means If youve been wondering about the term data engineering, your confusion is understandable 1 2 3 4 This book provides a snapshot of data engineering today  To the fullest extent, were focusing on the immutables of data engineeringwhat hasn t changed, and what won t likely change in the future There are no guarantees, as this field is rapidly evolving That said, we think weve captured a great framework to guide you on your data engineering journey for many years to come Before we define data engineering, you should understand a brief history of data engineering, how it s evolved, and where it fits into the general backdrop of data and technology  Evolution of the Data Engineer To understand data engineering today and tomorrow , it helps to have the context of how the field evolved This is not a history book, but looking to the past is invaluable in understanding where we are today , and where things are going There s a common theme that recurs constantly in data engineering what s old is new again The early days 1980 to 2000 , from data warehousing to the web The birth of the data engineer ar guably has its roots in data warehousing, dating as far back as the 1970s, with the business data warehouse taking shape in the 1980s, and Bill Inmon of ficially coining the term data warehouse in 1990  After the relational database and S QL were developed at I BM, Oracle popularized the technology  As nascent data systems grew , businesses needed dedicated tools and data pipelines for reporting and business intelligence B I T o help people properly model their business logic in the data warehouse, Ralph Kimball and Bill Inmon developed their respective data modeling techniques and approaches, which are still widely used today  Data warehousing ushered in the first age of scalable analytics, with new M PP systems massively parallel processing databases coming on the market and supporting unprecedented volumes of data Roles such as B I engineer , E TL developer , and data warehouse engineer addressed thevarious needs of the data warehouse Data warehouse and B I engineering was a precursor to today s data engineering, and still play a central role in the discipline Around the mid to late 1990s, the Internet went mainstream, creating a whole new generation of web first companies such as A OL, Altavista, Yahoo, Amazon The dot com boom spawned a ton of activity in web applications, as well as the backend systems to support themservers, databases, storage Much of the infrastructure was expensive, monolithic, and heavily licensed The vendors selling these backend systems didn t foresee the sheer scale of the data that web applications would produce The early 2000s The Birth of Contemporary Data Engineering Fast forward to the early 2000s the dot com bust of the late 90s left behind a small cluster of survivors Some of these companies, such as Y ahoo, Google, Amazon, would grow into powerhouse tech companies Initially , these companies continued to rely on the traditional monolithic, relational databases and data warehouses of the 1990s, pushing these systems to the limit As these systems buckled, new solutions and approaches were needed to handle the growing volume, variety , and velocity of data in a cost effective, scalable, reliable, and fault tolerant manner  Coinciding with the explosion of data, commodity hardwareservers, ram, disks, flash drives, etc.also became cheap and ubiquitous T o handle the explosion of data, several innovations allowed distributed computation and storage on massive computing clusters W ithout realizing the future implications, these innovations started decentralizing and breaking apart traditionally monolithic services This opened an era of big data, which Oxford Dictionary defines as extremely lar ge data sets that may be analyzed computationally to reveal patterns, trends, and associations, especially relating to human behavior and interactions Another popular and succinct description of big data are the 3 V s of datavelocity , variety , and volume In 2003 , Google published a paper on the Google File System and shortly thereafter in 2004 , a paper on Map Reduce , an ultra scalable data processing5paradigm In truth, big data has earlier antecedents in M PP data warehouses and data management for experimental physics projects, but Google s publications constituted a big bang for data technologies and the cultural roots of data engineering as we know it today  The Google papers inspired engineers at Y ahoo to develop and later open source Hadoop in 2006  The impact of Hadoop cannot be overstated Software engineers interested in lar ge scale data problems were drawn to the possibilities of Hadoop and its ecosystem of technologies As companies of all sizes and types saw their data grow into many terabytes and even petabytes, the era of the big data engineer was born Around the same time, Amazon had to keep up with their own exploding data needs, and created elastic computing environments Amazon E C2, infinitely scalable storage systems Amazon S3, highly scalable No SQ L databases Dynamo DB, streaming pipelines Kinesis and many other core data building blocks Amazon elected to of fer these services for both internal and external consumption through Amazon W eb Services A W S, which would go on to become the first popular public cloud A W S created an ultra flexible pay as you go resource marketplace by virtualizing and reselling vast pools of commodity hardware Instead of purchasing hardware for a data center , developers could simply rent compute and storage from A W S T urns out, A W S became a very profitable driver for Amazon s growth Other public clouds would soon follow , such as Google Cloud, Microsoft Azure, O VH, Digital Ocean The public cloud is ar guably one of the biggest innovations of the 21st century and spawned a revolution in the way software and data applications are developed and deployed The early big data tools and public cloud laid the foundation upon which today s data ecosystem is built The modern data landscapeand data engineering as we know it todaywould not exist without these innovations The 2000s and 2010s Big Data Engineering Open source big data tools in the Hadoop ecosystem quickly matured and spread from Silicon V alley companies to tech savvy companies all over the6 7world For the first time, businesses anywhere in the world could use the same bleeding edge big data tools developed by the top tech companies You could choose the latest and greatest Hadoop, Pig, Hive, Dremel, H Base, Cassandra, Spark, Presto, and numerous other new technologies that came on the scene T raditional enterprise oriented and G UI based data tools suddenly felt outmoded, and code was in vogue with the ascendance of Map Reduce The authors were around during this time, and it felt like old dogmas died a sudden death upon the altar of big data The explosion of data tools in the late 2000s and 2010s ushered in the big data engineer T o effectively use these tools and techniquesnamely the Hadoop ecosystem Hadoop, Y A RN, H DF S, Map Reduce, etcbig data engineers had to be proficient in software development and low level infrastructure hacking, but with a shifted focus Big data engineers were interested in maintaining huge clusters of commodity hardware to deliver data at scale While they might occasionally submit pull requests to Hadoop core code, they shifted their focus from core technology development to data delivery  They also had the responsibility of working with data scientists to write low level map reduce jobs in a variety of languages, but especially in Java Big data quickly became a victim of its own success As a buzzword, big data gained popularity during the early 2000s through the mid 2010s, peaking around 2014  Big data captured the imagination of companies trying to make sense of the ever growingvolumes of data, as well as the endless barrage of shameless marketing from companies selling big data tools and services Due to the immense hype, it was common to see companies using big data tools for small data problems, sometimes standing up a Hadoop cluster to process just a few gigabytes It seemed like everyone wanted in on the big data action As Dan Ariely tweeted, Big data is like teenage sex everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it To get an idea of the rise and fall of big data, see Figure 1 1 for a snapshot of Google T rends for the search term big data 8 Figur e 1 1  Google T rends for big data Despite the popularity of the term, big data has lost steam since the mid 2010 s What happened One wordsimplification Despite the power and sophistication of open source big data tools, managing them was a lot of work, and required a lot of attention Oftentimes, companies employed entire teams of big data engineerscosting millions of dollars a yearto babysit these platforms In addition, this new breed of data engineer had not been taught to work closely with business stakeholders, so it was a challenge to deliver insight the business needed Open source developers, clouds, and 3rd parties started looking for ways to abstract, simplify , and make big data available to all, without the high administrative overhead and cost of managing their own clusters, installing, configuring, and upgrading their own open source code, etc The term big data is essentially a relic to describe a certain time and approach to handling lar ge amounts of data In truth, data is moving at a faster rate than ever and growing ever lar ger, but big data processing has become so accessible that it no longer merits a separate term every company aims to solve its data problems, regardless of actual data size Big data engineers are now simply data engineers  The 2020s Engineering for the Data Life Cycle At the time of this writing, the data engineering role is evolving rapidly we expect this evolution to continue at a rapid clip for the foreseeable future Whereas data engineers historically spent their time tending to the low level details of monolithic frameworks such as Hadoop, Spark, or Informatica, the trend is moving toward the use of decentralized, modularized, managed, and highly abstracted tools Indeed, data tools have proliferated at an astonishing rate see Figure 1 2  New popular trends in the early 2020 s include The Modern Data Stack, which represents a collection of of f the- shelf open source and 3rd party products assembled to make the lives of analysts easier  At the same time, data sources and data formats are growing both in variety and size Data engineering is increasingly a discipline ofinteroperation, of interconnecting numerous technologies like lego bricks to serve ultimate business goals Figur e 1 2  Data tools in 2012 vs 2021 In fact, the authors believe that data engineers should focus on the abstraction of data processing stages in what we term the data engineering lifecycle see Figure 1 3  Figur e 1 3  The Data Engineering Lifecycle The data engineering lifecycle paradigm shifts the conversation away from technology , toward the data itself and the end goals that it must serve The stages of the lifecycle areGeneration Ingestion Transformation Serving Storage We put storage last because it plays a role in all the other stages W e also have a notion of under currents, i.e., ideas that are critical across the full lifecycle W e will cover the lifecycle more extensively in Chapter 2 , but we outline it here because it is critical to our definition of data engineering The data engineer we discuss in this book can be described more precisely as a data lifecycle engineer  With greater abstraction and simplification, a data lifecycle engineer is no longer encumbered by the gory details of yesterday s big data frameworks While data engineers maintain skills in low level data programming and use these as required, they increasingly find their role focused on things higher in the value chaindata management, Data Ops, data architecture, orchestration, and general data lifecycle management As tools and workflows simplify , weve seen a noticeable shift in the attitudes of data engineers Instead of focusing on who has the biggest data, open source projects and services are increasingly concerned with managing and governing data, making it easier to use and discover , and improving its quality  Data engineers are now conversant in acronyms such as C CP A and G DP R as they engineer pipelines, they concern themselves with privacy , anonymization, data garbage collection and compliance with regulations What s old is new again Now that many of the hard problems of yesterday s data systems are solved, neatly productized, and packaged, technologists and entrepreneurs have shifted focus back to the enterprisey stuff, but with an emphasis on decentralization and agility that contrasts with the traditional enterprise command and control approach W e view the9 10present as a golden age of data management Data engineers managing the data engineering lifecycle have better tools and techniques at their disposal than have ever before W e will discuss the data engineering lifecycle, and its undercurrents, in greater detail in the next chapter  Data Engineering Defined Okay , now that weve surveyed the recent past, were ready to lay out our definition of data engineering Let s unpack the common threads In general, a data engineer gets data, stores it, and prepares it for consumption by data scientists and analysts For the purpose of this book, we define data engineering as follows Data engineering The development, implementation, and maintenance of systems and processes that take in raw data and produce high quality , consistent information that supports downstream use cases, such as analysis and machine learning Data engineering is the intersection of data management, Data Ops, data architecture, orchestration, and software engineering Data engineers Manage the data engineering lifecycle see Figure 1 3 beginning with ingestion and ending with serving data for use cases, such as analysis or machine learning Put another way , data engineers produce reliable data that serves the business with predictable quality and meaning The data engineering workflow is built atop systems supporting the data model, underpinned by the management, and configuration of storage and infrastructure Data engineering systems and outputs are the backbones of successful data analytics and data science The data engineering workflow is roughly as follows see Figure 1 4  This is the general process through which all data in an or ganization flows Raw data is ingested from source systems, stored and processed, and served as conformed data models and datasets to consumers such as data scientists and data analysts W ell return to the data engineering workflow throughout the book Figur e 1 4  The Data Engineering W orkflow Data Engineering and Data Science Where does data engineering fit in with data science There s some debate, with some ar guing data engineering is a subdiscipline of data science W e believe data engineering is separate from data science and analytics They complement each other , but they are distinctly dif ferent Data engineering sits upstream from data science Data engineers serve data scientists and other data customers Figure 1 5  Figur e 1 5  Data engineering sits upstr eam fr om data science To justify this position, let s consider the Data Science Hierarchy of Needs see Figure 1 6  In 2017 , Monica Rogati wrote an article by this title that showed where A IM L sat in proximity to less sexy areas such as data movementstorage, collection, and infrastructure 1 1 Figur e 1 6  The Data Science Hierar chy of Needs Sour ce Rogati It is now widely recognized that data scientists spend an estimated 70% to 90% of their time on the bottom 3 parts of the hierarchygathering data, cleaning data, processing dataand only a small slice of their time on analysis and machine learning Rogati ar gues that companies need to build a solid data foundationthe bottom 3 levels of the hierarchybefore tackling areas such as A I and M L Data scientists aren t typically trained to engineer production grade data systems, and they end up doing this work haphazardly because they lack the support and resources of a data engineer  In an ideal world, data scientists should spend 90% of their time focused on the top layers of the pyramid analytics, experimentation, and machine learning When data engineers focus on these bottom parts of the hierarchy , they build a solid foundation upon which data scientists can succeed With data science driving advanced analytics and machine learning, data engineering straddles the divide between getting data and getting value from data see Figure 1 7  We believe data engineering will rise to be equal in importance and visibility to data science, with data engineers playing a vital role in making data science successful in production Figur e 1 7  A data engineer gets data and pr ovides value fr om data Data Engineering Skills and Activities The skillset of a data engineer encompasses what we call the undercurrents of data engineeringdata management, data ops, dataarchitecture, and software engineering It requires an understanding of how to evaluate data tools, and how they fit together across the data engineering lifecycle It s also necessary to know how data is produced in source systems, as well as how analysts and data scientists will consume and create value from data after it is processed and curated Finally , a data engineer juggles a lot of complex moving parts, and must constantly optimize along the axes of cost, agility , simplicity , reuse, and interoperability Figure 1 8  Well cover these topics in more detail in upcoming chapters Figur e 1 8  The balancing act of data engineering As we discussed, in the recent past, a data engineer was expected to know and understand how to use a small handful of powerful and monolithic technologies to create a data solution Utilizing these technologies Hadoop, Spark, T eradata, and many others often required a sophisticated understanding of software engineering, networking, distributed computing, storage, or other low level details Their work would be devoted to cluster administration and maintenance, managing overhead, and writing pipeline and transformation jobs, among other tasks Nowadays, the data tooling landscape is dramatically less complicated to manage and deploy  Modern data tools greatly abstract and simplify workflows As a result, data engineers are now focused on balancing the simplest and most cost ef fective, best of breed services that deliver value to the business The data engineer is also expected to create agile data architectures that evolve as new trends emer ge What are some things a data engineer does N OT do A data engineer typically does not directly build machine learning models, create reports or dashboards, perform data analysis, build K PIs, or develop software applications That said, a data engineer should have a good functioning understanding of all of these areas, in order to best serve stakeholders Data Maturity and the Data Engineer The level of data engineering complexity within a company depends a great deal on the company s data maturity  This, in turn, has a significant impact on a data engineer s day to day job responsibilities, and potentially on their career progression as well What is data maturity , exactly Data maturity is the progression toward higher data utilization, capabilities, and integration across the or ganization, but data maturity does not simply depend on the age or revenue of a company  An early stage startup can have greater data maturity than a 100 year oldcompany with annual revenues in the billions What matters is how data is leveraged as a competitive advantage There are many versions of data maturity models, such as Data Management Maturity D MM and others, and it s hard to pick one that is both simple and useful for data engineering So, well create our own simplified data maturity model In our data maturity model Figure 1 9 , we have three stagesstarting with data, scaling with data, and leading with data Let s look at each of these stages, and what a data engineer typically does at each stage Figur e 1 9  Our simplified data maturity model for a company12 Stage 1 Starting with data A company getting started with data is, by definition, in the very early stages of its data maturity  The company may have fuzzy , loosely defined goals, or no goals at all Data architecture is in the very early stages of planning and development Adoption and utilization are likely low or nonexistent The data team is small, often with a headcount in the single digits At this stage, a data engineer is often a generalist, and will typically play several other roles, such as data scientist or software engineer  A data engineer s goal is to move fast, get traction, and add value The practicalities of getting value from data are typically poorly understood, but the desire exists Reports or analyses lack formal structure, and most requests for data are ad hoc Machine learning is rarely successful at this stage, and so such projects are not recommended W eve seen countless data teams get stuck and fall short when they try to jump to machine learning without building a solid data foundation The authors half jokingly call themselves recovering data scientists, lar gely from personal experience with being involved in premature data science projects without adequate data maturity or data engineering support In or ganizations getting started with data, a data engineer should focus on the following Get buy in from key stakeholders, including executive management Ideally , the data engineer should have a sponsor for key initiatives to design and build a data architecture to support the company s goals Define the right data architecture usually solo, since a data architect likely isn t available This means determining business goals, and what competitive advantage youre aiming to achieve with your data initiative W ork toward a data architecture that supports these goals See Chapter 3 for our advice on good data architecture 13 Identify and audit data that will support key initiatives, and operate within the data architecture that you designed Build a solid data foundation upon which future data analysts and data scientists can generate reports and models that provide competitive value In the meantime, you may also have to generate these reports and models until this team is hired Things to watch out for This is a delicate stage with lots of pitfalls Organizational willpower may wane Getting quick wins will establish the importance of data within the organization Just keep in mind that quick wins will likely create technical debt Have a plan to reduce this debt, as it will otherwise add friction for future delivery  Be visible and continue getting support Avoid undif ferentiated heavy lifting Don t get caught in the trap of boxing yourself in with unnecessary technical complexity  Use of f- the shelf, turnkey solutions wherever possible Build custom solutions and code only where youre creating a competitive advantage Stage 2 Scaling with data At this point, a company has moved away from ad hoc data requests and has formal data practices Now the challenge is creating scalable data architectures and planning for a future where the company is truly data- driven Data engineering roles move from generalists to specialists, with people focusing on particular aspects of the data engineering lifecycle In or ganizations that are in Stage 2 of data maturity , a data engineer s goals are to Establish formal data practices Create scalable and robust data architectures Adopt Dev Ops and Data Ops practices Build systems that support machine learning Continue to avoid undif ferentiated heavy lifting and customizing only where there s a competitive advantage We will return to each of these goals later in the book Things to watch out for As we grow more sophisticated with data, there s a temptation to adopt bleeding edge technologies based on social proof from Silicon V alley companies This is rarely the best use of your time and ener gy Any technology decisions should be driven by the value that theyll deliver to your customers The main bottleneck for scaling is not cluster nodes, storage, or technology , but the data engineering team itself Focus on solutions that are simple to deploy and manage to expand your team s throughput Youll be tempted to frame yourself as a technologist, as a data genius who can deliver magical products Shift your focus instead to pragmatic leadershipbegin transitioning to the next maturity stage now  Communicate with other teams about the practical utility of data T each the or ganization how to consume and leverage data Stage 3 Leading with data At this stage, the company leads with data and is data driven The automated pipelines and systems created by data engineers allow people within the company to do self service analytics and machine learning Introducing new data sources is seamless and tangible value is derived Data engineers implement formal controls and practices to ensure data isalways available to the people and systems that need it Data engineering roles continue to specialize more deeply than in Stage 2  In or ganizations that are in Stage 3 of data maturity , a data engineer will continue building on prior stages, plus Create automation for the seamless introduction and usage of new data Focus on building custom tools and systems that leverage data as a competitive advantage Focus on the enterprisey aspects of data Data management, data serving, Data Ops, etc Deploy tools that expose and disseminate data throughout the or ganization, including data catalogs, data lineage tools, metadata management systems, etc Collaborate ef ficiently with software engineers, machine learning engineers, analysts, etc Create a community and environment where people can collaborate and speak openly , no matter what role or position youre in Things to watch out for At this stage, complacency is a significant danger  Once organizations reach Stage 3 , they must focus constantly on maintenance and improvement or they risk falling back to a lower stage Technology distractions are a bigger danger here than in the other stages There s a temptation to pursue expensive hobby projects that don t deliver value to the business Utilize custom built technology only where it provides a competitive advantage The Background and Skills of a Data Engineer Because data engineering is a relatively new discipline, there is little available in the way of formal training to enter the field Universities don thave a common data engineering path Although there are a handful of data engineering boot camps and online tutorials covering random topics, a common curriculum for the subject doesn t yet exist Those entering data engineering arrive with varying backgrounds in education, career , and skillset Everyone entering the field should expect to invest a significant amount of time in self study including reading this book Figur e 1 10  Data engineering is the fastest gr owing tech occupation 2020 If youre pivoting your career into data engineering see Figure 1 10 for recent growth of the field, weve found that the transition is easiest when moving from an adjacent field, such as software engineering, E TL development, database administration, data science, and data analysis These disciplines tend to be data aware and provide good context for data roles in an or ganization They also tend to equip folks with the relevant technical skills and context to solve data engineering problems Despite the lack of a formalized path, there is a requisite body of knowledge that we believe a data engineer should know in order to be successful By definition, a data engineer must understand both data and technology  With respect to data, this entails knowing about various best practices around data management On the technology end, a data engineer must be aware of various options for tools, their interplay , and their tradeof fs This requires a good understanding of software engineering, Data Ops, and data architecture Zooming out, a data engineer must also understand the requirements of data consumersdata analysts and data scientistsas well as the broader implications of data across the or ganization Data engineering is a holistic practicethe best data engineers view their responsibilities through both business and technical lenses Business Responsibilities The macro responsibilities we list below aren t just important for a data engineer , but for anyone working in a data or technology field Because a simple Google search will yield tons of resources to learn about these areas, we will simply list them for brevity Know how to communicate with nontechnical and technical people Communication is key , and you need to be able to establish rapport and trust with people across the or ganization W e suggest paying close attention to or ganizational hierarchies, who reports to whom, how14people interact with each other , and which silos exist These observations will be invaluable to your success Understand how to scope, and how to gather business and pr oduct requir ements Simply put, you need to know what to build and make sure that your stakeholders agree with your assessment In addition, develop a sense of how data and technology decisions impact the business Understand the cultural foundations of Agile, Dev Ops, and Data Ops Many technologists mistakenly believe these practices are solved through technology  This is dangerously wrong in fact, they are fundamentally cultural, requiring buy in across the or ganization Cost contr ol Youll be successful when you can keep costs low while providing outsized value Know how to optimize for time to value, the total cost of ownership, and opportunity cost Learn to monitor costs to avoid surprises Continuously learn The data field feels like it s changing at light speed People who succeed in it are great at picking up new things while sharpening their fundamental knowledge Theyre also good at filtering, determining which new developments are most relevant to their work, which are still immature, and which are just fads Stay abreast of the field and learn how to learn A successful data engineer always zooms out to understand the big picture, and how to achieve outsized value for the business Communication is key , both for technical and non technical people W e often see data teams succeed or fail based upon their communication with other stakeholders success or failure is very rarely a technology issue Knowing how tonavigate an or ganization, scope and gather requirements, control costs, and continuously learn will set you apart from the data engineers who rely solely on their technical abilities to carry their career  Technical Responsibilities At a high level, you must understand how to build architectures that optimize performance and cost, whether the components are prepackaged or homegrown Ultimately , architectures and constituent technologies are building blocks to serve the data engineering lifecycle Recall the stages of the data engineering lifecycle Figure 1 3 Data generation Data ingestion Data storage Data transformation Serving data The undercurrents of the data engineering lifecycle are Data management Data Ops Data architecture Software engineering Zooming in a bit, we discuss some of the tactical data and technology skills youll need as a data engineer here we will discuss these in much more detail in subsequent chapters People often askshould a data engineer know how to code Short answer yes A data engineer should have production grade software engineering chops W e note that the nature of software development projects undertaken by data engineers has changed fundamentally in the last few years A greatdeal of low level programming ef fort previously expected of engineers is now replaced by fully managed services, managed open source, and simple plug and play software as a service S AA S of ferings For example, data engineers now focus on high level abstractions, writing pipelines as code within an orchestration framework, or using dataframes inside Spark instead of having to worry about the nitty gritty of Spark internals Even in a more abstract world, software engineering best practices provide a competitive advantage, and data engineers who can dive into the deep architectural details of a codebase give their companies an edge when specific technical needs arise In short, a data engineer who can t write production grade code will be severely handicapped, and we don t see this changing anytime soon Data engineers remain software engineers, in addition to their many other roles What languages should a data engineer know W e divide data engineering programming languages into primary and secondary categories At the time of this writing, the primary languages of data engineering are S QL, Python, a J VM language usually Java or Scala, and Bash S QL The most common interface for databases and data lakes After briefly being sidelined by the need to write custom map reduce code for big data processing, S QL in various forms has reemer ged as the lingua franca of data Python The bridge language between data engineering and data science Further , a growing number of data engineer tools are written in Python or have Python A PIs It s known as the second best language at everything Python underlies popular data tools such as Pandas, Numpy , Airflow , S KLearn, T ensorflow , Pytorch, Py Spark, and countless others Python acts as the glue between underlying components and is frequently a first class A PI language for interfacing with a framework J VM languages, such as Java and Scala Popular for Apache open source projects such as Spark, Hive, Druid, and many more The J VM is generally more performant than Python and may provide access to lower -levelfeatures than a Python A PI For example, this is the case for Apache Spark and Beam If youre using a popular open source data framework, understanding Java or Scala will be very helpful Bash The command line interface for Linux operating systems Knowing Bash commands and being comfortable using C LI s will greatly improve your productivity and workflow when you need to script or perform O S operations Even today , data engineers frequently use command line tools like A wk or sed to process files in a data pipeline or call Bash commands from orchestration frameworks If youre using Windows, feel free to substitute Powershell for Bash T HE U NR EA SO NA BL E E FF EC TI VE NE SS O F S QL The advent of map reduce and the Big Data era relegated S QL to pass status Since then, a variety of developments have dramatically enhanced the utility of S QL in the data engineering life cycle Spark S QL, Big Query , Snowflake, and many other data tools can process massive amounts of data using declarative, set theoretic S QL semantics S QL is also supported by many streaming frameworks, such as Flink, Beam, and Kafka W e believe that competent data engineers should be highly proficient in S QL Are we saying that S QL is an end all be all language Not at all S QL is a powerful tool that can quickly solve many complex analytics and data transformation problems Given that time is a primary constraint for data engineering team throughput, a tool that combines simplicity and high productivity should be embraced Data engineers also do well to develop expertise in composing S QL with other operations, either within frameworks such as Spark and Flink or by using orchestration to combine multiple tools Data engineers should also learn modern S QL semantics for dealing with J SO N parsing and nested data, and consider leveraging a S QL management framework such as D BT Data Build Tool A proficient data engineer also recognizes when S QL is not the right tool for the job and can choose and code in a suitable alternative while a S QL expert could likely write a query to stem and tokenize raw text in an N LP natural language processing pipeline, coding in native Spark is a far superior alternative to this masochistic exercise Data engineers may also need to develop proficiency in secondary programming languages including R, Javascript, Go, Rust, CC, C, Julia, etc Developing in these languages is often necessary when they are popular across the company , or to use domain specific data tools For instance, Javascript has proven popular as a language for user -defined15functionsin cloud data warehouses, while C and Powershell are important in companies that leverage Azure and the Microsoft ecosystem K EE PI NG P AC E I N A F AS T M OV IN G F IE LD Once a new technology r olls over you, if your e not part of the steamr oller, your e part of the r oad Stewart Brand How do you keep your skills sharp in a rapidly changing field like data engineering Should you focus on the latest tools or deep dive into fundamentals Here s our advice focus on the fundamentals to understand what s not going to change pay attention to ongoing developments to understand wher e the field is going New paradigms and practices are introduced all the time, and it s incumbent on you to stay current Strive to understand how new technologies will be useful in the lifecycle The Continuum of Data Engineering Roles, from A to B Despite job descriptions that paint a data engineer as a unicorn who must possess every data skill imaginable, data engineers don t all do the same type of work or have the same skillset Just as data maturity is a helpful guide to understand the types of data challenges a company will face as it grows its data capability , its helpful to look at some key distinctions in the types of work data engineers do Though these distinctions are simplistic, it clarifies what data scientists and data engineers do, and avoids lumping either role into the unicorn bucket In data science, there s the notion of T ype A and T ype B data scientists Type A data scientistswhere A stands for Analysisfocus on understanding and deriving insight from data T ype B data scientists where B stands for Buildingshare similar backgrounds as T ype A data scientists, and also possess strong programming skills The T ype B Data Scientist builds systems that make data science work in production 16 Borrowing from the T ype A and T ype B data scientist continuum, well create a similar distinction for two types of data engineers Type A data engineers A stands for Abstraction  In this case, the data engineer avoids undif ferentiated heavy lifting, keeping data architecture as simple and abstract as possible and not re inventing the wheel T ype A data engineers manage the data engineering lifecycle using mostly or completely off the shelf products, managed services, and tools T ype A data engineers work at companies across industries, and across all levels of data maturity  Type B data engineers B stands for Build  Type B data engineers build data tools and systems that scale and leverage a company s core competency and competitive advantage In the data maturity range, a T ype B data engineer is more commonly found at companies in Stage 2 and 3 scaling and leading with data, or when an initial data use case is so unique and mission critical that custom data tools are required to get started Type A and T ype B data engineers may work in the same company , and they may even be the same person More commonly , a Type A data engineer is first hired to set the foundation, with T ype B data engineer skill sets either learned or hired as the need arises within a company  Data Engineers Inside an Organization Data engineers don t work in a vacuum Depending on what theyre working on, they will interact both with technical and non technical people, as well as face dif ferent directions internal and external Let s explore what data engineers do inside an or ganization, and who they interact with Internal Facing V ersus External Facing Data Engineers A data engineer serves several end users and faces many internal and external directions Figure 1 1 1  Since not all data engineering workloads and responsibilities are the same, it s important to understand who the data engineer serves Depending on the end use cases, a data engineer s primary responsibilities are external facing, internal facing, or a blend of the two Figur e 1 1 1  The dir ections a data engineer faces An external facing data engineer normally aligns with the users of external- facing applications, such as social media apps, Io T devices, and e commerce platforms This data engineer architects, builds, and manages the systems that collect, store, and process transactional and event data from these applications The systems built by these data engineers have a feedback loop from the application, to the data pipeline, then back to the application see Figure 1 12  We note that external facing data engineering comes with a special set of problems External facing query engines often handle much lar ger concurrency loads than internal facing systems engineers also need to consider putting tight limits on queries that users can run in order to limit the infrastructure impact of any single user and security is a much more complex and sensitive problem for external queries, especially if the data being queried is multi tenant, i.e., data from many customers is housed in a single table Figur e 1 12  External facing data engineer systems An internal facing data engineer typically focuses on activities crucial to the needs of the business and internal stakeholders Figure 1 13  Examples include the creation and maintenance of data pipelines and data warehouses for B I dashboards, reports, business processes, data science, and machine learning models Figur e 1 13  Internal facing data engineer External facing and internal facing responsibilities are often blended In practice, internal facing data is usually a prerequisite to external facing data The data engineer has two sets of users, with very dif ferent requirements for query concurrency , security , etc., as mentioned above Data Engineers and Other T echnical Roles In practice, the data engineering lifecycle cuts across many dif ferent domains of responsibility  Data engineers sit at the nexus of a variety of roles, interacting with many or ganizational units, either directly or through managers Let s look at who a data engineer may impact W ell start with a discussion of technical roles connected to data engineering Figure 1 14  Figur e 1 14  Key technical stakeholders of data engineering In a nutshell, the data engineer is a hub between data producers, such as software engineers and data architects, and data consumers such as data analysts, data scientists, and machine learning engineers In addition, data engineers will interact with those in operational roles, such as Dev Ops engineers Given the pace at which new data roles come into vogue analytics and machine learning engineers come to mind, this is by no means an exhaustive list Upstream stakeholders To be successful as a data engineer , you need to understand the data architecture youre using or designing, as well as the source systems producing the data youll need Below we discuss a few common upstream stakeholders youll encounter , such as data architects, software engineers, and devops engineers Data architects Data ar chitects function at a level of abstraction one step removed from data engineers Data architects design the blueprint for or ganizational data management, mapping out processes, and overall data architecture and systems They also serve as a bridge between the technical and non technical sides of an or ganization Successful data architects generally have battle scars from extensive engineering experience, allowing them to guide and assist engineers, while successfully communicating the challenges of engineering to non technical business stakeholders Data architects implement policies for managing data across silos and business units, steer global strategies such as data management and data governance, and guide major initiatives Data architects often play a central role in cloud migrations and greenfield cloud design, which at the time of this writing are either inflight initiatives or on a roadmap for many companies The advent of the cloud has shifted the boundary between data architecture and data engineering Cloud data architectures are much more fluid than on-17premisessystems, so architecture decisions that traditionally involved extensive study , long lead times, purchase contracts, and hardware installation are now often made during the implementation process, just one step in a lar ger strategy  Nevertheless, data architects will remain important visionaries in enterprises, working hand in hand with data engineers to determine the big picture of architecture practices and data strategies Depending on the data maturity and size of a company , a data engineer may overlap with or assume the responsibilities of a data architect Therefore, a data engineer should have a good understanding of architecture best practices and approaches Readers will note that we have placed data architects in the upstr eam stakeholder s section This is because data architects often help to design application data layers that are source systems for data engineers In general, architects may also interact with data engineers at various other stages of the data engineering lifecycle We will cover good data architecture in Chapter 3  Software engineers Softwar e engineers build the software and systems that run a business they are lar gely responsible for generating the internal data that data engineers will consume and process The systems built by software engineers typically generate application event data and logs, which are a major asset in their own right This internal data is in contrast to external data , pulled from S AA S platforms or partner businesses In well run technical organizations, software engineers and data engineers coordinate from the inception of a new project to design application data for consumption by analytics and machine learning applications A data engineer should work together with software engineers to understand the applications that generate data, the volume, frequency , and format of the data being generated, and anything else that will impact the data engineering lifecycle, such as data security and regulatory compliance In addition, data engineers and architects can coordinate on designing thedata bus that captures data from an application, whether this is a messaging queue, a log sink, or a batch extraction process Dev Ops engineers Dev Ops engineers often produce data through operational monitoring we classify them as upstream of data engineers for this reason, but they may also be downstream, consuming data through dashboards or they may interact with data engineers directly in coordinating operations of data systems Downstream stakeholders The modern data engineering profession exists to serve downstream data consumers and use cases In this section, well discuss how data engineers interact with a variety of downstream roles W ell also introduce a few different service models, including centralized data engineering teams and cross functional teams Chapter 8 will provide an in depth discussion of these topics Data scientists Data scientists build forward looking models to make predictions and recommendations These models are then evaluated on live data to provide value in various ways For example, model scoring might determine automated actions in response to real time conditions, recommend products to customers based on the browsing history in their current session, or make live economic predictions used by traders According to common industry folklore, data scientists spend between 40 and 80% of their time collecting, cleaning, and preparing data In the authors experience, these numbers are representative of poor data science and engineering practices In particular , many popular and useful data science frameworks can become bottlenecks if they are not scaled up appropriately  Data scientists who work exclusively on a single workstation force themselves to downsample data, making data preparation significantly more complicated and potentially compromising the quality of the models that they produce Furthermore, locally developed code and environments18are often dif ficult to deploy in production, and a lack of automation significantly hampers data science workflows If data engineers are doing their job and collaborating successfully , data scientists shouldn t spend their time collecting, cleaning, and preparing data after initial exploratory work data engineers should do this work The need for production ready data science is a major driver behind the emer gence of the data engineering profession Data Engineers should help data scientists to enable a path to production In fact, the authors moved from data science to data engineering in recognition of this fundamental need Data engineers work to provide the data automation and scale that make data science more ef ficient Data analysts Data analysts or business analysts seek to understand business performance and trends Whereas data scientists are forward looking, a data analyst typically focuses on the past or present Data analysts usually run S QL queries in a data warehouse or a data lake They may also utilize spreadsheets for computation and analysis, and various business intelligence tools such as Power BI, Looker , or T ableau Data analysts are domain experts in the data they work with frequently , and therefore become intimately familiar with data definitions, characteristics, and quality problems A data analyst s typical downstream customers are business users, management, and executives Data engineers work with data analysts to build pipelines for new data sources required by the business Data analysts subject matter expertise is invaluable in improving data quality , and they frequently collaborate with data engineers in this capacity  Machine learning engineers and A I researchers Machine learning engineers M L Engineers overlap with both data engineers and data scientists Machine learning engineers develop advanced machine learning techniques, train models, and design and maintain the infrastructure running machine learning processes in a scaled productionenvironment M L engineers often have advanced working knowledge of machine learning and deep learning techniques, as well as frameworks such as Py T orch or T ensorflow  Machining learning engineers also understand the hardware, services, and systems required to run these frameworks both for model training and model deployment at a production scale Increasingly , machine learning flows run in a cloud environment where M L engineers can spin up and scale infrastructure resources on demand or rely on managed services As mentioned above, the boundaries between M L engineering, data engineering, and data science are fuzzy  Data engineers may have some Dev Ops responsibilities over M L systems, and data scientists may work closely with M L engineering in designing advanced machining learning processes The world of M L engineering is growing at a rapid pace and parallels a lot of the same developments occurring in data engineering Whereas several years ago the attention of machine learning was focused on how to build models, M L engineering now increasingly emphasizes incorporating best practices of M L Operations M LOps and other mature practices previously adopted in software engineering and Dev Ops A I resear chers work on new , advanced machine learning techniques A I researchers may work inside lar ge technology companies, specialized intellectual property startups Open AI, Deep Mind, or academic institutions Some practitioners are partially dedicated to research in conjunction with M L engineering responsibilities inside a company  Those working inside specialized machine learning labs are often 100% dedicated to research Research problems may tar get immediate practical applications or more abstract demonstrations of artificial intelligence Alpha Go and G PT-3are great examples of machine learning research projects W eve provided some references in the further r eading section at the end of the chapter  A I researchers in well funded or ganizations are highly specialized and operate with supporting teams of engineers to facilitate their work Forexample, see job listings for Open AI or Deep Mind Machine learning engineers in academia usually have fewer resources, but rely on teams of graduate students, postdocs, and university staf f to provide engineering support Machine learning engineers who are partially dedicated to research often rely on the same support teams for research and production Data Engineers and Business Leadership So far , weve discussed technical roles a data engineer interacts with But data engineers also operate more broadly as or ganizational connectors, often in a non technical capacity  Businesses have come to rely increasingly on data as a core part of many products, or as a product in itself Data professionals, such as data engineers, now participate in strategic planning and lead key initiatives that extend beyond the boundaries of I T  Data engineers often support data architects by acting as the glue between the business and data scienceanalytics Data in the C suite C level executives are increasingly involved in data and analytics as these are recognized as central assets for modern businesses C EOs now concern themselves with initiatives that were once the exclusive province of I T , such as major cloud migrations or deployment of a new customer data platform This is certainly the case at tech companies such as Google, Amazon, and Facebook, but also in the wider business world Chief executive officer C EOs at non tech companies generally don t concern themselves with the nitty gritty of data frameworks and software Rather , they define a vision in collaboration with technical C suite roles and company data leadership Data engineers provide a window into what s possible with data That is, data engineers and their managers maintain a map of what data is available to the or ganization both internally and from third parties in what timeframe They are also tasked to study major data architectural changes in collaboration with other engineering roles For example, data engineers areoften heavily involved in cloud migrations, migrations to new data systems, or deployment of streaming technologies Chief information officer A chief information of ficer C IO is the senior C suite executive responsible for information technology within an or ganization it is an internal facing role A C IO must possess deep knowledge of both information technology and business processeseither alone is insuf ficient C IOs direct the information technology or ganization, setting ongoing policies while also defining and executing major initiatives under the direction of the C EO In or ganizations with well developed data culture, C IOs often collaborate with data engineering leadership If an or ganization is not very high in its data maturity , a C IO will typically help shape its data culture C IOs will work with engineers and architects to map out major initiatives and make strategic decisions on the adoption of major architectural elements, such as E RP and C RM systems, cloud migrations, data systems, and internal facing I T Chief technology officer A chief technology of ficer C T O is similar to a C IO but faces outward A C TO owns the key technological strategy and architectures for external- facing applications, such as mobile, web apps, and Io T , all critical data sources for data engineers The C T O is likely a skilled technologist and has a good sense of software engineering fundamentals, system architecture, and much more Data engineers often report directly or indirectly through a C TO Chief data officer The chief data of ficer C DO was created in 2002 at Capital One in recognition of the growing importance of data as a business asset The C DO is responsible for a company s data assets and strategy  C DOs are focused on the business utility of data but should have a strong technical grounding C DOs oversee data products, strategy , and initiatives, as well as corefunctions such as master data management and data privacy  Occasionally , C DOs oversee business analytics and data engineering Chief analytics officer The chief analytics of ficer C AO is a variant of the C DO role Where both roles exist, the C DO focuses on the technology and or ganization required to deliver data, where the chief analytics of ficer is responsible for analytics, strategy , and decision making for the business A C AO may oversee data science and machine learning as well, though this lar gely depends on whether or not the company has a C DO or C T O role Chief algorithms officer A chief algorithms of ficer C AO- 2 is a very recent innovation in the C- suite, a highly technical role focused specifically on data science and machine learning The C AO- 2 s typically have experience as individual contributors and team leads in data science or machine learning projects Frequently , they have a background in machine learning research and a related advanced degree C AO- 2 s are expected to be conversant in current machine learning research and have deep technical knowledge of their company s machine learning initiatives In addition to creating business initiatives, they provide technical leadership, set research and development agendas, build research teams, etc Data engineers and project managers Data engineers often work on lar ge initiatives, potentially spanning many years As we write this book, many data engineers are working on cloud migrations, migrating pipelines and warehouses to the next generation of data tools Other data engineers are starting greenfield, creating new data architectures from scratch, able to choose from a breathtaking number of best of breed architecture and tooling options These lar ge initiatives often benefit from project management in contrast to product management, which we discuss below Where data engineersfunction in an infrastructure and service delivery capacity , project managers direct traf fic and serve as gatekeepers Most project managers operate according to some variation of Agile and Scrum, with W aterfall still appearing occasionally  Business never sleeps, and business stakeholders often have a significant backlog of things they want to be addressed, and new initiatives they want to launch Project managers must filter a long list of requests and prioritize key deliverables to keep projects on track and better serve the company as a whole Data engineers interact with project managers, often in planning sprints for projects, as well as ensuing standups related to the sprint Feedback goes both ways, with data engineers informing project managers and other stakeholders about progress and blockers, and project managers balancing the cadence of technology teams against the ever changingneeds of the business Data engineers and product managers Product managers oversee product development, often owning product lines In the context of data engineers, these products are called data products Data products are either built from the ground up or are incremental improvements upon existing products As the broader corporate world has adopted a data centric focus, data engineers interact more frequently with product managers  Similar to project managers, product managers balance the activity of technology teams against the needs of the customer and business Data engineers and other management roles Data engineers interact with a variety of managers beyond project and product managers However , these interactions usually follow either the services model or the cross functional model That is, data engineers either serve a variety of incoming requests as a centralized team or work as a resource assigned to a particular manager , project, or product For more information on data teams and how to structure them, we recommend John Thompson s Building Analytics T eams and Jesse Anderson s Data T eams see the further r eading section at the end of the chapter Both books provide strong frameworks and perspectives on the roles of executives with data, who to hire, and how to construct the most effective data team for your company  Conclusion This chapter provided you with a brief overview of the data engineering landscape, including Defining data engineering, and describing what data engineers do Describing the types of data maturity a company may Type A and T ype B data engineers Who data engineers work with A stab at the future of data engineering We hope that the first chapter has whetted the reader s appetite, whether they are practitioners of software development, data science, machine learning engineering, or business stakeholders, entrepreneurs, venture capitalists, etc Of course, there is a great deal still to elucidate in subsequent chapters In Chapter 2 , we will cover the data engineering lifecycle in detail, followed by architecture in Chapter 3  In subsequent chapters, we will get into the nitty gritty of technology decisions for each part of the lifecycle The entire data field is in flux, and the aim in each chapter is to focus on the immutables, or at least perspectives that will be valid for many years to come in the midst of relentless change 1 .altexsoft.comblogdatasciencewhat is data engineering explaining data- pipeline data warehouse and data engineer -role 2 Jesse Anderson, .jesse-anderson.com201806the two types of data engineering 3 Maxime Beauchemin, Is Data Engineering O Reilly 2020 , .comlibraryviewwhat is- data9781492075578ch01.html 5 .lexico.comendefinitionbig_data 6 .wired.com201 110how yahoo spawned hadoop 7 8 .comdanarielystatus287952257926971392langen 9 Data Ops aka Data Operations W e will return to this topic throughout the book, starting in Chapter 2  For more information, read the Data Ops Manifesto 10 California Consumer Privacy Act and General Data Protection Regulation 1 1 12 13 .linkedin.compulsewhat recovering data scientist joe reis 14 4db57b6cce35 15 .getdbt.com 16 two year-journey as a data scientist at twitter f0c13298aee6 17 .dataversity .netdata architect vs data engineer 18 There are a variety of references for this notion data scientists spend- 80 of their -time cleaning data turns out no .datanami.com20200706data prep still dominates data scientists time survey- finds  This cliche is widely known, but there s healthy debate around its validity in dif ferent practical settings Chapter 2  The Data Engineering Lifecycle A N OT E F OR E AR LY R EL EA SE R EA DE RS With Early Release ebooks, you get books in their earliest formthe authors raw and unedited content as they writeso you can take advantage of these technologies long before the of ficial release of these titles This will be the second chapter of the final book If you have comments about how we might improve the content andor examples in this book, or if you notice missing material within this chapter , please reach out to the authors at book_feedbackternarydata.com  The major goal of this book is to encourage you to move beyond viewing data engineering as a specific collection of data technologies The data landscape is seeing an explosion of new data technologies and practices, with higher levels of abstraction and ease of use Some of these technologies will succeed, and most will fade into obscurity at an ever faster rate Because of increased technical abstraction, data engineers will increasingly become data lifecycle engineers , thinking and operating in terms of the principles of data lifecycle management In this chapter , youll learn about the data engineering lifecycle , which is the central theme of this book  The data engineering lifecycle is our framework describing cradle to grave data engineering  You will also learn about the undercurrents of the data engineering lifecycle, which are key foundations that support all data engineering ef forts What Is the Data Engineering Lifecycle In simple terms, the data engineering lifecycle is the series of stages that turn raw data ingredients into a useful end product, ready for consumption by analysts, machine learning engineers, etc In this chapter , we introduce the major stages of the data engineering lifecycle, focusing on core concepts and saving details of each stage to later chapters We divide the data engineering lifecycle into the following five stages Figure 2 1 Generation source systems Ingestion Storage Transformation Serving data In general, the data engineering lifecycle starts by getting data from source systems, storing it, and transforming and serving data to analysts, data scientists, machine learning engineers, and others The stages in the middle ingestion, storage, transformationcan get a bit jumbled And that s ok Although we split out the distinct parts of the data engineering lifecycle, it s not always a neat, continuous flow  Various stages of the lifecycle may repeat themselves, occur out of step, or weave together in interesting ways Acting as a bedrock are undercurrents Figure 2 1 that cut across multiple stages of the data engineering lifecycledata management, Data Ops, data architecture, orchestration, and software engineering No part of the data engineering lifecycle can properly function without each of these undercurrents Figur e 2 1  Components and under currents of the data engineering lifecycle The Data Lifecycle V ersus the Data Engineering Lifecycle You may be wondering what the dif ference is between the overall data lifecycle and the data engineering lifecycle There s a subtle distinction between the two The data engineering lifecycle is a subset of the full data lifecycle that is owned by data engineers see Figure 2 2  Whereas the full data lifecycle encompasses data across its entire lifespan, the data engineering lifecycle focuses on the stages a data engineer controls, namely data generation in source systems to serving the data for analytics and machine learning use cases Figur e 2 2  The data engineering lifecycle is a subset of the full data lifecycle Generation Source Systems The data engineer needs to understand how source systems work, how they generate data, the frequency and velocity of the data, and the variety of data they generate A major challenge in modern data engineering is thatengineers must work with and understand a dizzying array of data source systems As an illustration, let s look at two common source systems, one very traditional and the other a modern example Figure 2 3 illustrates a traditional source system, with applications backed by a database This source system pattern became popular in the 1980s with the explosive success of relational databases The application database pattern remains popular today with various modern evolutions of software development practices For example, with microservices, applications often consist of many small servicedatabase pairs rather than a single monolith No SQ L databases like Mongo DB, Cosmos DB, Spanner , and Dynamo DB are compelling alternatives to traditional R DB MS systems Figur e 2 3  Source system example an application database Lets look at another example of a source system Figure 2 4 illustrates an Io T swarm, where a fleet of devices circles sends data messages rectangles to a central collection system This type of system isincreasingly common as Io T devicessensors, smart devices, and much moreproliferate in the wild Figur e 2 4  Source system example an Io T Swarm and Messaging Queue Evaluating source systems key engineering considerations Below are some important characteristics of source systems that data engineers must think about This is by no means an exhaustive list, but rather a starting set of evaluation questions What are the basic characteristics of the data source Is it an application A swarm of Io T devices How does the source handle state At what rate is data generated How many events per second How many G B per hour What level of consistency can data engineers expect from the output data If youre running data quality checks against the output data, how often do data inconsistencies occurnulls where they aren t expected, bad formatting, etcdeviate from the norm How often do errors occur Will the data contain duplicates Will some data values arrive late, possibly much later than other messages produced at the same time What is the schema of the ingested data W ill data engineers need to join across several tables or even several systems to get a full picture of the data If schema changessay , a new column is addedhow is this dealt with and communicated to downstream stakeholders How frequently should data be pulled from the source system For stateful systems, e.g a database tracking customer account information, is data provided as periodic snapshots or as update events from change data capture C DC What s the logic for how changes are performed, and how are these tracked in the source databaseWhowhat is the data provider that will transmit the data for downstream consumption Will reading from a data source impact its performance Does the source system have upstream data dependencies What are the characteristics of these upstream systems Are data quality checks in place to check for late or missing data Sources produce data that is consumed by downstream systems This includes human generated spreadsheets, I OT sensors, web and mobile applications, and everything in between Each source has its unique volume and cadence of data generation A data engineer should understand how the source generates data, including relevant quirks or nuances Data engineers also need to understand the limits of the source systems they interact with For example, will queries to feed analytics cause performance issues with an application One of the most challenging nuances of source data is a schema  The schema defines the hierarchical or ganization of data Logically , we can think of data at the level of a full source system, drilling down into individual tables, all the way to the structure of individual fields The schema of data shipped from source systems is handled in a variety of ways One popular model is schemaless  Schemaless doesn t mean the absence of schemarather , it means that the schema is defined by the application as data is written, whether to a messaging queue, a flat file, a blob, or a document database such as Mongo DB A more traditional model built on relational database storage uses a fixed schema enforced in the database, which application writes must conform to Either of these models presents challenges for data engineers Schemas change over time in fact, schema evolution is encouraged in the agile approach to software development T aking raw data input in the source system schema and transforming this into output useful for analytics is a key component of the data engineer s job This job becomes more challenging as the source schema evolves There are numerous ways to transmit data from a source, including Programmatically Message brokers A PIs R PC Streams Output files We will dive into source systems in greater detail in Chapter 5  Ingestion After you understand the data source and the characteristics of the source system youre using, you need to gather the data The second stage of the data engineering lifecycle is data ingestion from source systems In our experience, source systems and ingestion represent the biggest bottlenecks of the data engineering lifecycle The source systems are normally outside of your direct control, and might randomly become unresponsive or provide data of poor quality  Or, your data ingestion service might mysteriously stop working for any number of reasons As a result, data flow stops or delivers bad data for storage, processing, and serving Unreliable source and ingestion systems have a ripple ef fect across the data engineering lifecycle Assuming youve answered the big questions listed above about source systems, and youre in good shape, youre now ready to ingest data Let s cover some key things to think about Key engineering considerations for the ingestion phase When preparing to architect or build a system, here are some primary questions to ask yourself related to the ingestion stage What s the use case for the data Im ingestingCan I re use this data, versus having to create multiple versions of the same dataset Where is the data going What s the destination How frequently will I need to access the data In what volume will the data typically arrive What format is the data in Can my downstream storage and transformation systems handle this format Is the source data in good shape for immediate downstream use If so, for how long, and what may cause it to be unusable If the data is from a streaming source, does the data need to be transformed before it reaches its destination If so, would an in flight transformation, where the data is transformed within the stream itself, be appropriate These are just a sample of the things youll need to think about with ingestion, and well cover those questions and more in Chapter 6  Before we leave, let s briefly turn our attention to two major data ingestion paradigmsbatch versus streaming, and push versus pull Batch versus streaming Virtually all data we deal with is inherently streaming That is, data is nearly always produced and updated continually at its source Batch ingestion is simply a specialized and convenient way of processing this stream in lar ge chunks, for example handling a full day s worth of data in a single batch Streaming ingestion allows us to provide data to downstream systems whether other applications, databases, or analytics systemsin a continuous, real time fashion Here, real time or near real time means that the data is available to a downstream system a short time after it is produced, for example, less than one second later  The latency required to qualify as real time varies by domain and requirements Batch data is ingested either on a predetermined time interval or as data reaches a preset size threshold Batch ingestion is a one way dooronce data is broken into batches, the latency for downstream consumers is inherently constrained Due to factors that limited the ways that data could be processed, batch wasand still isa very popular way to ingest data for downstream consumption, particularly in the areas of analytics and machine learning However , the separation of storage and compute in many systems, as well as the ubiquity of event streaming and processing platforms, make continuous processing of data streams much more accessible and increasingly popular  The choice lar gely depends on the use case and expectations for data timeliness Key considerations for batch versus stream ingestion Should you go streaming first Despite the attractiveness of a streaming first approach, there are many tradeof fs to understand and think about Below are some questions to ask yourself when determining if streaming ingestion is an appropriate choice over batch ingestion If I ingest the data in real time, can downstream storage systems handle the rate of data flow Do I need true, to the millisecond real time data ingestion Or would a micro batch approach work where I accumulate and ingest data, say every minute What are my use cases for streaming ingestion What specific benefits do I realize by implementing streaming If I get data in real time, what actions can I take on that data that would be an improvement upon batch Will my streaming first approach cost more in terms of time, money , maintenance, downtime, and opportunity cost than simply doing batch Is my streaming pipeline and system reliable and redundant in the event of infrastructure failureWhat tools are most appropriate for the use case Should I use a managed service Kinesis, Pubsub, Dataflow, or stand up my own instances of Kafka, Flink, Spark, Pulsar , etc If I do the latter , who will manage it What are the costs and tradeof fs If Im deploying a machine learning model, what benefits do I have with online predictions, and possibly continuous training Am I getting data from a live production instance If so, what s the impact of my ingestion process on this source system As you can see, streaming first might seem like a good idea, but it s not always straightforward there are inherently extra costs and complexities Many great ingestion frameworks do handle both batch and micro-batch ingestion styles That said, we think batch is a perfectly fine approach for many data ingestion very common use cases such as model training and weekly reporting, which are inherently batch oriented Adopt true real time streaming only after identifying a business use case that justifies the extra complexity  Push versus pull In the push model of data ingestion, a source system writes data out to a target, whether that be a database, object storage, or a file system In the pull model, data is retrieved from the source system In practice, the line between the push and pull paradigms can be quite blurry often data is both pushed and pulled as it works its way through the various stages of a data pipeline Consider , for example, the E TL extract, transform, load process, commonly used in batch oriented ingestion workflows The extract part of E TL makes it clear that were dealing with a pull ingestion model In traditional E TL, the ingestion system queries a current table snapshot on a fixed schedule In another example, consider continuous change data capture C DC, which is achieved in a few dif ferent ways One common method triggers amessage every time a row is changed in the source database This message is pushed to a queue where it is picked up by the ingestion system Another common C DC method uses binary logs, which record every commit to the database The database pushes to its logs The ingestion system reads the logs but doesn t directly interact with the database otherwise This adds little to no additional load to the source database Some versions of batch C DC use the pull pattern For example, in timestamp based C DC an ingestion system queries the source database and pulls the rows that have changed since the previous update With streaming ingestion, data bypasses a backend database and is pushed directly to an endpoint, typically with some kind of data buf fering such as a queue This pattern is useful with fleets of Io T sensors emitting sensor data Rather than relying on a database to maintain the current state, we simply think of each recorded reading as an event This pattern is also growing in popularity in software applications as it simplifies real time processing, allows app developers to tailor their messages for the needs of downstream analytics, and greatly simplifies the lives of data engineers Well discuss ingestion best practices and techniques in depth in Chapter 6  Storage After ingesting data, you need a place to store it Choosing a storage solution is key to success in the rest of the data lifecycle, and it s also one of the most complicated stages of the data lifecycle for a variety of reasons First, modern data architectures in the cloud often leverage several storage solutions Second, few data storage solutions function purely as storage, with many also supporting complex transformation queries even object storage solutions may support powerful query capabilities e.g A W S S3 Select Third, while storage is a stage of the data engineering lifecycle, it frequently touches on other stages, such as ingestion, transformation, and serving Storage frequently occurs in multiple places in a data pipeline, with storage systems crossing over with processing, serving, and ingestion stages Forexample, cloud data warehouses can store data, process data in pipelines, and serve it to analysts streaming frameworks such as Kafka and Pulsar can function simultaneously as ingestion, storage, and query systems for messages and object storage is a standard layer for the transmission of data In general with storage, data moves and goes somewhere, gets stored temporarily or permanently , then goes somewhere else, gets stored again, and so on Evaluating storage systems key engineering considerations Here are a few key engineering questions to ask when choosing a storage system for a data warehouse, data lakehouse, database, object storage, etc Is this storage solution compatible with the required write speeds for the architecture Will storage create a bottleneck for downstream processes Do you understand how this storage technology works Are you utilizing the storage system optimally , or committing unnatural acts For instance, are you applying a high rate of random access updates in an object storage system, an antipattern with significant performance overhead Will this storage system handle anticipated future scale Y ou should take into account all capacity limits on the storage system total available storage, read operation rate, write volume, etc Will downstream users and processes be able to retrieve data in the required S LA Are you capturing metadata about schema evolution, data flows, data lineage, and so forth Metadata has a significant impact on the utility of data Metadata represents an investment in the future, dramatically enhancing discoverability and institutional knowledge to streamline future projects and architecture changes Is this a pure storage solution object storage or does it support complex query patterns i.e., a cloud data warehouse Is the storage system schema agnostic object storage Flexible schema Cassandra Enforced schema A cloud data warehouse For data governance, how are you tracking master data, golden records data quality , and data lineage W ell have more to say on these in the data management subsection of this chapter  How are you handling compliance and data sovereignty For example, can you store your data in certain geographical locations, but not others Data access frequency Not all data is accessed in the same way  Retrieval patterns will greatly vary, based upon the type of data being stored and queried This brings up the notion of the temperatures of data Data access frequency will determine what temperature your data is Data that is most frequently accessed is called hot data  Hot data is commonly retrieved many times per day, perhaps even several times per second in systems that serve user requests This is data that should be stored for fast retrieval, where fast is relative to the use case Lukewarm data is data that might be accessed every so often, say every week or month Cold data is seldom queried and is appropriate for storing in an archival system Cold data is often data that is retained for compliance purposes, or in case of a catastrophic failure in another system In the old days, cold data would be stored on tapes and shipped to remote archival facilities In cloud environments, vendors of fer specialized storage tiers with extremely low monthly storage costs, but high prices for data retrieval Selecting a storage system What type of storage solution should you use This depends on your use cases, data volumes, frequency of ingestion, format, and size of the data being ingested essentially , the key considerations listed above There isnot a one size fits all universal storage recommendation Every storage technology has its tradeof fs Let s quickly list some of the common storage systems that a data engineer will encounter  There are countless varieties of storage technologies, and it s easy to be overwhelmed when deciding the best option for your data architecture Here are several popular storage options Relational database management systems R DB MS Data lake Data warehouse Data lakehouse Streaming systems with data retention capabilities Graph database In memory Redis, Memcached, etc High performance No SQ L databases Rocks DB, etc Feature stores Data catalog Metadata stores Spreadsheets Chapter 7 will cover storage best practices and approaches in greater detail, as well as the crossover between storage and other lifecycle stages Transformation The next stage of the data engineering lifecycle is transformation  As we mentioned, data can be ingested in its raw form, with no transformations performed In most cases, however , data needs to be changed from its original form into something useful for downstream use cases W ithoutproper transformations, data will not be in an appropriate form for reports, analysis, or machine learning T ypically , the transformation stage is where data begins to create value for downstream user consumption There are a whole host of data transformation types, which we will cover extensively in chapter 8  Immediately after ingestion, basic transformations map data into correct types changing ingested string data into numeric and date types for example, put records into standard formats and remove bad records Later stages of transformation may transform the data schema and apply normalization Downstream, we can apply lar ge scale aggregation for reporting or featurize data for machine learning processes Key considerations for the transformation phase When thinking about data transformations within the context of the data engineering lifecycle, it helps to consider the following W ell cover transformations in depth in Chapter 8  What s the cost and R OI of the transformation All transformations should have an associated business value attached Is the transformation expensive from a time and resource perspective What value will the transformation bring downstream In other words, what is the R OI of a transformation Is the transformation as simple and self isolated as possible What business rules do the transformations support During transformation, am I minimizing data movement between the transformation and the storage system Data can be transformed in batch, or while streaming in flight As mentioned in the section on ingestion, virtually all data starts life as a continuous stream batch is just a specialized way of processing a data stream Batch transformations are overwhelmingly popular , but given the growing popularity of stream processing solutions such as Flink, Spark,Beam, etc., as well as the general increase in the amount of streaming data, we expect the popularity of streaming transformations to continue growing, perhaps entirely replacing batch processing in certain domains soon Logically , we treat transformation as a standalone area of the data engineering lifecycle, but the realities of the lifecycle can be much more complicated in practice T ransformation is often entangled in other phases of the data engineering lifecycle T ypically , data is transformed in source systems or during ingestion For example, a source system may add an event timestamp to a record before forwarding it to an ingestion process Or a record within a streaming pipeline may be enriched with additional fields and calculations before it s sent to a data warehouse T ransformations are ubiquitous in various parts of the lifecycle Data preparation, data wrangling, and cleaningall of these transformative tasks add value to end consumers of data Business logic is a major driver of data transformation Business logic is critical for obtaining a clear and current picture of business processes A simple view of raw retail transactions might not be useful in itself without adding the logic of accounting rules so that the C FO has a clear picture of financial health In general, ensure a standard approach for implementing business logic across your transformations Data featurization for machine learning is another data transformation process Featurization intends to extract and enhance features of data that will be useful for training machine learning models Featurization is something of a dark art, combining domain expertise to identify which features might be important for prediction, with extensive experience in data science For this book, the main point to take away is that once data scientists determine how to featurize data, featurization processes can be automated by data engineers in the transformation stage of a data pipeline Transformation is a very deep subject, and we cannot do it justice in this very brief introduction Chapter 8 will delve into the various practices and nuances of data transformations Serving Data for Analytics, Machine Learning, and Reverse E TL Youve reached the last stage of the data engineering lifecycle Now that the data has been ingested, stored, and transformed into coherent and useful structures, it s time to get value from your data Getting value from data means dif ferent things to dif ferent users Data has value when it s used for practical purposes Data that is not consumed or queried is simply inert Data vanity projects are a major risk for companies Many companies pursued vanity projects in the big data era, gathering massive datasets in data lakes that were never consumed in any useful way  The cloud era is triggering a new wave of vanity projects, built on the latest data warehouses, object storage systems, and streaming technologies Data projects must be intentional across the lifecycle What is the ultimate business purpose of the data so carefully collected, cleaned, and stored Data serving is perhaps the most exciting part of the data engineering lifecycle This is where the magic happens This is where machine learning engineers can apply the most advanced modern techniques For now , lets take a look at some of the popular uses of dataanalytics, machine learning, and reverse E TL Analytics Analytics is the core of most data endeavors Once your data is stored and transformed, youre ready to generate reports, dashboards, and do ad hoc analysis on the data Whereas the bulk of analytics used to encompass business intelligence B I, it now includes other facets such as operational analytics and customer facinganalytics Figure 2 5  Let s briefly touch on these variations of analyticsFigur e 2 5  Types of Analytics Business intelligence Business intelligence B I marshalls collected data to describe the past and current state of a business B I requires the processing of raw data using business logic Note that data serving for analytics is yet another area where the stages of the data engineering lifecycle can get tangled As we mentioned earlier , business logic is often applied to data in the transformation stage of the data engineering lifecycle, but a logic on- read approach has become increasingly popular  That is, data is stored in a cleansed but fairly raw form, with minimal business logic post- processing A B I system maintains a repository of business logic and definitions This business logic is used to query the data warehouse so that reports and dashboards accord with business definitions 55 As a company grows its data maturity , it will move from ad hoc data analysis to self service analytics, which allows democratized data access to business users, without the need for I T to intervene The capability to do self service analytics assumes that data is in a good enough place that people across the or ganization can simply access the data themselves, slice and dice it however they choose and get immediate insights from it W e find that though self service analytics is simple in theory , its incredibly dif ficult to pull of f in practice The main reason is that poor data quality and or ganizational silos get in the way ofallowing widespread use of analytics, free of gatekeepers such as I T or reporting departments Operational analytics Operational analytics focuses on the fine grained details of operations, promoting actions that a user of the reports can act upon immediately  For example, operational analytics could be a live view of inventory , or real time dashboarding of website health In this case, data is consumed in a real time manner , either directly from a source system, from a streaming data pipeline such as A W S Kinesis or Kafka, or aggregated in a real time O LA P solution like Druid The types of insights in operational analytics dif fer from traditional B I since operational analytics is focused on the present and doesn t necessarily concern historical trends Customer -facinganalytics You may wonder why weve broken out customer facinganalytics separately from B I In practice, analytics provided to customers on a S AA S Software as a Service platform come with a whole separate set of requirements and complications Internal B I faces a limited audience and generally presents a handful of unified views Access controls are critical, but not particularly complicated Access is managed using a handful of roles and access tiers With customer facinganalytics, the request rate for reports, and the corresponding burden on analytics systems, go up dramatically access control is significantly more complicated and critical Businesses may be serving separate analytics and data to thousands or more customers Each customer must see their data and only their data Where an internal data access error at a company would likely lead to a procedural review , a data leak between customers would be considered a massive breach of trust, likely leading to media attention and a significant loss of customers Minimize your blast radius related to data leaks and securityvulnerabilities Apply tenant or data level security within your storage, and anywhere else there s a possibility of data leakage M UL TI TE NA NC Y Many modern storage and analytics systems support multi-tenancy in a variety of ways Engineers may choose to house data for many customers in common tables to allow a unified view for internal analytics and machine learning This data is presented externally to individual customers through the use of logical views with appropriately defined controls and filters It is incumbent on engineers to understand the minutiae of multitenancy in the systems they deploy to ensure absolute data security and isolation Machine learning Machine learning is one of the most exciting technology revolutions of our time Once or ganizations reach a high level of data maturity , they can begin to identify problems that are amenable to machine learning and start organizing a machine learning practice The responsibilities of data engineers overlap significantly in analytics and machine learning, and the boundaries between data engineering, machine learning engineering, and analytics engineering can be fuzzy  For example, a data engineer may need to support Apache Spark clusters that facilitate both analytics pipelines and machine learning model training They may also need to provide a Prefect or Dagster system that orchestrates tasks across teams, and support metadata and cataloging systems that track data history and lineage Setting these domains of responsibility , and the relevant reporting structures is a critical or ganizational decision One recently developed tool that combines data engineering and M L engineering is the featur e stor e e.g F EA ST and T ecton, among others Feature stores are designed to reduce the operational burden for machine learning engineers, by maintaining feature history and versions, supportingfeature sharing between teams, and providing basic operational and orchestration capabilities, such as backfilling In practice, data engineers are part of the core support team for feature stores to support M L engineering Should a data engineer be familiar with machine learning It certainly helps Regardless of the operational boundary between data engineering, M L engineering, and business analytics, etc., data engineers should maintain operational knowledge about the teams they work with A good data engineer is conversant in the fundamental machine learning techniques and related data processing requirements deep learning, featurization, etc., in the use cases for models within their company , and the responsibilities of the or ganization s various analytics teams This helps to maintain ef ficient communication and facilitate collaboration Ideally , data engineers will build tools in collaboration with other teams that neither team is capable of building on its own This book cannot possibly cover machine learning in depth There s a growing ecosystem of books, videos, articles, and communities if youre interested in learning more we include a few additional references in the further r eading section at the end of this chapter  Below are some considerations for the Serving Data phase, specific to machine learning Is the data of suf ficient quality to perform reliable feature engineering Quality requirements and assessments are developed in close collaboration with teams consuming the data Is the data discoverable Can data scientists and machine learning engineers easily find useful data Where are the technical and or ganizational boundaries between data engineering and machine learning engineering This organizational question has major architectural implications The dataset properly represents ground truth and isn t unfairly biased While machine learning is exciting, our experience is that companies often prematurely dive into it Before investing a ton of resources into machine learning, take the time to build a solid data foundation This means setting up the best systems and architecture across the data engineering lifecycle, as well as the machine learning lifecycle More often than not, get good with analytics before moving to machine learning Many companies have seen their machine learning dreams dashed because they undertook initiatives without appropriate foundations in place On the other hand, data engineers should keep the serving stage of the data engineering lifecycle in mind at all times as theyre building out the other stages, both as motivation and to steer and shape their architectures and initiatives Reverse E TL Reverse E TL has long been a practical reality in data, viewed as an antipattern that we didn t like to talk about or dignify with a name Reverse E TL is the process of taking processed data from the output side of the data engineering lifecycle and feeding it back into source systems as shown in Figure 2 6  In reality , this flow is extremely useful, and often necessary reverse E TL allows us to take analytics, scored models, etc., and feed these back into production systems or S AA S platforms Marketing analysts might calculate bids in Excel using the data in their data warehouse, then upload these bids to Google Ads This process was often quite manual and primitive Figur e 2 6  Reverse E TL As weve written this book, several vendors have embraced the concept of reverse E TL and built products around it, such as Hightouch and Census Reverse E TL remains nascent as a field, but we suspect that it is here to stay Reverse E TL has become especially important as businesses rely increasingly on S AA S and external platforms For example, businesses may want to push certain metrics from their data warehouse to a customer data platform or customer relationship management C RM system Advertising platforms are another common use case, as in the Google Ads example Expect to see more activity in the Reverse E TL, with an overlap in both data engineering and M L engineering The jury is out whether the term reverse E TL will stick And the practice may evolve Some engineers claim that we can eliminate reverse E TL by handling data transformations in an event stream, and sending those events back to source systems as needed Realizing widespread adoption of this pattern across businesses is another matter  The gist is that transformed data will need to be returned to source systems in some manner , ideally with the correct lineage and business process associated with the source system The Major Undercurrents Across the Data Engineering Lifecycle Data engineering is rapidly maturing Whereas prior cycles of data engineering simply focused on the technology layer , the continued abstraction and simplification of tools and practices have shifted this focus Data engineering now encompasses far more than tools and technology  The field is now moving up the value chain, incorporating traditional enterprise practices such as data management and cost optimization, and newer practices like Data Ops W eve termed these practices under currents data management, Data Ops, data architecture, orchestration, and software engineeringthat support every aspect of the data engineering lifecycle Figure 2 7  We will give a brief overview of these undercurrents and their major components, which youll see in more detail throughout the book Figur e 2 7  The major under currents of data engineering Data Management Data management Y oure probably thinking that sounds very corporate Old school data management practices are making their way into data and machine learning engineering What s old is new again Data management has been around for decades but didn t get a lot of traction in data engineering until recently  We sense the motivation for adopting data management is because data tools are becoming simpler to use, and a data engineer needs to manage less complexity  As a result, the data engineer is moving up the value chain toward the next rung of best practices Data best practices once reserved for very lar ge companiesdata governance, master data management, data quality management, metadata management, etc are now filtering down into companies of all sizes and maturity levels As we like to say , data engineering is becoming enterprisey This is a great thingAccording to the D AM A D MB OK, which we consider to be the definitive book for enterprise data management, Data management is the development, execution, and supervision of plans, policies, programs, and practices that deliver , control, protect, and enhance the value of data and information assets throughout their lifecycle That s a bit verbose, so let s look at how it ties to data engineering, specifically  Data engineers manage the data lifecycle, and data management encompasses the set of best practices that data engineers will use to accomplish this task, both technically and strategically  Without a framework for managing data, data engineers are simply technicians operating in a vacuum Data engineers need the broader perspective of data s utility across the or ganization, from the source systems to the C Suite, and everywhere in between Why is data management important Data management demonstrates that data is a vital asset to daily operations, just as businesses view financial resources, finished goods, or real estate as assets Data management practices form a cohesive framework that everyone can adopt to make sure that the or ganization is getting value from data and handling it appropriately  There are quite a few facets to data management, including the following Data governance, including discoverability , security , and accountability Data modeling and design Data lineage Storage and operations Data integration and interoperability Data lifecycle management Data systems for advanced analytics and M L Ethics and privacy1 While this book is in no way an exhaustive resource on data management, lets briefly cover some salient points from each area, as they relate to data engineering Data governance According to Data Governance The Definitive Guide, Data governance is, first and foremost, a data management function to ensure the quality , integrity , security , and usability of the data collected by an or ganization We can expand on that definition and say that data governance engages people, processes, and technologies to maximize data value across an organization, while protecting data with appropriate security controls Effective data governance is developed with intention and supported by the organization When data governance is accidental and haphazard, the side effects can range from untrusted data to security breaches, and everything in between Being intentional about data governance will maximize the organization s data capabilities and value generated from data It will also hopefully keep a company out of headlines for questionable or downright reckless data practices Think of the typical example where data governance is done poorly  A business analyst gets a request for a report, but doesn t know what data to use to answer the question He or she may spend hours digging through dozens of tables in a transactional database, taking wild guesses at what fields might be useful The analyst compiles a report that is directionally correct, but isn t entirely sure that the report s underlying data is accurate or sound The recipient of the report also questions the validity of the data In the end, the integrity of the analystand all data in the company s systemsis called into question The company is confused about its performance, making business planning next to impossible Data governance is a foundation for data driven business practices and a mission critical part of the data engineering lifecycle When data governance is practiced well, people, processes, and technologies are all aligned to treat data as a key driver of the business And, if data issues occur , they are promptly handled and things carry on 2 The core categories of data governance are discoverability , security , and accountability  Within these core categories are subcategories, such as data quality , metadata, privacy , and much more Let s look at each core category in turn Discoverability In a data driven company , data must be available and discoverable End users should have quick and reliable access to the data they need to do their jobs They should know where the data comes from Golden source, how it relates to other data, and what the data means There are some key components to data discoverability , including metadata management and master data management Let s briefly describe these components Metadata Metadata is data about data, and it underpins every section of the data engineering lifecycle Metadata is exactly the data needed to make data discoverable and governable We divide metadata into two major categories auto generated and human generated Modern data engineering revolves around automation, but too often, metadata collection is still a manual, error prone process Technology can assist with this process, removing much of the error prone work of manual metadata collection W ere seeing a proliferation of data catalogs, data lineage tracking systems and metadata management tools Tools can crawl databases to look for relationships and monitor data pipelines to track where data comes from and where it goes A low fidelity manual approach is to use an internally led ef fort where metadata collection is crowdsourced by various stakeholders within the or ganization These data management tools will be covered in depth throughout the book, as they undercut much of the data engineering lifecycle Metadata becomes a byproduct of data and data processes However , key challenges remain In particular , interoperability and standards are still3lacking Metadata tools are only as good as their connectors to data systems, and their ability to share metadata with each other  In addition, automated metadata tools should not entirely take humans out of the loop Data has a social elementeach or ganization accumulates social capital and knowledge around processes, datasets, and pipelines Human oriented metadata systems focus on the social aspect of metadata This is something that Airbnb has emphasized in their various blog posts on data tools, particularly their original Data Portal concept Such tools should provide a place to disclose data owners, data consumers, domain experts, etc Documentation and internal wiki tools provide a key foundation for metadata management, but these tools should also integrate with automated data cataloging as mentioned above For example, data scanning tools can generate wiki pages with links to relevant data objects Once metadata systems and processes exist, data engineers can consume metadata in all kinds of useful ways Metadata becomes a foundation for designing pipelines and managing data throughout the lifecycle D MB OK identifies several main categories of metadata that are useful to data engineers Business metadata Technical metadata Operational metadata Reference metadata Lets briefly describe each category of metadata Business metadata Business metadata relates to how data is used in the business, including business and data definitions, data rules and logic, how and where data is used, the data owners, and so forth A data engineer uses business metadata to answer non technical questions about who, what, where, and how For example, a data engineer4may be tasked with creating a data pipeline for customer sales analysis But, what is a customer Is it someone who s purchased in the last 90 days Or someone who s purchased at any time the business has been open T o use the correct data, a data engineer would refer to business metadata data dictionary or data catalog to look up how a customer is defined Business metadata provides a data engineer with the right context and definitions to properly use data Technical metadata Technical metadata describes the data created and used by systems across the data engineering lifecycle It includes the data model and schema, data lineage, field mappings, pipeline workflows, and much more A data engineer uses technical metadata to create, connect, and monitor various systems across the data engineering lifecycle Here are some common types of technical metadata that a data engineer will use Pipeline metadata often produced in orchestration systems Data lineage Schema Orchestration is a central hub that coordinates workflow across various systems Pipeline metadata captured in orchestration systems provides the details of the workflow schedule, system and data dependencies, configurations, connection details, and much more Data lineage metadata tracks the origin and changes to data, and its dependencies, over time As data flows through the data engineering lifecycle, it evolves through transformations and combinations with other data Data lineage provides an audit trail of data s evolution as it moves through various systems and workflows Schema metadata describes the structure of data that is stored in a system such as a database, a data warehouse, a data lake, or a file system it is one of the key dif ferentiators across dif ferent types of storage systems Objectstores, for example, don t manage schema metadatainstead, this must be managed in a system like the Hive Metastore On the other hand, cloud data warehouses manage schema metadata for engineers and users These are just a few examples of technical metadata that a data engineer should know about This is not a complete list, and well cover additional aspects of technical metadata throughout the book Operational metadata Operational metadata describes the operational results of various systems and includes statistics about processes, job ids, application runtime logs, data used in a process, error logs, etc A data engineer uses operational metadata to determine whether a process succeeded or failed and the data involved in the process Orchestration systems can provide a limited picture of operational metadata, but the latter still tends to be scattered across many systems A need for better quality operational metadata, and better metadata management, is a major motivation for next generation orchestration and metadata management systems Reference metadata Reference metadata is data used to classify other data This is also referred to as lookup data Standard examples of reference data are internal codes, geographic codes, units of measurement, internal calendar standards Note that much of reference data is fully managed internally , but items such as geographic codes might come from external standard references Reference data is essentially a standard for interpreting other data, so if it changes at all, this change happens slowly over time Security People and or ganizational structure are always the biggest security vulnerabilities in any company  When we hear about major security breaches in the media, it quite often turns out that someone in the company ignored basic precautions, fell victim to a phishing attack, or otherwiseacted in an irresponsible manner  As such, the first line of defense for data security is to create a culture of security that pervades the or ganization All individuals who have access to data must understand their responsibility in protecting the sensitive data of the company and its customers Security must be top of mind for data engineers, and those who ignore it do so at their peril Data engineers must understand both data and access security , at all times exercising the principle of least privilege The principle of least privilege means giving a user or system access only to the data and resources that are essential to perform an intended function A common anti-pattern we see with data engineers with little security experience is to give admin access to all users Please avoid this Give users only the access they need to do their jobs today , nothing more Don t operate from a root shell when youre just looking for files that are visible with standard user access In a database, don t use the superuser role when youre just querying tables visible with a lesser role Imposing the principle of least privilege on ourselves can prevent a lot of accidental damage, and also keeps you into a security first mindset Data security is also about timingproviding data access to only the people and systems that need to access it, and only for the duration necessary to perform their work  Data should be protected at all times from unwanted visibilityboth in flight and at restusing techniques such as encryption, tokenization, data masking, obfuscation or simple robust access controls Data engineers must be competent security administrators, as security falls in their domain A data engineer should understand security best practices, both for cloud and on prem Knowledge of user and identity access management I AMroles, policies, groups, network security , password policies, and encryption are good places to start Throughout the book, well highlight areas where security should be top of mind in the data engineering lifecycle Data accountability5 Data accountability means assigning an individual to be responsible for governing some portion of data The responsible person then coordinates the governance activities of other stakeholders Fundamentally , it is extremely dif ficult to manage data quality if no one is accountable for the data in question Note that people accountable for data need not be data engineers In fact, the accountable person might be a software engineer , product manager , or serve in another role In addition, the responsible person generally doesn t have all the resources necessary to maintain data quality  Instead, they coordinate with all people who touch the data, including data engineers Data accountability can happen at a variety of levels accountability can happen at the level of a table or a log stream but could be as fine grained as a single field entity that occurs across many tables An individual may be accountable for managing a customer I D across many dif ferent systems For the purposes of enterprise data management, a data domain is the set of all possible values that can occur for a given field type, such as in this I D example This may seem excessively bureaucratic and fastidious, but in fact, can have significant implications for data quality  Data quality Can I trust this data Everyone in the business Data quality is the optimization of data toward the desired state, and orbits the question What do you get compared with what you expect Data should conform to the expectations in the business metadata Does the data match the definition agreed upon by the business A data engineer is responsible for ensuring data quality across the entire data engineering lifecycle This involves performing such tasks as data quality tests, ensuring data conformance to schema expectations, data completeness and precision, and much more According to Data Governance The Definitive Guide, data quality is defined by three main characteristicsaccuracy , completeness, timelinessAccuracy Is the collected data factually correct Are there duplicate values Are the numeric values accurate Completeness Are the records complete Do all required fields contain valid values Timeliness Are records available in a timely fashion Each of these characteristics is quite nuanced For example, when dealing with web event data, how do we think about bots and web scrapers If we intend to analyze the customer journey , then we must have a process that lets us separate human from machine generated traf fic Any bot generated events misclassified as human present data accuracy issues, and vice versa A variety of interesting problems arise concerning completeness and timeliness In the Google paper introducing the dataflow model see further reading, the authors give the example of an of fline video platform that displays ads The platform downloads video and ads while a connection is present, allows the user to watch these while of fline, then uploads ad view data once a connection is present again This data may arrive late, well after the ads are watched How does the platform handle billing for the ads Fundamentally , this is a problem that can t be solved by purely technical means Rather , engineers will need to determine their standards for late- arriving data and enforce these uniformly , possibly with the help of various technology tools In general, data quality sits across the boundary of human and technology problems Data engineers need robust processes to collect actionable human feedback on data quality , while also using technology tools to preemptively detect quality issues before downstream users ever see them W ell cover these collection processes in the appropriate chapters throughout this book M AS TE R D AT A M AN AG EM EN T M DM Master data is data about business entities such as employees, customers, products, locations, etc As or ganizations grow lar ger and more complex through or ganic growth and acquisitions, and as they collaborate with other businesses, maintaining a consistent picture of entities and identities becomes more and more challenging M DM isn t always strictly under the purview of data engineers, but they must always be aware of it if they don t own M DM, they will still collaborate on M DM initiatives Master data management M DM is the practice of building consistent entity definitions known as golden r ecords Golden records harmonize entity data across an or ganization and with its partners Master data management is a business operations process that is facilitated by building and deploying technology tools For example, a master data management team might determine a standard format for addresses, then work with data engineers to build an A PI to return consistent addresses and a system that uses address data to match customer records across divisions of the company  Master data management reaches across the full data cycle into operational databases As such, it may fall directly under the purview of data engineering, but is often the assigned responsibility of a dedicated team that works across the or ganization Data modeling and design To derive business insights from data, through business analytics and data science, the data must be in a usable form The process for converting data into a usable form is known as data modeling and design Where we traditionally think of data modeling as a problem for D BAs and E TL developers, data modeling can happen almost anywhere in an or ganization Firmware engineers develop the data format of a record for an Io T device,or web application developers design the J SO N response to an A PI call or a My SQ L table schema these are all instances of data modeling and design Data modeling has become more challenging due to the variety of new data sources and use cases For instance, strict normalization doesn t work well with event data Fortunately , a new generation of data tools increases the flexibility of data models, while retaining logical separations of measures, dimensions, attributes, and hierarchies Cloud data warehouses support the ingestion of enormous quantities of denormalized and semistructured data, while still supporting common data modeling patterns, such as Kimball, Inmon, and Data V ault Data processing frameworks such as Spark can ingest a whole spectrum of data ranging from flat structured relational records to raw unstructured text W ell discuss these transformation patterns in greater detail in Chapter 8  With the vast variety of data that engineers must cope with, there is a temptation to throw up our hands and give up on data modeling This is a terrible idea with harrowing consequences, made evident when people murmur of the W OR N write once, read never access pattern or refer to a data swamp Data engineers need to understand modeling best practices, and also develop the flexibility to apply the appropriate level and type of modeling to the data source and use case Data lineage As data moves through its lifecycle, how do you know what system af fected the data, or what the data is composed of as it gets passed around and transformed Data lineage describes the recording of an audit trail of data through its lifecycle, tracking both the systems that process the data, as well as what data it depends upon Data lineage helps with error tracking, accountability , and debugging of both data and the systems that process it It has the obvious benefit of giving an audit trail for the data lifecycle but also helps with compliance For example, if a user would like their data deleted from your systems, having the data lineage of that user s data allows you to know where that data is stored and its dependencies Though data lineage has been around for a long time in lar ger companies with strict compliance standards for tracking its data, it s now being more widely adopted in smaller companies as data management becomes mainstream A data engineer should be aware of data lineage, as it s becoming a key piece both for debugging data workflows, as well as tracing data that may need to be removed wherever it resides Data integration and interoperability Data integration and interoperability is the process of integrating data across tools and processes As we move away from a single stack approach to analytics towards a heterogeneous cloud environment where a variety of tools process data on demand, integration, and interoperability occupy an ever widening swath of the data engineer s job Increasingly , integration happens through general purpose A PIs rather than custom database connections A pipeline might pull data from the Salesforce A PI, store it to Amazon S3, call the Snowflake A PI to load it into a table, call the A PI again to run a query , then export the results to S3 again where it can be consumed by Spark All of this activity can be managed with relatively simple Python code that talks to data systems rather than handling data directly  While the complexity of interacting with data systems has decreased, the number of systems and the complexity of pipelines has increased dramatically  Engineers starting from scratch quickly outgrow the capabilities of bespoke scripting and stumble into the need for orchestration Orchestration is one of our undercurrents, and we discuss it in detail below  Data lifecycle management The advent of data lakes encouraged or ganizations to ignore this data archival and destruction Why discard data when you can simply add more storage and archive it eternally T wo changes have encouraged engineers to pay more attention to what happens at the end of the data engineering lifecycle First, data is increasingly stored in the cloud This means pay as you go storage costs, instead of lar ge block capex expenditures for an on premises data lake When every byte shows up on a monthly A W S statement, C FOs see opportunities for savings Cloud environments make data archival a relatively straightforward process Major cloud vendors of fer archival specific object storage classes such as Amazon Glacier and Google Cloud Coldline Storage that allow long term data retention at an extremely low cost, assuming very infrequent access it should be noted that data retrieval isnt so cheap, but that s for another conversation These storage classes also support extra policy controls to prevent accidental or deliberate deletion of critical archives Second, privacy and data retention laws such as G DP R and C CP A now require data engineers to actively manage data destruction, with consideration to the right to be for gotten Data engineers must be aware of what consumer data they retain and must have procedures in place to destroy data in response to requests and compliance requirements Data destruction is straightforward in a cloud data warehouse SQ L semantics allow deletion of rows conforming to a where clause Data destruction was more challenging in data lakes, where write once, read- many was the default storage pattern T ools such as Hive Acid and Delta Lake now allow easy management of deletion transactions at scale New generations of metadata management, data lineage, and cataloging tools will also streamline the end of the data engineering lifecycle Ethics and privacy Ethical behavior is doing the right thing when no one else is watching Aldo Leopold The last several years of data breaches, misinformation, and mishandling of data make one thing cleardata impacts people Data used to live in the Wild W est, freely collected and traded like baseball cards Those days are long gone Whereas the ethical and privacy implications of data were once considered a nice to have, like security , theyre now central to the generaldata lifecycle Data engineers need to do the right thing when no one else is watching because someday , everyone will be watching W e hope that more organizations will encourage a culture of good data ethics and privacy  How do ethics and privacy impact the data engineering lifecycle Data engineers need to ensure that datasets mask personally identifiable information P II and other sensitive information bias can be identified and tracked in datasets as they are transformed Regulatory scrutinyand penalties for failing to complyis only growing for data Make sure your data assets are compliant with a growing number of data regulations, such as G DP R and C CP A At the time of this writing, the E U is working on a G DP R for A I W e don t expect any slowdown in the popularity of ethics, privacy , or resultant regulation, so please take this seriously  Because ethics and privacy are top of mind, we will have some tips to ensure youre baking ethics and privacy into the data engineering lifecycle Orchestration We think that or chestration matters because we view it as r eally the center of gravity of both the data platform as well as the data lifecycle, the softwar e development lifecycle as it comes to data Nick Schrock, founder of Elementl While Nick s definition of the data lifecycle is somewhat dif ferent from ours, we fundamentally agree with his idea Not only is orchestration a central Data Ops process, but orchestration systems are a critical part of the engineering and deployment flow for data jobs, just as tools like Jenkins are critical to the continuous delivery and deployment of software So, what is orchestration Orchestration is the process of coordinating many jobs to run as quickly and ef ficiently as possible on a scheduled cadence People often mistakenly refer to orchestration tools, like Apache Airflow , as schedulers  This isn t quite accurate, and there are key dif ferences A pure scheduler , such as cron, is aware only of time An orchestration engine, on the other hand, builds in metadata on job dependencies, generally in the form of a D AG directed acyclic graph The D AG can be run once, or bescheduled to run at a fixed interval of daily , weekly , every hour , every five minutes, etc As we discuss orchestration throughout this book, we assume that an orchestration system stays online with high availability  This allows the orchestration system to sense and monitor constantly without human intervention, and to run new jobs any time they are deployed An orchestration system monitors jobs that it manages and kicks of f new tasks as internal D AG dependencies are completed It can also monitor external systems and tools to watch for data to arrive and criteria to be met When certain conditions go out of bounds, the system also sets error conditions and sends alerts through email or other channels Commonly , you might set an expected completion time for overnight daily data pipelines, perhaps 10 am If jobs are not done by this time, alerts go out to data engineers and data consumers Orchestration systems typically also build in job history capabilities, visualization, and alerting Advanced orchestration engines can backfill new D AGs or individual tasks as they are added to a D AG They also support dependencies over a time range For example, a monthly reporting job might check that an E TL job has been completed for the full month before starting Orchestration has long been a key capability for data processing but was not often top of mind nor accessible to anyone except the lar gest companies Enterprises used a variety of tools to manage job flows, but these were expensive, out of reach of small startups, and generally not extensible Apache Oozie was extremely popular in the 2010s, but it was designed to work within a Hadoop cluster and was dif ficult to use in a more heterogeneous environment Facebook developed Data Swarm for internal use in the late 2000s this inspired popular tools such as Airflow , which Airbnb open sourced in 2014  Airflow was open source from its inception, a key strength to its strong adoption The fact that it was written in Python made it highly extensible to almost any use case imaginable While there are many other interestingopen source orchestration projects such as Luigi and Conductor , Airflow is arguably the mindshare leader for the time being Airflow arrived just as data processing was becoming more abstract and accessible, and engineers were increasingly interested in coordinating complex flows across multiple processors and storage systems, especially in cloud environments At the time of this writing, several nascent open source projects aim to mimic the best elements of Airflow s core design, while improving on it in key areas Some of the most interesting examples at the moment are Prefect and Dagster , which aim to improve the portability and testability of D AGs to allow engineers to more easily move from local development to production There s also Ar go, which is built around Kubernetes primitives, and Metaflow , an open source project out of Netflix that aims to improve data science orchestration Which framework ultimately wins the mindshare for orchestration is still to be determined We also want to mention one interesting non open source orchestration platform, Datacoral Datacoral has introduced the idea of auto orchestration through data definitions That is, one can define a table in Datacoral by using a S QL statement the orchestrator analyzes the source tables in the query to determine upstream dependencies, then triggers a table refresh as these dependencies are met W e believe that auto orchestration will become a standard feature of orchestration engines in the future We also point out that orchestration is strictly a batch concept The streaming alternative to orchestrated task D AGs is the streaming D AG Streaming D AGs remain challenging to build and maintain, but next generation streaming platforms such as Pulsar aim to dramatically reduce the engineering and operational burden W e will talk more about these developments in the chapter on transformation Data Ops Data Ops maps the best practices of Agile methodology , Dev Ops, and statistical process control S PC to data Whereas Dev Ops aims to improve the release and quality of software products, Data Ops does the same thingfor data products Data products dif fer from software products because of how data is used A software product provides specific functionality and technical features for end users By contrast, a data product is built around sound business logic and metrics, whose users make decisions or build models that perform automated actions A data engineer must understand both the technical aspects of building software products to provide good user experiences to downstream users, as well as the business logic, quality , and metrics that will create excellent data products not just for immediate downstream users, but for users across the business Like Dev Ops, Data Ops borrows much from lean manufacturing and supply chain, mixing people, processes, and technology to reduce time to value As Data Kitchen the experts in Data Ops describes it, Data Ops is a collection of technical practices, workflows, cultural norms, and architectural patterns that enable Rapid innovation and experimentation delivering new insights to customers with increasing velocity Extremely high data quality and very low error rates Collaboration across complex arrays of people, technology , and environments Clear measurement, monitoring, and transparency of results Lean practices such as lead time reduction, minimizing defects, and the resulting improvements to quality , and productivity are things we are glad to see gaining momentum both in software and data operations First and foremost, Data Ops are a cultural habit, and the data engineering team needs to adopt a cycle of communicating and collaborating with the business, breaking down silos, continuously learning from successes and mistakes, and rapid iteration Only when these cultural habits are set in place can the team get the best results from technology and tools The reverse is rarely true 6 Depending on a company s data maturity , a data engineer has some options to build Data Ops into the fabric of the overall data engineering lifecycle If the company has no pre existing data infrastructure or practices, Data Ops is very much a greenfield opportunity that can be baked in from day one W ith an existing project or infrastructure that lacks Data Ops, a data engineer can begin adding Data Ops into workflows W e suggest first starting with observability and monitoring to get a window into the performance of a system, then adding in automation and incident response In a data mature company , a data engineer may work alongside an existing Data Ops team to improve the data engineering lifecycle In all cases, a data engineer must be aware of the philosophy and technical aspects of Data Ops There are three core technical elements to Data Opsautomation, monitoring and observability , and incident response see Figure 2 8  Let s look at each of these pieces, and how they relate to the data engineering lifecycle Figur e 2 8  The thr ee pillars of Data Ops Automation Automation enables reliability and consistency in the Data Ops process and allows data engineers to quickly deploy new product features and functionality , as well as changes to existing parts of the workflow  Data Ops automation has a similar framework and workflow to Dev Ops, consisting of change management environment, code, and data version control, continuous integrationcontinuous deployment C IC D, and configuration as code X as code Like Dev Ops, Data Ops practices monitor and maintain the reliability of technology and systems data pipelines,orchestration, etc., with the added dimension of checking for data quality , datamodel drift, metadata integrity , and much more Lets briefly discuss the evolution of Data Ops automation within a hypothetical or ganization An or ganization with a low level of Data Ops maturity often attempts to schedule data processes using cron jobs This works well for a while As data pipelines become more complicated, several things are likely to happen If the cron jobs are hosted on a cloud instance, the instance may have an operational problem, causing the jobs to stop running unexpectedly  As the spacing between jobs becomes tighter , a job will eventually run long, causing a subsequent job to fail or produce out of- date data Engineers may not be aware of job failures until they hear from analysts that their reports are out of date As the or ganization s data maturity grows, data engineers will typically adopt an orchestration framework, perhaps Airflow or Dagster  Data engineers are aware that Airflow itself presents an operational burden, but this eventually outweighs the operational dif ficulties of cron jobs Engineers will gradually migrate their cron jobs to Airflow jobs Now , dependencies are checked before jobs run More jobs can be packed into a given time because each job can start as soon as upstream data is ready rather than at a fixed, predetermined time The data engineering team still has room for operational improvements For instance, eventually , a data scientist deploys a broken D AG, bringing down the Airflow webserver and leaving the data team operationally blind After enough such headaches, the data engineering team realizes that they need to stop allowing manual D AG deployments In their next phase of operational maturity , they adopt automated D AG deployment D AGs are tested before deployment, and monitoring processes ensure that the new D AGs start running properly  In addition, data engineers block the deployment of new Python dependencies until installation is validated After automation is adopted, the data team is much happier and experiences far fewer headaches One of the tenets of the Data Ops Manifesto is Embrace Change This does not mean change for the sake of change, but goal oriented change At each stage of our automation journey , there are opportunities for operational improvement Even at the high level of maturity that weve described here, there is further room for improvement Engineers might choose to embrace a next generation orchestration framework that builds in better metadata capabilities Or they might try to develop a framework that builds D AGs automatically based on data lineage specifications The main point is that engineers constantly seek to implement improvements in automation that will reduce their workload and increase the value that they deliver to the business Observability and monitoring As we tell our clients, data is a silent killer W eve seen countless examples of bad data lingering in reports for months or years Executives may make key decisions from this bad data, only to discover a substantial time later that the data was wrong The outcomes are usually bad, and sometimes catastrophic for the business Initiatives are undermined and destroyed, years of work wasted In some of the worst cases, companies may be led to financial or technical disaster by bad data Another horror story is when the systems that create the data for reports randomly stop working, resulting in reports being delayed by several days The data team doesn t know until theyre asked by stakeholders why reports are late or producing stale information Eventually , different stakeholders lose trust in the capabilities of the data system and start their quasi data team The result is a ton of dif ferent unstable systems, reports, and silos If youre not observing and monitoring your data, and the systems that produce the data, youre inevitably going to experience your own data horror story  Observability , monitoring, logging, alerting, tracing are all critical to getting ahead of any problems that will occur along the data engineering lifecycle W e recommend you incorporate statistical process control S PC to understand whether events being monitored are out of line, and which incidents are worth responding to Well cover many aspects of monitoring and observability throughout the data engineering lifecycle in later chapters Incident response A high functioning data team using Data Ops will be able to quickly ship new data products But mistakes will inevitably happen A system may have downtime, a new data model may break downstream reports, a machine learning model may become stale and provide bad predictions, and countless other things may happen that interrupt the data engineering lifecycle Incident response is about using the automation and observability capabilities mentioned above to rapidly identify root causes of an incident, and resolve the incident as reliably and quickly as possible Incident response isn t just about technology and tools, though they are immensely useful It s also about open and blameless communication, both on the data engineering team, and across the or ganization As W erner Vogels is famous for saying, Everything breaks all the time Data engineers must be prepared for disaster , and ready to respond as swiftly and efficiently as possible Even better , data engineers should proactively find issues before the business reports them T rust takes a long time to build and can be lost in minutes In the end, incident response is as much about retroactively responding to incidents as proactively addressing them before they happen Data Ops summary At this point, Data Ops is still a work in progress Even so, practitioners have done a good job of adapting the principles of Dev Ops to the data domain and mapping out an initial vision through the Data Ops Manifesto and other resources Data engineers would do well to implement Data Ops practices a high priority in all of their work The upfront ef fort will see a significant long term payof f through faster delivery of products, better reliability and accuracy of data, and greater overall value for the business The state of operations in data engineering is still quite immature compared with software engineering Many data engineering tools, especially legacymonoliths, are not automation first That said, there s a recent movement to adopt automation best practices across the data engineering lifecycle T ools like Airflow have paved the way for a new generation of automation and data management tools Because the landscape is changing so fast, well take a wait and see approach before we suggest specific tools The general practices we describe for Data Ops are aspirational, and we suggest companies try to adopt them to the fullest extent possible, given the tools and knowledge available today  Data Architecture A data architecture reflects the current and future state of data systems that support the long term data needs and strategy of an or ganization Because the data requirements of an or ganization will likely change very rapidly , and new tools and practices seem to arrive on a near dailybasis, data engineers must understand good data architecture W e will cover data architecture in depth in Chapter 3 , but we want to highlight how data architecture is an undercurrent of the data engineering lifecycle A data engineer should first understand the needs of the business and gather requirements for new use cases Next, a data engineer needs to translate those requirements to design new ways to capture and serve data, balanced for cost and operational simplicity  This means knowing the tradeof fs with design patterns, technologies, and tools in the areas of source systems, ingestion, storage, transformation, and serving data This doesn t imply that a data engineer is a data architect, as these are typically two separate roles If a data engineer works alongside a data architect, the data engineer should be able to deliver on the data architect s designs, as well as provide architectural feedback W ithout a dedicated data architect, however , a data engineer will realistically be entrusted to be the default data architect W e will deep dive into good data architecture in Chapter 3  Software Engineering Software engineering has always been a central skill for data engineers In the early days of contemporary data engineering 20002010 , data engineers worked on low level frameworks and wrote map reduce jobs in C, C, and Java At the peak of the big data era the mid 2010s, engineers started using frameworks that abstracted away these low level details This abstraction continues today  Data warehouses support powerful transformations using S QL semantics T ools like Spark have become more user friendly over time, transitioning away from R DD based coding towards easy to use dataframes Despite this abstraction, software engineering is still a central theme of modern data engineering W e want to briefly discuss a few common areas of software engineering that apply to the data engineering lifecycle Core data processing code Though it has become more abstract and easier to manage, core data processing code still needs to be written and it appears throughout the data engineering lifecycle Whether in ingestion, transformation, or data serving, data engineers need to be highly proficient and productive in frameworks and languages such as Spark, S QL, or Beam we reject the notion that S QL is not code Its also imperative that a data engineer understand proper code testing methodologies, such as unit, regression, integration, end to end, smoke testing, and others Especially with the growing number of tools and modularization, and the automation involved with Data Ops, data engineers need to ensure their systems work end to end Development of open source frameworks Many data engineers are heavily involved in the development of open source frameworks They adopt these frameworks to solve specific problems in the data engineering lifecycle, then continue to develop the framework code to improve the tools for their use cases and to make a contribution back to the community  In the big data era, we saw a Cambrian explosion of data processing frameworks inside of the Hadoop ecosystem These tools were primarily focused on the transformation and serving parts of the data engineering lifecycle Data engineering tool speciation has not ceased or even slowed down, but the emphasis has shifted up the ladder of abstraction, away from direct data processing This new generation of open source tools assists engineers in managing, enhancing, connecting, optimizing, and monitoring data For example, Apache Airflow dominated the orchestration space from 2015 on Now , a new batch of open source competitors Prefect, Dagster , Metaflow , etc has sprung up to fix perceived limitations of Airflow , providing better metadata handling, portability , dependency management, etc Before data engineers begin engineering new internal tools, they would do well to survey the landscape of projects Keep an eye on the total cost of ownership T CO and opportunity cost associated with implementing a tool There is a good chance that some open source project already exists to address the problem that theyre looking to solve, and they would do well to collaborate on an existing project rather than reinventing the wheel Streaming Streaming data processing is inherently more complicated than batch, and the tools and paradigms are ar guably less mature As streaming data becomes more pervasive in every stage of the data engineering lifecycle, data engineers face a host of interesting software engineering problems For instance, data processing tasks such as joins that we take for granted in the batch processing world often become more complicated in a real time environment, and more complex software engineering is required Engineers must also write code to apply a variety of windowing methods Windowing allows real time systems to calculate useful metrics such as trailing statistics there are a variety of techniques for breaking data into windows, all with subtly dif ferent nuances Engineers have many frameworks to choose from, including various function platformsOpen Faa S, A W S Lambda, Google Cloud Functions for handling individual events or dedicated stream processors Apache Spark, Beam, or Pulsar for analyzing streams for reporting and real time actions Infrastructure as code Infrastructur e as code Ia C applies software engineering practices to the configuration and management of infrastructure The infrastructure management burden of the big data era has decreased precipitously as companies have migrated to managed big data systems Databricks, E MR and cloud data warehouses When data engineers have to manage their infrastructure in a cloud environment, they increasingly do this through Ia C frameworks rather than manually spinning up instances and installing software Several general purpose T erraform, Ansible and cloud platform specific A W S Cloudformation, Google Cloud Deployment Manager frameworks allow automated deployment of infrastructure based on a set of specifications Many of these frameworks can manage services as well as infrastructure For example, we can use Cloud Formation to spin up Amazon Redshift, or Deployment Manager to start Google Cloud Composer managed Airflow There is also a notion of infrastructure as code with containers and Kubernetes, using tools like Helm These practices are a key part of Dev Ops, allowing version control and repeatability of deployments Naturally , these capabilities are extremely valuable throughout the data engineering lifecycle, especially as we adopt Data Ops practices Pipelines as code Pipelines as code are the core concept of modern orchestration systems, which touch every stage of the data engineering lifecycle Data engineers use code typically Python to declare data tasks and dependencies between them The orchestration engine interprets these instructions to run steps using available resources General purpose problem solving In practice, regardless of which high level tools they adopt, data engineers will run into corner cases throughout the data engineering lifecycle that requires them to solve problems outside the boundaries of their chosen tools, and to write custom code Even when using frameworks like Fivetran, Airbyte, or Singer , data engineers will encounter data sources without existing connectors and need to write something custom They should be proficient enough in software engineering to understand A PIs, pull and transform data, handle exceptions, etc Conclusion Most discussions weve seen in the past about data engineering involve technologies but miss the bigger picture of the data lifecycle management As technologies become more abstract and do more heavy lifting, a data engineer has the opportunity to think and act on a higher level The data engineering lifecyclesupported by its undercurrentsis an extremely useful mental model for or ganizing the work of modern data engineering We break the data engineering lifecycle into the following stages Generation source systems Ingestion Storage Transformation Serving Several themes cut across the data engineering lifecycle, as well These are the under currents of the data engineering lifecycle At a high level, the undercurrents are Data management Data Ops Data architecture Software engineering Orchestration A data engineer has several top level goals across the data lifecycle produce optimum R OI and reduce costs financial and opportunity, reduce risk security , data quality and maximize data value and utility  In the next chapter , well discuss how these elements impact the design of good architecture Subsequently , weve devoted a chapter to each of the stages of the data engineering lifecycle 1 Evren Eryurek, Uri Gilad, V alliappa Lakshmanan, Anita Kibunguchy Grant, and Jessi Ashdown, Data Governance The Definitive Guide O Reilly, .comlibraryviewdata governance the9781492063483 2 Eryurek et al 3 Eryurek et al 4 5 gwikiPrinciple_of_least_privilege 6 Choosing Technologies Across the Data Engineering Lifecycle A N OT E F OR E AR LY R EL EA SE R EA DE RS With Early Release ebooks, you get books in their earliest formthe authors raw and unedited content as they writeso you can take advantage of these technologies long before the of ficial release of these titles This will be the fourth chapter of the final book If you have comments about how we might improve the content andor examples in this book, or if you notice missing material within this chapter , please reach out to the authors at book_feedbackternarydata.com  Modern data engineering suf fers from an embarrassment of riches Data technologies to solve nearly any problem are available as turnkey of ferings consumable in almost every wayopen source, managed open source, proprietary software, proprietary service, etc However , its easy to get caught up in chasing bleeding edge technology while losing sight of the core purpose of data engineeringdesigning robust and reliable systems to carry data through the full life cycle and serve it according to the needs of end users Just as structural engineers carefully choose technologies and materials to realize an architect s vision for a building, data engineers are tasked with making appropriate technology choices to shepherd data through the lifecycle to best serve data applications and users In the last chapter , we discussed good data architecture, and why it matters W e now explain how to choose the right technologies to serve this architecture Data engineers must choose good technologies to make the best possible data product W e feel the criteria to choose a good data technology is simple does it add value to a data pr oduct and the br oader business A lot of people confuse architecture and tools Architectur e is strategic tools ar e tactical We sometimes hear , our data architecture are tools X, Y , and Z This is the wrong way to think about architecture Architecture is the top level design, roadmap, and blueprint of data systems that satisfy the strategic aims for the business Architecture is the What, Why and When T ools are used to make the architecture a reality tools are the How We often see teams going of f the rails and choosing technologies before mapping out an architecture The reasons for this are variedshiny object syndrome, resume driven development, a lack of expertise in architecture but in practice, this prioritization of technology often means that they don t choose architecture, but instead cobble together a kind of Rube Goldber g machine of technologies This is exactly backward W e strongly advise against choosing technology before getting your architecture right Architectur e first T echnology second This chapter discusses our tactical plan for making technology choices once we have an architecture blueprint in hand Below are some considerations for choosing data technologies across the data engineering lifecycle Cost optimization and adding value to the business Friction to deliver Team ability and maturity Speed to market Today versus the future immutable versus transitory technologies Location cloud, on premises, hybrid cloud, multi-cloud Build versus buy Monolith versus modular Serverless versus servers The undercurrents of the data engineering lifecycle, and how they impact choosing technologies Cost Optimization Total Cost of Ownership and Opportunity Cost In a perfect world, wed get to experiment with all of the latest, coolest technologies with no consideration of cost, time investment, or value added for the business In reality , budgets and time are finite, and cost is a major constraint for choosing the right data architectures and technologies Y our organization expects a positive return on investment R OI from your data projects, so it s critical that you understand the basic costs you can control Technology is a major cost driver , so your technology choices and management strategies will have a significant impact on your budget W e look at costs through two main lensestotal cost of ownership and opportunity cost Total Cost of Ownership Total cost of ownership T CO is the total estimated cost of an initiative, including the direct and indirect costs of products and services utilized Direct costs are the costs you can directly attribute to an initiative, such as the salaries of a team working on the initiative, or the A W S bill for all services consumed Indirect costs, also known as overhead, are independent of the initiative and must be paid regardless of where theyre attributed Apart from direct and indirect costs, how something is purchased has a big impact on a budget s bottom line These expenses fall into two big groups capital expense and operational expense Capital expenses , also known as capex, require an upfront investment Payment is required today  Before the cloud, companies would typically purchase hardware and software upfront through lar ge acquisition contracts, and house this infrastructure in huge server rooms, or pay to house their servers in a colocation facility  These upfront investments in hardware and softwarecommonly in the hundreds of thousands to millions of dollars, or higherwould be treated as assets, and slowly depreciate over time From a budget perspective, capital would need to be available to pay for the entire purchase upfront or funding secured through a loan vehicle In addition, there s the extra cost of maintaining the hardware and software over the lifetime of those assets Capex is generally reserved for long term investments, where there s a well thought out plan for achieving a positive R OI on the ef fort and expense put forth Operational expenses , known as opex, are almost the opposite of capex Whereas capex is long term focused, opex is short term Opex can be pay as you go, or similar , and allows a lot of flexibility  Opex has the added benefit of potentially being closer to a direct cost, making it easier to attribute to a data project Until recently , opex simply wasn t an option for lar ge data projectsdata warehouse systems required multimillion dollar contracts This has all changed with the advent of the cloud, where data platform services allow engineers to pay on a consumption based model In general, we find that opex allows for a far greater ability for engineering teams to choose both their software and hardware Cloud based services allow data engineers to iterate quickly with dif ferent software and technology configurations, often very inexpensively  Data engineers need to be pragmatic about flexibility  The data landscape is changing too quickly to invest in long term hardware that inevitably goes stale, can t easily scale, and potentially hampers a data engineer s flexibility to try new things Given the upside for flexibility and low initial costs, we urge data engineers to take an opex first appr oach center ed on the cloud and flexible technologies Opportunity Cost Data engineers often fail to evaluate opportunity cost when they undertake a new project In our opinion, this is a massive blind spot Opportunity cost is the cost of choosing one thing at the expense of something else As it relates to data engineering, if you choose Data Stack A, that means youve chosen the benefits of Data Stack A over all other options, ef fectively excluding Data Stack B through Z Y oure now committed to Data Stack A, and everything it entailsthe team to support it, training, setup, maintenance, and so forth But in an incredibly fast moving field like data, what happens when Data Stack A becomes obsolete Can you still move to Data Stack B A big question in a world where newer and better technologies arrive on the scene at an ever -fasterratehow quickly and cheaply can you move to something newer and better Does the expertise youve built up on Data Stack A translate to the next wave Or are you able to swap out components of Data Stack A and buy yourself some time and options The first step to minimizing opportunity cost is evaluating it with eyes wide open W eve seen countless data teams get stuck with technologies that, while they seemed good at the time, are either not flexible for future growth, or are simply obsolete Inflexible data technologies ar e a lot like bear traps Theyr e easy to get into and extr emely painful to escape Don t let this happen to you Today Versus the Future Immutable Versus Transitory Technologies Where are you today What are your goals for the future Y our answers to these questions should inform the decisions you make about your architecture, and thus the technologies used within that architecture T oo often, we see data engineers and architects scoping and building for the future whose date and specific needs are not well defined These intentions are noble but often lead to over architectingand over engineering It often happens that tooling chosen for the futur e is stale and out of date when thisfuture arrives the future frequently looks little like what we envisioned years before As many life coaches would tell you, focus on the present choose the best technology for the moment and near future, but in a way that supports future unknowns and evolution This is done by understanding what is likely to change, and what tends to stay the same There are two classes of tools to considerimmutable and transitory  Immutable technologies might be components that underpin the cloud, or languages and paradigms that have stood the test of time In the cloud, examples of immutable technologies are object storage, networking, servers, and security  Object storage such as A W S S3 and Azure Blob Storage will be around from today until the end of the decade, and probably much longer  Storing your data in object storage is a wise choice Object storage continues to improve in various ways, and constantly of fers new options, but your data will be safe and usable in object storage for the foreseeable future For languages, S QL and Bash have been around for many decades, and we dont see them disappearing anytime soon Immutable technologies benefit from the Lindy Ef fect, which says the longer a technology has been established, the longer it will continue to be used Think, for example, of relational databases, C , or the x86 processor architecture W e suggest applying the Lindy Ef fect as a litmus test to determine if a technology has immutable potential Transitory technologies are those that come and go The typical trajectory begins with a lot of hype, followed by meteoric growth in popularity , then a slow descent into obscurity  The Javascript frontend landscape is a classic example of this How many Javascript frontend frameworks have come and gone between 2010 and 2020 Backbone, Ember and Knockout were popular in the early 2010 s, Angular JS in the mid 2010 s, and React and Vue have massive mindshare today  What s the popular frontend framework 3 years from now Who knows On the data front, new well funded entrants and open source projects arrive on the scene every day  Every vendor will say their product is going to1change the industry and make the world a better place Sadly , the vast majority of these companies and projects won t ultimately get traction, and will fade into obscurity  Top V C s are making big money bets, knowing that most of their data tooling investments will fail How can you possibly know what technologies to invest in for your data architecture It s hard Just consider the number of technologies in Matt T urck s infamous depictions of the machine learning, A I, and data M AD landscape Figure 3 1 2 3 Figur e 3 1  Matt T urcks M AD data landscape Even relatively successful technologies often fade into obscurity quickly after a few years of rapid adoption, a victim of their success For instance, Hive was met with rapid uptake because it allowed both analysts and engineers to query massive data sets without coding complex Map Reduce jobs by hand Inspired by the success of Hive, but wishing to improve on its shortcomings, engineers developed Spark SQ L, Presto, and other technologies Hive now appears primarily in legacy deployments This is the general cycle of data technologiesincumbents reign for a time, then get disrupted and replaced by new technologies that also reign for a time, and so on Our Advice Given the rapid pace of tooling and best practice changes, we suggest evaluating tools on a two year basis Figure 3 2  Whenever possible, find the immutable technologies along the data engineering lifecycle, and use those as your base Build transitory tools around the immutables Figur e 3 2  Use a two year time horizon to r eevaluate your technology choices Location On Premises, Cloud, Hybrid, Multi- Cloud, and More Companies now have numerous options when they decide where to run their technology stacks A slow shift toward the cloud in the last decade now culminates in a veritable stampede of companies spinning up workloads on A W S, Azure, and G CP  Many C T Os now view their decisions around technology hosting as having existential significance for their organizations If they move too slowly , they risk being left behind by their more nimble competition on the other hand, a poorly planned cloud migration could lead to poorly functioning technology and catastrophic costs Let s look at the main places to run your technology stackon- premises, the cloud, hybrid cloud, and multi-cloud On premises While new startups are increasingly born in the cloud, on premises systems are still very much the default for established companies Essentially , this means that they own their own hardware, which may live in data centers that they own, or in leased colocation space In either case, companies are operationally responsible for their hardware and the software that runs on it As hardware fails, they have to repair or replace it They also have to manage upgrade cycles every few years as new , updated hardware is released and older hardware ages and becomes less reliable They also must ensure that they have enough hardware to handle peaks for an online retailer , this means hosting enough capacity to handle the load spikes of Black Friday  For data engineers in char ge of on premises systems, this means buying lar ge enough systems to allow reasonable performance for peak load and lar ge jobs without overbuying and spending excessively  On the one hand, established companies have established operational practices that have served them well for some time If a company that relies on information technology has been in business for some time, this means that they have managed to juggle the cost and personnel requirements ofrunning their own hardware, managing software environments, deploying code from dev teams, running databases, and big data systems, etc On the other hand, established companies see their younger , more nimble competition scaling rapidly and taking advantage of cloud managed services They also see established competitors making forays into the cloud, allowing them to temporarily scale up to huge clusters for massive data jobs, or turn on temporary server capacity for Black Friday  Companies in competitive sectors generally don t have the option to stand still Competition is fierce, and there s always the threat of being disrupted by more agile competition, often backed by significant venture capital dollars Plus, many lar ge incumbent companies are also modernizing and moving their workloads to the cloud Every company must keep its existing systems running ef ficiently while deciding what moves to make next This could involve adopting newer Dev Ops practices, such as containers, Kubernetes, microservices, and continuous deployment while keeping their own hardware running on premises, or it could involve a full migration to the cloud, as discussed below  Cloud The cloud flips the on premises model on its head Instead of purchasing hardware, you simply rent hardware and managed services from a cloud provider A W S, Azure, Google Cloud, etc These resources can often be reserved on an extremely short term basisvirtual machines typically spin up in less than a minute, and subsequent usage is billed in per second increments This allows cloud users to dynamically scale resources in a way that is inconceivable with on premises servers In a cloud environment, engineers can quickly launch projects and experiment without worrying about long lead time hardware planning they can begin running servers as soon as their code is ready to deploy  This makes the cloud model extremely appealing to startups who are tight on budget and time The early era of cloud was dominated by Infrastructure as a Service Iaa S offerings, i.e., products such as virtual machines, virtual disks, etc that are essentially rented slices of hardware Slowly , weve seen a shift toward Platform as a Service Paa S, while Software as a Service Saa S products continue to grow at a rapid clip Paa S includes Iaa S products, but adds more sophisticated managed services to support applications Examples are managed databases Amazon R DS, Google Cloud S QL, managed streaming platforms Amazon Kinesis and S QS, managed Kubernetes Google Kubernetes Engine, Azure A KS, etc Paa S services can allow engineers to ignore the operational details of managing individual machines, and deploying and configuring frameworks across distributed systems They often provide turnkey access to complex systems with simple scaling and minimal operational overhead Saa S of ferings move one additional step up the ladder of abstraction Typically , Software as a Service provides a fully functioning enterprise software platform with very little operational management Examples of Saa S include Salesforce, Google W orkspace, Of fice 365 , Zoom, Fivetran, etc Both the major public clouds and third parties of fer Saa S platforms Saa S covers a whole spectrum of enterprise domains, including video conferencing, data management, ad tech, of fice applications, C RM systems, and so on It can be dif ficult to define the boundary between Paa S and Saa S offerings a C RM platform may be an important component of backend application systems, and Fivetran functions as a critical piece of data infrastructure We also mention serverless in this section, which is increasingly important in Paa S and Saa S of ferings Serverless products generally of fer fully automated scaling from 0 up to extremely high usage rates They are billed on a pay as you go basis and allow engineers to operate with no operational awareness of underlying servers Many people quibble with the term serverlessafter all, the code must run somewhere In practice, serverless platforms usually run on highly distributed multi tenant infrastructure over many servers A W S Lambda and Google Big Query are best in classserverless of ferings in the cloud functions and cloud data warehouse domains respectively  Cloud services have become increasingly appealing to established businesses with existing data centers and I T infrastructure Dynamic, seamless scaling is extremely valuable to businesses that deal with seasonality i.e retail business coping with Black Friday load and web traffic load spikes The C OV ID 19 Global Pandemic has also been a major driver of cloud adoption, as numerous businesses have had to cope with substantially increased web traf fic, and internal I T infrastructure has groaned under the load of mass remote work Before we discuss the nuances of choosing technologies in the cloud, let s first discuss why migration to the cloud requires a dramatic shift in thinking, specifically on the pricing front Enterprises that migrate to the cloud often make major deployment errors by not appropriately adapting their practices to the cloud pricing model Let s take a bit of a detour into cloud economics A brief detour on cloud economics To understand how to use cloud services ef ficiently , you need to know how clouds make money  This is actually an extremely complex concept and one on which cloud providers of fer little transparency  Our guidance in this section is lar gely anecdotal, and won t cover the full scale of what youll likely encounter in your own experience Cloud services and credit default swaps Recall that credit default swaps rose to infamy after the 2007 global financial crisis Roughly speaking, a credit default swap was a mechanism for selling dif ferent tiers of risk attached to an asset i.e a mortgage It is not our intention to understand this idea in any detail, but rather to present an analogy wherein many cloud services are similar to financial derivatives cloud providers not only slice hardware assets into small pieces through virtualization, but they also sell these pieces with varying technical characteristics and risks attached While providers are extremely tight lipped about many details of their internal systems, there are massive opportunities for optimization and scaling by understanding cloud pricing and exchanging notes with other users Lets look at the example of cloud archival storage Google Cloud Platform openly admits that their archival class storage runs on the same clusters as standard cloud storage, yet the price per G B per month of archival storage is roughly 117th that of standard storage How is this possible Here, we are giving our best educated guess When we purchase cloud storage, we think in terms of cost per G B, but each disk in a storage cluster has three dif ferent assets that cloud providers and consumers make use of First, it has a certain storage capacity , say 10 T B Second, it supports a certain number of I OPs inputoutput operations per second, say 100  Third, disks support a certain maximum bandwidth, which is the maximum read speed for optimally or ganized files A magnetic drive might be capable of reading at 200 M Bs For a cloud provider , any of these limits I OPs, storage capacity , bandwidth is a potential bottleneck For instance, the cloud provider might have a disk storing 3 T B of data, but hitting maximum I OP S There s an alternative to leaving the remaining 7 T B empty sell the empty space without selling I OPs Or more specifically , sell cheap storage space and expensive I OPs to discourage reads Much like traders of financial derivatives, cloud vendors also deal in risk In the case of archival storage, vendors are selling a type of insurance, but one that pays out for the insurer rather than the policy buyer in the event of a catastrophe Similar considerations apply to nearly any cloud service T o provide another example, compute is no longer a simple commodity in the cloud as it is with on premises hardware Rather than simply char ging for C PU cores, memory , and features, cloud vendors monetize characteristics such as durability , reliability , longevity , and predictability a variety of compute platforms discount their of ferings for workloads that are ephemeral A W SLambda or can be arbitrarily interrupted when capacity is needed elsewhere Google Compute Engine Interruptible Instances Cloud on premises This heading may seem like a silly tautology in reality , the belief that cloud services are just like familiar on premises servers is an extremely common cognitive error that plagues cloud migrations and leads to horrifying bills This cognitive error demonstrates a broader issue in tech that we refer to as the curse of familiarity  Generally , any technology product is designed to look like something familiar to facilitate ease of use and adoption but, any technology product has subtleties and wrinkles that users must learn to identify and work witharound after the initial adoption phase Moving on premises servers one by one to virtual machines in the cloud known as simple lift and shiftis a perfectly reasonable strategy for the initial phase of cloud migration, especially where a company is facing some kind of financial clif f, such as the need to sign a significant new lease or hardware contract if existing hardware is not shut down However , companies that leave their cloud assets in this initial state are in for a rude shock on a direct comparison basis, long running servers in the cloud are significantly more expensive than their on premises counterparts The key to finding value in the cloud is to understand and optimize for the cloud pricing model Rather than deploying a set of long running servers capable of handling full peak load, use autoscaling to allow workloads to scale down to minimal infrastructure when loads are light, and up to massive clusters during peak times Or take advantage of A W S spot instances or A W S Lambda to realize discounts through more ephemeral, less durable workloads Data engineers can also realize new value in the cloud not by simply saving money , but by doing things that were simply not possible in their on- premises environment The cloud gives data engineers the opportunity to spin up massive compute clusters quickly in order to run complex transformations, at scales that are unaf fordable for on premises hardware Data gravity In addition to basic errors such as following on premises operational practices in the cloud, data engineers need to watch out for other aspects of cloud pricing and incentives that frequently catch users unawares V endors want to lock you into their of ferings For cloud platforms, this means that getting data onto the platform is cheap or free, but getting data out can be extremely expensive Be aware of data egress fees and their long term impacts on your business before you get blindsided by a lar ge bill Data gravity is realonce data lands in a cloud, the cost to extract it and migrate processes can be very high Hybrid Cloud As more and more established businesses migrate into the cloud, the hybrid cloud model is growing in importance V irtually no business can migrate all of its workloads overnight The hybrid cloud model assumes that an organization will maintain some workloads outside the cloud indefinitely  There are a variety of reasons to consider a hybrid cloud model Organizations may believe that they have achieved operational excellence in certain areas, for instance with their application stack and associated hardware Thus, they may choose to migrate only specific workloads where they see immediate benefits in the cloud environment They might choose to migrate their Hadoop stack to ephemeral cloud clusters, reducing the operational burden of managing software for the data engineering team and allowing rapid scaling for lar ge data jobs This pattern of putting analytics in the cloud is particularly attractive because data flows primarily in one direction, minimizing data egress costs Figure 3 3  That is, on premises applications generate event data that can be pushed to the cloud essentially for free The bulk of data remains in the cloud where it is analyzed, while smaller amounts of data might be pushed back for deploying models to applications, reverse E TL, etc Figur e 3 3  Hybrid cloud data flow model that minimizes egr ess costs There s also a new generation of managed hybrid cloud service of ferings, including Google Anthos and A W S Outposts These services allow customers to locate cloud managed servers in their data centers Thus, they can take advantage of auto scaling with the E C2 Management Console, or spin up Google Cloud Composer , while still realizing low latency and network access control of having servers within their own walls These services integrate seamlessly with the cloud of ferings of A W S and G CP respectively , allowing customers to burst data and application workloads to the cloud as the need arises Multi cloud Multi-cloud simply refers to the practice of deploying workloads to multiple clouds, as opposed to deliberately choosing and using only a single cloud for the sake of simplicity  Companies may have a variety of motivations for multi cloud deployments S AA S platforms often wish to offer their services close to existing customer cloud workloads Snowflakeand Databricks are two or ganizations that of fer multi cloud services for this reason This is especially critical for data intensive applications, where network latency and bandwidth limitations hamper performance, and data egress costs can be prohibitive Another common motivation for employing a multi cloud approach is to take advantage of the best services across several clouds Customers might want to handle their Google Ads and Analytics data on G CP and deploy Kubernetes through Google G KE They might adopt Azure specifically for Microsoft workloads And A W S has several best in class services i.e A WS Lambda and enjoys huge mindshare, making it relatively easy to hire A WS proficient engineers There are several disadvantages to a multi cloud methodology  As we just mentioned, data egress costs and networking bottlenecks are critical And going multi-cloud can introduce significant complexity  Companies must now manage a dizzying array of services across several clouds, and cross cloud integration and security present a huge challenge Among other things, it may become necessary to integrate virtual networks across several clouds A new generation of cloud of clouds services aims to facilitate multi- cloud with reduced complexity by not only of fering services across clouds, but seamlessly replicating data between clouds, or managing workloads on several clouds through a single pane of glass A Snowflake account runs in a single cloud region, but customers can readily spin up accounts in G CP , A WS, or Azure Snowflake provides simple scheduled data replication between these accounts In addition, the Snowflake interface is essentially the same in all of these accounts, removing the training burden associated with switching between cloud native services such as Amazon Redshift and Azure Synapse Google Big Query Omni will deliver similar functionality once it goes into general availability , extending the G CP native experience to multiple clouds In fact, were seeing a growing trend of cloud of clouds services offered directly by the public clouds Google Anthos allows users to runworkloads on A W SAzure, and Amazon E KS Anywhere of fers similar functionality for AzureG CP  The cloud of clouds space is evolving rapidly within a few years of this book s publication, there will be far more such services available, and data engineers and architects would do well to maintain awareness of this quickly changing landscape Decentralized Blockchain and the Edge Though not widely used now , its worth briefly mentioning a new trend that might become popular over the next decadedecentralized computing Whereas today s applications mainly run on premises and in the cloud, the rise of blockchain, W eb 3 0 , and edge computing may possibly invert this paradigm While there are not a lot of popular examples to point at right now, its worth keeping platform decentralization in the back of your mind as you consider technologies Be Cautious with Repatriation Arguments As we were writing this book, Sarah W ang and Martin Casado published an article for Andreesen Horowitz that generated significant sound and fury in the tech space This article was widely interpreted as a call for repatriation of cloud workloads to on premises servers In fact, they make a somewhat more subtle ar gument that companies should expend significant resources to control cloud spend, and should consider repatriation as a possible option We want to take a moment to dissect one part of their discussion W ang and Casado cite Dropbox s repatriation of significant workloads from A W S to their servers as a case study for companies considering the same move or assessing retention of existing data centers You are not Dropbox, nor are you Cloudflare We believe that this case study is frequently used without appropriate context, and is a compelling example of the false equivalence logical4fallacy  Dropbox provides very specific services where ownership of hardware and data centers can of fer a competitive advantage Companies should not rely excessively on Dropbox s example when assessing cloud and on premises deployment options First, it s important to understand that Dropbox stores a vast quantity of data The company is tight lipped about exactly how much data they host, but they do say that it is many exabytes and that it continues to grow  Second, Dropbox handles a vast amount of network traf fic W e know that their bandwidth consumption in 2017 was significant enough for the company to add hundreds of gigabits of Internet connectivity with transit providers regional and global I SPs, and hundreds of new peering partners where we exchange traf fic directly rather than through an I SP Their data egress costs would be extremely high in a public cloud environment Third, Dropbox is essentially a cloud storage vendor , but one with a highly specialized storage product that combines characteristics of object and block storage Dropbox s core competence is a dif ferential file update system that can ef ficiently synchronize actively edited files between users while minimizing network and C PU usage As such, their product is not a good fit for object storage, block storage or other standard cloud of ferings Dropbox has instead benefited from building a custom, highly integrated software and hardware stack Fourth, while Dropbox moved their core product to their own hardware, they continue to build out other workloads on A W S In ef fect, this allows them to focus on building one highly tuned cloud service at extraordinary scale rather than trying to replace numerous dif ferent services they can focus on their core competence in cloud storage and data synchronization, while of floading management of software and hardware in other areas, such as data analytics Other frequently cited success stories where companies have built outside the cloud include Backblaze and Cloudflare, and these of fer similar lessons Backblaze began life as a personal cloud data backup product, but has since begun to of fer its own object storage similar to Amazon S3 They currently5 6 7store over an exabyte of data Cloudflare claims to provide services for over 25 million internet properties, with points of presence in over 200 cities and 51 Tbps terabits per second of total network capacity  Netflix of fers yet another useful example Netflix is famous for running its tech stack on A W S, but this is only partially true Netflix does run video transcoding on A W S, accounting for roughly 70% of its compute needs in 2017  They also run their application backend and data analytics on A W S However , rather than using the A W S content distribution network, they have built out a custom C DN in collaboration with internet service providers, utilizing a highly specialized combination of software and hardware For a company that consumes a substantial slice of all internet traffic, building out this critical infrastructure allowed them to cost effectively deliver high quality video to a huge customer base These case studies suggest that it makes sense for companies to manage their own hardware in very specific circumstances The biggest modern success stories of companies building and maintaining hardware involve extraordinary scale exabytes of data, Tbps of bandwidth, etc., and narrow use cases where companies can realize a competitive advantage by engineering highly integrated hardware and software stacks Thus, Apple might gain a competitive edge by building its own storage system for i Cloud, but there is less evidence to suggest that general purpose repatriation ef forts will be beneficial Our Advice From our perspective, we are still at the beginning of the transition to cloud, and thus the evidence and ar guments around workload placement and migration will continue to evolve rapidly  Cloud itself is changing, with a shift from the I AA S infrastructure as a service model built around Amazon E C2 that drove the early growth of A W S, toward more managed service of ferings such as A W S Glue, Google Big Query , Snowflake, etc Weve also seen the emer gence of new workload placement abstractions On premises services are becoming more cloud like Think Kubernetes, which abstracts away hardware details And hybrid cloud services such as8 9 10 1 1 12 Google Anthos and A W S Outposts allow customers to run fully managed services within their own walls, while also facilitating tight integration between local and remote environments Further , the cloud of clouds is just beginning to take shape, fueled by third party services, and increasingly the public cloud vendors themselves Choose technologies for the present, but look toward the future In sum, it is a very dif ficult time to plan workload placements and migrations The decision space will look very dif ferent in five to ten years It is tempting to take into account every possible future architecture permutation We believe that it is critical to avoid this endless trap of analysis Instead, plan for the present Choose the best technologies for your current needs and concrete plans for the near future Choose your deployment platform based on real business needs, while focusing on simplicity and flexibility  In particular , don t choose a complex multi cloud or hybrid cloud strategy unless there s a compelling reason to do so Do you need to serve data near customers on multiple clouds Do industry regulations require you to house certain data in your own data centers Do you have a compelling technology need for specific services on two dif ferent clouds If these scenarios don t apply to you, then choose a simple single cloud deployment strategy  On the other hand, have an escape plan As weve emphasized before, every technologyeven open source softwarecomes with some degree of lock- in A single cloud strategy has significant advantages of simplicity and integration, but also significant lock in In this instance, were really talking about mental flexibility , the flexibility to evaluate the current state of the world and imagine alternatives Ideally , your escape plan will remain locked behind glass, but preparing this plan will help you to make better decisions in the present, and give you a way out if things do go wrong in the future Build Versus Buy Build versus buy is a very old debate in technology  The ar gument for build is that you have end to end control over the solution, and are not at the mercy of a vendor or open source community  The ar gument supporting buy comes down to resource constraints and expertisedo you have the expertise to build a solution that s better than something already available Either decision comes down to T CO, opportunity cost, and whether the solution provides a competitive advantage to your or ganization If youve caught on to a theme in the book so far , its that we suggest investing in building and customizing when doing so will provide a competitive advantage Otherwise, stand on the shoulders of giants and use what s already available in the market Given the number of open source and paid servicesboth of which may have communities of volunteers or highly paid teams of amazing engineersyoure foolish to try to build everything yourself As we often ask, When you need new tires for your car , do you get the raw materials, build the tires from scratch, and install them yourself Or do you go to the tire shop, buy the tires, and let a team of experts install them for you If youre like most people, youre probably buying tires and having someone install them The same ar gument applies to build vs buy  Weve seen teams whove built their own database from scratch Upon closer inspection, a simple open source R DB MS would have served their needs much better  Imagine the amount of time and money invested in this homegrown database T alk about low R OI for T CO and opportunity cost This is where the distinction between the T ype A and T ype B data engineer comes in handy  As we pointed out earlier , Type A and T ype B roles are often embodied in the same engineer , especially in a small or ganization Whenever possible, lean towar d Type A behavioravoid undiffer entiated heavy lifting, and embrace abstraction Use open sour ce frameworks, or if this is too much tr ouble, look at buying a suitable managed or pr oprietary solution In either case, there are plenty of great, modular services to choose from Its worth mentioning the shifting reality of how software is adopted within companies Whereas in the past, I T used to make most of the software purchase and adoption decisions in a very top down manner , these days, the trend is for bottom up software adoption in a company , driven by developers, data engineers, data scientists, and other technical roles Technology adoption within companies is becoming an or ganic, continuous process Lets look at some options for open source and proprietary solutions Open Source Software O SS Open source software O SS is a software distribution model where softwareand the underlying codebaseis made available for general use, typically under certain licensing terms Oftentimes, O SS is created and maintained by a distributed team of collaborators Most of the time, O SS is free to use, change, and distribute, but with specific caveats For example, many licenses require that the source code of open source derived software be included when the software is distributed The motivations for creating and maintaining O SS vary  Sometimes O SS is organic, springing from the mind of an individual or a small team who create a novel solution and choose to release it into the wild for public use Other times, a company may make a specific tool or technology available to the public under an O SS license There are two main flavors of O SScommunity managed, and commercial O SS Community managed O SS O SS projects succeed when there s a strong community and vibrant user base Community managed O SS is a very common path for O SS projects With popular O SS projects, the community really opens up high rates of innovations and contributions from developers all over the world Some things to consider with a community managed O SS projectMindshar e Avoid adopting O SS projects that don t have traction and popularity  Look at the number of Git Hub stars, forks, and commit volume and recency  Another thing to pay attention to is community activity on related chat groups and forums Does the project have a strong sense of community A strong community creates a virtuous cycle of strong adoption It also means that youll have an easier time getting technical assistance, and finding talent qualified to work with the framework Project management How is the project managed Look at Git issues and ifhow theyre addressed Team Is there a company sponsoring the O SS project Who are the core contributors Developer r elations and community management What is the project doing to encourage uptake and adoption Is there a vibrant Slack community that provides encouragement and support Contributing Does the project encourage and accept pull requests Roadmap Is there a project roadmap If so, is it clear and transparent Hosting and maintenance Do you have the resources to host and maintain the O SS solution If so, what s the T CO and opportunity cost versus buying a managed service from the O SS vendorGiving back to the community If you like the project and are actively using it, consider investing in the project Y ou can contribute to the codebase, help fix issues, give advice in the community forums and chats If the project allows donations, consider making one Many O SS projects are essentially community service projects, and the maintainers often have full time jobs in addition to helping with the O SS project It s a labor of love that sadly doesn t afford the maintainer a living wage If you can af ford to donate, please do so Commercial O SS Sometimes O SS has some drawbacks Namely , you have to host and maintain the solution in your environment Depending on the O SS application youre using, this may either be trivial or extremely complicated and cumbersome Commercial vendors try to solve this management headache by hosting and managing the O SS solution for you, typically as a cloud S AA S of fering Some examples of such vendors include Databricks Spark, Confluent Kafka, D BT Labs dbt, and many , many others This model is called Commercial O SS C OS S T ypically , a vendor will offer the core of the O SS for free, while char ging for enhancements, curated code distributions, or fully managed services A vendor is often af filiated with the community O SS project As an O SS project becomes more popular , the maintainers may create a separate business for a managed version of the O SS This typically becomes a cloud S AA S platform built around a managed version of the open source code This is a very common trendan O SS project becomes very popular , an affiliated company raises truckloads of V C money to commercialize the O SS project, and the company scales as a fast moving rocketship At this point, there are two options for a data engineer  You can continue using the community managed O SS version, which you need to continue maintaining on your own updates, servercontainer maintenance, pullrequests for bug fixes, etc Or , you can pay the vendor and let them take care of the administrative management of the C OS S product Some things to consider with a commercial O SS project Value Is the vendor of fering a better value than if you managed the O SS technology yourself Some vendors will add lots of bells and whistles to their managed of ferings that aren t available in the community O SS version Are these additions compelling to you Delivery model How do you access the service Is the product available via download, A PI, or webmobile U I Be sure youre easily able to access the initial version and subsequent releases Support Support cannot be understated, and it s sadly often opaque to the buyer  What is the support model for the product, and is there an extra cost for support Oftentimes, vendors will sell support for an additional fee Be sure you clearly understand the costs of obtaining support Releases and bug fixes Is the vendor transparent about their release schedule, improvements, and bug fixes Are these updates easily available to you Sales cycle and pricing Often, a vendor will of fer on demand pricing, especially for a S AA S product, and of fer you a discount if you commit to an extended agreement Be sure to understand the tradeof fs of paying as you go versus paying upfront Is it worth it to pay a lump sum, or is your money better spent elsewhere Company finances Is the company viable If the company has raised V C funds, you can check their funding on sites like Crunchbase How much runway does the company have, and will they still be in business in a couple of years Logos versus r evenue Is the company focused on growing the number of customers logos, or is it trying to grow revenue Y ou may be surprised by the number of companies that are primarily concerned with growing their customer count without the revenue to establish sound finances Community support Is the company truly supporting the community version of the O SS project How much are they contributing to the community O SS codebase There s been controversy with certain vendors co opting O SS projects, and subsequently providing very little value back to the community  Proprietary W alled Gardens While O SS is extremely common, there is also a giant market for non O SS technologies Some of the biggest companies in the data industry sell closed source products Let s look at two major types of proprietary walled gardensindependent companies and cloud platform of ferings Independent offerings The data tool landscape has seen exponential growth over the last several years Every day , it seems like there are new independent of ferings for data tools, and there s no indication of a slowdown anytime soon W ith the ability to raise funds from V Cs who are flush with capital, these data companies can scale, and hire great teams in engineering, sales and marketing This presents a situation where users have some great product choices in the marketplace while having to wade through endless sales and marketing clutter  Quite often, a company selling a data tool will not release it as O SS, instead offering a black box solution Although you won t have the transparency of a pure O SS solution, a proprietary independent solution can work quite well, especially as a fully managed service in the cloud For comparison, Databricks uses the Apache Spark codebase with a proprietary management layer Google Big Query is a proprietary solution, not of fering access to the underlying codebase Some things to consider with an independent of fering Inter operability Make sure it inter -operateswith other tools youve chosen OS S, other independents, cloud of ferings, etc Interoperability is key , so make sure you are able to try before you buy  Mindshar e and market shar e Is the solution popular Does it command a presence in the marketplace Does it enjoy positive customer reviews Documentation and support Problems and questions will inevitably arise Is it clear how to solve your problem, either through documentation or support Pricing Is the pricing clearly understandable Map out low , medium, and high probability usage scenarios, with respective costs Are you able to negotiate a contract, along with a discount Is it worth it If you sign a contract, how much flexibility do you lose, both in terms of negotiation and the ability to try new options Are you able to obtain contractual commitments on future pricing Longevity Will the company survive long enough for you to get value from its product If the company has raised money , check sites like Crunchbaseand see what their funding situation is like Look at user reviews Ask friends and post questions on social networks about other users experiences with the product Make sure you know what youre getting into Cloud platform service offerings Cloud vendors develop and sell their own proprietary services for storage, databases, etc Many of these solutions are internal tools used by respective sibling companies For example, Amazon created the database Dynamo DB to overcome the limitations of traditional relational databases and handle the lar ge amounts of user and order data as Amazon.com grew into a behemoth They later of fered the Dynamo DB service solely on A W S it s now an extremely popular product used by companies of all sizes and maturity levels Clouds will often bundle their products to work well together  By creating a strong integrated ecosystem, each cloud is able to create stickiness with its user base In addition to internal innovations that make their way to end users, as we discussed earlier , clouds are also keen followers of strong O SS and independent of ferings If a cloud vendor sees traction with a particular product or project, expect that they will of fer their own version The reason is simple Clouds make their money through consumption More of ferings in a cloud ecosystem mean a greater chance of stickiness and increased spending by customers Some things to consider with a proprietary cloud of fering Performance versus price comparisons Is the cloud of fering substantially better than an independent or O SS version What s the T CO of choosing a cloud s offering Purchase considerations On demand pricing can be expensive Are you able to lower your cost by purchasing reserved capacity , or entering into a long term commitment agreementOur Advice Build versus buy comes back to knowing your competitive advantage, and where it makes sense to invest resources toward customization In general, we favor O SS and C OS S by default, which frees you up to focus on improving those areas where these options are insuf ficient Focus on a few areas where building will add significant value or reduce friction substantially  Its worth mentioningdon t treat internal operational overhead as a sunk cost There s great value in upskilling your existing data team to build sophisticated systems on managed platforms rather than babysitting on- premises servers Always think about how a company makes money , especially the sales and customer experience people This will generally indicate how youre treated during the sales cycle, as well as when youre a paying customer  Monolith Versus Modular Monoliths versus modular systems is another longtime debate in the software architecture space Monolithic systems are self contained, often performing multiple functions under a single system The monolith camp favors the simplicity of having everything in one place It s easier to reason about a single entity , and you can move faster because there are fewer moving parts The modular camp leans toward decoupled, best of breed technologies performing tasks at which they are uniquely great Especially given the rate of change in products in the data world, the ar gument is you should aim for interoperability among an ever changingarray of solutions What approach should you take in your data engineering stack Let s explore the tradeof fs of a monolithic vs a modular approach Monolith The monolith Figure 3 4 has been a technology mainstay for decades The old days of waterfall meant that software releases were huge, tightly coupled, and moved at a slow cadence Lar ge teams worked together to deliver a single working codebase Monolithic data systems continue to this day, with older software vendors such as Informatica, and open source frameworks such as Spark Figur e 3 4  The monolith tightly couples its services The pros of the monolith are it s easy to reason about, and there s lower cognitive burden and context switching since everything is self contained Instead of dealing with dozens of technologies, you deal with one technology , and typically one principle programming language Monoliths are a good option if you want simplicity in reasoning about your architecture and processes Of course, there are cons with the monolith For one, it s brittle Due to the vast number of moving parts, updates and releases take longer , and tend to bake in the kitchen sink If there s a bug in the system and hopefully the software s been thoroughly tested before release, it can have a deleterious effect on the entire system User induced problems also happen with monoliths For example, we saw a monolithic E TL pipeline that took 48 hours to run If anything brokeanywhere in the pipeline, the entire process had to restart Meanwhile, anxious business users were waiting for their reports, which were already 2 days late by default Breakages were common enough that the monolithic system was eventually thrown out Multitenancy in a monolithic system can also be a significant problem It can be dif ficult to isolate the workloads of multiple users in a monolithic system In an on prem data warehouse, one user definedfunction might consume enough C PU to slow down the system for other users In Hadoop, Spark, and Airflow environments, conflicts between dependencies required by dif ferent users are a frequent source of headaches See the distributed monolith discussion that follows Another con about monolithsif the vendor or open source project dies, switching to a new system will be very painful Because all of your processes are contained in the monolith, extracting yourself out of that system, and onto a new platform, will be costly in both time and money  Modularity Modularity Figure 3 5 is an old concept in software engineering, but modular distributed systems truly came into vogue with the rise of microservices in the 2010s Instead of relying on a massive monolith to handle your needs, why not break apart systems and processes into their own self contained areas of concern Microservices can communicate via A PIs, allowing developers to focus on their domains while making their applications accessible to other microservices This is the trend in software engineering and is increasingly seen in modern data systems as well Figur e 3 5  Modularity each service is decoupled Major tech companies have been key drivers in the microservices movement Google engineers invented Linux containers, inspired by other operating systems such as Solaris, to allow the decomposition of applications into smaller pieces The famous Bezos A PI mandate decreases coupling between applications, allowing refactoring and decomposition Bezos also imposed the two pizza rule No team should be so lar ge that two pizzas can t feed the whole group Ef fectively , this means that a team will have at most 5 members This cap also limits the complexity of a team s domain of responsibility , in particular the codebase that it can manage Where a lar ge, monolithic application might entail a team of 100 people, dividing developers into small teams of 5 requires that this lar ge application be broken into small, manageable, loosely coupled pieces In a modular microservice environment, components are swappable a service written in Python can be replaced with a Java service Service customers need only worry about the technical specifications of the service A PI, not behind the scenes details of implementation Data processing technologies have shifted toward a modular model by providing strong support for interoperability  In the world of data lakes and lakehouses, data is stored in object storage in a standard format such as Parquet Any processing tool that supports the format can read the data and write processed results back into the lake for processing by another tool Tools such as Big Query , Snowflake, and Redshift don t use a data lake architecture and don t make data directly accessible to external tools However , all of these support interoperation with object storage, through importexport using standard formats, and external tables, i.e., queries run directly on data in a data lake In today s data ecosystem, new technologies arrive on the scene at a dizzying rate, and most get stale and outmoded quickly  Rinse and repeat The ability to swap out tools as technology changes are invaluable In addition, we view data modularity as a more powerful paradigm than monolithic data engineering It allows engineers to choose the best technology for each job, or even for each step of a pipeline The cons of modularity are that there s more to reason about Instead of handling a single system of concern, now you potentially have countless systems to understand and operate Interoperability is a potential headache hopefully , these systems all play nicely together  In fact, it is this very problem that led us to break orchestration out as a separate undercurrent, instead of placing it under data management Orchestration is also important for monolithic data architectureswitness the success of Control M in the traditional data warehousing space But orchestrating five or ten tools is dramatically more complex than orchestrating one Orchestration becomes the glue that binds these tools together  The Distributed Monolith Pattern The distributed monolith pattern is a distributed architecture that still suf fers from many of the limitations of monolithic architecture The basic idea is that one runs a distributed system with separate services to perform different tasks, but services and nodes share a common set of dependencies andor a common codebase One standard example is a traditional Hadoop cluster  A standard Hadoop cluster can host a number of dif ferent frameworks simultaneously , such as Hive, Pig, Spark, etc In addition, the cluster runs core Hadoop components Hadoop common libraries, H DF S, Y arn, Java In practice, a cluster generally has one version of each component installed Essentially , a standard on prem Hadoop system entails managing a common environment that works for all users and all jobs Managing upgrades and installations is a significant challenge Forcing jobs to upgrade dependencies risks breaking them maintaining two versions of a framework entails extra complexity  Our second example of a distributed monolith pattern is Apache Airflow  The Airflow architecture is highly decoupled and asynchronous Interprocess communications are passed through a backend database, with afew dif ferent services web server , scheduler , executor interacting The web server and executor can be scaled horizontally to multiple copies The problem with this architecture is that every service needs to run the same Airflow codebase with the same dependencies Any executor can execute any task, so a client library for a single task run in one D AG must be installed on the whole cluster  As a consequence, Google Cloud Composer managed Airflow on Google Cloud has a huge number of installed dependencies to handle numerous dif ferent databases, frameworks, and A PIs out of the box The pip package manager shows many dependency conflicts even in the base environment The installed packages are essentially a house of cards Installing additional libraries to support specific tasks sometimes works, and sometimes breaks the whole installation One solution to the problems of the distributed monolith is ephemeral infrastructure in a cloud setting That is, each job gets its own temporary server or cluster , with its own dependencies installed Each cluster remains highly monolithic, but conflicts are dramatically reduced by separating jobs This pattern is now quite common for Spark with services like Amazon E MR and Google Cloud Dataproc A second solution is to properly decompose the distributed monolith into separate software environments using containers W e have more to say on containers below in our section on Serverless vs Servers A third solution is to retain the distributed monolith, but pull functionality out into microservices This approach has become common with Airflow code with complex dependency requirements is moved into external containers or cloud functions to keep the core environment simple Our Advice While monoliths are certainly attractive due to ease of understanding and reduced complexity , this comes at a big cost The cost is the potential loss of flexibility , opportunity cost, and high friction development cycles Here are some things to consider when evaluating monoliths versus modular Inter operability  Architect for sharing and interoperability  Avoid the bear trap Something that is very easy to get into might be very painful or impossible to extricate yourself from Flexibility  Things are moving so fast in the data space right now  Committing to a monolith reduces flexibility and reversible decisions Serverless Versus Servers A big trend for cloud providers is serverless, which allows developers and data engineers to run applications without managing servers behind the scenes For the right use cases, serverless provides a very quick time to value In other cases, it might not be a good fit Let s look at how to evaluate whether serverless is right for you Serverless Though serverless has been around for quite some time, the serverless trend kicked of f in full force with A W S Lambda in 2014  W ith the promise of executing small chunks of code on an as needed basis, without having to run a server , serverless exploded in popularity  The main reasons for the popularity of serverless are cost and convenience Instead of paying the cost of a server , why not just pay when your code is evoked There are many flavors of serverless Though function as a service F A AS is wildly popular , serverless systems actually predate the advent of A W S Lambda As an example, Google Cloud s Big Query is a serverless datawarehouse, in that there s no backend infrastructure for a data engineer to manage Just load data into the system and start querying Y ou pay for the amount of data your query consumes, as well as a small cost to store your data This payment modelpaying for consumption and storageis becoming more and more prevalent, particularly with serverless workloads Another example of serverless is Google App Engine, originally released in 2008  When does serverless make sense As with many other cloud services, it depends and data engineers would do well to understand the details of cloud pricing to predict when serverless deployments are going to become expensive Looking specifically at the case of A W S Lambda, various engineers have found hacks to run batch workloads at incredibly low costs On the other hand, serverless functions suf fer from an inherent overhead inef ficiency  Every invocation involves spinning up some new resources and running some initialization code, overhead that does not apply to a long running service Handling one event per function call at a high event rate can be catastrophically expensive As with other areas of ops, it s critical to monitor and model That is, monitor to determine cost per event in a real world environment, and model by using this cost per event to determine overall costs as event rates grow in the future Similar considerations apply to understanding costs for other serverless models Containers In conjunction with serverless and microservices, containers are one of the most significant trending operational technologies as of this writing In fact, containers play a role in both Containers are often referred to as lightweight virtual machines Where a traditional virtual machine wraps up an entire operating system, a container packages an isolated user space, i.e a filesystem and one or a few processes many such containers can coexist on a single host operating system This provides some of the principal benefits of virtualization i.e dependency and code isolation without the overhead of carrying around an entire operating system kernel 13 A single hardware node can host numerous containers with fine grained resource allocations At the time of this writing, containers continue to grow in popularity , along with Kubernetes, a container management system Serverless environments typically run on containers behind the scenes indeed, Kubernetes is a kind of serverless environment because it allows developers and ops teams to deploy microservices without worrying about the details of the machines where they are deployed Containers provide a partial solution to problems of the distributed monolith, mentioned earlier in this chapter  Hadoop now supports containers, allowing each job to have its own isolated dependencies And as we mentioned, many Airflow users extract jobs into containers W AR NI NG Container clusters do not provide the same level of security and isolation of fered by full virtual machines Container escapebroadly , a class of exploits where code in a container gains privileges outside the container at the O S levelremains an unsolved problem While Amazon E C2 is a truly multi tenant environment with V Ms from many customers hosted on the same hardware, a Kubernetes cluster should only host code within an environment of mutual trust, e.g inside the walls of a single company  Various flavors of container platforms add additional serverless features Containerized function platforms Open Faas, Google Cloud Run run containers as ephemeral units triggered by events, rather than persistent services This gives users the simplicity of A W S Lambda with the full flexibility of a container environment in lieu of the highly restrictive Lambda runtime And services such as A W S Far gate and Google App Engine run containers without the need to manage a compute cluster as is required for Kubernetes These services also fully isolate containers, preventing the security issues associated with multi-tenancy  Abstraction will continue working its way across the data stack Consider the impact of Kubernetes on cluster management While you can manage your Kubernetes clusterand many engineering teams do soeven Kubernetes is a managed service Quite a few commercial S AA S productsuse Kubernetes behind the scenes What you end up with is essentially a serverless service When Servers Make Sense Why would you want to run a server instead of using serverless There are a few reasons Cost is a big factor  Serverless makes less sense when the usage and cost exceed the constant cost of running and maintaining a server Figure 3 6  However , at a certain scale, the economic benefits of serverless diminish, and running servers becomes more attractive Figur e 3 6  Cost of serverless vs using a server Customization, power , and control are other major reasons to favor servers over serverless Some serverless frameworks can be underpowered or limited for certain use cases Here are some things to consider when using servers, particularly when youre in the cloud, where server resources are ephemeral Expect servers to fail Server failure will happen A void using a special snowflake server that is overly customized and brittle, as this introduces a glaring vulnerability in your architecture Instead, treat servers as transient resources that you can create as needed and then delete If yourapplication requires specific code to be installed on the server , use a boot script, or build an image Deploy code to the server through a C IC D pipeline Use clusters and auto scaling Take advantage of the cloud s ability to grow and shrink compute resources on demand As your application grows its usage, cluster your application servers, and use auto scaling capabilities to automatically expand your application as demand grows Treat your infrastructur e as code Automation doesn t just apply to servers and should extend to your infrastructure whenever possible Deploy your infrastructure servers or otherwise using deployment managers such as T erraform, A W S Cloud Formation, Google Cloud Deployment Manager , and others Consider using containers For more sophisticated or heavy duty workloads where simple startup scripts won t cut it, consider using containers and something like Kubernetes Our Advice Here are some key considerations to help you determine whether serverless is right for you Workload size and complexity Serverless works best for simple, discrete tasks and workloads It s not as suitable if you have a lot of moving parts, or if you require a lot of compute or memory horsepower  In that case, consider using containers and a container workflow orchestration framework like Kubernetes Execution fr equency and duration How many requests per second will your serverless application process How long will each request take to process Cloud serverless platforms have limits on execution frequency , concurrency , and duration If your application can t function neatly within these limits, it is time to consider a container orientedapproach Requests and networking Serverless platforms often utilize some form of simplified networking and don t support all cloud virtual networking features, such as V PCs and firewalls Language What language do you typically use If it s not one of the of ficially supported languages supported by the serverless platform, then you should consider containers instead Runtime limitations Serverless platforms don t give you full operating system abstractions Instead, youre limited to a specific runtime image Cost Serverless functions are extremely convenient, but also inef ficient when handling one event per call This works fine for low event rates, but costs rise rapidly as the event count increases This scenario is a frequent source of surprise cloud bills In the end, abstraction tends to win W e suggest looking at using serverless first, and serverswith containers and orchestration if possibleonly after serverless makes no sense for your use case Undercurrents and How They Impact Choosing Technologies As weve seen in this chapter , there s a lot for a data engineer to consider when evaluating technologies Whatever technology you choose, be sure to understand how it supports the undercurrents of the data engineering lifecycle Let s briefly review them again Data Management Data management is a broad area, and concerning technologies, it isn t always obvious whether a technology adopts data management as a principal concern For example, a third party vendor may use data management best practicessuch as regulatory compliance, security , privacy , data quality , and governancebehind the scenes, but expose only a limited U I layer to the customer  In this case, while evaluating the product, it helps to ask the company about their data management practices Here are some sample questions you should ask How are you protecting data against breaches, both from the outside and from within What is your product s compliance with G DP R, C CP A, and other data privacy regulations Do you allow me to host my data to be compliant with these regulations How do you ensure data quality , and that Im viewing the right data in your solution There are many other questions to ask, and these are just a few of the ways to think about data management as it relates to choosing the right technologies These same questions should also apply to the O SS solutions youre considering Data Ops Problems will happen They just will A server or database may die, a cloud s region may have an outage, you might deploy buggy code, bad datamight be introduced into your data warehouse, and any number of unforeseen problems When evaluating a new technology , how much control do you have over the deployment of new code, how will you be alerted if there s a problem, and how are you going to respond when there s a problem The answer to this lar gely depends on the type of technology youre considering If the technology is O SS, then youre likely responsible for setting up monitoring, hosting, and code deployment How will you handle issues What s your incident response If youre using a managed of fering, much of the operations are out of your control Consider the vendor s S LA, how they alert you to issues, and whether theyre transparent about how theyre addressing the issue, an E T A to a fix, etc Data Architecture As we discussed in Chapter 3 , good data architecture means assessing tradeof fs and choosing the best tools for the job, while keeping your decisions reversible W ith the data landscape morphing at warp speed, the best tool for the job is a moving tar get The main goals are to avoid unnecessary lock in, ensure interoperability across the data stack, and produce high R OI Choose your technologies accordingly  Orchestration Through most of this chapter , we have actively avoided discussing any particular technology too extensively  We will make an exception for orchestration because the space is currently dominated by one open source technology , Apache Airflow  Maxime Beauchemin kicked of f the Airflow project at Airbnb in 2014  Airflow was developed from the beginning as a non commercial open source project The framework quickly grew significant mindshare outside Airbnb, becoming an Apache Incubator project in 2016 , and a full Apache sponsored project in 2019  At present, Airflow enjoys many advantages, lar gely due to its dominant position in the open source marketplace First, the Airflow open source project is extremely active, with a high rate of commits, and a quick response time for bugs and security issues and the project recently released Airflow 2 , a major refactor of the codebase Second, Airflow enjoys massive mindshare Airflow has a vibrant, active community on many communications platforms, including Slack, Stack Overflow , and Git Hub Users can easily find answers to questions and problems Third, Airflow is available commercially as a managed service or software distribution through many vendors, including G CP , A WS, and Astronomer .io Airflow also has some downsides Airflow relies on a few core non scalable components the scheduler and backend database that can become bottlenecks for performance, scale, and reliability the scalable parts of Airflow still follow a distributed monolith pattern See Monolith vs Modular above Finally , Airflow lacks support for many data native constructs, such as schema management, lineage, and cataloging and it is challenging to develop and test Airflow workflows We will not attempt an exhaustive discussion of Airflow alternatives here, but just mention a couple of the key orchestration contenders at the time of writing Prefect and Dagster each aim to solve some of the problems discussed above by rethinking components of the Airflow architecture W e also mention Datacoral, which intrigues us with its concept of metadata first architecture Datacoral allows for automated workflow construction through analysis of data flows between queries We highly recommend that anyone choosing an orchestration technology study the options discussed here They should also acquaint themselves with activity in the space, as there will almost certainly be new developments by the time you read this Software Engineering As a data engineer , you should strive for simplification and abstraction across the data stack Buy or use pre built open source whenever possible Eliminating undif ferentiated heavy lifting should be your big goal Focus your resourcescustom coding and toolingon areas that give you a strong competitive advantage For example, is hand coding a database connection between My SQ L and your cloud data warehouse a competitive advantage to you Probably not This is very much a solved problem Pick an of f the shelf solution open source or managed S AA S, versus writing your own database connector  The world doesn t need the millionth 1 My SQ L to cloud data warehouse connector  On the other hand, why do customers buy from you Y our business very likely has something special about the way it does things Maybe it s a particular algorithm that powers your fintech platform, or similar  By abstracting away a lot of the redundant workflows and processes, you can continue chipping away , refining, and customizing the things that really move the needle for the business Conclusion Choosing the right technologies is no easy task, and especially at a time when new technologies and patterns emer ge on a seemingly daily basis, today is possibly the most confusing time in history for evaluating and selecting technologies Choosing technologies is a balance of use case, cost, build versus buy , and modularization Always approach technology the same way as architectureassess trade of fs and aim for reversible decisions Now that were armed with knowledge, let s dive into the first layer of the data engineering lifecyclesource systems, and the data they generate 1 As the authors were working on this chapter in September 2021 , C was in position 1 on the T IO BE index .tiobe.comtiobe index 2 Silicon V alley Making the W orld a Better Place3 4 repatriation optimization 5 6 pocket infrastructure 7 8 .backblaze.comcompanyabout.html 9 .cloudflare.comwhat is cloudflare 10 .comblog2017124the eternal cost savings of netflixs internal spot- market.html 1 1 .com2019digitalnewsnetflix loses title top downstream bandwidth- application- 1203330313 12 .theinformation.comarticlesapples spending on google cloud storage on track- to soar 50 this year 13 See on aws lambdaChapter4 Ingestion A N OT E F OR E AR LY R EL EA SE R EA DE RS With Early Release ebooks, you get books in their earliest formthe authors raw and unedited content as they writeso you can take advantage of these technologies long before the of ficial release of these titles This will be the seventh chapter of the final book If you have comments about how we might improve the content andor examples in this book, or if you notice missing material within this chapter , please reach out to the authors at book_feedbackternarydata.com  You just learned the various source systems youll likely encounter as a data engineer , as well as ways to store data Let s now turn our attention to the patterns and choices that apply to ingesting data from a variety of source systems W e will discuss what data ingestion is, the key engineering considerations for the ingestion phase, the major patterns for both batch and streaming ingestion, technologies youll encounter , who youll work with as you develop your data ingestion pipeline, and how the undercurrents feature in the ingestion phase see Figure 4 1  Figur e 4 1  Data engineering life cycle What Is Data Ingestion Data moves from source systems into storage, with ingestion as an intermediate step Figure 4 2  Data ingestion implies data movement and is a key component of the data engineering lifecycle Figur e 4 2  Data fr om System 1 is ingested into System 2 Its worth quickly contrasting data ingestion with data integration  Whereas data ingestion is the movement of data from point A to B, data integration combines data from disparate sources into a new dataset For example, you can use data integration to combine data from a C RM system, advertising analytics data, and web analytics to create a user profile, which is saved to your data warehouse Furthermore, using reverse E TL, you can send this newly created user profile back to your C RM so salespeople can use the data for prioritizing leads Data integration will be described more fully in Chapter 8 , where we discuss data transformations reverse E TL is covered in Chapter 9  W e also point out that data ingestion is dif ferent from internal ingestion within a system, where data stored in a database is copied from one table to another we consider this to be another part of the general process of data transformation covered in Chapter 8  D AT A P IP EL IN ES D EF IN ED Data pipelines begin in source systems, but ingestion is the stage where data engineers begin actively designing data pipeline activities In the data engineering space, there is a good deal of ceremony around different data movement and processing patterns, with older patterns such as E TL extract, transform, load, newer patterns such E L T extract, load, transform, and new names for long established practices reverse E TL All of these concepts are encompassed in the idea of a data pipeline It is important to understand the details of these various patterns, but also know that a modern data pipeline could encompass all of these As the world moves away from a traditional monolithic approach with very rigid constraints on data movement, towards an ecosystem of cloud services that are assembled like lego bricks to realize that products, data engineers prioritize using the right tools to accomplish the desired outcome rather than adhering to a narrow philosophy of data movement In general, here s our definition of a data pipeline A data pipeline is the combination of ar chitectur e, systems, and processes that move data thr ough the stages of the data engineering lifecycle Our definition is deliberately fluidand intentionally vagueto allow data engineers to plug in whatever they need to accomplish the task at hand A data pipeline could be a traditional E TL system, where data is ingested from an on premises transactional system, passed through a monolithic processor , and written into a data warehouse Or it could be a cloud based data pipeline that pulls data from 100 dif ferent sources, combines data into 20 wide tables, trains five dif ferent machine learning models, deploys them into production, and monitors ongoing performance A data pipeline should be flexible enough to fit any needs along the data engineering lifecycle Lets keep this notion of data pipelines in mind as we proceed through the ingestion chapter  Key Engineering Considerations for the Ingestion Phase When preparing to architect or build an ingestion system, here are some primary considerations and questions to ask yourself related to data ingestion What s the use case for the data Im ingesting Can I reuse this data and avoid ingesting multiple versions of the same dataset Where is the data going What s the destination Frequency How often should the data be updated from the source What is the expected data volume What format is the data in Can downstream storage and transformation accept this format Is the source data in good shape for immediate downstream use That is, is the data of good quality What post processing is required to serve it What are data quality risks e.g could bot traffic to a website contaminate the data If the data is from a streaming source, does it require in flight processing for downstream ingestion These questions undercut both batch and streaming ingestion and apply to the underlying architecture youll create, build, and maintain Regardless of how often the data is ingested, youll want to consider these factors when designing your ingestion architectureBounded versus unbounded Frequency Synchronous versus asynchronous Serialization and deserialization Throughput and elastic scalability Reliability and durability Payload Push versus pull patterns Lets look at each of these Bounded V ersus Unbounded As you might recall from Chapter 3 , bounded and unbounded data Figure 4 3 , data comes in two formsbounded and unbounded Unbounded data is data as it exists in reality , where events happen when they happen, either sporadically or continuous, ongoing and flowing Bounded data is a convenient way of bucketing data across some sort of boundary , such as time Figur e 4 3  Bounded vs unbounded data Let us adopt this mantra All data is unbounded until we bound it Like many mantras, this one is not precisely true 100% of the time the grocery list that I scribbled this afternoon is truly bounded data However , the idea is correct for practical purposes for the vast majority of data that we handle in a business context That is, an online retailer will process customer transactions 24 hours a day until the business fails, the economy grinds to a halt, or the sun explodes Business processes have long imposed artificial bounds on data by cutting discrete batches, but always keep in mind the true unboundedness of your data streaming ingestion systems are simply a tool for preserving the unbounded nature of data so that subsequent steps in the lifecycle can also process it continuously  Frequency One of the key decisions the data engineers must make in designing data ingestion processes is the data ingestion frequency  Ingestion processes can be a batch, micro-batch, or real time Ingestion frequencies vary dramatically , on the spectrum from slow to fast Figure 4 4  On the slow end, a business might ship its tax data to an accounting firm once a year  On the faster side, a change data capture system could retrieve new log updates from a source database once a minute Even faster , a system might ingest events from Io T sensors continuously , and process these within seconds In a company , data ingestion frequencies are often mixed, depending on the use case and technologies involved Figur e 4 4  The spectrum of slow to fast ingestion, fr om batch to r eal time We note that real time ingestion patterns are becoming increasingly common W e initially put real time in quotes because no ingestion system is truly real time Any database, queue, pipeline, etc has some inherent latency in delivering data to a tar get system It is more accurate to speak of near r eal time , but we often use the term real time for the sake of brevity  The near real time pattern generally does away with an explicit update frequency events are processed in the pipeline either one by one as they arrive or in micro-batches i.e batches over very short time intervals For this book, we will use real time and streaming interchangeably  Even with a streaming data ingestion process in place, batch processing downstream is quite common At the time of this writing, machine learning models are typically trained on a batch basis, although continuous online training is becoming more prevalent Rarely do data engineers have the option to build a purely near real time pipeline with no batch components Instead, they choose where batch boundaries will occur , i.e., wherein the data engineering lifecycle data will be broken into batches Once data reaches a batch process, the batch frequency becomes a bottleneck for all downstream processing you move at the cadence of how the batch moves We also note that streaming systems are an or ganic best fit for many modern data source types In Io T applications, the common pattern is for each sensor to write events or measurements as they happen While this data can be written into a database, a streaming ingestion platform such as Amazon Kinesis or Apache Kafka is a better fit for the application Software applications can adopt similar patterns, by writing events to a message queue as they happen rather than waiting for an extraction process to pull events and state information from a backend database This pattern works extremely well for event driven architectures that are already exchanging messages through queues And again, streaming architectures generally coexist with batch processing Synchronous V ersus Asynchronous Ingestion Ingestion systems can have a series of dependent steps synchronous systems, or operate without any dependencies asynchronous systems When we discuss dependencies in this context, were describing whether the completion of one step prevents downstream steps from starting With synchronous ingestion, the source, ingestion, and destination have hard dependencies and are tightly coupled As you can see in Figure 4 5 , each stage of the data engineering lifecycle has processes A, B, and C that are directly dependent upon each other  If Process A fails, Processes B and C cannot start This type of synchronous workflow is common in older E TL systems where data extracted from a source system must then be transformed before being loaded into a data warehouse If the ingestion or transformation process fails for any reason, the entire process must be replayed until it s successful W eve seen instances where the transformation process itself is a series of dozens of synchronous workflows, sometimes taking over 24 hours to finish If any step of that transformation workflow failed and it occasionally would fail, the entire transformation process needed to be restarted from the beginningFigur e 4 5  Synchr onous data ingestion With asynchronous ingestion, dependencies can now operate at the level of individual events, much as they would in a software backend built from microservices Figure 4 6  For example, take the example of a web application that emits events into an Amazon Kinesis Data Stream here acting as a buf fer the stream is read by Apache Beam, which parses and enriches events, then forwards them to a second Kinesis Stream Kinesis Firehose rolls up events and writes objects to Amazon S3 The big idea is that rather than relying on asynchronous processing, where a batch process runs for each stage as the input batch closes and certain time conditions are met, each stage of the asynchronous pipeline can process data items as they become available in parallel across the Beam cluster  The processing rate depends on available resources The Kinesis Data Stream acts as the shock absorber , moderating the load so that event rate spikes will not overwhelm downstream processing When the event rate is low and any backlog has cleared, events will move through the pipeline very quickly  Note that we could modify the scenario and use a Kinesis Data Stream for storage, eventually extracting events to S3 before they expire out of the stream Figur e 4 6  Asynchr onous data ingestion Serialization and Deserialization Moving data from source to sink involves serialization and deserialization Serialization means encoding the data from a source and preparing datastructures for transmission and intermediate storage stages See the more extensive discussion of serialization in the appendix on Serialization and Compr ession Throughput and Scalability In theory , your ingestion should never be a bottleneck In practice, this is easier said than done As your data volumes grow and requirements change, data throughput and system scalability become extremely critical Design your systems to flexibly scale and shrink to match the desired data throughput Monitoring is key , as well as knowledge of the behavior of the upstream systems you depend upon and how they generate data Y ou should be aware of the number of events generated per time interval youre concerned with eventsminute, eventssecond, and so on, as well as the average size of each event Y our data pipeline should be able to handle both the frequency and size of the events youre ingesting Where youre ingesting data from matters a lot If youre receiving data as its generated, will the upstream system have any issues that might impact your downstream ingestion pipelines For example, suppose a source database goes down When it comes back online and attempts to backfill the lapsed data loads, will your ingestion be able to keep up with this sudden influx of backlogged data Another thing to consider is your ability to handle bursty data ingestion Data generation is very rarely done in a constant fashion, and often ebbs and flows Built in buf fering is required to collect events during rate spikes to prevent data from getting lost Even in a dynamically scalable system, buffering bridges the time while the system scales Buf fering also allows storage systems to accommodate bursts These days, it s worth using managed services that handle the throughput scaling for you While you can manually accomplish these tasks by adding more servers, shards, or workers, this isn t necessarily value add work Much of this heavy lifting is now automated Don t reinvent the data ingestion wheel if you don t have to Reliability and Durability Reliability and durability are especially important in the ingestion stages of data pipelines Reliability entails high uptime and appropriate failover for ingestion systems Durability entails making sure that data isn t lost or corrupted Some data sources e.g., Io T devices may not retain data if it is not correctly ingested Once lost, it is gone for good In this sense, the reliability of ingestion systems leads directly to the durability of generated data If data is ingested, downstream processes can theoretically run late if they break temporarily  Our advice is to evaluate the risks and build an appropriate level of redundancy and self healing based on the impact and cost of losing data Will your ingestion process continue if an A W S zone goes down How about a whole region How about the power grid or the internet Reliability and durability have both direct and indirect costs building a highly redundant system can entail big cloud bills while keeping a team on call 24 hours a day takes a toll on your team and resources Don t assume that you can build a system that will reliably and durably ingest data in every possible scenario Even the massive budget of the U S federal government can t guarantee this In many extreme scenarios, ingesting data actually won t matter  For example, if the internet goes down, there will be little to ingest even if you build multiple data centers in under ground bunkers with independent power  Always evaluate the tradeof fs and costs of reliability and durability  Payload The payload is the dataset youre ingesting and has characteristics such as kind, shape, size, schema and data types, and metadata Let s look at some of these characteristics to get an idea of why this matters Kind The kind of data you handle directly impacts how it s handled downstream in the data engineering lifecycle Kind consists of type and format Data has a typetabular , image, video, text, etc The type directly influences the format of the data, or how it is expressed in bytes, name, and file extension For example, a tabular kind of data may be in formats such as C SV or Parquet, with each of these formats having dif ferent byte patterns for serialization and deserialization Another kind of data is an image, which has a format of J PG or P NG and is inherently binary and unstructured Shape Every payload has a shape that describes its dimensions Data shape is critical across the data engineering lifecycle For instance, an image s pixel and R GB dimensions are necessary for deep learning applications As another example, if youre trying to import a C SV file into a database table, and your C SV has more columns than the database table, youll likely get an error during the import process Here are some examples of the shapes of different kinds of data Tabular The number of rows and columns in the dataset, commonly expressed as M rows and N columns Semi structur ed J SO N The key value pairs, and depth of nesting that occurs with sub-elements Unstructur ed text Number of words, characters, or bytes in the text body Images The width, height, and R GB color depth e.g., 8 bits per pixel Uncompr essed Audio Number of channels e.g., two for stereo, sample depth e.g., 16 bits per sample, sample rate e.g., 48 k Hz, and length e.g., 10003 seconds Size The size of the data describes the number of bytes of a payload A payload may range in size from single bytes to terabytes, and lar ger To reduce the size of a payload, it may be compressed into a variety of formats such as Z IP, T AR, and so on See the discussion of compression in the appendix on Serialization and Compr ession  Schema and data types Many data payloads have a schema, such as tabular and semi structured data As weve mentioned earlier in this book, a schema describes the fields and types of data that reside within those fields Other types of data such as unstructured text, images, and audio will not have an explicit schema or data types, though they might come with technical file descriptions on shape, data and file format, encoding, size, etc You can connect to databases in a variety of ways, i.e file export, change data capture, J DB CO DB C, etc The connection is the easy part of the process The great engineering challenge is understanding the underlying schema Applications or ganize data in a variety of ways, and engineers need to be intimately familiar with the or ganization of the data and relevant update patterns to make sense of it The problem has been somewhat exacerbated by the popularity of O RM object relational mapping, which automatically generates schemas based on object structure in languages such as Java or Python Structures that are natural in an object oriented language often map to something messy in an operational database Data engineers may also need to familiarize themselves with the class structure of application code for this reason Schema is not only for databases As weve discussed, A PIs present their schema complications Many vendor A PIs have nice reporting methods that prepare data for analytics In other cases, engineers are not so lucky and the A PI is a thin wrapper around underlying systems, requiring engineers togain a deep understanding of application internals to use the data This situation is more challenging than dealing with complex data internal to an organization because communication is more dif ficult Much of the work associated with ingesting from source schemas happens in the transformation stage of the data engineering lifecycle, which we discuss in Chapter 8  W eve placed this discussion here because data engineers need to begin studying source schemas as soon they plan to ingest data from a new source Communication is critical for understanding source data, and engineers also have the opportunity to reverse the flow of communication and help software engineers improve data where it is produced W ell return to this topic later in this chapter , in the section on Who youll work with  Detecting and handling schema changes in upstream and downstream systems Schema changes occur frequently in source systems and often are well out of the control of data engineers Examples of schema changes include the following Adding a new column Changing a column type Creating a new table Renaming a column Its becoming increasingly common for ingestion tools to automate the detection of schema changes, and even auto update tar get tables Ultimately , this is something of a mixed blessing Schema changes can still break pipelines downstream of staging and ingestion Engineers must still implement strategies to automatically respond to changes, and alert on changes that cannot be accommodated automatically  Automation is great, but the analysts and data scientists who rely on this data should be informed of the schema changes that violate existingassumptions Even if automation can accommodate a change, the new schema may adversely af fect the performance of reports and models Communication between those making schema changes and those impacted by these changes is as important as reliable automation that checks for schema changes Schema registries In streaming data, every message has a schema, and these schemas may evolve between producers and consumers A schema registry is a metadata repository used to maintain schema and data type integrity in the face of constantly evolving schemas It essentially describes the data model for messages, allowing consistent serialization and deserialization between producers and consumers Schema registries are used in Kafka, A W S Glue, and others Metadata In addition to the obvious characteristics weve just covered, a payload often contains metadata, which we first discussed in Chapter 2  Metadata is data about data  Metadata can be as critical as the data itself One of the major limitations of the early approach to the data lakeor data swamp, which could turn it into a data superfund sitewas a complete lack of attention to metadata W ithout a detailed description of the data, the data itself may be of little value W eve already discussed some types of metadata e.g schema and will address them many times throughout this chapter  Push V ersus Pull Patterns We introduced the concept of push vs pull when we introduced the data engineering lifecycle in Chapter 2  Roughly speaking, a push strategy Figure 4 7 involves a source system sending data to a tar get, while a pull strategy Figure 4 8 entails a tar get reading data directly from a source As we mentioned in that discussion, the lines between these strategies are blurry  Figur e 4 7  Pushing data fr om sour ce to destination Figur e 4 8  Pushing data fr om sour ce to destination We will discuss push versus pull in each subsection on ingestion patterns, and give our opinionated reasoning on when a pattern is push or pull Lets dive in Batch Ingestion Patterns It is often convenient to ingest data in batches This means that data is ingested by either taking a subset of data from a source system, based either on a time interval or size of accumulated data Figure 4 9  Figur e 4 9  Time interval batch ingestion Time interval batch ingestion is extremely common in traditional business E TL for data warehousing This pattern is often used to process data once aday overnight during of f hours to provide daily reporting, but other frequencies can also be used Size based batch ingestion Figure 4 10 is quite common when data is moved from a streaming based system into object storageultimately , the data must be cut into discrete blocks for future processing in a data lake Size based ingestion systems such as Kinesis Firehose can break data into objects based on a variety of criteria, such as size bytes of the total number of events Figur e 4 10  Size based batch ingestion Some commonly used batch ingestion patterns, which well discuss in this section, includeSnapshot or dif ferential extraction File based export and ingestion E TL versus E L T Data migration Snapshot or Differential Extraction Data engineers must choose whether to capture full snapshots of a source system or dif ferential updates W ith full snapshots, engineers grab the full current state of the source system on each update read W ith the dif ferential update pattern, engineers can pull only the updates and changes since the last read from the source system While dif ferential updates are ideal for minimizing network traf fic, tar get storage usage, etc., full snapshot reads remain extremely common due to their simplicity  File Based Export and Ingestion Data is quite often moved between databases and systems using files That is, data is serialized into files in an exchangeable format, and these files are provided to an ingestion system W e consider file based export to be a push based ingest pattern This is because the work of data export and preparation is done on the source system side File based ingestion has several potential advantages over a direct database connection approach For security reasons, it is often undesirable to allow any direct access to backend systems W ith file based ingestion, export processes are run on the data source side This gives source system engineers full control over what data gets exported and how the data is pre processed Once files are complete, they can be provided to the tar get system in a variety of ways Common file exchange methods are object storage i.e Amazon S3, Azure Blob Storage, S FT P , E DI, or S CP  E TL V ersus E L T In Chapter 3 , we introduced E TL and E L T, which are both extremely common ingest, storage, and transformation patterns youll encounter in batch workloads For this chapter , well cover the Extract E and the Load L O parts of E TL and E L T transformations will be covered in Chapter 8 Extract Extract means getting data from a source system While extract seems to imply pulling data, it can also be push based Extraction may also entail reading metadata and schema changes Load Once data is extracted, it can either be transformed E TL before loading it into a storage destination or simply loaded into storage for future transformation When loading data, you should be mindful of the type of system into which youre loading, the schema of the data, and the performance impact of loading Transform Well cover transformations in much more detail in Chapter 8  Know that data can be transformed after it s been ingested, but before it s loaded into storage This is common in classic E TL systems that do in memory transformations as part of a workflow  Its also common with event streaming frameworks such as Kafka, which uses the Kafka Stream A PI to transform and join data before persisting the output to storage Inserts, Updates, and Batch Size Batch oriented systems often perform poorly when users attempt to perform a large number of small batch operations rather than a smaller number of large operations For example, while it is a common pattern to insert one row at a time in a transactional database, this is a bad pattern for many columnar databases as it forces the creation of many small, suboptimal files,and forces the system to run a high number of create object operations Running a lar ge number of small in place update operations is an even bigger problem because it forces the database to scan each existing column file to run the update Understanding the appropriate update patterns for the database youre working with Also, understand that certain technologies are purpose built for high insert rates Systems such as Apache Druid and Pinot can handle high insert rates Single Store can manage hybrid workloads that combine O LA P and O L T P characteristics Big Query performs poorly on a high rate of standard inserts, but extremely well if data is fed in through its stream buffer Know the limits and characteristics of your tools Data Migration Migrating data to a new database or a new environment is not usually trivial, and data needs to be moved in bulk Sometimes this means moving data sizes that are 100s of T Bs or much lar ger, often involving not just the migration of specific tables, but moving entire databases and systems Data migrations probably aren t a regular occurrence in your role as a data engineer , but it s something you should be familiar with As is so often the case for data ingestion, schema management is a key consideration If youre migrating data from one database system to another , say T eradata to Big Query , no matter how closely the two databases resemble each other , there are nearly always subtle dif ferences in how they handle schema Fortunately , it is generally easy to test ingestion of a sample of data and find schema issues before undertaking a full table migration Most modern data systems perform best when data is moved in bulk rather than as individual rows or events File or object storage is often a good intermediate stage for moving data Also, one of the biggest challenges of database migration is not the movement of the data itself, but the movement of data pipeline connections from the old system to the new one Streaming Ingestion Patterns Another way to ingest data is from a stream This means that data is ingested continuously  Let s look at some patterns for ingesting streaming data Types of T ime While time is an important consideration for all data ingestion, it becomes that much more critical and subtle in the context of streaming, where we view data as continuous and expect to consume it shortly after it is produced Let s look at the key types of time youll run into when ingesting datathe time the event is generated, and when it s ingested and processed Figure 4 1 1 Figur e 4 1 1  Event, ingestion, and pr ocess time Event time Event time is the time at which an event is generated in a source system, including the timestamp of the original event itself Upon event creation, there will be an undetermined time lag before the event is ingested and processed downstream Always include timestamps for each phase through which an event travels Log events as they occur , and at each stage of timewhen theyre created, ingested, and processed Use these timestamp logs to accurately track the movement of your data through your data pipelines Ingestion time After data is created, it is ingested somewhere Ingestion time is the time at which an event is ingested from source systems, into a message queue, cache, memory , object storage, a database, or any place else that data is stored see Chapter 6  After ingestion, data may be processed immediately , within minutes, hours, or days, or simply persist in storage indefinitely  We will cover the details of storage in Chapter 8  Processing time Processing time is the time at which data is processed, which is typically some sort of transformation Y oull learn more about various kinds of processing and transformations in Chapter 8  Late arriving data A group of events might occur around the same time frame, but because of various circumstances, might be late in arriving for ingestion This is called late arriving data and is common when ingesting data Y ou should be aware of late arriving data, and the impact on downstream systems and uses For example, if you assume that ingestion or processing time is the same as the event time, you may get some very strange results if your reports or analysis depend upon an accurate portrayal of when events occur  Key Ideas We spend a good deal of time on stream processing in Chapter 8 , where we discuss data transformation W ell quickly introduce key streaming ingestion ideas here and present a more extensive discussion there Streaming ingestion and storage systems Streaming storage collects messages, log entries, or events and makes them available for downstream processing T ypical examples are Apache Kafka, Amazon Kinesis Data Streams, or Google Cloud PubSub Modern streaming storage systems support basic producerconsumer patterns publishersubscriber, but also support replay , i.e., the ability to playback a time range of historic data Producers and consumers Producers write data into streaming storage, while consumers read from the stream In practice, a streaming pipeline may have many consumers and producers at various stages The initial producer is usually the data source itself, whether a web application that writes events into the stream or a swarm of Io T devices Clusters and partition In general, modern steaming systems distribute data across clusters In many cases, data engineers have some control of how the data is partitioned across the cluster through the use of a partition key  Choice of ingestion partition key can be critical in preparing data for downstream consumption, whether for preprocessing that is a key part of the ingestion stage or more complex downstream processing Often, we set upstream processing nodes to align with partitions in the stream messaging system Topics A topic is simply a data stream a streaming storage system can support a large number of separate topics streams simultaneously , just as a relational database supports many tables Streaming Change Data Capture While there are batch versions of change data capture C DC, we are primarily interested in streaming change data capture The primary approaches are C DC by logging, wherein each writes to the database is recorded in logs and read for data extraction, and the database trigger pattern, where the database sends some kind of signal to another systemeach time it makes a change to a table For streaming ingestion, we assume that the C DC process writes into some kind of streaming storage system for downstream processing Real time and Micro-batch Considerations for Downstream Destinations While streaming ingestion generally happens in a stream storage system, the preprocessing part of ingestion e.g data parsing can happen in either a true stream processor or a micro batch system Micro-batch processing is essentially high speed batch processing, where batches might happen every few seconds Apache Spark often processes data in micro-batches, though there are newer more continuous modes as well The micro-batch approach is perfectly suitable for many applications On the other hand, if you want to achieve operational monitoring that is much closer to real time, a continuous approach might be more appropriate Ingestion Technologies Now that weve described some of the major patterns that underlie ingestion in general, we turn our attention to the technologies youll use for data ingestion W ell give you a sample of the types of ingestion technologies youll encounter as a data engineer  Keep in mind the universe of data ingestion technologies is vast and growing daily  Although we will cite common and popular examples, it is not our intention to provide an exhaustive list of technologies and vendors, especially given how fast the discipline is changing Batch Ingestion T echnologies To choose appropriate technologies, you must understand your data sources see Chapter 5  In addition, you need to choose between full replication and change tracking W ith full replication, we simply drop the old data inthe tar get and fully reload from the source Change tracking takes many forms, but often it entails pulling rows based on update timestamp and merging these changes into the tar get In addition, preprocessing should be considered a part of ingestion Preprocessing is data processing that happens before the data is truly considered to be ingested In streaming pipelines, raw events often arrive in a rather rough form directly from a source application and must be parsed and enriched before they can be considered fully ingested In batch processing scenarios, similar considerations apply , with data often arriving as raw string data, then getting parsed into correct types Also, think about Fin Ops and cost management early in the design of your ingestion architecture Fin Ops entails designing systems and human processes to make costs manageable Think about the implications of scaling up your solution as data scales, and design for cost ef ficiency  For example, instead of spinning up full priced on demand E C2 instances in A WS, instead, build in support for E C2 spot instances where this applies Ensure that costs are visible and monitored Direct Database Connection Data can be pulled from databases for ingestion by querying and reading over a network connection Most commonly , this connection is made using O DB C or J DB C O DB C 1 0 was released in 1992  O DB C Open Database Connectivity uses a driver hosted by a client accessing the database to translate commands issued to the standard O DB C A PI into commands issued to the database The database returns query results over the wire, where they are received by the driver and translated back into a standard form, and read by the client For purposes of ingestion, the application utilizing the O DB C driver is an ingestion tool The ingestion tool may pull data through many small queries or a single lar ge query  The ingestion tool might pull data once a day or once every five minutes J DB C Java Database Connectivity was released as a standard by Sun Microsystems in 1997  J DB C is conceptually extremely similar to O DB C a Java driver connects to a remote database and serves as a translation layer between the standard J DB C A PI and the native network interface of the target database It might seem a bit strange to have a database A PI dedicated to a single programming language, but there are strong motivations for this The J VM Java V irtual Machine is standard, portable across hardware architectures and operating systems, and provides the performance of compiled code through a J IT just in time compiler The J VM is far and away from the most popular compiling virtual machine for running code in a portable manner  J DB C provides extraordinary database driver portability  O DB C drivers are shipped as O S and architecture native binaries database vendors must maintain versions for each architectureO S version that they wish to support On the other hand, vendors can ship a single J DB C driver that is compatible with any J VM language Java, Scala, Clojure, Kotlin, etc and J VM data framework i.e Spark J DB C has become so popular that it is also used as an interface for non J VM languages such as Python The Python ecosystem provides translation tools that allow Python code to talk to a J DB C driver running on a local J VM Returning to the general concept of direct database connections, both J DB C and O DB C are used extensively for data ingestion from relational databases V arious enhancements are used to accelerate data ingestion Many data frameworks can parallelize several simultaneous connections and partition queries to pull data in parallel On the other hand, nothing is freeusing parallel connections also increases the load on the source database J DB C and O DB C were long the gold standards for data ingestion from databases However , these connection standards are beginning to show their age for many data engineering applications These connection standards struggle with nested data, and they send data as rows This means that native nested data has to be re encoded as string data to be sent over thewire, and columns from columnar databases must be re serialized as rows Many alternatives have emer ged for lower friction data export As discussed in the section on file based export, many databases now support native file export that bypasses J DB CO DB C and exports data directly in modern formats Alternatively , many databases Snowflake, Big Query , Arango DB, etc.provide direct R ES T A PIs J DB C connections should generally be integrated with other ingestion technologies For example, we commonly use a reader process to connect to a database with J DB C, write the extracted data into multiple objects, then orchestrate ingestion into a downstream system see Figure 4 12  The reader process can run in a fully ephemeral cloud instance or directly in an orchestration system Figur e 4 12  An ingestion pr ocess r eads fr om a sour ce database using J DB C, then writes objects into object storage A tar get database not shown can be trigger ed to ingest the data with an A PI call from an or chestration system S FT P Engineers rightfully cringe at the mention of S FT P occasionally , we even hear instances of F TP being used in production Regardless, S FT P is still a practical reality for many businesses That is, they work with partner businesses that either consume or provide data using S FT P , and are not willing to rely on other standards T o avoid data leaks, security analysis is critical in these situations If S FT P is required, it can be combined with extra network security , such as only allowing authorized I P addresses to access the network, or even passing all traf fic over a V PN Object storage In our view , object storage is the most optimal and secure way to handle file exchange Public cloud storage implements the latest security standards, has an extremely robust track record, and provides high performance movement of data Well discuss object storage much more extensively in Chapter 6  At a basic level, object storage is a multi tenant system in public clouds, and it supports storing massive amounts of data This makes object storage ideal for moving data in and out of data lakes, moving data between teams, and transferring data between or ganizations Y ou can even provide short term access to an object with a signed U RL, giving a user short term permission S CP S CP secure copy is a file exchange protocol that runs over an S SH connection S CP can be a secure file transfer option if it is configured correctly  Again, adding additional network access control defense in depth to enhance S CP security is highly recommended E DI Another practical reality for data engineers is E DI Electronic Data Interchange The term is vague enough that it could refer to any method of data movement In practice, it is used in modern parlance to refer to rather archaic means of file exchange, such as by email, or on a flash drive Data engineers will find that some of their data sources do not support more modern means of data transport, often due to archaic I T systems, or human process limitations They can at least enhance E DI through automation For example, they can set up a cloud based email server that saves files onto company object storage as soon as they are received This can trigger orchestration processes to ingest and process data This is much more robust than an employee downloading the attached file and manually uploading it to an internal system, something that we still frequently see Databases and file export Engineers should be aware of how the source database systems handle file export For many transactional systems, export involves lar ge data scans that put a significant load on the database Source system engineers must assess when these scans can be run without af fecting application performance and might opt for a strategy to mitigate the load Export queries can be broken down into smaller exports by querying over key ranges or one partition at a time Alternatively , a read replica can reduce load Read replicas are especially appropriate if exports happen many times a day , and exports coincide with high source system load Some modern cloud databases are highly optimized for file export Snowflake allows engineers to set up a warehouse compute cluster just for export, with no impact on other warehouses running analytics Big Query handles file exports using a backend service, completely independent of its query engine A W S Redshift gives you the ability to issue a S QL U NL OA D command to dump tables into S3 object storage In all cases, file export is the best practice approach to data movement, recommended by the vendor over J DB CO DB C connections for cost and performance reasons Practical issues with common file formats Engineers should also be aware of the file formats that theyre using to export At the time of this writing, C SV is nearly universal, but also extremely error prone Namely , C SV s default delimiter is also one of the most common characters in the English languagethe comma But it gets worse C SV is by no means a uniform format Engineers must stipulate delimiter , quote characters, escaping, etc to appropriately handle the export of string data C SV also doesn t natively encode schema information, nor does it directly support modern nested structures C SV file encoding and schema information must be configured in the tar get system to ensure appropriate ingestion Auto detection is a convenience feature provided in many cloud environments but is not appropriate for production ingestion As a best practice, engineers should record C SV encoding and schema details in file metadata More modern export formats include Parquet, A vro, Arrow , and O RC or J SO N These formats natively encode schema information and handle arbitrary string data with no special intervention Many of them also handle nested data structures natively , so that J SO N fields are stored using internal nested structures rather than simple strings For columnar databases, columnar formats Parquet, Arrow , O RC allow more ef ficient data export because columns can be directly transcoded between formats, a much lighter operation than pivoting data into rows C SV , Avro Modern formats are also generally more optimized for query engines The Arrow file format is designed to map data directly into processing engine memory , providing high performance in data lake environments The disadvantage of these newer formats is that many of them are not natively supported by source systems Data engineers are often forced to work with C SV data, and then build robust exception handling and error detection to ensure data quality on ingestion See the appendix on serialization and compr ession for a more extensive discussion of file formats S SH Strictly speaking, S SH is not an ingestion strategy , but a protocol that is used in conjunction with other ingestion strategies W e use S SH in a few different ways First, S SH can be used for file transfer with S CP , as mentioned earlier  Second, S SH tunnels are used to allow secure, isolated connections to databases Application databases should never be directly exposed on the internet Instead, engineers can set up a bastion host, i.e., an intermediate host instance that can connect to the database in question This host machine is exposed on the internet, although locked down for extremely limited access from only specified I P addresses to specified ports T o connect to the database, a remote machine first opens an S SH tunnel connection to the bastion host, then connects from the host machine to the database Shell The shell is the interface by which you may execute commands to ingest data In practice, the shell can be used to script workflows for virtually any software tool, and shell scripting is still used extensively in ingestion processes For example, a shell script might read data from a database, reserialize the data into a dif ferent file format, upload it to object storage, and trigger an ingestion process in a tar get database While storing data on a single instance or server is not highly scalable, many of our data sources are not particularly lar ge, and such approaches work just fine In addition, cloud vendors generally provide robust C LI based tools It is possible to run complex ingestion processes simply by issuing commands to the A W S C LI As ingestion processes grow more complicated, and the service level agreement grows more stringent, engineers should consider moving to a true orchestration system instead A PIs The bulk of softwar e engineering is just plumbing Karl Hughes As we mentioned in Chapter 5 , A PIs are a data source that continues to grow in importance and popularity  A typical or ganization may have hundreds of external data sources SA AS platforms, partner companies, etc The hard reality is that there is no true standard for data exchange over A PIs Data engineers can expect to spend a significant amount of time reading documentation, communicating with external data owners, and writing and maintaining A PI connection code Three trends are slowly changing this situation First, many vendors provide A PI client libraries for various programming languages that remove much of the complexity of A PI access Google was ar guably a leader in this space, with Ad W ords client libraries available in various flavors Many other vendors have since followed suit Second, there are numerous data connector platforms available now as proprietary software, open source, or managed open source These platforms provide turnkey data connectivity to many data sources forunsupported data sources, they of fer frameworks for writing custom connectors See the section below on managed data connectors The third trend is the emer gence of data sharing discussed in Chapter 5 , i.e., the ability to exchange data through a standard platform such as Google Big Query , Snowflake, Redshift, or Amazon S3 Once data lands on one of these platforms, it is straightforward to store it, process it, or move it somewhere else Data sharing has had a significant and rapid impact in the data engineering space For example Google now supports direct sharing of data from a variety of its advertising and analytics products Google Ads, Google Analytics, etc to Big Query  Any business that advertises online can now reallocate software development resources away from Google product data ingestion and focus instead on everything downstream When data sharing is not an option and direct A PI access is necessary , don t reinvent the wheel While a managed service might look like an expensive option, consider the value of your time and the opportunity cost of building A PI connectors when you could be spending your time on higher value work In addition, many managed services now support building custom A PI connectors This may take the form of providing A PI technical specifications in a standard format, or of writing connector code that runs in a serverless function framework e.g A W S Lambda while letting the managed service handle the details of scheduling and synchronization Again, these services can be a huge time saver for engineers, both for development and ongoing maintenance Reserve your custom connection work for A PIs that aren t well supported by existing frameworksyou will find that there are still plenty of these to work on There are two main aspects of handling custom A PI connections software development and ops Follow software development best practices you should use version control, continuous delivery , automated testing, etc In addition to following Dev Ops best practices, consider an orchestration framework, which can dramatically streamline the operational burden of data ingestion Webhooks Webhooks, as we discussed in Ch 5 , are often referred to as reverse A PIs For a typical R ES T data A PI, the data provider gives engineers A PI specifications that they use to write their data ingest code The code makes requests and receives data in responses With a webhook Figure 4 13 , the data provider documents an A PI request specification, but this specification is implemented by the data consumer  The consumer sets up an endpoint, and the data source calls the endpoint, delivering data in requests T ypically , webhooks deliver one event at a time the consumer is responsible for ingesting each request and handling data aggregation, storage, processing, etc Webhook based data ingestion architectures can be brittle, dif ficult to maintain, and inef ficient Data engineers can build more robust webhook architectureswith lower maintenance and infrastructure costsby using appropriate of f the shelf tools A common pattern uses a serverless function framework i.e A W S Lambda to receive incoming events, a streaming message bus to store and buf fer messages i.e A W S Kinesis, a stream processing framework to handle real time analytics i.e Apache Flink, and an object store for long term storage i.e Amazon S3 Figur e 4 13  A basic webhook ingestion ar chitectur e built fr om cloud services Using serverless services r educes operational over head Youll notice that this architecture does much more than simply ingesting the data This underscores the fact that ingestion is highly entangled with the other stages of the data engineering lifecycleit is often impossible to define your ingestion architecture without also making decisions about storage and processing We note that this architecture can be simplified if one does not need real time analytics However , trying to simplify too much creates new problems For instance, writing directly from a Lambda serverless function to S3 might produce a massive number of objects, with an extremely high request rate to S3, and potentially high costs This is where data engineers need to understand the critical role played by each component in the pipeline T o simplify the architecture in a more sane way , we could remove the streaming analytics system, accumulate events into the messaging bus, and periodically query a lar ge chunk of data over a time range to write it into S3 Better yet, we could use a serverless tool like Kinesis Firehose, which automates the process of event rollup to create S3 objects of a reasonable size Legacy data flow management tools We would be remiss not to mention data flow management tools such as Informatica and T alend, to name a few  This category of tool is designed to serve several stages of the data engineering lifecycle, including ingestion, and processing In practice, these systems also integrate some orchestration capabilities See the undercurrents section in this chapter , and corresponding discussions throughout the book Such systems include connectors to a variety of sources, and allow users to define data pipelines, often using a visual interface that represents processing stages with icons Processing can be set on a schedule Each processing stage runs once its upstream dependencies are met Of course, there are generally ingestion steps at the beginning of each pipeline These systems typically employ two main processing models W e have already discussed both E TL and E L T Systems based on the E TL modelingest data internally , transform it and write it back to external storage, such as a cloud data warehouse T ools that employ the E L T model handle most processing in an external system such as a cloud data warehouse Web interface Web interfaces for data access remain a practical reality for data engineers We frequently run into situations where not all data and functionality in a S AA S software as a service platform is exposed through automated interfaces such as A PIs and file drops Instead, someone must manually access a web interface, generate a report and download a file to a local machine Potentially , some automation can be applied by using web interface simulation frameworks such as simulation, but web interfaces remain an area of high friction for data ingestion For example, you might be able to automate the clicking of a web interface with Selenium But whenever possible, find another approach Managed Data Connectors In the section on ingesting data from A PIs, we mention the emer gence of managed data connector platforms and frameworks The goal of these tools is to provide a standard set of connectors that are available out of the box to spare data engineers much detailed plumbing to connect to a particular source For now , Fivetran has emer ged as a category leader in this space, but there are many up and coming competitors, both proprietary and open source Generally , proprietary options in the space allow users to set a tar get and source, set permissions and credentials, configure an update frequency , and begin syncing data Data syncs are fully managed and monitoredif data synchronization fails, users will receive an alert The open source options in the space are usually part of a product, and available in supported and unsupported flavors Stitch started this trend by introducing the Singer Python framework Singer provided a standard set of abstractions for creating data connectors Engineers could then use Singerto manage and run their data synchronization or send their connectors to Stitch to run within their fully managed platform V arious competitors to Stitch have emer ged Some utilize the Singer framework others, such as Airbyte and Meltano, have introduced new open source data connector frameworks We note also that even proprietary data connector engines allow the creation of custom connections with some coding ef fort Fivetran allows this through various serverless function frameworks, including A W S Lambda and its similar competitors Engineers write function code that receives a request, pulls data, and returns it to Fivetran Fivetran takes care of orchestration and synchronization to the tar get database These are just a few of many options for managed connectors, and we expect that this space of S AA S managed services and O SS that support the development of custom connectors will continue to grow  Web scraping Web scraping is another common data source for engineers Any search engine must scrape the web to analyze links, build an index, etc However , many other businesses also rely on web scraping to survey web content that may be relevant to their business activities A full discussion of web scraping is well beyond the scope of this book There are many books specific to the subject W e recommend that readers look at web scraping books in O Reilly s catalog and beyond There are also numerous online resources, in video tutorials, blog posts, etc Here is some top level advice to be aware of before undertaking any web scraping project First, learn to be a good citizen Don t inadvertently create a denial of service attack, and don t get your I P address blocked Understand how much traf fic youre generating and pace your web crawling activities appropriately  Just because you can spin up thousands of simultaneous Lambda functions to scrape doesn t mean you should in fact, excessive web scraping could lead to the disabling of your A W S account Second, be aware of the legal implications of your activities Again, generating denial of service attacks has legal implications Activities that violate terms of service may cause legal headaches for your employer  Web scraping has interesting implications for the processing stage of the data engineering lifecycle there are various things that engineers should think about at the beginning of a web scraping project What do you intend to do with the data Are you just pulling key fields from the scraped H TM L using Python code, then writing these values to a database Do you intend to maintain the full H TM L code of the scraped websites and process this data using a framework like Spark These decisions may lead to very different architectures downstream of ingestion Transfer appliances for data migration For truly big data 100 T B or more, transferring data directly over the internet may be a slow and extremely expensive processat this scale, the fastest, most ef ficient way to move data is not over the wire, but by truck Cloud vendors of fer the ability to send your data via a physical box of hard drives Simply order a storage devicecalled a transfer applianceload your data from your servers, then send it back to the cloud vendor who will upload your data into their cloud The suggestion is to consider using a transfer appliance if your data size hovers around 100 T B On the extreme end, A W S even of fers Snowmobile, a transfer appliance in a semi-trailer  Snowmobile is intended to lift and shift an entire data center , where data sizes are in the petabytes or greater  Transfer appliances are particularly useful for creating hybrid cloud or multi cloud setups For example, Amazon s data transfer appliance A W S Snowball supports both import and export T o migrate into a second cloud, users can export their data into a Snowball device, then import it into a second transfer appliance to move data into G CP or Azure This might sound like an awkward process, but even when it s feasible to push data over the internet between clouds, data egress fees make this an extremely expensive proposition, and physical transfer appliances are a much cheaper alternative when the data volumes are significant Keep in mind that transfer appliances and data migration services are one time data ingestion events, and not suggested for ongoing workloads If youve got workloads that require constant movement of data in either a hybrid or multi cloud scenario, your data sizes are presumably batching or streaming much smaller data sizes, on an ongoing basis Streaming Ingestion T echnologies There are a few main technologies for ingestion and temporary storage of streaming data As we discuss streaming technologies, it is more useful to look at the class of all frameworks that collect and buf fer messages and consider them in terms of various features they of fer Well describe major characteristics that data engineers should consider for these systems, and cite a few examples This discussion is by no means exhaustivewe discuss only a few examples and a handful of standard technologies But these characteristics are a good framework for research as you choose a streaming ingestion technology  Decide what characteristics you need for your data applications and evaluate technologies according to your needs Wed like to note that the terminology in the streaming ingestion space can be confusing Apache Kafka has variously described itself as a distributed event streaming platform or a distributed commit log Per Amazon, Kinesis Data Streams is a serverless streaming data service Rabbit MQ calls itself a message broker  Apache Kafka may or may not be a message queue depending on which author you refer to And so on Focus on the bigger picture and context of where various technologies fit in terms of functionality and utility and take vendor descriptions with a grain of salt Horizontal scaling For our purposes, were mostly interested in streaming ingest frameworks that support horizontal scaling, which grows and shrinks the number of nodes based on the demand placed upon your system Horizontal scaling enhances data scale, reliability , and durability  In some cases, a single node solution may work just fine for small streams but consider the reliability and durability implications Stream partitions Kafka and Kinesis partition shard streams to support horizontal scaling, and they allow users to specify an explicit partition key that uniquely determines which shard a message belongs to Each consuming server can consume from a specific shard Google Cloud PubSub hides all details of partitioning subscribers simply read messages at the level of a topic Explicit partitioning is a blessing and a curse, entailing engineering advantages and challengessee the discussion of stream partitions in Chapter 8  Subscriber pull and push Kafka and Kinesis only support pull subscriptions That is, subscribers read messages from a topic and confirm when they have been processed In addition, to pull subscriptions, PubSub and Rabbit MQ support push subscriptions, allowing these services to write messages to a listener  Pull subscriptions are the default choice for most data engineering applications, but you may want to consider push capabilities for specialized applications Note that pull only message ingestion systems can still push if you add an extra layer to handle this Operational overhead As with any data engineering technology , operational overhead is a key consideration W ould you rather manage your streaming data pipelines, or outsource this work to a team of dedicated engineers On one end of the spectrum, Google Cloud PubSub is a fully managed service with no knobs to turn just create a topic and a consumer , give a payload, and youre good to go On the other end, Apache Kafka is packaged in a variety of ways, all the way from raw , hot of f the presses open source, to a fully managed serverless of fering from Confluent Autoscaling Autoscaling features vary from platform to platform Confluent Cloud will autoscale up to 100 MBs, after which some intervention is required to set the scale More ef fort is required if you run your clusters Amazon Kinesis Data Streams recently added a feature that automatically scales the number of shards PubSub is fully autoscaling Message size This is an easily overlooked issue one must ensure that the streaming framework in question can handle the maximum expected message size For example, Amazon Kinesis supports a maximum message size of 1 M B Kafka defaults to this maximum size but can be configured for a maximum of 20 M B or more Configurability may vary on managed service platforms Replay Replay is a key capability in many streaming ingest platforms Replay allows readers to request a range of messages from the history  A system must support replay for us to truly consider it a streaming storage system Rabbit MQ deletes messages once they are consumed by all subscribers storage capabilities are for temporary buf fering only for long term storage, a separate tool is required to consume messages and durably write them Replay also essentially lets us hybridize batch and stream processing in one system A key parameter for engineering decisions is maximum message retention time Google Cloud PubSub supports retention periods of up to 7 days, Amazon Kinesis Data Streams retention can be turned up to 1 year , and Kafka can be configured for indefinite retention, limited by available disk space Kafka also supports the option to write older messages to cloud object storage, unlocking virtually unlimited storage space and retention Fanout Fanout entails having more than one consumer per data stream This is very useful in the context of complex streaming applications, where we mightwant to feed the same data to multiple tar gets Note that this is dif ferent from having multiple shards and consuming these on dif ferent downstream servers Fanout entails multiple consumers per shar d, where each consumer gets its bookmark determining the current position in the stream Each streaming ingestion platform has its limits for fanout For example, a Kinesis Data Stream supports up to 20 consumers Exactly once delivery In some cases, streaming ingestion systems such as PubSub may send events to consumers more than once This is known as at least once delivery and is a consequence of consistency challenges in a distributed system Kafka recently added support for exactly once delivery  However , before relying on this feature, make sure that you read the documentation carefully to understand the exact configuration requirements and performance implications Also, it s important to realize that there are various ways for records to be processed multiple times in a streaming system even if your streaming storage system support exactly once delivery  For example, if a publisher crashes after writing a record into the stream before receiving confirmation that the record was consumed, it may write a second copy of the record when it comes back online And if a subscriber crashes after processing a record but before confirming to the storage system that it has completed processing, the record may get read a second time In general, think through the implications of duplicate records in your stream processing applications Designing for idempotency will allow your system to properly deduplicate records On the other hand, an occasional duplicate record may not be an issue in your system Record delivery order The notion of record delivery order is challenging in a distributed system Amazon Kinesis orders records in single shards, but this does not provide any guarantees for behavior across a whole topic, especially as the number of shards increases PubSub provides no ordering guarantees instead, 1 2engineers are advised to use a tool like Google Cloud Dataflow Apache Beam if they want to order records We give essentially the same advice that we gave in the context of at least once delivery Understand the ordering characteristics of your streaming ingest system and think through the implications of data arriving out of order  Stream processing Some streaming storage systems support direct processing without using an external processing tool See Chapter 8  For example, Kafka supports a variety of operations with the K SQ L query language Google Cloud PubSub and Kinesis Data Streams rely on external tools for processing Streaming D AGs Apache Pulsar has introduced an enhanced notion of stream processing that can be extremely useful for data engineers W ith Kafka, engineers can potentially stitch topics together into a D AG directed acyclic graph to realize complex data processing, but this would also require custom services and code Pulsar builds in streaming D AGs as a core abstraction, supporting complex processing and transformations without ever exiting the Pulsar system Multisite and multiregional It is often desirable to integrate streaming across several locations for enhanced redundancy and to consume data close to where it is generated For example, it might be desirable to have streaming storage running across several regions to improve latency and throughput for messages sent by an Io T swarm with millions of devices An Amazon Kinesis Data Stream runs in a single region Google Cloud PubSub Global Endpoints allow engineers to create a single endpoint, with Google automatically storing data in the nearest region While Kafka supports a notion of site to site replication through Mirror Maker , the corearchitecture of Pulsar is designed to specifically support multi cluster use cases Who Youll Work With Data ingestion sits at several or ganizational boundaries In the development and management of data ingestion pipelines, data engineers will work with both data producers and data consumers Upstream Data Producers In practice, there is often a significant disconnect between those responsible for generating data typically software engineersand the data engineers who will prepare this data for analytics and data science Software engineers and data engineers usually sit in separate or ganizational silos if they think about data engineers at all, they usually see them simply as downstream consumers of the data exhaust from their application, not as stakeholders We see this current state of af fairs as a problem, but also a significant opportunity  Data engineers can improve the quality of the data that they ingest by inviting software engineers to be stakeholders in data engineering outcomes The vast majority of software engineers are well aware of the value of analytics and data science but don t necessarily have aligned incentives to directly contribute to data engineering ef forts However , simply improving communication is a great first step Often, software engineers have already identified potentially valuable data for downstream consumption Opening a channel of communication encourages software engineers to get data into shape for consumers, and to communicate about data changes to prevent pipeline regressions Beyond communication, data engineers can highlight the contributions of software engineers to team members, executives, and especially product managers Involving product managers in the outcome and treating downstream data processed as part of a product encourages them to allocatescarce software development to collaboration with data engineers Ideally , software engineers can work partially as extensions of the data engineering team this allows them to collaborate on a variety of projects, such as creating an event driven architecture to enable real time analytics Downstream Data Consumers Who is the ultimate customer for data ingestion Data engineers tend to focus on data practitioners and technology leaders such as data scientists, analysts, and chief technical of ficers They would do well to also remember their broader circle of business stakeholders such as marketing directors, vice presidents over the supply chain, chief executive of ficers, etc Too often, we see data engineers pursuing sophisticated projects real time streaming buses, big data systems while digital marketing managers next door are left downloading Google Ads reports manually  View data engineering as a business, and recognize who your customers are Often, there is significant value in basic automation of ingestion processes, especially for or ganizations like marketing that control massive budgets and sit at the heart of revenue for the business Basic ingestion work may seem boring, but delivering value to these core parts of the business will open up more budget and more exciting opportunities for data engineering in the long term Data engineers can also invite more executive participation in this collaborative process For good reason, the notion of data driven culture is quite fashionable in business leadership circles, but it is up to data engineers and other data practitioners to provide executives with guidance on the best structure for a data driven business This means communicating the value of lowering barriers between data producers and data engineers while supporting executives in breaking down silos and setting up incentives that will lead to a more unified data driven culture Once again, communication is the watchword Honest communication early and often with stakeholders will go a long way to making sure your data ingestion adds value Undercurrents Virtually all the undercurrents touch the ingestion phase, but well emphasize the most salient ones here Security Moving data introduces security vulnerabilities because you have to move data between locations The last thing you want is for the data to be captured or compromised while it is being moved Consider where the data lives and where it is going Data that needs to move within your V PC should use secure endpoints, and never leave the confines of the V PC If you need to send data between the cloud and an on premises network, use a V PN or a dedicated private connection This might cost money , but the security is a good investment If your data traverses the public internet, make sure the transmission is encrypted Don t ever send data unencrypted over the public internet Data Management Naturally , data management begins at data ingestion This is the starting point for lineage and data cataloging from this point on, data engineers need to think about master data management, ethics, privacy , compliance, etc Schema changes Schema changes remain, from our perspective, an unsettled issue in data management The traditional approach is a careful command and control review process W orking with clients at lar ge enterprises, we have been quoted lead times of six months for the addition of a single field This is an unacceptable impediment to agility  On the opposite end of the spectrum is Fivetran, where schema changes are completely automatic Any schema change in the source triggers tar gettables to be recreated with the new schema This solves schema problems at the ingestion stage, but can still break downstream pipelines One possible solution, which the authors have ruminated on for a while, is an approach pioneered by Git version control When Linus T orvalds was developing Git, many of his choices were inspired by the limitations of C VS Concurrent V ersions System C VS is completely centralizedit supports only one current of ficial version of the code, stored on a central project server  To make Git a truly distributed system, T orvalds used the notion of a tree, where each developer could maintain their processed branch of the code and then mer ge to or from other branches A few years ago, such an approach to data was unthinkable Data warehouse systems are typically operated at close to maximum storage capacity  However , in big data and cloud data warehouse environments, storage is cheap One may quite easily maintain multiple versions of a table with dif ferent schemas and even dif ferent upstream transformations T eams can maintain multiple development versions of a table using orchestration tools such as Airflow schema changes, upstream transformation and code changes, etc can appear in development tables before of ficial changes to the main table Data ethics, privacy , and compliance Clients often ask for our advice on encrypting sensitive data in databases This generally leads us to ask a very basic question do you need the sensitive data that youre trying to encrypt As it turns out, in the process of creating requirements and solving problems, this question often gets overlooked Data engineers should train themselves to always ask this question when setting up ingestion pipelines They will inevitably encounter sensitive data the natural tendency is to ingest it and forward it to the next step in the pipeline But if this data is not needed, why collect it at all Why not simply drop sensitive fields before data is stored Data cannot leak if it is never collected Where it is truly necessary to keep track of sensitive identities, it is common practice to apply tokenization to anonymize identities in model training and analytics But engineers should look at where this tokenization is applied If possible, hash data at ingestion time In some cases, data engineers cannot avoid working with highly sensitive data Some analytics systems must present identifiable sensitive information Whenever they handle sensitive data, engineers must act under the highest ethical standards In addition, they can put in place a variety of practices to reduce the direct handling of sensitive data Aim as much as possible for touchless pr oduction where sensitive data is involved This means that engineers develop and test code on simulated or cleansed data in development and staging environments, but code deployments to production are automated Touchless production is an ideal that engineers should strive for , but situations inevitably arise that cannot be fully solved in development and staging environments Some bugs may not be reproducible without looking at the live data that is triggering a regression For these cases, put a broken glass process in place, i.e., require at least two people to approve access to sensitive data in the production environment This access should be tightly scoped to a particular issue, and come with an expiration date Our last bit of advice on sensitive data be wary of naive technological solutions to human problems Both encryption and tokenization are often treated like privacy magic bullets Most modern storage systems and databases encrypt data at rest and in motion by default Generally , we don t see encryption problems, but data access problems Is the solution to apply an extra layer of encryption to a single field, or to control access to that field After all, one must still tightly manage access to the encryption key  There are legitimate use cases for single field encryption, but watch out for ritualistic applications of encryption On the tokenization front, use common sense and assess data access scenarios If someone had the email of one of your customers, could they easily hash the email and find the customer in your data Thoughtlesslyhashing data without salting and other strategies may not protect privacy as well as you think Data Ops Reliable data pipelines are the cornerstone of the data engineering lifecycle When they fail, all downstream dependencies come to a screeching halt Data warehouses and data lakes aren t replenished Data scientists and analysts can t effectively do their jobs the business is forced to fly blind Ensuring your data pipelines are properly monitored is a key step toward reliability and ef fective incident response If there s one stage in the data engineering lifecycle where monitoring is critical, it s in the ingestion stage Weak or nonexistent monitoring means the pipelines may or may not be working In our work, weve seen countless examples of reports and machine learning models being generated from stale data In one extreme case, an ingestion pipeline failure wasn t detected for six months One might question the concrete utility of the data in this instance This was very much avoidable through proper monitoring What should you monitor Uptime, latency , data volumes processed are a good place to start If an ingestion job fails, how will you respond This also applies to third party services In the case of these services, what youve gained in terms of lean operational ef ficiencies reduced headcount is replaced by systems you depend upon being outside of your control If youre using a third party service cloud, data integration service, etc, how will you be alerted if there s an outage What s your response plan in the event a service you depend upon suddenly goes of fline Sadly , there s not a universal response plan for third party failures If you can failover to other servers discussed in the data architecture section as well, preferably in another zone or region, definitely set this up If your data ingestion processes are built internally , do you have the correct testing and deployment automation to ensure the code will function in production And if the code is buggy or fails, can you roll back to a working versionData quality tests We often refer to data as a silent killer  If quality , valid data is the foundation of success in modern business, using bad data to make decisions is much worse than having no data at all bad data has caused untold damage to businesses Data is entropic it often changes in unexpected ways without warning One of the inherent dif ferences between Dev Ops and Data Ops is that we only expect software regressions when we deploy changes, while data often presents regressions on its own Dev Ops engineers are typically able to detect problems by using binary conditions Has the request failure rate breached a certain threshold How about response latency In the data space, regressions often manifest as subtle statistical distortions Is a change in search term statistics a result of customer behavior Of a spike in bot traf fic that has escaped the net Of a site test tool deployed in some other part of the company Like systems failures in the world of Dev Ops, some data regressions are immediately visible For example, in the early 2000s, Google provided search terms to websites when users arrived from search In 201 1 , not provided started appearing as a search term in Google Analytics  Analysts quickly saw not provided bubbling to the tops of their reports The truly dangerous data regressions are silent and can come from inside or outside a business Application developers may change the meaning of database fields without adequately communicating with data teams Changes to data from third party sources may go unnoticed In the best case scenario, reports break in obvious ways Often, business metrics are distorted unbeknownst to decision makers Traditional data testing tools are generally built on simple binary logic Are nulls appearing in a non nullable field Are new , unexpected items showing up in a categorical column Statistical data testing is a new realm, but one is likely to grow dramatically in the next five years 3 Orchestration Ingestion generally sits at the beginning of a lar ge and complex data graph given that ingestion is the first stage of the data engineering lifecycle, ingested data will flow into many more data processing steps, and data from many sources will flow and mingle in complex ways As weve emphasized throughout this book, orchestration is a key process for coordinating these steps Organizations in an early stage of data maturity may choose to deploy ingestion processes as simple scheduled cron jobs However , it is important to recognize that this approach is brittle and that it can slow the velocity of data engineering deployment and development As data pipeline complexity grows, true orchestration is necessary  By true orchestration, we mean a system capable of scheduling full task graphs rather than individual tasks An orchestration can start each ingestion task at the appropriate scheduled time Downstream processing and transform steps begin as ingestion tasks are completed Further downstream, processing steps lead to further processing steps Software Engineering The ingestion stage of the data engineering lifecycle is engineering intensive This stage sits at the edge of the data engineering domain and often interfaces with external systems, where software and data engineers have to build a variety of custom plumbing Behind the scenes, ingestion is incredibly complicated, often with teams operating open source frameworks like Kafka or Pulsar , or in the case of some of the biggest tech companies, running their own forked or homegrown ingestion solutions As weve discussed in this chapter , various developments have simplified the ingestion process, such as managed data ingest platforms like Fivetran and Airbyte, and managed cloud services like A WS Lambda and Kinesis Data engineers should take advantage of the best available toolsespecially managed ones that do a lot of the heavy lifting for youbut also develop high competency in softwaredevelopment Even for simple serverless functions, it pays to use proper version control and code review processes and to implement appropriate tests When writing software, your code needs to be decoupled A void writing monolithic systems that have tight dependencies on the source or destination systems For example, avoid writing G ET queries to an R DB MS with an O RM This pattern tightly couples your ingestion pull to the O RM, which is tightly coupled to the R DB MS Conclusion Okay , weve made it to the end of ingestion In your work as a data engineer , ingestion will likely consume a significant part of your ener gy and effort At heart, ingestion is plumbing, connecting pipes to other pipes, ensuring that data flows consistently and securely to its destination At times, the minutiae of ingestion may feel tedious, but without ingestion, the interesting applications of data analytics, machine learning, etc cannot happen As weve emphasized, were also in the midst of a sea change toward streaming data pipelines This is an opportunity for data engineers to discover interesting applications for streaming data, communicate these to the business and deploy exciting new technologies 1 .confluent.ioblogsimplified robust exactly one semantics in kafka- 2 5 2 idempotent data transformations518 3 gdark google search terms not provided one year -laterAbout the Authors Joe Reis is a business minded data nerd who s worked in the data industry for 20 years, with responsibilities ranging from statistical modeling, forecasting, machine learning, data engineering, data architecture, and almost everything else in between Joe is the C EO and Co-Founder of Ternary Data, a data engineering and architecture consulting firm based in Salt Lake City , Utah In addition, he volunteers with several technology groups and teaches at the University of Utah In his spare time, Joe likes to rock climb, produce electronic music, and take his kids on crazy adventures Matt Housley is a data engineering consultant and cloud specialist After some early programming experience with Logo, Basic and 6502 assembly , he completed a Ph D in mathematics at the University of Utah Matt then began working in data science, eventually specializing in cloud based data engineering He co founded T ernary Data with Joe Reis, where he leverages his teaching experience to train future data engineers and advise teams on robust data architecture Matt and Joe also pontificate on all things data on The Monday Morning Data Chat.\n",
      "\n",
      "\n",
      "\n",
      "S RH University Heidelberg Electrical Engineering B Eng Status 1120231 Vanessa Lehr Your contact person 49 6221 6799 799 studyinheidelbergsrh.de Prof Dr Felix Mller Study Programme Director felix.moellersrh.de Your motivation You are ready for progress As an electrical engineer, you will be directly involved in the devel opment of innovative products, such as the development of hard ware and software for electric vehicles, the programming of mobile devices, and the control of high speed trains and medical devices Your prospects You drive innovation in the modern information society Electrical engineers are being sought and recruited for innovative tasks in all sectors Electromobility is one of the fields in which you will be in great demand in the years ahead In your future profession, you will typically focus on one major project or several small sub-projects You will often be involved in the entire process from planning and development to the construction design and manufacture of new devices, equipment and systems in the field of electronics and electrical engineering Your potential fields of work include Research and development Planning and project planning Testing and quality control inspection bodies such as the technical inspection agency T V Technical sales sale of technical products Electrical Engineering Bachelor of Engineering SR H University Heidelberg S RH University Heidelberg Electrical Engineering B Eng Status 1120232 At a glance Degree Bachelor of Engineering B Eng Credit points 210 E CT S Start of academic programme Winter semester Duration of study 7 semesters Tuition fees 690 per month One time enrolment fee of 750 One time enrolment fee of 1 , 000 for applicants from Non E EA countries without permanent residence permit State recognition Accredited and state recognised Admission requirements A general higher education entrance qualification Abitur, a subject restricted higher education entrance qualification fachgebundene Hochschulreife or an entrance qualification for studies at universities of applied sciences Fachhochschulreife Alternatively at least two years of relevant vocational training and a minimum of three years professional experience, plus the aptitude test Or a master craftsmans diploma Meisterbrief or a technician qualification Technikerabschluss Successful participation in the selection process Course content and skills You acquire the skills required to successfully manage projects Once you have graduated with a degree in Electrical Engineering from S RH University Heidelberg, you will have all the skills you need to operate successfully in the project planning, development and maintenance of complex systems You will also have the opportunity to specialise in areas of high social relevance electromobility, energy technologies and mobile robotics You engage in practice based learning Besides enabling you to acquire the expertise you need, we specifically prepare you for entering the workforce from the very beginning working alone or as part of a team, you solve practical problems in a variety of exercises and interdisciplinary projects As a result, you will be prepared for the challenges of the future The entire range of engineering positions in industry and public institutions will be open to you We continually adapt your teaching content to current developments By studying at our university, you can therefore be sure of having excellent opportunities in a future oriented job market Apply now Scan the Q R code SR H University Heidelberg Electrical Engineering B Eng Status 1120233 Your study programme Instead of getting bogged down with lots of subjects, you concentrate fully on a five week block module in each case Each block concludes with an examination This sustainable process helps you to achieve optimal learning outcomes Semester 01 Mathematics and Natural Sciences I Mathematics and Natural Sciences I IFoundations of Electrical Engineering I Foundations of Electrical Engineering I I Examination Credits Kls Prs I 8 E CT S Kls Kls I 8 E CT S Kls Kls I 8 E CT S Kls I 8 E CT S 02 Foundations of Computer Science Innovation and Economics Analogue Electronics Electronics Development Examination Credits T PL I 8 E CT S St A I 8 E CT S Kls Lab I 8 E CT S P A I 6 E CT S 03 Production and Project Management Sensors and Actuators Interconnection Engineering Design Project Examination Credits P A I 8 E CT S D IV I 8 E CT S Kls I 8 E CT S P A Prs I 8 E CT S 04 Business Administration Internship Examination Credits Kls Te I 4 E CT S P B I 27 E CT S 05 Software Engineering Embedded Systems Systems Theory Control Systems Engineering Examination Credits T PL I 8 E CT S T PL I 8 E CT S Kls Lab I 8 E CT S T PL I 8 E CT S 06 Information Processing and Transmission I Information Processing and Transmission I I Elective I Elective I I Examination Credits T PL I 8 E CT S Lab I 8 E CT S D IV I 8 E CT S D IV I 8 E CT S 07 Digital Engineering Bachelors Thesis and ColloquiumIn addition, English lectures from Semester 1 to Semester 4 Examination Credits Kls I 8 E CT S Th Ko I 15 E CT S Kls Written Exam Prs Presentation T PL Technical Problem Solving St A Student Research Project Lab Laboratory PA Project Work D IV Various Types of Exams Te Test P B Internship Report Th Thesis Ko Colloquium Electives Please refer to the next page for module content within your elective Explanation The university reserves the right to make changes S RH University Heidelberg Electrical Engineering B Eng Status 1120234 Electives Sharpen your profile Your choice of elective enables you to focus on your personal interests After completing your basic studies, you can choose an elective in Semester 6 to explore your interests in greater depth Automotive Engineering Life Science Engineering Elective I E CT S Electrical Components 8 Elective I E CT S Anatomy and Device Technology 8 Elective I I E CT S Powertrains 8 Elective I I E CT S Vital Sign Acquisition 8\n",
      "erweiterung Health City 202324 The living cell almost always contains, locked in its interior, the visible or invisible products of its physiological activity or its nourishment Albrecht Kossel 1910 Nobel Prize winner for medicine from HeidelbergHeidelberg Not only has the famous Heidelberg Castle cast its spell on guests from around the globe the entire city is nestled in an enchanting environment and is clearly in the pulse of its time With a historical Old Town that has so much to offer an abundance of small alleys, each of which tells its own story, and a location that Goethe called ideal, Heidel berg has its place in the hearts of innumerable visitors However, Heidelberg, at the same time, is a modern city of science and is deemed a light house of Life Sciences the University, the European Molecular Biology Laboratory, the German Cancer Research Center and the Heidelberg Max Planck Institutes are some of the leading research institutes in the world, making Heidelberg an international renowned science location The cultural wealth of Heidelberg is just as colorful and diverse as its visitors from around the world offering dance, theater, music, and many events in a diverse cultural landscape that leaves hardly anything to be desired This, not least, also applies to gastronomy and hotels From vibrant student pubs to elegant gourmet restaurants, from trendy hotels to five star accommodation excellence It is not surprising that barely anyone who has ever been here will keep Heidelberg close to his or her heart heidelberg4you instagram.comheidelberg4you HE ID EL BE RG4you youtube.comheidelberg4you Heidelberg4you facebook.comHeidelberg Dear Guests, Dear Family Members, Young, versatile, open minded, and international all of these are attributes that fully de scribe Heidelberg and its 160 , 000 citizens First, however, Heidelberg also is a city of international renown thanks to its excellent standing in science and research Germanys oldest university, the Ruperto Carola is a beacon of knowledge and education It received the status of Univer sity of Excellence in 2007  The notable institutes, with subjects spanning humanities, social, and legal sciences, as well as natural and life sciences, including medicine, are where todays and tomorrows leaders are teaching and studying Heidelberg University Hospital promises focused expertise as well It enjoys global recognition for its outstanding research, teaching, and medical care It is hardly surprising that researchers and patients alike, find it one of the most important points of contact in terms of health About 13 , 800 employees from 130 nations are currently working in over 100 different profes sions at the Heidelberg University Hospital On a daily basis, they work together to help severely ill patients from around the world, who profit not only from a comprehensive range of cutting edge, successful treatments, but also from ever new ideas and impulses in patient care, research, and teaching Top facilities, such as the National Centrum for Tumor Diseases N CT, the German Cancer Research Center Deutsches Krebsforschungszentrum, D KF Z, the Zentrum fr Molekulare Biologie Heidelberg Z MB H, four Max Planck Institutes, the European Molecular Biology Laboratory E MB L, or the Center for Integrative Infectious Disease Research C II D make Heidelberg a globally leading site for international research at the highest level These and other institutes research new and improved treatment methods all year round to pre serve or restore your health to the best degree Nonetheless, not only is medical care outstanding internationally in Heidelberg, but our beautiful city in itself will contribute to your recovery on top With 70% of forest and green spaces, a moderate climate, and an urban flair, Heidelberg offers one of the highest lifestyle values in Germany Embedded in the Heidelberg city forest, certified as a recreational fo rest, the world renowned Heidelberg Schloss Castle, the Philosophenweg Philosophers Walk, the historical Altstadt Old Town, the Bahnstadt as the latest and most innovative addition to the citys quarters, and many culinary offers are waiting to be explored These are just a few examples of what makes Heidelberg so attractive Millions of visitors flock to our beautiful city every year, strolling on one of the longest pedestrian zones in Europe At a length of 1 6 kilometers, the Hauptstrae Main Street with its enchanting small side alleys will invite you to a unique selection of local and international products alike that will leave hardly any shopping wish unfulfilled This brochure will give you an overview of the medical services of the Heidelberg University Hospital, while at the same time presenting a selection of hotels that specialize in hosting patients accompanying families On top, we are delighted to give you some inspiration to help you make your time in Heidelberg interesting and diverse based on your own preferences The Heidelberg Marketing Gmb H service team will gladly assist you in planning your individual stay Feel free to contact us directly and tell us about your personal travel requirements Welcome to our beautiful city Yours, Mathias Schiemer C EO Heidelberg Marketing Gmb H Heidelberg Congress Health City 5 6 Health City Outstanding areas of expertise High end oncology all disciplines, for example surgical treatment chemotherapy innovative radiation therapy including proton and carbon ion therapy stem cell transplantation comprehensive care in the National Center for Tumor Diseases Heidelberg Heart and vascular diseases Neurology and neurosurgery Metabolic and endocrine diseases Orthopedics and Traumatology Gynecology Pediatrics Services for international patients Coordination of treatment requests to all medical departments Multilingual team at the International Office Certified interpreters Support in obtaining a medical visa to travel to Germany Friendly, English speaking nursing personal All doctors speak fluent English medical reports are provided in English Special requests regarding meals are taken into consideration Prayer room for Muslim patients At a glance 1 , 163 , 400 outpatient treatment cases year 85 , 500 inpatient treatment cases year 2 , 500 beds 50 medical specialist departments 13 , 800 employees, including 1 , 900 professors and doctors data source annual report 2022 , rounded Heidelberg University Hospital Health City 7 Heidelberg University Hospital is one of the leading medical centers in Europe and offers inpatients and outpatients an innovative and effective diagnosis and therapy for all complex diseases Every year, many thousands of patients from all over Germany and various other countries worldwide travel to Heidelberg for medical treatment Renowned professors, distin guished physicians and nursing staff, state of the art equipment, as well as the proximity and interdisciplinary cooperation of the specialist depart ments guarantee the highest standards of medical care Progress and innovation are essential for promising medical treatment Heidelberg University Hospital and its partner research institutes, such as the world renowned German Cancer Research Center, pursue a common aim the development of new forms of therapy and their quick implemen tation for the benefit of the patient Contact and Information Heidelberg University Hospital International Office Im Neuenheimer Feld 400 69120 Heidelberg, Germany Phone 49 6221 56 6243 Fax 49 6221 56 33955 international.officemed.uni heidelberg.de university hospital.com Altstadt Old Town Main Station Heidelberg Castle Karlstor PhilosophenwegPhilosophersWalk Uferstrae Hauptstrae Plck Plck Ziegelhuser Landstrae Theodor Heuss Brcke Ernst W alz Brcke Berliner Strae Neuenheimer Landstrae Neckarstaden Bergheimer Strae Iqbal Ufer Eppelheimer Strae Chaisenweg Bismarck- platz Klingenteichstrae Rohrbacher Strae Alte Brcke Old Bridge Neckar Neckarfor pedestrians only Neckar Kurfrsten Anlage Neuenheim Bergh eim Weststad t Sdstadt Pfaffeng rund Old T own C Chhaaiisseennweegg 8 Recommended Hotels and Apartments Recommended Hotels and Apartments Enjoy our hospitality In an unfamiliar city, it is particularly important for those accompanying a hospital patient to be able to stay in a friendly and comfortable accommodation setup Therefore, on the following pages, we provide a list of selected Heidelberg hotels and apartments specialized in looking after international guests and providing personalized service, a high level of quality and genuine hospitality excellence from 3 to 5 star hotels to cosy apartments in leafy surroundings to luxury boutique apartments Our hosts are looking forward to welcoming you warmly Map is not true to scale Note For your information 1 km approx 0 62 miles and 1 sqm 10 76 ft1 21 28 9 10 11 12 133 4 5 6 773 11 9 10 64 12 Der Europische Hof Heidelberg Boutique Suites Heidelberg Rafaela Hotel Heidelberg Heidelberg Marriott Hotel Atlantic Hotel Heidelberg Premier Inn Heidelberg City Zentrum Premier Inn Heidelberg City Bahnstadt Hilton Hotel Heidelberg Hotel Bergheim 41 Apartments Zweite Heimat Heidelberg Boarding House Luise5 Heidelberg Hospitality Boarding House Midori Guesthouse Heidelberg University Hospital8 513 Recommended Hotels and Apartments 9 Der Europische Hof Heidelberg Comments made by our guests that the Europischer Hof Heidelberg, a privately managed hotel, felt like home to them are the nicest compliments for us and are a testament to our claim of being the friendliest luxury hotel in Germany The entire team strives to look after each and every guest in a unique atmosphere and with passionate, individualized and enthusiastic service Sincere excellence in service is a matter of attitude Moreover We love what we do Dr Caroline von Kretschmann, Managing Director Your home in Heidelberg People undergoing medi cal treatment need an environment in which they feel comfortable and in which they receive special care Who could possibly do this better than a private hotel that has been family managed for generations While in Heidelberg, we can offer you a place to stay safe where you can truly feel at home Our individu ally furnished rooms and suites provide you and your family with very pleasant surroundings The Fine Dining Restaurant Kurfrstenstube offers culinary delights tailored to your desires and needs In addition, you can relax in our P AN OR AM A S PA with roof terrace Whenever you visit Heidelberg we look forward to welcoming you Der Europische Hof Heidelberg Friedrich Ebert Anlage 1 , 69117 Heidelberg Phone 49 6221 5150 reservationseuropaeischerhof.com Hygiene Safety Concept 119 individual appointed Rooms and Suites 4 luxuriously equipped Serviced Apartments 1 Fine Dining Restaurant Die Kurfrstenstube 1 Bar Smokers Lounge Terrace Courtyard 10 Conference Venues P AN OR AM A S PA with Roof Terrace 200 Parking Spaces in the basement Personalized Service Multilingual Staff Central Location University Hospital 4 km Frankfurt Airport 85 km 10 Recommended Hotels and Apartments Boutique Suites Heidelberg H OM E M EE TS H OT EL The Boutique Suites Heidelberg expresses a luxurious and urban lifestyle with a professional focus on long term stays for medical reasons You are safe with us The hotels concept adapts perfectly to the new situation on safe travel Each single suite features an independent resi dential ventilation unit and autonomous AC to protect you and help to focus on the essential We look forward to welcome you and guarantee you an atmosphere of privacy and comfort, where your well being is most important Tamimu Gibreel, Chef Concierge Y OU R W EL LB EI NG I S O UR P RI OR IT Y As one of the most exclusive Boutique Suite Hotels in the whole region, we offer extra spacious serviced apartments for international guests with high demands It is our priority to make you feel as comfortable as possible Settle down and gain strength in one of our exclusively designed sui tes with state of the art technology Experi ence a dedicated and personalized service, enjoy your privacy and independence and discover highest living comfort with premium equipment at the Boutique Suites Heidelberg Alte Zigarrenmanufaktur Boutique Suites Heidelberg Rmerstrasse 80 , 69115 Heidelberg Phone 49 6221 659369 welcomeboutiquesuitesheidelberg.com 3 residences 100125 sqm 20 suites sized 40100 sqm, with kitchen, living and dining area Accessible apartments for long term guests Superior suites with 2 bathrooms Private parking places right in front of the building Personal concierge service Private panorama sauna and private gym University hospital 3 km Ethianum 1 5 km Your well being is important to us As a result, we align our actions with it We want you to feel right at home here to ensure that you will enjoy coming back Thanks to your guest ratings, we have received numerous awards in the last two years We look forward to your visit and will gladly adjust to your indi vidual needs Enza Occhipinti Apfel, Managing Director In its central, idyllic location with a modern air, Rafaela Hotel is located near the hospitals and research facilities in the Neuenheimer Feld, and very close to the popular Neckarwiese banks of the Neckar and the Altstadt Old Town with its tourist sights Gastronomy beckons from a small market square right outside the hotel door, com plete with outdoor seating, French, Italian, and sustainable specialties stores even though this is a calm side street Rafaela Hotel is a new building with elevator, air conditioning, and level access showers The University Hospital is seven minutes away by taxicar, twelve by bus, and can be reached on foot in 25 minutes Rafaela Hotel Heidelberg Lutherstrasse 17 , 69120 Heidelberg Phone 49 6221 6743300 inforafaela-hotel.com Air conditioning Free high speed Wi Fi 50 flat screen T V Refrigerator Safe Central location Calm side street Idyllic environment Modern new building 2019 University Hospital 1 75 km Recommended Hotels and Apartments 11 Rafaela Hotel Heidelberg 12 Recommended Hotels and Apartments Heidelberg Marriott Hotel At the Heidelberg Marriott Hotel we understand individual needs Our associates are just as diverse, with different cultural backgrounds as our guests are, and therefore ready to cater for all your needs with that typical Heidelberg spirit to serve Being part of the biggest travel company in the world with more than 8000 hotels, you can rest assured that the Heidelberg Marriott Hotel observes the highest health safety standards and with views overlooking the Neckar River, both body and mind, can take a well deserved break from your daily hassle Lukas Bechtel, Director of Sales Marketing The Heidelberg Marriott Hotel is located on the banks of the river Neckar within walking distance of the old town and the picturesque Heidelberg shopping mall Some of our quiet rooms overlook the river facing the well known University Hospitals The Restaurant Grill 16 , with its fantastic sun deck, offers regional as well as international specialties but is mainly famous for its fabulous steaks Besides our generous American breakfast buffet, we also offer the most common Ara bian breakfast items Our Chefs are happy to provide advice in dietary cooking and will not hesitate to fulfil any special requests you may have To make your time in Heidelberg as pleasant as possible we offer you free access to our indoor pool, health club and sauna If wanted you can also gain access to our unique execu tive lounge, featuring a free bar with snacks during lunch and dinner, as well as a rooftop terrace Heidelberg Marriott Hotel Vangerowstrasse 16 , 69115 Heidelberg Phone 49 6221 9080 salesoffice.heidelbergmarriott.com Marriott.com Right next to the Neckar River 234 quiet rooms and 14 suites, some with a water view Arabian T V channels Free Wi Fi Restaurant Grill 16 with sun deck Room service Water taxi during summer months Indoor swimming pool, fitness center with sauna University Hospital 2 0 km Opened in 2023 , the A TL AN TI C Hotel Heidelberg impresses with its central location, its state of the art facilities, comfort, and an unobtrusive service that always focuses on the guest In our brand new Grand Suite and in our suites, Heidelberg is at your feet Enjoy the magnificent, unique view over the city or the Rhine Neckar metropolitan region We look forward to welcoming the world to the A TL AN TI C Hotel Heidelberg Stephan Sporer, Hotel Director At the northern German A TL AN TI C Hotels Group, you can expect exceptional 4 to 5 star comfort and individual full service In direct proximity to the new Heidelberg Congress Center and the railway station, the 4 star supe rior A TL AN TI C Hotel Heidelberg offers 310 sty lish rooms and suites, a spa and wellness area, a restaurant, and a rooftop bar with a terrace and views over Heidelberg and the surround ing area State of the art conference rooms and a 350 sqm flexible ballroom on the 13th floor round off the facilities A TL AN TI C Hotel Heidelberg Europaplatz 1 69115 Heidelberg email heidelbergatlantic hotels.de For reservations Phone 49 0 421 944888 535 email heidelbergatlantic-hotels.de 1 Grand Suite with 105 sqm with possibility to extend by an additional suite to 153 sqm 19 suites with 48 sqm 3 wheelchair accessible rooms 310 rooms and suites, with air conditioning, coffee machine, minibar and safe 350 sqm ballroom on the 13th floor and additional 7 event facilities Free W IF I Restaurant and bar on the 15th floor Room service Laundry service Wellness and fitness area Parking spaces University Hospital 2 5 km Ethianum 1 5 km Recommended Hotels and Apartments 13 Atlantic Hotel Heidelberg 14 Recommended Hotels and Apartments Premier Inn Heidelberg City Bahnstadt Premier Inn Hotel Heidelberg City Zentrum A good nights sleep is our top priority, which is why we do everything for your well being and promise you a restful night We offer premium quality at economy prices because Premier Inn rooms are not only comfortable and functional by our international standards, they are also very affordable Everything Premium Except for the price Try it out yourself at our two hotels in Heidelberg our warm, intercultural teams look forward to meeting you Miriam Saalmann, Cluster Hotel Manager Our two Premier Inn hotels in Heidelberg are ide ally located nearby the university hospitals as well as the famous sights in the old town The proximi ty to the main railway station and the connection to the motorways also ensure excellent connec tions to the airports in Frankfurt and Stuttgart or to exciting excursion destinations in the surroun ding area After an eventful day, you will be welco med in a spacious and air conditioned room whe re you can relax and submerge in a sound sleep in your comfortable Hypnos bed W e offer special conditions for your stay with a medical back ground reach out to us when booking Premier Inn Heidelberg Bahnstadt Czernyring 26 28 , 69115 Heidelberg Phone 49 6221 6484896 email Heidelberg City Bahnstadtpremierinn.com Premier Inn Heidelberg City Zentrum Kurfrsten Anlage 23 , 69115 Heidelberg Phone 49 6221 6484899 email Heidelberg Citypremierinn.com two premium economy hotels 259 rooms in total bathrooms with walk in shower three wheelchair accessible rooms kettle with tea coffee Lounge with bar menu Breakfast buffet full English Relax and work zones Free W LA N Parking spaces for a fee Premier Inn Heidelberg City Premier Inn Heidelberg Bahnstadt Recommended Hotels and Apartments 15 Warm hospitality in the centre of Heidelberg Newly opened in 2023 , this hotel with 244 elegant and comfortable rooms and suites enjoys a prime loca tion It is only a few minutes away from the old town as well as from the university hospital and medical facilities Enjoy your meal in the hotels restaurant or the exclusive atmosphere in the hotel bar or execu tive lounge Hilton Heidelberg Kurfrstenanlage 1 , 69115 Heidelberg Phone 49 06221 3521 320 infohiltonheidelberg.com Hotel Heidelberg In the city centre of Heidelberg 244 elegant rooms and suites Restaurant and bar Executive lounge Fitness centre Free Wi Fi 130 underground parking spaces in the building Direct connection to public transport Frankfurt Airport 85 km, shuttle available University hospital 4 km Contemporary living in a protected monument Our 16 individual and very well equipped apart ments are furnished in high quality to make you comfortable The area offers many restau rants, cafs and stores to cover your daily needs The good infrastructure connection will comfortably take you to any sights, event venues and hospitals Zweite Heimat Heidelberg Gmb H Werderstrasse 43 , 69120 Heidelberg Phone 49 6221 6561943 wohnenzweiteheimatheidelberg.de Zweite Heimat Heidelberg 4 locations in direct proximity of the city center 40 90 sqm 1 3 rooms Winter garden balcony terrace garden use Fully furnished kitchen Towels and bed linen free of charge Washing machine and tumble dryer free of charge Parking spaces available partly for a fee Wi Fi included University Hospital 1 5 km The Bergheim 41 is easily accessible by both car and public transportation and is in the immediate vicinity of Bismarckplatz, the beginning of the pedestrian shopping area The hotel has 32 rooms and studios of contemporary design The university hospitals are just minutes away and can easily be reached by car or public transportation located directly in front of our hotel Hotel Bergheim 41 Bergheimer Strasse 41 , 69115 Heidelberg Phone 49 6221 750040 infobergheim41.de Bergheim 41 32 rooms and studios Junior suite with in room sauna and roof terrace Located just 300 meters from the pedestrian mall Kaffeekultur B41 on site coffee shop High speed Wi Fi up to 100 Mbit s Roof garden with view of the Castle Individually controlled air conditioning University Hospital 3 2 km The Boardinghouse Luise5 is a unique jewel Enjoy modern and spacious accommodation each unit 50 sqm at the very heart of Heidelberg and refurbis hed historical building, just behind the A TO S Clinic Calmly located in short distance from the main street is ideal for short trips, long stays and business travelers Meanwhile main sights, the Neckar River promenade, restaurants and public transport stops are easily reached Boardinghouse Luise5 Luisenstrasse 5 , 69115 Heidelberg Phone 49 6221 655 6114 welcomeluise5 boardinghouse.de boardinghouse.de Boardinghouse Luise5 6 spacious two room apartments spread over 50 sqm for up to 4 persons Light living room with extensible sofa bed, dining table and working area Fully furnished kitchen Free Wi Fi and L ED T V in each room Close to Bismarckplatz, main shopping area and Atos Clinic Onsite parking spaces are available per request with applicable daily fee Long stay guests can benefit from our washing machine and dryer Pets are welcome with additional daily costs High quality and sustainable living, relaxing and meeting in an energy efficient passive house standard at the gates of Heidelberg, and close to Heidelberg University Hospital Midori the green guesthouse Friedrich Ebert Strasse 4 69221 Dossenheim Heidelberg Phone 49 6221 87 29 80 infomidori-guesthouse.com guesthouse.com Midori Guesthouse 66 feel good rooms 4 spacious suites free W Lan free garage Public transport only 2 minutes Panorama lounge roof terrace Short or long term stay Breakfast Bistro Our beautiful serviced apartments are centrally located between the University Hospital and the Old Town The 1 room apartments approx 25 m are equipped with a double bed, 49 inch smart T V, hotel safe, kitchenette, and a bathroom with ground level shower The 2 room suites approx 40 m also have a living room with a sofa bed Book your temporary home Heidelberg Hospitality Boarding House Kurfrsten Anlage 47 51 , 69115 Heidelberg Phone 49 06221 710 318 infoheidelberg hospitality.de hospitality.de Heidelberg Hospitality Boarding House Contact free check in available Free Wi Fi use Non smoking hotel Coffee machine refrigerator Rooms with connecting door Elevator University Hospital 1 5 km Main train station 0 5 km Old town 1 7 km 16 Recommended Hotels and Apartments MU SE UM H EI DE LB ER G D AI LY 10 A M 6 P MWhat the B OD Y are all about Airport Health Wedding Limousine Sprinter Be it in an elegant limousine, a comfortable Sprinter or a classic Mercedes Benz vintage car T LS is your trusted automotive partner for all travel connections to Heidelberg and in the Rhine Neckar region Our health shuttles are happy to drive patients with and without medical equipment to clinical treatments and check up appointments Barrier free and e-wheelchair accessible rides can be provided on request More information about T LS Heidelberg is available online at or by phone 49 6221 770077 We look forward to meeting youT LS Travel well with us Special Activity Programs Experience Heidelberg with its famous trio of the castle, the Neckar River, and the Old Town in a unique way Wander through the castle gardens take a boat ride, a guided tour of the town, a shopping tour, or a trip on a segway We have compiled a number of exciting and in teresting experiences for you to make your stay as pleasant as possible Of course, you can combine our special activity programs with overnight hotel stays in all inclusive packages We are here to help make your wishes come true Please contact us we would be deligh ted to talk with you in person 16 Special Activity Programs Experiences Zoo Heidelberg Find the red panda hidden in the branches or watch our elephant males taking a dip there is a lot to see in our zoo Majestic lions, elegant tigers or our cuddly brown bears, comical meerkats, ancient giant tortoises a centu ry of lifetime experiences in every shell, our gorilla fami ly or the cool chimpanzee gang just a sample of our wide variety of fascinating animals Botanical Garden Heidelberg The Botanical Gardens of the University of Heidelberg, one of the oldest in Germany, was originally established in 1593 as a Hortus Medicus for the cultivation of medi cinal herbs Nowadays, this institution is important for its contribution to botanical studies The greenhouses con tain unique plant collections e.g Old and New World succulents and orchids Walking along the outdoor sec tions, visitors can enjoy the fascinating vegetation of the raised bog, heath bog, alpine garden, and weedy vi neyard, as well as a systematic section Shopping Heidelbergs famous shopping area takes you through the middle of the Old Town Over one kilometer in length, the main street is one of the longest pedestrian malls in Europe and full of temptations at every turn from indi vidual owner operator shops and department stores with a wide product range to branch stores in internatio nal labels Also, in the little side streets peeling off the pedestrian mall and in the suburbs of Heidelberg there are surprises waiting at every corner here you will find antiquarian bookshops, galleries, young boutiques and quaint little shops Service and more information Heidelberg Marketing Gmb H, Phone 49 6221 58 40 223225 , health-cityheidelberg-marketing.de, marketing.com Special Activity Programs 17 Guided Tours Old Town Tour Winding alleys and historic squares, fascina ting museums and galleries Heidelbergs Old Town offers a wide range of things to do and see A guided tour around the Old Town brings the past to life Whether the Heiliggeistkirche Church of the Holy Spirit, the Jewish Quarter, Germanys oldest university with the Studen tenkarzer students prison, or any of the many sights, every corner of the Old Town has an in teresting story to tell The route also takes you along the main street, one of the longest pedestrian zones in Europe Recommended duration 1 5 to 2 hours Bus Tours Get to know the city with a guided tour along the Neckar River to the Heidelberg Castle During the tour, you can immerse yourself in the history of the world famous building and its breathtaking story Alternatively, take a ride on the convertible sightseeing bus with its in formative commentary and numerous tips for additional activities in the city Boat Tours Taking a boat tour through the narrow Neckar valley, with its hillsides full of vineyards and de ciduous forests, is quite an experience It is also possible to book tours in combination with guided tours through the Old Town or the monastery Castle Tour It towers majestically above the roofs of the Old Town and impresses millions of people year after year the Heidelberg Castle With a tour through its courtyard and garden, you can dive in the colorful history of the world famous structure A visit to the Great Barrel, the largest wine barrel in the world ever to be filled, is also featured on the program no inside areas Recommended duration of 1 5 or 2 hours Segway Tour A Segway tour through the city provides entire ly new perspectives A guide will accompany you on the route along the Neckar River to the historic Neuenheim district From here, you will continue in the direction of the zoo and through the Neuenheimer Feld campus with its impressive university and hospital buil dings The highlight is the journey along the Philosophenweg Philosophers Walk Themed Tours Varieties of theme tours are available, inclu ding a guided tour of the famous University or the traditional Christmas market, or the Philo sophers Walk to mention but a few Duration of 2 hours Service and more information Heidelberg Marketing Gmb H, Phone 49 6221 58 40 223225 , health-cityheidelberg-marketing.de, marketing.com Gltig ab Valid from Family Heidelberg CA RD Heidelberg in your pocket Discounts with attractive partners from gastronomy, retail, culture and leisure With castle ticket including funicular railway Free use of public transport in the city area Combo ticket one time free admission to the University Museum, the Student Prison and the special exhibitionstarting from 26 Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from18 Special Activity Programs Culture and Festival Town Dance, theater, music our cultural scene presents itself center stage Heidelberg has much to offer as a festival town and invites cultural enthusiasts to get to know its many facets March 15 April 13 , 2024 International Music Festival Heidelberger Frhling Heidelberg Spring The Heidelberger Frhling Heidelberg Spring is an idiosyncratic hotspot for music F A Z and hosts over 100 events including star spangled classical concerts, innovative productions which experiment with dance or multi-media, as well as unconventional concert formats and genre cross-overs Of parti cular interest is the song category April 16 May 5 , 2024 Heidelberger Stckemarkt Play Market The avant garde of theater presents outstanding premieres from the German speaking area New plays are shown and social discourse is initiated The international highlight is the annually changing guest country Sweden will be our guest country in 2024  Though still barely known as a theatre country in Germany, it stands out with its vibrant and innovative scene June 1 and September 7 , 2024 Heidelberg Castle Illuminations with Fireworks Every year the legendary Castle Illuminations capture the imagination of thousands of people there is hardly another city that can boast such magical nights Bengali flares slowly bathe Heidelberg Castle in a mysterious red firelight, as if the ruins were on fire once again in their long history When the glow of the castle slowly dies down, the second part of the spectacle begins the brilliant fireworks over the Neckar The banks of the Neckar and the Philosophers Walk around 10 pm are the best locations from which to watch the Castle Illuminations marketing.com June 9 July 28 , 2024 Heidelberger Schlossfestspiele Heidelberg Castle Festival Heidelbergs theater and orchestra make the famous castle ruins reverberate with the sounds of music, fun, and dramatic plays alike The successful musical comedy Im Weissen Rssl The White Horse Inn and the famous vampire Dracula will be played in the Schlosshof Castle Courtyard A diverse concert program with tomorrows masters and the best international soloists ensures a choice of entertainment under the open sky that has yet to find its equal End of July beginning of August, 2024 Metropolink Festival of Urban Art The Metropolink Festival is displaying expansive wall paintings by internationally acclaimed practi tioners of urban art in the whole city area Convertible bus tours take visitors to the various decora ted houses and facades Public tours during the festival and tours for groups are available all year The city turns into a public art space and Heidelberg shows that it has truly earned the title of U NE SC O Creative City Please note that events may be cancelled or change due to unforeseen circumstances 20 Culture and Festival Town August 24 25 , 2024 Summer at the river For two days, a promenade will be created on between the Alte Brcke and Marstall, where people can dance, sing, play or just chill Two stages invite you to linger and offer a various programme from brass music to jazz and Dixieland to party hits Stroll, relax, and enjoy the river Boat trips offer magical per spectives of the city, and the Childrens Area on Krahnenplatz offers fun, games and excitement with a hands on programme marketing.com September 28 29 , 2024 Heidelberger Herbst Heidelberg Autumn Festival Live Music, the Craft Market, Flea Market, culture, and stalls with regional specialties Heidelbergs greatest city festival on the last weekend in September always enjoys the dependable cooperation of many associations, gastronomes, ,and cultural organizers who all contribute to making it such a success By the evening, the Old Town becomes a huge open air concert, offering music for every taste and venues all over the Old Town marketing.com Beginning of October beginning of November, 2024 International Enjoy Jazz Festival Enjoy Jazz means high quality festival weeks with an extraordinary program hosting many different artists and music genres The festival welcomes artists from all nations on the stages of the festival cities of Heidelberg, Mannheim and Ludwigshafen Focusing on high quality jazz, the festival in the Rhine- Neckar metropolitan region has found international renown as a high class musical event It takes over the stages to present outstanding musicians for a period of nearly seven weeks October 2 13 , 2024 Heidelberg Wine Village Viticulture in Heidelberg is characterised by a centuries old tradition Heidelberg benefits from a mild climate and offers vines on sun drenched slopes everything they need to thrive and produce high- quality wines Enjoy local wines and delicious drops from the region in the heart of the Old Town This event is accompanied by regional cuisine and atmospheric live music marketing.com November 25 December 22 , 2024 Heidelberg Christmas Market The aroma of roasted almonds, cinnamon and hot chestnuts wafts through the winding alleyways of the Old Town It is Christmas time the Heidelberg Old Town revels in Christmas magic The around 140 decorated booths in the Old Town and Germanys most beautiful ice rink open against the impressive setting of the Castle, and in the blink of an eye the market fills with the people of Heidelberg as well as many regional and international guests The atmosphere is captivating the stalls nestled idyllically amongst the historic squares of the Old Town shed a warm glow along one of the longest pedestrian zones in Europe marketing.com Culture and Festival Town 21 22 Your way to Health City Your journey to Health City Located in the south west of Germany the heart of Europe Heidelberg is also a convenient 80 km distance from Frankfurt International Airport 120 km from Stuttgart Airport and around 90 km from the Karlsruhe Baden Baden Airport It is so easy to get here We hope to see you soon Europ e Berlin 100 km German y Baden Wrttember g F SC paper was produced with green electricity Imprint Heidelberg Marketing Gmb H Neuenheimer Landstrasse 5 69120 Heidelberg Germany The Heidelberg Marketing Gmb H is a subsidiary of the City of Heidelberg Layout a B Grafik Artem Bathauer Photos Cover page Tobias Schwerdt, Heidelberg University Hospital Hendrick Schrder Page 3 , 4 , 5 , 15 , 16 , 17 , 19 , 20 , 21 Tobias Schwerdt Page 6 , 7 Heidelberg University Hospital Hendrick Schrder Page 9 14 Hotel photos of the respective accommodation Page 20 Susanne Reichardt, Steffen Schmid Page 21 Jrgen Spachmann 2024 All contents, in particular texts, photographs and graphics, are protected by  Unless expressly stated otherwise, Heidelberg Marketing Gmb H owns the  Heidelberg Marketing Gmb H Neuenheimer Landstrasse 5 69120 Heidelberg Germany Phone 49 6221 58 40 223225 health-cityheidelberg-marketing.de Heidelberg University Hospital International Office Im Neuenheimer Feld 400 69120 Heidelberg Germany Phone 49 6221 56 6243 international.officemed.uni heidelberg.de\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 Data Visualization and Storytelling 12 Design Basics and Designing Interactive Dashboards Module responsible by Prof Dr Swati Chandna , Room L GS 6 , 2nd floor, Arc 215 , email swati.chandnasrh.de Phone 49 6221 6799 - 223 2 Data Visualization and Storytelling 3 Advanced Data Visualization Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 3 First Steps into Case Studies Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 223 4 Case Study 1 Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 5 Master Thesis Project Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 6 Data Engineering 1 Big Data Databases Module responsible by Prof Dr Binh Vu , Room L GS 6 , 2nd floor, Arc 215 , email binh.vusrh.de , Phone 49 6221 6799 - 208 7 Data Engineering 2 Big Data Architectures Module responsible Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 8 Big Data Programming Python Module responsible Dr Ing Kamellia Reshadi , Room L GS 6 , A ND 2 , Arc 213 , email kamellia.reshadisrh.de Phone 49 6221 6799 216 9 Data Management 1 Data Acquisition and Data Cleaning Module responsible by Prof Dr Theodoros Soldiers , Room L GS 6 , 2nd floor, Arc 215 , E -mail theodoros.soldatossrh.de Phone 49 6221 6799 213 10 Data Management 2 Data Curation and Data Management Module responsible by Prof Dr Binh Vu, Room L GS 6 , 2nd floor, Arc 215 , E -Mail binh.vusrh.de, Phone 49 6221 6799 - 208 11 Data Analytics 1 Statistics and Machine Learning Module responsible by Prof Dr Theodoros Soldiers, Room L GS 6 , 2nd floor, Arc 215 , E -mail theodoros.soldatossrh.de Phone 49 6221 6799 213 12 Data Analytics 2 Text Mining and Natural Language Processing Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 13 Data Analytics 3 Deep Learning Module responsible by Prof Dr Binh Vu, Room L GS 6 , 2nd floor, Arc 215 , E -Mail binh.vusrh.de, Phone 49 6221 6799 - 208 14 Privacy, Ethics and International Law Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 15 Case Study 2 Module responsible by Prof Dr Swati Chandna, Room L GS 6 , 2nd floor, Arc 215 , E -mail swati.chandnasrh.de Phone 49 6221 6799 - 223 16 A DS A Internship Module responsible by Prof Dr Binh Vu, Room L GS 6 , 2nd floor, Arc 215 , E -Mail binh.vusrh.de, Phone 49 6221 6799 - 208 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S RH University Heidelberg Tuition fees effective from April 1 , 2023 E EA students Programme Monthly fees E EA students Master Applied Computer Science Master o f Science 750 Applied Data Science and Analytics Master of Science 790 Architecture Design for the Built Environment Master of Arts 750 Dance Movement Therapy Master of Arts 690 Global Business and Leadership Master of Arts 770 Information Technology Master of Engineering 770 International Business and Engineering Master of Engineering 770 International Business and Engineering with pre course Master of Engineering 770 International Management and Leadership Master of Arts 770 Music Therapy Master of Arts 690 Water Technology Master of Engineering 770 Water Technology with pre course Master of Engineering 770 Bachelor International Business Bachelor of Arts 750 One -timeregistration fee 750 Tuition fees in this pricelist are valid from 1 April 2023 for E EA students, starting with the Winter Semester 2023  Payment is required in advance, as specified in the table per semester, per year, or full programme S RH reserves the right to modify the fees at any time Tuition fee discounts for up -frontpayment One year in advance 3% Full programme in advance 5% Academic leave For academic leaves there is an administration charge of 50 exceptions parental leaveillnesscare E EA European Economic Area S RH University Heidelberg General Fees Students from E EA countries are required to pay a one time registration fee of 750 at the beginning of the degree programme This applies to all degree programmes  Students who completed a Bachelors degree at S RH University Heidelberg and then immediately begin a Masters degree programme are exempted from this fee It also does not apply for participants in a module programme or auditing students Tuition fees and advance payments The monthly tuition fees cover all regular courses and events associated with the degree programme in which the student is enrolled as well as the language courses offered by the university Fees must be paid for the full duration of the degree programme, and at a minimum for the standard study period Regelstudienzeit For residency reasons, leave cannot be granted for a voluntary internship to students required to have a visa Any other planned leave must be agreed in advance with the responsible auth ority for the residency permit Leave cannot be granted during mandatory internship periods because mandatory internships are a required component of the study programm and form part of the standard study period If a student completes a voluntary internship without having been granted leave during this period, normal study fees apply Tuition fee discounts for up -frontpayment-Oneyear in advance 3% Fullprogramme in advance 5% Reduction of the tuition fees The tuition fees for the standard period of study may be reduced if the student has a close family member sibling, parents, children, spouse who is attending or attended S RH Universit ies, the student has completed a Bachelors degree at one of the S RH Universities and has now been accepted to a Masters degree programme In such cases, a discount of 10% will be applied to the tuition fees for the standard period of study All employees of S RH Universities are eligible for a 10% employee discount to the tuition fees for the standard period of study in cases in which the student is in employment subject to full social security contributions at the S RH universities The reduc tions of the tuition fees mentioned above may not be combined, nor applied in combination with in -housescholarships Scholarship rules The current rules governing scholarships at S RH University Heidelberg are either publicly available on the university website during the scholarship application period or can be requested from the Admission Office at S RH University Heidelberg Module programm e Auditing students Participants in a module programme pay a fee of 150credit point E CT S corresponding to the credit points of the module in which they are enroled Auditing students pay a fee of 120credit point E CT S corresponding to the credit points of the classmodule they are auditing This is only possible if the capacity of the classmodule maximum number of participants is not exceeded due to the auditing stu dents participation and the applicable fees are paid in advance upon conclusion of the contract Auditing students are not eligible to participate in examinations Otherwise, the provisions of the enrolment agreement and the programme relatedstatutes of S RH University Heidelberg apply at all times Adrian Thny, Carsten Diener Managing Director President Important information disclaimer This translation is intended to provide information on fee relatedmatters to non German speakers Only the German version of the scale of fees is legally binding.\n",
      "Semester fees non E EA students 6 , 450 , 00 6 , 450 , 00 5 , 100 , 00 5 , 100 , 00 5 , 500 , 00 6 , 450 , 00 6 , 450 , 00 6 , 450 , 00 5 , 500 , 00 5 , 100 , 00 6 , 450 , 00 6 , 450 , 00 5 , 100 , 00 Business Engineering 5 , 500 , 00 Electrical Enginnering 5 , 500 , 00 Mechanical Engineering 5 , 500 , 00 General Fees Academic leave For academic leaves there is an administration charge of 50 exceptions parental leaveillnesscare E EA European Economic Area For students from non E EA countries without a permanent residence permit, the registration fee is 1 , 000  This applies to all degree programmes  Students who completed a Bachelors degree at S RH University Heidelberg and then immediately begin a Masters degree programme are exempted from this fee It also does not apply for participants in a module programme or auditing students Tuition fees and advance payments The monthly tuition fees cover all regular courses and events associated with the degree programme in which the student is enrolled as well as the language courses offered by the university Fees must be paid for the full duration of the degree programme, and at a minimum for the standard study period Regelstudienzeit Students from Non E EA countries without permanent residence permit applying for a degree programme are required to pay the tuition fees for one semester in advance From the second semester onwards the payments for the remaining semesters of study are due six weeks before the end of the period of study already paid for The payments have to be done semesterwise For residency reasons, leave cannot be granted for a voluntary internship to students required to have a visa Any other planned leave must be agreed in advance with the responsible authority for the residency permit Leave cannot be granted during mandatory internship periods because mandatory internships are a required component of the study programm and form part of the standard study period If a student completes a voluntary internship without having been granted leave during this period, normal study fees apply Tuition fee discounts for up front payment-Oneyear in advance 3% Fullprogramme in advance 5% Reduction of the tuition fees The tuition fees for the standard period of study may be reduced if the student has a close family member sibling, parents, children, spouse who is attending or attended S RH Universities, the student has completed a Bachelors degree at one of the S RH Universities and has now been accepted to a Masters degree programme In such cases, a discount of 10% will be applied to the tuition fees for the standard period of study All employees of S RH Universities are eligible for a 10% employee discount to the tuition fees for the standard period of study in cases in cases in which the student is in employment subject to full social security contributions at the S RH universities The reductions of the tuition fees mentioned above may not be combined, nor applied in combination with in house scholarships Scholarship rules The current rules governing scholarships at S RH University Heidelberg are either publicly available on the university website during the scholarship application period or can be requested from the Admission Office at S RH University Heidelberg Module programme Auditing students Participants in a module programme pay a fee of 150credit point E CT S corresponding to the credit points of the module in which they are enroled Auditing students pay a fee of 120credit point E CT S corresponding to the credit points of the classmodule they are auditing This is only possible if the capacity of the classmodule maximum number of participants is not exceeded due to the auditing students participation and the applicable fees are paid in advance upon conclusion of the contract Auditing students are not eligible to participate in examinations Otherwise, the provisions of the enrolment agreement and the programme related statutes of S RH University Heidelberg apply at all times Adrian Thny, Carsten Diener Managing Director President Important information disclaimer This translation is intended to provide information on fee related matters to non German speakers Only the German version of the scale of fees is legally binding Bachelor International Business Bachelor of Arts One time registration fee 1 , 000  Tuition fees in this pricelist are valid from 1 April 2023 for applicants from non E EA countries, starting with the Winter Semester 2023  Payment is required in advance, as specified in the table per semester, per year, or full programme S RH reserves the right to modify the fees at any time Tuition fee discounts for up front payment One year in advance 3% Full programme in advance 5%International Business and Engineering with pre-course Master of Engineering International Management and Leadership Master of Arts Music Therapy Master of Arts Water Technology Master of Engineering Water Technology with pre-course Master of EngineeringArchitecture Design for the Built Environment Master of Arts Dance Movement Therapy Master of Arts Global Business and Leadership Master of Arts Information Technology Master of Engineering International Business and Engineering Master of EngineeringTuition fees effective from April 1 , 2023 non E EA students Programme Master Applied Computer Science Master of Science Applied Data Science and Analytics Master of Science\n",
      "\n",
      "\n",
      "City Tours 2024 49 62215840 - 223 - 225 I NF OR MA TI ON T IC KE TS guideheidelberg marketing.de marketing.com Tickets Booking Heidelberg has many stories to tell Our guides will give you a special insight into the colourful variety of the city They lead you through the idyllic alleys of the Old Town or visit the world famous castle with you If you prefer to get to know the city from another perspective, you may experience Heidelberg within a Segway Tour or a City Sightseeing Tour in the Cabriobus The Heidelberg Marketing team will be happy to assist you in finding the perfect tour for you Tickets are available at the Tourist Information in the Rathaus town hall, at the Neckar mnzplatz and at the Hauptbahnhof main station Notes Since the number of participants is limited, it is suggested to make a reservation by phone or via the Heidelberg Marketing website For reservations, please pick up your tickets 30 minutes before the tour starts Tip For group bookings, date, topic and language of the tour can of course also be arranged individually Information and service hotline Monday Thursday 900 am 500 pm Friday 900 am 300 pm subject to change Please find the opening hours of our Tourist Information on our website 49 62215840 - 223 - 225 I NF OR MA TI ON T IC KE TS guideheidelberg-marketing.de Eligibility criteria for the reduced price school and university students up to 28 years old, people with disabilities and a disabled persons pass and own ers of the H DC AR D  One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons identity card as well as of children and young people up to 18 years of age with a disability is free of charge Children up to and including 5 years are free of charge Children from 6 to 14 years pay a reduced price Tickets are available at our Tourist Information Walking Tour of the Old Town The oldest part of Heidelberg has a lot more to offer than just the Alte Brcke Old Bridge, the picture perfect alleys and the unique view to the most famous ruin in the world The Old Town is vibrant with its charming squares and the mixture of people of all ages and countries They meet in many small cafs and pubs The unique cultural programs of our museums and theaters also attract our visitors Discover Heidelberg with your guide and really get to know the city Price 12 , discount price 10 Duration 1 5 hours Times April October German daily 1030 am, additionally Friday 600 pm and Saturday 230 pm, English Thursday Saturday 1030 am November March German Friday 230 pm and Saturday 1030 am Meeting point Neckarmnzplatz, in front of the Tourist Information Note There is no Walking Tour of the Old Town at 230 pm during the Old Town Festival Heidelberger Herbst Heidel berg Autumn on September 28 , 2024  City Tour with Guided Castle Tour Open view on the Heidelberg scenery Act one The focus shifts to the romantic castle, which is majestically enthroned above the Karlsplatz Enjoy a comfortable city tour by bus, accompanied by a tour guide Act two takes you up to Heidelberg Castle In addition to an exterior tour, you will visit the inner courtyard and the famous Great Barrel Take advantage of your castle ticket with a visit of the German Pharmacy Museum and the funicular ride back to the Old Town Kornmarkt station Price 30 , discount price 27 , 28 with H DC AR D , including castle ticket funicular railway castle courtyard admission Duration 2 hours Times April October German Friday, Saturday and Whitsunday 130 pm November March German Saturday 130 pm Meeting point Neckarmnzplatz, information board bus stop Note There is no City and Castle Sightseeing Tour during the Old Town Festival Heidelberger Herbst Heidelberg Autumn on September 28 , 2024  Eligibility criteria for the reduced price school and university students up to 28 years old and people with disabilities and a disabled persons pass One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons identity card as well as of children and young people up to 18 years of age with a disability is free of charge Chil dren up to and including 5 years are free of charge Children from 6 to 14 years pay a reduced price Tickets are available at our Tourist Information.including castle ticket The University in the Old Town Founded by Prince Elector Ruprecht I in 1386 , the Ruper to Carola is Germanys oldest university and one of the most venerable education facilities in Europe The tour conveys not only the universitys history but also provides an insight into student life It takes you from the university library via the Peterskirche St Peters Church, the oldest church in Heidelberg, to the Alte Aula Old Auditorium and the histor ical Studentenkarzer Student Prison Visit the place that once made people suffer From 1778 to 1914 , students were punished here for trivial offenses Price 17 , discount price 15 including admission to the Alte Aula Old Auditorium and the Studentenkarzer Student Prison Duration 1 5 2 hours Times April October German Saturday 1100 am Meeting point Neckarmnzplatz, in front of the Tourist Information Minimum number of participants 3 people Eligibility criteria for the reduced price school and university students up to 28 years old, people with disabilities and a disabled persons pass and own ers of the H DC AR D  One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons identity card as well as of children and young people up to 18 years of age with a disability is free of charge Children up to and including 5 years are free of charge Children from 6 to 14 years pay a reduced price Tickets are available at our Tourist Information.including 1 mulled wine punch Christmas Market Tour Take a walk through the Old Town and immediately perceive the Christmas spirit with the scent of roasted almonds and mulled wine Lovingly arranged booths spread over various historical squares The unique backdrop with Heidelberg Cas tle towering above the Old Town creates an outstanding atmo sphere and makes the Heidelberg Christmas Market one of the most fairy tale like events in Germany The Advent themed guided tour provides interesting information on the regions Christmas and pre Christmas traditions The walk starts at the Neckar mnzplatz and passes the most beautiful corners of the Old Town ending at the Universitts platz University Square Price 17 , discount price 15 Duration 1 5 hours Times Advent Saturdays November 30 , December 7 , 14 and 21 , 2024 German 430 pm Meeting point Neckarmnzplatz, in front of the Tourist Information Minimum number of participants 3 people Notes The price includes one cup of mulled wine or fruit punch per person at our booth on Universittsplatz Uni versity Square You only pay the deposit on site and get it back when you return the mug Of course, you can also keep the limited Heidelberg mug as a souvenir of your visit Eligibility criteria for the reduced price school and university students up to 28 years old, people with disabilities and a disabled persons pass and owners of the H DC AR D  One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons identity card as well as of children and young people up to 18 years of age with a disability is free of charge Children up to and including 5 years are free of charge Children from 6 to 14 years pay a reduced price Tickets are available at our Tourist Information Cabriobus Sightseeing Tour The open roof of the convertible bus offers new perspec tives and is a boon for all photographers Enjoy an unre stricted view for your pictures without window reflections Take a relaxed ride along the citys sights while an audio guide in multiple languages keeps you informed The tour starts and ends at the Karlsplatz Carls Square and shows you several highlights before heading to Weststadt and onwards to Neuenheim with its magnificent villas Price 13 , discount price 8 12 , 12 with H DC AR D Duration approx 40 minutes Languages German, English, Chinese, Dutch, French, Italian, Japanese, Korean, Russian, Spanish Times March daily 1000 am 400 pm departure every full hour April October daily 1000 am 500 pm departure every full and half hour November and December Thursday Sunday 1100 am 400 pm departure every full hour Meeting point Karlsplatz Carls Square, information board bus stop Notes Tickets are available at our Tourist Information at the Hauptbahnhof main station, in the Rathaus town hall and at the Neckarmnzplatz, from the driver cash, credit card, debit card or online at There is no tour during the Old Town Festival Heidelberger Herbst Heidelberg Autumn on September 28 , 2024  Children younger than 6 are free of charge, children aged 6 to 12 pay 8  People with disabilities and a disabled persons pass and owners of the H DC AR D pay 12  Delectable Heidelberg Tour Fall in love with the culinary delights of romantic Heidelberg Take a tour through the picturesque Old Town with an aperitif matured under the sun of Baden and a regional 3 course menu in two traditional restaurants The time and journeys between the courses and places are entertain ingly spiced up with history and stories, magical elixirs, and are topped off with a sweet Heidelberg treat Price 98 per person, no reductions including aperitif, appetizer, main course, dessert, two sur prises and a guided tour Drinks are charged separately in the respective locationsrestaurants Duration approx 3 hours Times March 16 , April 6 , April 20 , May 4 , May 18 , June 8 , June 15 , July 13 , July 27 , August 3 , August 31 , September 14 , Sep tember 21 , October 12 , October 26 , November 9 , Novem ber 16 , 2024 German 530 pm Meeting point Marktplatz Market Square, Herkulesbrunnen Hercules Fountain Minimum number of participants 10 people Notes Booking required Please note, that the tour is not suitable for children Segway Tour Highly Philosophical The segway tour reveals an entirely new perspective Glide along the Neckar River to the historical and trendy quarter Neuenheim accompanied by a tour guide Afterwards, you move on to the zoo, the Neuenheimer Feld with its impres sive university and hospital buildings and up to the Philo- sophenweg Philosophers Walk This Heidelberg sight is not easily seen at first glance The former vintners path was once taken by scholars who desired the inspiration of the Heidelberg Trio The harmony of the baroque city, the Heidelberg Castle and the Neckar River Enjoy this ex traordinary view Price 59 , 54 with H DC AR D including helmet fee and segway license Duration 1 hours Times February and November German English daily 100 pm depending on weather conditions March October German English daily 930 am, 100 pm and 400 pm Meeting point Neckarmnzplatz Requirements minimum age 14 years old, size at least 1 40 m, weight 45 115 kg Minimum number of participants 2 people Notes Booking required For group inquiries of twenty guests and more, the price may change due to increased effort and logistics Segway Tour All in 360 Discover Heidelberg and the Neckar valley on a city safari that provides a mixture of everything Your tour starts at the Neckarmnzplatz and continues to the Philosophenweg Philosophers Walk with its fantastic view of the city The second place steeped in history is the Neuburg monastery, which was founded in 1130 and looks back on a fascinating past Head towards the Kpfel and Ziegelhausen to the other side of the Neckar River and right up to the Wolfsbrunnen Proceed in the direction of Heidelberg Castle Enjoy the great view of the castle garden above the castle before continuing to the Klingenteich Passing the Jewish cemetery, the tour takes you back to the Old Town and your starting point at the Neckarmnzplatz Price 69 , 64 with H DC AR D including helmet fee and segway license Duration 2 5 hours Times February and November German English daily 1230 pm depending on weather conditions March October German English daily 930 am, 1230 pm and 330 pm Meeting point Neckarmnzplatz Requirements minimum age 14 years old, size at least 1 40 m, weight 45 115 kg Minimum number of participants 2 people Notes Booking required For group inquiries of twenty guests and more, the price may change due to increased effort and logistics With castle ticket including Bergbahn funicular railway Including combo ticket one time free admission to the University Museum, the Student Prison and the special exhibition Free use of public transport in the urban area Discounts on all public tours except Delectable Heidelberg Discounts with attractive partners from gastronomy, retail, culture and leisure Heidelberg CA RD sale Tourist Information in the town hall Market Square, Tourist Information at the Neckarmnzplatz, Tourist Information at the main station, Kthe Wohlfahrt Hauptstrae 124 and in many Heidelberg hotels Heidelberg C AR D Put Heidelberg in your pocket starting from 26 Gltig ab Valid from 4 Gltig ab Valid from 2 Family Gltig ab Valid from 2 Gltig ab Valid from 1 Imprint Heidelberg Marketing Gmb H Neuenheimer Landstrae 5 69120 Heidelberg Telefon 49 6221 58 40200 Telefax 49 6221 58 40209 infoheidelberg-marketing.de marketing.com The Heidelberg Marketing Gmb H is a subsidiary of the City of Heidelberg Photos Tobias Schwerdt 2024 All contents, in particular texts, photo graphs and graphics, are protected by  Unless expressly stated otherwise, Heidelberg Marketing Gmb H owns the  Kornmarkt Cabriobus City tour Neckarmnzplatz Molkenkur Knigstuhl Bergbah n funicular railwayKarlstorAltstadt Bussemergasse Kl Mantelgasse Groe Mantelgasse Haspelgasse Wehrsteg Floring Krmergasse Mittelbadgasse Apothekergasse Fischerg  Semmelsg Steingasse Leyerg Obere Neckarstrae Mnchg asse Dreiknigsstr Kettengasse Schulgasse Grabengasse Sandgasse Theaterstra e Friedrichstr ae Mrzgasse Akademiestra e Neugasse Rohrbacher Strae Bismarckstrae Nadlerstr ae St An na Gasse Fahrtgasse Thibautstra e Pfaffengasse Am Brckentor Zoo Heiliggeiststr Marstallstr ae Schigasse Bauamtsgasse Ziegelgasse Brunnengasse Bienenstrae Karpfengasse Unter e Stra e Fischmarkt Marsiliusplat z Richard Hauser- Platz Stadthalle Friedrich- Ebert Platz Bismar ck- Platz Rathaus Kornmarkt Schlangenweg Unter e Neckarstra e Landfriedstr ae Neuenheimer Landstr ae Ziegelhuser Landstr ae Neckarstaden Friedrich Ebert Anlage Gaisbergtunne l Schlossberg- tunnel Friedrich Ebert Anlage Kurfrsten Anlage Neckarstaden Schurmanstr ae Uferstr ae Neuenheimer Landstrae Plck Plck Hauptstr ae Bergheimer St rae Bahnhofst rae Hauptstr ae Merianstr ae Ingrimstr ae Zwingerst ra e Un t Fauler Pel z Ober er Fauler Pelz Neue Schlossstr ae Theodor Heuss Brcke Karl Theodor B rcke Brckenstrae Neue Schlossstrae Mrc henpar adies Fairy Tale Par adiseSchlierbach Rohrbach Neuenheim Neckarwiese Hirschgasse Alte Brcke Old BridgeFugngerbergan g pedestrian cr ossing Karlsplatz Neckarmnz- platz Marktplatz Universitts- platz Schloss Solar po wered boat Heidelberger Schloss CastleNeuenheimer Feld Kliniken Hospital Hauptbahnhof main stationKloster Benedictine abbe y Stift Neubur g Philosophenweg Philosophers W alk Kirchheim Weststadt Ziegelhausen Weisse Flotte Footpath Pier Bus tours Public toilets Tourist Information HD Card saleonly entry and exit Parking garage Meeting point Funicular railway Bus parking\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "erweiterung Accommodations Packages 2023modern modern elegant elegant delicious delicious central central traditional traditional cultural cultural classical classicalfamous famous diverse diverse Heidelberg Marketing Gmb H Neuenheimer Landstrasse 5 69120 Heidelberg Germany Phone 49 6221 5840 226 Fax 49 6221 5840 222 reservationheidelberg-marketing.de marketing.com The service team is pleased to help you Monday Thursday 900 am-600pm Friday 900 am-300pm subject to change Welcome Let your journey through Heidelberg begin there is a lot to discover In this brochure, we have put together short trips that you should not miss We provide complete packages on various subjects that include matching hotels to guarantee your perfect stay Experience our beautiful city on the Neckar River with its world famous castle ruin, the romantic Old Town and the legendary atmosphere of a young university city Enjoy the magic of special places and a plethora of events during any season No matter if you want to stay close to the city center, have a unique, romantic or family- like accommodation, you will certainly find something that suits your taste Our team will be happy to assist you in finding your ideal accommodation Our city is delicious, romantic, scientific, captivating, cultural and much more Choose what you want to explore first Our package offers will let you experience Heidelberg just as you like it Look forward, for example, to special events such as our Heidelberg Castle Illuminations or discover Heidelbergs unique natural scenery Contact us We will turn your stay into a unique experience visit Heidelberg, the city, where so many have left their hearts before Have fun exploring Your Heidelberg Marketing team heidelberg4you instagram.comheidelberg4you HE ID EL BE RG4you youtube.comheidelberg4you Heidelberg4you facebook.comHeidelberg Discover Heidelberg how its done 1 Choosing your trip tell us when you would like to ar rive, the number of rooms needed, the number of people traveling, and the name of the package from this brochure 2 Finding your accommodation we will make you an offer with matching hotels for your stay in Heidelberg 3 Choosing your hotel pick a hotel from our sugges- tions We will book your accommodation and send you a booking confirmation 4 Welcome to Heidelberg Before arriving you will re ceive all documents for your package at the Tourist Infor mation at the Heidelberg main station We look forward to meeting you Our partner hotels are listed on page 22  Bookings are possible on request, and based on availability, for up to 14 persons If your chosen hotel category preferred ho tel is no longer available, the prices may change Please note that you have to request your package at the latest 3 work ing days before your planned arrival Tip Save at the weekend with a stay at a first class hotel thanks to our first class weekend rate see note first class week end in the price tables This rate covers arrival on Friday at the earliest and depar ture on Monday at the latest public holi days are treated as weekends Note Please note that, according to legal regulations, after the conclusion of contracts regarding domestic tourism services, particularly in the case of con tracts for accommodations, guided tours and cultural events There is no statutory right of withdrawal, except that there could be a cancellation which generally involves a fee in line with the agreed terms and conditions or of the statutory provisions 4 Packages with overnight stay The Heidelberg CA RD is already included in all packages and offers you numerous advantages and discounts within the entire city area Benefit from the following included services castle ticket including admission to the castle courtyard, the bar- rel cellar as well as the German Pharmacy Museum and the fare for the funicular railway trip to the castle with continuation of the trip to the Molkenkur station return trip including one stop combination ticket one time free entrance to the University Museum, the Student Prison and the special exhibition free use of public transport in Heidelberg numerous discounts on tours, museums, leisure activities, restaurants and shops Further information can be found hereHeidelberg C AR D Heidelberg in your pocket Note The Heidelberg Card can also be booked without a package with best price guarantee Main area Heidelberg Heidelberg CA RD Packages with overnight stay 5 Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from Family Gltig ab Valid from Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de This journey will let you experience why so many have lost their hearts to our city Explore the picturesque and charis matic alleys during a walking tour of the Old Town and see the many small places that make Heidelberg one of the most beautiful cities The busy activity in the his torical city core will draw you in and make you forget your everyday life in its unique atmosphere A trip aboard a solar powered boat on the Neckar River offers a very special view of the scenery Surely, you too will love our city, which is inseparably connec ted to German RomanticismIncluded services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Participation in the public walking tour of the Old Town 1 5 hours Ticket for a trip on the solar boat Neckarsonne 6 Packages with overnight stay For an additional charge, the boat trip can also be booked as a 3 hour castle tour round trip We will be happy to assist you in planning Bookable from April to October 2023 Lose your heart in Heidelberg Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 125 185 240 Single room 165 260 350 First class weekend Double room 135 210 280 Single room 190 310 420 First class Double room 145 230 310 Single room 210 350 480 Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Explore Heidelberg in all directions by bus and with a very special view of the backdrop of Heidelbergs Old Town Accompanied by a tour guide, this City and Castle Sight seeing Tour along the Neckar River will take you up to Heidel berg Castle During the subsequent outdoor tour, your guide will introduce you to the history of the fasci nating building You will also visit the inner courtyard and the famous Great Barrel After the tour, you can visit the German Pharmacy Museum and then enjoy the ride on the funicular railway back to the Old Town which is included in your castle ticket Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay City and Castle Sightseeing Tour including castle ticket 2 hours Packages with overnight stay 7 Bookable all year round for 2 people and more Crisscross through Heidelberg Schedule for the City and Castle Sightseeing Tour, meeting point Neckarmnzplatz information panel at the bus stop April October German Friday, Saturday and Whitsunday 130 pm except September 30 , 2023 November March German Saturday 130 pm Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 130 190 245 Single room 180 280 370 First class weekend Double room 140 215 285 Single room 205 325 435 Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de 8 Packages with overnight stay Bookable at the weekends of June 3 , July 8 and September 2 , 2023 Heidelberg Castle Illumination The Heidelberg Castle Illuminations in June, July and Sep tember cast their spell on thousands every year Bengali fires illuminate the world famous ruin in a mysterious red light, in memory of the destruction of the castle in the Palatinate War of Succession The origin of the subsequent fireworks from the Alte Brcke Old Bridge, however, is a romantic one Prince Elector Frederic V had fireworks held in 1613 for his new wife Elisabeth Stuart, thereby founding this tradition that is now more than four hundred years old Experience the Heidelberg Castle Illumination from the front seats as the highlight of a 1 5 hour boat trip into the Neckar valley When the anchor is lowered at the Alte Brcke, you will have a gorgeous view of the spectacle Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Fireworks boat trip on Saturday evening with the Weisse Flotte Heidelberg 2 course menu on board Participation in the public walking tour of the Old Town 1 5 hours Schedule for the boat trip to the Heidelberg Castle Illumination Admission as of 700 pm, departure at 800 pm, end approx 1100 pm The seats on the ship are reserved for you Drinks are charged separately and paid on site Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 195 255 310 Single room 240 330 430 First class weekend Double room 205 280 345 Single room 265 375 485 Combine your visit to B UG A 23 in Mannheim with an event ful and at the same time relaxing stay in Heidelberg and book our event specific package In addition to a day ticket for the German National Garden Show, this package includes a varied program in our at tractive city Benefit from the advantages of our Heidel berg Card, which allows you, among other things, to visit the world famous castle ruin as well as the Student Prison of the University of Heidelberg, the oldest university in Germany Get to know the charm of the historic Old Town on a guided tour and enjoy a 3 course menu in a restau rant in the heart of the Old Town setting sit back and re live the experience Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Day ticket for the German National Garden Show 2023 B UG A, one time access to the exhibition areas Participation in the public walking tour of the Old Town 1 5 hours 3 course menu in a restaurant in the Old Town Bookable from April 14 to October 8 , 2023 B UG A flower pot German National Garden Show Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Packages with overnight stay 9 Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 170 235 285 Single room 220 310 400 First class weekend Double room 185 255 325 Single room 240 355 470 First class Double room 195 275 355 Single room 260 395 530 Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de 10 Packages with overnight stay Visit Heidelberg in summer and enjoy sunbathing in the meadows along the Neckar River, or explore the roman tic Old Town in mild evening temperatures In the warm sea son, life is exuberant in Heidelbergs twisty alleys The many cozy cafs and pubs offer places in the summer air and in the middle of our citys busy activity Many cultural highlights will make your summer in Heidelberg a memo rable experience Because the sun is shining so much nicer here, we will pay for the third night of your stay Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Participation in the public walking tour of the Old Town 1 5 hours Ticket for a trip on the solar boat Neckarsonne 50 minutes Bookable from July 21 to August 27 , 2023 Summer Special stay 3 nights, pay for 2 Price per person in Hotel category Type of room 1 night 2 oder 3 nights Comfort Double room 120 185 Single room 170 260 First class weekend Double room 135 220 Single room 190 340 First class Double room 145 245 Single room 210 390 The city puzzle game City Quest offers a truly unique opportunity to see the city through new eyes Whether as a couple, with the family or with friends the game pro vides the perfect combination of fun and exploration The ideal group size is two to six people An app for smart phone or tablet is used to navigate during the puzzle tour and to input the solution In addition, a City Quest bag with equipment needed to solve the puzzles will be handed out The game starts in the heart of Heidelbergs Old Town On the route of about three kilometers, you will find eleven other stations All players must prove their creativity, co-operation and powers of deduction in order to master the tricky puzzlesIncluded services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Digital City Quest scavenger hunt App based 2 5 hours Bookable all year round for 2 people or more City Quest Heidelbergs interactive quiz The app is required for the rally and can be downloaded in advance via AndroidI OS The download is free A deposit payment of 50 in cash for the City Quest bag is obligatory The deposit will be made at Hostel Lotte, Burgweg 3 , 69117 Heidelberg Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 110 175 240 Single room 155 250 345 First class weekend Double room 125 200 275 Single room 180 300 420 First class Double room 135 225 310 Single room 200 345 485 Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Packages with overnight stay 11 Would you like to complement your stay in Heidelberg with a themed culinary experience on board of a ship If so, come aboard and enjoy this package You will stay overnight in a single or double room in a com fort or first class hotel In the evening, you can round off the day with a theme cruise on the ship including a menu In ad dition to theme evenings such as an Italian evening or a Kurpflzer evening, where typical regional dishes are served, a relaxed Grill Chill cruise, or even a Captains Dinner with wit and charm, you can choose from various dates and theme cruises From April to October, the package can also be booked including a public tour of the Old Town for an ad ditional charge Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Themed boat trip including dinner on a ship of the Weisse Flotte Heidelberg Bookable all year round A HO Y welcome aboard for a culinary delight Drinks are charged separately and paid on site Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de 12 Packages with overnight stay Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 180 235 295 Single room 240 320 410 First class weekend Double room 190 260 330 Single room 250 360 470 First class Double room 200 280 360 Single room 270 400 530 An overview of all available dates and trips can be found here Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Packages with overnight stay 13 Bookable from April to October 2023 for 4 people and more Delectable Heidelberg Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 205 255 315 Single room 250 340 425 First class weekend Double room 220 285 355 Single room 275 385 495 Besides the breathtaking scenery, Heidelberg offers a lot for all senses This package will show you the culinary side of the city A delightful tour through our Old Town will let you ex plore the most deli cious sides of Heidelberg The exclusive tour Delectable Heidelberg will let you enjoy a perfectly co ordinated ex perience An aperitif matured under the sun of Baden, a regional 3 course menu in two traditional Heidel berg restaurants, topped off with a sweet Heidelberg treat Time and paths between the courses will be spiced up enter tainingly with enjoyable stories A ride in the solar boat will complete your stay with a unique view of the city This is a tour for true connoisseurs who want to fall in love with Hei delberg for good Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Participation in the public walking tour Delectable Heidelberg through the Old Town 3 hours, only in German Ticket for a trip on the solar boat Neckarsonne 50 minutes Times Delectable Heidelberg tour, meeting point Marktplatz Market Square, Herkulesbrunnen Hercules Fountain starts at 530 pm, April 122 , May 627 , June 1024 , July 129 , August 526 , September 923 and October 714 , 2023 Drinks are charged separately and paid on site 14 Packages with overnight stay Get to know Heidelberg from its quiet side and discover the unique natural scenery that makes our romantic city on the Neckar River so worth living in Starting from the Bis marckplatz, along the famous Philosophenweg Philoso phers Walk and, continuing through Neuenheimer Schweiz, an impressive nature reserve, you will reach the idyllic Benedictine Abbey Stift Neuburg There you will have earned a cool, regional organic beer, brewed in the small Brauerei zum Klosterhof the Abbey courtyard brewery with the spring water of the monastery Afterwards, you can either walk along the Neckar River back to the Old Town or take the boat back to the city center for an additional fee, season April October An overnight stay including break fast and the Heidelberg C AR D round off your unique stay Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Digital hiking route via Komoot 1 5 hours, family friendly beginners tour 1 bottle 1 liter of regional organic beer from the Heidelberg Klosterhof brewery per person Bookable all year round for 2 people or more Strolling around Heidelberg App required Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 110 170 230 Single room 155 255 340 First class weekend Double room 125 195 265 Single room 185 300 405 First class Double room 135 215 295 Single room 205 340 470 Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Arrangements mit bernachtung 15 Bookable from November 2023 to March 2024 Winter Dream stay 3 nights, pay for 2 In the winter months, Heidelberg proves itself to be a partic ularly inspiring destination Our castle ruin and the twisty Old Town alleys are very atmospheric in the cold season, inviting you to a relaxed stroll before warming up again in one of the many restaurants or laid back, cozy student pubs The winter months are also full of exiting events and cultural highlights for a diverse city experience Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Traditional Heidelberger Studentenkuss students kiss Price per person in Hotel category Type of room 1 night 2 oder 3 nights Comfort Double room 105 165 Single room 150 245 First class weekend Double room 115 200 Single room 180 320 First class Double room 125 225 Single room 200 380 except for Advent weekends 16 Packages with overnight stay Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Price per person in Hotel category Type of room 2 nights 3 nights Comfort Double room 205 265 Single room 295 380 First class weekend Double room 230 300 Single room 340 450 Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de In the Advent season, the romantic charisma of our city is topped once again From the end of November until just before Christmas Eve, the Christmas market covers the entire Old Town During these weeks, around 130 booths create a sea of lights, bring festive music and the scent of winter treats A guided tour of the Christmas Market will familiarize you with the magic and the special offers of our markets A 3 course menu in our Christmas themed Old Town makes your stay just perfect Included services 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Participation in the public Christmas Walking Tour on Saturday 1 5 hours Traditional 3 course menu in a restaurant in the Old Town Packages with overnight stay 17 Bookable at Advent weekends Advent experience Bookable from January to June and September to December 2023 Theater and Orchestra Heidelberg Our renowned theater and orchestra of Heidelberg wel comes you to an exciting cultural package The en-sembles offer something for everyone musical theater, concerts, plays, dance and a dedicated child and youth theater Choose from a diverse schedule with modern plays, popu lar classics and exiting newcomers The pack- age also in cludes a 3 course menu in a restaurant in our beautiful Old Town Lean back and reminisce about your experiences Find the current schedule here Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Tickets for a performance at the Theater and Orchestra Heidelberg category I I Traditional 3 course menu in a restaurant in the Old Town 18 Packages with overnight stay Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 160 220 270 Single room 200 300 390 First class weekend Double room 175 245 315 Single room 225 345 455 First class Double room 185 265 345 Single room 255 395 515 Bookable from June 11 to July 30 , 2023 Heidelberger Schlossfestspiele Heidelberg Castle Festival Sights youve never seen before The combination of the incredible flair of our castle ruin with its legendary pre-sentations and highclass concerts Every summer, the Heidelberg Castle Festival draws many culture connois seurs to our castle With this package you become part of this fascination and it lets you be there for a play of your choice A walking tour of the Old Town and a 3 course menu will bring you summer days you will surely remem ber forever Find the current schedule here Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Tickets for a performance of the Heidelberg Castle Festival category I I Traditional 3 course menu in a restaurant in the Old Town Participation in the public walking tour of the Old Town 1 5 hours Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Packages with overnight stay 19 Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 180 240 295 Single room 220 320 405 First class weekend Double room 190 265 325 Single room 250 360 470 First class Double room 200 285 360 Single room 270 400 530 Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de 20 Packages with overnight stay Bookable from March 17 to April 15 , 2023 Heidelberger Frhling Heidelberg Spring Music festival Music lovers hearts beat faster in Heidelberg, especially in the springtime From March to April, the classical music scene celebrates a festival with international renown With over 100 events and 47 , 000 visitors, the Heidelberg Spring is one of the largest festivals of its kind in Germany On this journey you will be our guest at a concert Choose your favourite event from the top class program and complete your concert evening with a culinary 3 course menu in our beautiful Old Town The current festival program can be found here Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Tickets for a concert category I I 3 course menu in a restaurant in the Old Town Participation in the public walking tour of the Old Town 1 5 hours Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 200 260 310 Single room 240 330 430 First class weekend Double room 215 280 350 Single room 265 375 495 First class Double room 225 305 380 Single room 285 415 555 Except for opening and closing concert Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Accommodated in the Art Nouveau ambience of the former public bath in Bergheim, you can get some fascinating in sights into the Anatomy of Happiness The K RP ER W EL TE N museum has exhibited Gunter von Hagens unique plastina tions of human and animal bodies since September 2017  Preserved in life like positions, the exhibits illustrate our anatomy better than ever before Come for an exciting journey and learn what striving for happiness is all about Included services 1 , 2 or 3 nights with breakfast in a single or double room in the hotel categories comfort or first class Heidelberg CA RD for the duration of your stay Entrance ticket to the exhibition of the B OD Y W OR LD S M US EU M in the Altes Hallenbad Heidelberg with audio-guide Traditional 3 course menu in a restaurant in the Old Town Bookable all year round B OD Y W OR LD S M US EU M K RP ER WE LT EN The Anatomy of Happiness Opening hours Monday Sunday 1000 am 600 pm last admission 500 pm, closed on December 24  Subject to changes Price per person in Hotel category Type of room 1 night 2 nights 3 nights Comfort Double room 155 215 265 Single room 205 290 380 First class weekend Double room 170 240 310 Single room 230 350 450 First class Double room 180 260 340 Single room 250 390 510 Packages with overnight stay 21 Participating Partner Hotels 2023 Our package trips let you choose between the comfort and first class categories The overview lists all participating partner hotels, classified by these categories We are happy to support you in choosing your perfect accommodation do not participate in the Winter Dream and Summer Special packages not listed in this brochure Hotel page Bayrischer Hof 24 Gasthaus Backmulde 26 Hackteufel 26 Hotel am Rathaus 27 Hotel am Schloss 28 Hotel Heidelberg Lounge 28 Kulturbrauerei Heidelberg 29 Staycity Aparthotel Heidelberg 31 The Heidelberg Exzellenz Hotel 32 Hotel page Hotel Zum Ritter St Georg A TL AN TI C Hotel Heidelberg 24 Bergheim 41 Hotel im Alten Hallenbad 25 Hilton Heidelberg 27 N H Heidelberg 29 Plaza Premium Heidelberg 30 Qube Hotel Bahnstadt 30 Qube Hotel Bergheim 30 Rafaela Hotel Heidelberg 31 22 Participating Partner Hotels Hotel category comfort Hotel category first class Hotel facilities Restaurant Charging station for electrical vehicles Bar Free coach parking Conference rooms Bicycle parking Elevator Terrace garden Air conditioning Fitness room Wi Fi Sauna Garage Swimming pool Parking Pets welcome S R Single Room D R Double Room Distance to the Old Town 1 Old Town Directly in the town center 2 City center The Old Town is within 5 15 minutes walking distance 3 Urban area The Old Town is within 20 30 minutes walking distance and can be reached easily by public transport within 5 10 minutes 4 Suburbs Districts within the city border the Old Town can be reached easily by public transport within 15 20 minutes Legend distances Highway Neckarmnzplatz Tourist Information Main station Nearest public transport stop Hotel rating under D EH OG A German Hotel and Restaurant Association Accommodation establishments which are not marked with stars have not undergone the voluntary rating process No inference to their standard is intended Tourist basic standards Standard middle standards Comfort raised standards First Class high standards Luxury the highest standards Superior additional rating Accommodations We will gladly advise you on our hotels and book your stay in Heidelberg All the hotel information provided and the room prices are a reference only We will make you an individual offer on request Find a city map on pages 36 and 37  Accommodations 23 Hotel category first class Arthotel Heidelberg Grabengasse 7 , 69117 Heidelberg, Phone 49 6221 6500601 Boutique Design Hotel a combination of a her- itage listed building and an architecturally attractive modern building unique in the heart of Heidel bergs Old Town Our hotel has its own parking facil ities, and a few minutes walk away you will find the sights Our rooms boast all the modern comforts, our restaurant R OM ER an array of culinary delights and our function rooms the perfect atmo sphere for your celebration Heidelbergs sights are right outside the door Rooms 23 D R 139 232 , S R 119 202 , 1 Loft room Suite 195 390 Distance approx 3 5 km 2 8 km 900 m Peterskirche Universittsplatz 200 m1 5200 15 , 5024 h 24 Hotels Hotels Bayrischer Hof Rohrbacher Strasse 2 , 69115 Heidelberg, Phone 49 6221 8728803 This hotel is personally run and boasts an enviable central location in the heart of Heidelberg Cross over Bismarckplatz Bismarck Square directly in front of the hotel and after 100 m you find yourself in the magic of Heidelbergs Old Town with its romantic castle and quaint twisty lanes Rooms 49 D R from 79 , 7 S R from 69 Distance approx 4 km 1 5 km 2 5 km Bismarckplatz 100 m2 1224 h A TL AN TI C Hotel Heidelberg Europaplatz 1 , 69115 Heidelberg, Phone 49 421 944888535 Pre-Opening2 Exceptional 4 to 5 star comfort and individual full service await you at the northern German influ enced A TL AN TI C Hotels Group In close proximity to the new Heidelberg Congress Center and the train station, the 4 star superior A TL AN TI C Hotel Heidelberg will offer 310 stylish rooms and suites, a spa and wellness area, a restaurant, and a rooftop bar with a terrace and a view over Heidelberg and the surrounding area from mid 2023  State of- the art conference rooms and a 350 square meter flexible ballroom on the 13th floor round off the facilities Rooms 310 D R from 199 , S R from 179 Distance approx 1 km 0 1 km 4 1 km Gadamerplatz 100 m3 8600 Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Hotels 25 Bergheim 41 Hotel im Alten Hallenbad Bergheimer Strasse 41 , 69115 Heidelberg, Phone 49 6221 7500405 The Bergheim 41 is a contemporary city hotel Your perfect choice for both business and leisure accom modation The hotel has 32 comfort rooms, 1 junior suite with sauna, rooftop garden with castle view and the Bergheim 41 Kaffeekultur cof- fee shop Short distances for shopping or sight seeing are guaranteed as the pedestrian area and the Old Town are only 250 m away from the hotel Rooms 31 D R 124 174 , S R 109 159 , 1 Junior Suite 280 330 Distance approx 3 9 km 2 4 km 2 1 km Altes Hallenbad 50 m2 11 , 5024 h Berggasthof Knigstuhl Knigstuhl 2 , 69117 Heidelberg, Phone 49 6221 41603004 Berggasthof Knigstuhl is located on the summit of Heidelbergs local mountain and offers 62 rooms The facilities include a restaurant, a hotel bar, a 24 hour reception and free Wi Fi in all ar- eas The 4 star property also provides a sauna and a terrace Private parking with and without charge is available at the hotel All rooms at the hotel are equipped with a desk and include a safe and minibar Rooms 6 Economy Rooms from 99 , 28 Classic Rooms from 124 , 24 Deluxe Rooms from 154 , 4 Junior Suites from 174 Distance approx 4 km 3 6 km 1 6 km Knigstuhl 200 m4 3150 1524 h 1524 h City Partner Hotel Hollnder Hof Neckarstaden 66 , 69117 Heidelberg, Phone 49 6221 605006 Thanks to our unique location in the heart of the Old Town, our guests can enjoy a divine view of the Alte Brcke Old Bridge, the Neckar River and the Philosophenweg Philosophers Walk Two dif ferent breakfast options, our extensive breakfast buffet and the small business breakfast, as well as a range of air conditioned rooms round off what we have to offer You can reach all of Heidelbergs top attractions within a few minutes on foot Rooms 26 D R from 109 , 12 S R from 91 , 1 Suite 216 233 Distance approx 4 km 4 km 300 m Alte Brcke 50 m1 Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Der Europische Hof Heidelberg Friedrich Ebert Anlage 1 , 69117 Heidelberg, Phone 49 6221 51507 Tradition meets emotion spoil yourself in a wel coming, family run luxury hotel in its third and fourth generation 100 rooms, 14 suites, 3 executive suites all timelessly furnished with elegance and individu ality Enjoy French modern interpreted dish es in our fine dining restaurant Die Kurfrsten stube or delight yourself with Mediterranean del icacies in our summer restaurant terrace Find refreshment, rest and relaxation in our P AN OR AM A S PA The von Kretschmann family welcomes you Rooms 60 D R 258 470 , 40 S R 209 347 , 14 Junior Suites 474 566 , 3 Executive Suites 626 756 Distance approx 4 km 1 4 km 1 8 km Bismarckplatz 350 m1 10200 2324 h 26 Hotels Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg-marketing.de Gasthaus Backmulde Schiffgasse 11 , 69117 Heidelberg, Phone 49 6221 536608 We are sure you will feel right at home here a place where both your soul and your taste buds will be pampered We will see to your needs and wishes, making your stay a relaxing and comfortable expe rience After a good nights rest, you will be ready to enjoy everything our premises have to offer Be our guest and leave your daily routine behind It will be our pleasure to look after you Rooms 3 D R 138 , 17 Comfort D R 148 , 5 S R 120 130 , 2 Junior Suites 180 Distance approx 3 km 4 1 km 700 m Stadthalle 300 m1 2100 1524 h Hackteufel Steingasse 7 , 69117 Heidelberg, Phone 49 6221 9053809 The privately managed house with family like at mosphere, located at the core of Heidelbergs Old Town between the Heiliggeistkirche Church of the Holy Spirit and the Alte Brcke Old Bridge, invites you to enjoy stylish life The 12 hotel rooms and the holiday apartment have been lovingly and indivi dually furnished They come with L CD T V and free Wi Fi The cozy restaurant and the wine tavern welcome you with regional cuisine and a well stocked wine cellar Welcome Rooms 9 D R from 109 , 2 S R from 85 , Apartment from 189 , Junior Suite from 189 Distance approx 3 km 3 km 800 m Alte Brcke 100 m1 824 h Heidelberg Marriott Hotel Vangerowstrasse 16 , 69115 Heidelberg, Phone 49 6221 908010 Heidelberg Marriott Hotel is the perfect place for a few days of relaxation or your event Idyllically loca ted on the banks of the Neckar River, getting to and exploring Heidelberg is convenient You can reach the Old Town on foot or by boat from the jetty at the hotel Visit our sun terrace, the restaurant Grill 16 or the bar, and relax in the sauna, the fitness center or by the pool Highest hygiene stan dards and free Wi Fi are a matter of course 248 rooms, 7 function rooms for max 330 persons Rooms 125 D R from 149 , 64 Balcony rooms Neckar River view from 169 , 45 Executive Rooms from 209 , 11 Junior Suites from 239 , 3 Executive Suites from 439 Distance approx 500 m 800 m 3 km Betriebshof 500 m3 7330 2524 h Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Hilton Heidelberg Kurfrsten Anlage 1 , 69115 Heidelberg, Phone 49 6221 352132011 Friendly hospitality in the center of Heidelberg Newly opening in 2023 , the hotel with its 244 elegant and high quality equipped rooms and suites benefits from a prime location It is only a few minutes away from the banks of the Neckar River and the main attractions such as the Old Town and Heidelberg Castle After an exciting day you can indulge yourself in the hotels own restaurant or enjoy the exclusive atmosphere in the hotel bar or executive lounge Rooms 108 D R from 139 , 79 Deluxe Rooms from 169 , 47 Executive Rooms from 239 , 4 Junior Suites from 289 , 6 One Bedroom Suites from 339 Distance approx 4 km 1 2 km 2 km Seegarten 10 m2 7300 2524 h Hotel am Rathaus Heiliggeiststrasse 1 , 69117 Heidelberg, Phone 49 6221 1473012 The hotel is situated in the heart of the Old Town, directly at the market square The pedestrian ised High Street and the Church of the Holy Spirit are just around the corner All the historic sights as well as the original student pubs and Heidelberg Castle are also just a short walk away Parking is available in the carpark P12 200 m away Rooms 14 D R 125 170 , 3 S R 89 130 , 1 Three bed 155 189 , 1 Four bed 170 199 Distance approx 3 5 km 3 km 350 m Rathaus Bergbahn 200 m1 1024 h Hotels 27 Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Hotel am Schloss Zwingerstrasse 20 , 69117 Heidelberg, Phone 49 6221 1417013 Relax in the pleasant atmosphere of the Hotel am Schloss From our rooms, you have a divine view over the Old Town of Heidelberg On our roof ter race, you can sit directly at the foot of Heidelberg Castle Our breakfast buffet guarantees a good start into the day There is a parking garage in the building Rooms 15 D R 125 170 , 4 S R 97 130 , 2 Three bed 155 180 , 1 Four bed 170 210 Distance approx 4 km 4 km 600 m Rathaus Bergbahn 2 m1 1024 h Hotel Berger Erwin Rohde Strasse 8 , 69120 Heidelberg, Phone 49 6221 40160814 Located in Heidelbergs central suburb of Neuen heim, the family run art nouveau villa invites you to come and relax around the heated outdoor pool in the peaceful garden setting Our hotel and the nearby annex with apartments are only a few min utes from the Old Town, the university campus and hospitals Come and enjoy the personal atmosphere in our individually designed hotel rooms and if you are here for a longer stay, make yourself at home in our luxuriously designed apartments Rooms 4 D R 120 160 , 2 S R 95 105 , 2 Family Rooms from 175 Distance approx 2 4 km 2 km 2 1 km Mnchhofschule 100 m2 1224 h Hotel Heidelberg Lounge Heuauer Weg 35 37 , 69124 Heidelberg, Phone 49 6221 354441515 At the gates of the city, in the Kirchheim district, you will find a cozy atmosphere and an ideal location The city center is easily reached within 20 minutes by public transport, jogging and cycling paths start at the hotel You can park your bike in the bike garage with e-bike charging possibility, for your car there is also an e charging station After an eventful day, relax in the lounge with delicious drinks, tarte flambe dishes to share, on sunny days also on the terrace Rooms 24 D R from 99 , 10 S R from 75 Distance approx 5 km 4 km 7 km Kirchheim Friedhof 50 m4 28 Hotels Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Hotel Krokodil Kleinschmidtstrasse 12 , 69115 Heidelberg, Phone 49 6221 739297016 The Hotel Krokodil is situated in the peaceful West stadt, only a short walk from the historic Old Town This hotel offers quiet and spacious rooms that really are value for money Enjoy the pleasant atmosphere and friendly service of a smaller, and more personal hotel A delicious breakfast buffet awaits you in the morning In our in house restau rant, you will be spoiled with regional specialties Rooms 13 D R from 115 , 3 S R from 90 Distance approx 2 km 500 m 2 km Stadtbcherei 200 m2 12 , 5024 h Kulturbrauerei Heidelberg Leyergasse 6 , 69117 Heidelberg, Phone 49 6221 50298017 The Kulturbrauerei Heidelberg hotel is located at the foot of Heidelberg Castle in the heart of Heidelbergs Old Town Our rooms are spread across the old brewery building, the newly erec ted annex and above the inn Wirtshaus Zum Seppl All rooms have Wi Fi and are non smoking Rooms 34 D R 141 199 Distance approx 7 km 4 km 100 m Rathaus Bergbahn 200 m1 1524 h N H Heidelberg Bergheimer Strasse 91 , 69115 Heidelberg, Phone 49 6221 1327018 The N H Heidelberg in the city center of Heidelberg is the ideal starting point to discover one of the most romantic and beautiful cities in Germany and the metropolitan region between the Rhine and Neckar River Heidelberg is famous for its pioneering science and research, its history, well known com panies, and elite universities The hotel has 168 rooms in both a new annex and a historic building, which once be- longed to the Heidelberg brewery Rooms 158 D R from 109 , 10 S R from 109 Distance approx 1 km 1 km 2 5 km Volkshochschule 100 m2 16400 2424 h Hotels 2930 Hotels P LA ZA Premium Heidelberg Sofienstrasse 6 8 , 69115 Heidelberg, Phone 49 6221 435392019 The recently opened P LA ZA Premium, which is situated close to the historic Old Town, offers you the perfect place for all kinds of leisure activities and excursions The central and quiet location of our hotel, is ideal for families, city and business travelers Rooms 87 D R Comfort from 134 , 70 D R Premium from 149 , 87 S R Comfort from 119 , 70 S R Premium from 134 , 10 Three bed from 174 , 1 Junior Suite from 214 , 7 Suites from 244 , 1 Presidential Suite from 304 Distance approx 5 km 1 7 km 1 8 km Bismarckplatz 200 m2 1724 h Qube Hotel Bahnstadt Grne Meile 21 , 69115 Heidelberg, Phone 49 6221 63900020 All rooms at the new Qube Bahnstadt have ge nerous glass sections that let in plenty of natural light The hotels high level equipment includes the latest presentation technology and fiberglass high- speed internet Quality and design enjoy high priori ty in all 84 rooms with their oak wood parquet floors, leather armchairs, walnut desks and natural stone bathtubs The Qube Bahnstadt places special value on the gastronomy area with restaurant, bar and cozy lounge The beautiful roof terrace is a special highlight Rooms 84 D R from 108 Distance approx 700 m 700 m 4 6 km Gadamerplatz 300 m3 4100 14 , 8024 h Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Qube Hotel Bergheim Bergheimer Strasse 74 , 69115 Heidelberg, Phone 49 6221 18799021 The boutique hotel captivates with its sophisticated interior and sustainable features The privately run hotel Qube Bergheim places great importance on quality, service and friendliness Excellent location in the town center Expansive restaurant area with lounge and bar Underground parking garage in the hotel Rooms 26 D R 108 208 , 11 S R 98 188 , 16 Superior 138 228 , 9 Deluxe 158 238 Distance approx 700 m 700 m 2 6 km Volkshochschule 20 m2 270 14 , 8024 h seven Days Hotel Boarding House Eppelheimer Strasse 14 , 69115 Heidelberg, Phone 49 6221 7530023 The seven Days is a unique combination of hotel and boarding house Each studio has a balcony and a fully equipped kitchenette with microwave oven, induction hob and coffee machine The studios are available for one night or longer stays, e g if you are here on a stay for weeks or months In our break fast restaurant, we offer a sumptuous breakfast buffet Rooms 68 D R from 89 , 68 S R from 79 , 11 Penthouse from 99 , 6 Apartments from 119 Distance approx 500 m 300 m 4 km Czernybrcke Sd 100 m3 1024 h 1024 h Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Rafaela Hotel Heidelberg Lutherstrasse 17 , 69120 Heidelberg, Phone 49 6221 674330022 Central, idyllic, modern Rafaela Hotel is located in direct proximity to the popular Neckar meadow and the Old Town with its tourist sights, as well as close to the Neuenheimer Feld with its research fa cilities and hospitals A small market square right outside of the hotel door beckons with gastronomy offers, outdoor chairs and French, Italian as well as sustain able specialties offers down a calm side street Parking places are available 300 m away in a public parking garage Rooms 24 D R 109 183 , 24 S R 90 162 , 3 Family Rooms 113 216 Distance approx 4 km 2 2 km 2 km Brckenstrasse 240 m2 360 Hotels 31 Staycity Aparthotel Heidelberg Speyerer Strasse 7 9 , 69115 Heidelberg, Phone 49 6221 3600024 The Staycity Aparthotel Heidelberg convinces with its proximity to the historic Old Town The hotel rooms are bright and generously furnished as well as air- conditioned and offer a separate shower and toilet area, hairdryer, L CD T V, safe, and free Wi Fi The apartments are additionally equipped with a kitche nette and thus suitable for a longer stay Reception and lobby bar are open 247 90 hotel parking spaces with e-car charging stations Rooms 215 D R from 80 , 84 Apartments from 109 Distance approx 3 km 1 km 4 km Montpellierbrcke 200 m3 1524 h Information and booking 49 6221 5840 226 , reservationheidelberg marketing.de Bauernhof Koch Holger und Marianne Koch, Bahnhofstrasse 50 , 68535 Edingen Neckarhausen, Phone 49 6203 85715 , infobauernhof koch edingen.de26 Whether you are planning a vacation in the countryside, a city trip or a business stay, from our farm you can easily reach the entire metropolitan region Our vaca tion apartments are designed to be largely barrier free and also offer sufficient space for families with children They each have their own terrace with a small garden All apartments are air conditioned, parking spaces for our guests are available free of charge Size 50 sqm, Rooms 2 , Bedrooms 1 , Beds 1 4 , Price from 80 4 Boardinghouse Luise5 Dr Friedrich Betzer, Luisenstrasse 5 , 69115 Heidelberg, Phone 49 6221 6556114 , welcomeluise5 boardinghouse.de27 Spacious apartments in a beautifully renovated historic building, in the direct neighborhood of the A TO S Klinik hospital Tastefully furnished, approx 50 sqm for up to 4 self caterers 1 separate bedroom 1 living dining area with fully equipped kitchen and 1 extensible sofa bed 1 daylit bathroom with shower toilet washing machine and dryer in the basement free Wi Fi 2 T Vs per apartment parking space at the house for a fee Size 50 sqm, Rooms 2 , Bedrooms 1 , Beds 1 4 , Price from 1292 The Heidelberg Exzellenz Hotel Rohrbacher Strasse 29 , 69115 Heidelberg, Phone 49 6221 915025 Our charming, individually appointed and personally run city hotel offers you a new lifestyle concept and an extraordinary design The centrally located art nouveau villa is only a few minutes walk from Heidelbergs Old Town Some of our rooms have a fully equipped kitchenette To make your stay more comfortable, we also have hotel parking facilities Rooms 35 D R from 115 , 13 S R from 82 Distance approx 3 km 2 km 2 km Hans Bckler Strasse 50 m2 130 1524 h 1524 h Holiday Apartments, Aparthotels and Hostels32 Hotels, Holiday Apartments, Aparthotels and Hostels Ferienbauernhof Gstehaus Fieer Family Fieer, Kirchheimer Hof 7 , 69124 Heidelberg, Phone 49 176 70709736 , infoferienbauernhof heidelberg.de28 In the middle of the field and yet in close proximity to the city lies our farm The old tobacco barn was converted into 4 modern, fully furnished vacation apartments 1 5 persons can be accommodated Around the farm there are many walking and cycling paths to get plenty of free space The little guests also get their moneys worth Small animals and a large outdoor playground make childrens hearts beat faster Size 70 75 sqm, Rooms 3 , Bedrooms 2 , Beds 1 5 , Price from 1004 Ferienwohnung Peperoni Heidelberg Ilona Jung, Rainweg 20 , 69118 Heidelberg, Phone 49 6221 808332 , c.i.jungarcor.de29 Our holiday apartment Peperoni is located in the green Ziegelhausen district, offering good traffic connection to all sights and event locations There are many hiking paths in close proximity, such as the Philosophenweg Philosophers Walk leading to Heidelbergs Old Town A bakery, an indoor swimming pool and the Frs tendamm bus stop are 200 m away Bus lines 33 , 34 and 37 will take you to any relevant destination in Heidelberg every 20 minutes For further information please also visit our website Size 42 sqm, Rooms 1 , Bedrooms 1 , Beds 1 4 , Price from 594 Holiday Apartments, Aparthotels and Hostels 33 Ferienwohnung Rittmller Ursula Rittmller, Stiftweg 28 , 69118 Heidelberg, Phone 49 6221 801147 , rainer.rittmuellert online.de30 Our property is in a beautiful rural setting only 2 3 minutes walk from the forest, tennis courts, indoor pool with sauna, solarium and sunbathing area hiking trails and direct bus connection to Heidelberg, the Old Town and the hospital Both apartments have a kitchen, shower and W C, separate access, terrace with use of the garden, cable T V and Wi Fi Size 30 60 sqm, Rooms 1 3 , Bedrooms 1 2 , Beds 1 4 , Price from 60 4 Gstehaus Komm Milvia Komm, Am Bchenbuckel 6 , 69118 Heidelberg, Phone 49 6221 20602 , gaestehaus kommt online.de31 Our accommodations are situated in a quiet area near the forest, offering stimula ting walks The holiday apartments have high quality furnishings fitted kitchen, bathrooms and toilet, including towels and bed linens, some with terrace, balcony and a lounge in the flower garden, S AT T V and free Wi Fi Two direct bus connec tions will take you to the Old Town Your dog is welcome here as well We look forward to your visit Size 32 84 sqm, Rooms 1 2 with kitchen bathroom, Bedrooms 0 1 , Beds 2 4 , Price from 60434 Holiday Apartments, Aparthotels and Hostels Weingut Bauer Ingrid Bauer, Dachsbuckel Winzerhof 1 , 69126 Heidelberg, Phone 49 6221 381931 , fewoheidelberger dachsbuckel.de34 The six apartments of the Bauer Winery are modernly furnished and very quiet In addition to a fully equipped kitchenette and internet access via Wi Fi, all apartments have a large living area Situated in the middle of the vineyards of the Heidelberg Dachsbuckel they offer the ideal starting point for a relax ing vacation in Heidelberg Size 45 80 sqm, Rooms 2 4 , Bedrooms 1 3 , Beds 3 7 , Price from 65 4 Heidelberg A PH EA RT ME NT S Marcus Weigl, Hauptstrasse 161 , 69117 Heidelberg, Phone 49 152 53182564 , mailheidelberg apheartments.com32 In the heart of the Old Town you will find four beautiful, newly renovated vacation apartments, which are ideal for short trips as well as for long term stays Staying in a romantic baroque house, you can easily reach all sights and institutions by foot, go shopping, stroll around and end the day in one of the traditional restaurants your vacation apartment and your dreamy comfortable bed are only a few steps away Size 92109 sqm, Rooms 3 4 with kitchen bathroom, Bedrooms 2 3 , Beds 1 7 , Price from 130 1 Zweite Heimat Heidelberg Monika Ihlenfeld, Werderstr 43 , Moltkestr 7 , Gaisbergstr 29 , Rmerstr 24 , Heidelberg, Phone 49 6221 6561943 , wohnenzweiteheimatheidelberg.de35 We want our guests to feel right at home This is why we value high quality living and furnishings We have good traffic connections to all sights and event locations You can cover your daily needs from nearby restaurants, cafs and stores Equipment T V, Hi Fi system, Wi Fi, fully equipped kitchen, washing machine, tumble dryer, bathtub or shower, toilet, towels, bed linen, terrace balcony conservatory Size 45 95 sqm, Rooms 1 3 with kitchen bathroom, Bedrooms 1 2 , Beds 1 6 , Price from 89 32 Steffis Hostel Stefanie Munz, Alte Eppelheimer Strasse 50 , 69115 Heidelberg, Phone 49 6221 7782772 , infohostelheidelberg.de33 You can find us just a few meters away from the Hauptbahnhof main station, on the premises of the Landfriedkomplex Landfried Complex, which are pro tected as a monument We offer a comfortable atmosphere and contact with travelers from around the world You can choose between one of our lovingly furnished private rooms and a bed in a spacious room with multiple beds All rooms share clean showers and toilets centrally situated along the hallway Wi Fi, parking place 24 h 8 , bicycles, guest kitchen, lounge Rooms 10 D R from 70 , 4 S R from 60 , 15 dormitories 3 10 beds from 243 Further information about accommodations in Heidelberg can be found on our website Verkehrsverein Ziegelhausen e V The beautiful landscape of the Neckar Valley east of Heidelberg, in the area of the district Ziegelhausen Peterstal, already impressed well knownpoets, scholars and composers in the 18th and 19th centuries Due to the very good transport connections, you can reach the sights, the old town, the famous Heidelberg Castle and the world famous Heidelberg University Hospital in a short time We are happy to help you finding a suitable holiday home Verkehrsverein Ziegelhausen e V Peterstaler Strae 1 , 69118 Heidelberg Telefon 49 06221 8006 49 email infoverkehrsverein ziegelhausen.de Website ziegelhausen.de Facebook verkehrsverein.ziegelhausen We are here for you Monday 1000 am-1200pm Wednesday 1000 am-1200pm 300 pm 500pm Thursday 300 pm 500pm Friday 1000 am-1200pm 300 pm 500pm Saturday 1000 am-1200pm Heidelberg C AR D Put Heidelberg in your pocket With castle ticket including funicular railway Combination ticket one time free admission to the University Museum, the Student Prison and the special exhibition Free travel on public transport in the city area Discounts with attractive partners, on our public guided Old Town tours and city tours by bus with a castle tour Available at all Tourist Information Online order and detailed information at Marketing Gmb H Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from Familyfrom 24 36 Overview map Overview map A 5 A 656 A 5 Friedrich Ebert Anlage Neuenheimer Landstrae Ziegelhuser Landstraeuhsheimer Landstrae Bergheimer Strae Rohrbacher Strae Rm erstrae Speyerer Strae Kirchheimer Weg Schwetzinger Strae Ziegelhausen Schlierbach Altstadt Neuenheim Handschuhsheim Bergheim Bahnstadt Pfaffengrund Weststadt Wieblingen Neuenheimer Feld Sdstadt Boxberg Rohrbach Kirchheim Emmertsgrund Heidelberger Schloss Mannheim 14 3 2 432 Distance to the Old Town 1 Old Town Directly in the town center 2 City center The Old Town is within 5 15 minutes walking distance 3 Urban area The Old Town is within 20 30 minutes walking distance and can be reached easily by public transport within 5 10 minutes 4 Suburbs Districts within the city border the Old Town can be reached easily by public transport within 15 20 minutes 1 14 7204 17 10232 15 8215 18 11243 16 922619 12 1325 Hotels Arthotel Heidelberg A TL AN TI C Hotel Heidelberg Bayrischer Hof Berggasthof Knigstuhl Bergheim 41 Hotel im Alten Hallenbad City Partner Hotel Hollnder Hof Der Europische Hof Heidelberg Gasthaus Backmulde Hackteufel Heidelberg Marriott Hotel Hilton Heidelberg Hotel am Rathaus Hotel am Schloss Hotel Berger Hotel Heidelberg Lounge Hotel Krokodil Kulturbrauerei Heidelberg N H Heidelberg P LA ZA Premium Heidelberg Qube Hotel Bahnstadt Qube Hotel Bergheim Rafaela Hotel Heidelberg seven Days Hotel Boarding House Staycity Aparthotel Heidelberg The Heidelberg Exzellenz Hotel7 2022316 28 342425 153 111951821 10 33272214 41869 12 131729 3235b 35a 35c35d30 3126 Map is not true to scale City center map 37 City center map Uferstrae Ziegelhuser Landstrae Hauptstrae Plck Plck Philosophenweg Hirschgasse Karlstor Heidelberger Schloss Neuenheim Handschuhsheim Bergheim Weststadt Alte Brcke Altstadt Neuenheimer Landstrae Handschuhsheimer Landstrae Theodor Heuss Brcke Bismar ck- Platz Neckarstaden Iqbal Ufer Bergheimer Strae e Berliner Strae Ernst W alz Brcke Kurfrsten Anlage Rohrbacher Strae13 3 42 2 26 3229 3527 333028 3431 Holiday Apartments, Aparthotels and Hostels Bauernhof Koch Boardinghouse Luise5 Ferienbauernhof Gstehaus Fieer Ferienwohnung Peperoni Heidelberg Ferienwohnung Rittmller Gstehaus Komm Heidelberg A PH EA RT ME NT S Steffis Hostel Weingut Bauer Zweite Heimat Heidelberg3 23 20 2421625111921 18522 869 12 1317 114 7 332732 10 35d35a35b 35c4 Map is not true to scale 38 Public City Tours Public City Tours Walking Tour of the Old Town Fall in love with Heidelberg Find out why Heidelberg is one of the most popular destinations in the world Take a walking tour of the Old Town to get to know our city and its fascinating sights and special features the Heiliggeistkirche Church of the Holy Spirit, Jesuits quarter, Germanys oldest university or the Studentenkarzer Student Prison Become part of the romantic spirit of the twisty alleys and discover history around every corner Price 12 , 10 discount price Duration 1 5 hours Times April October German daily 1030 am, additionally Friday 600 pm and Saturday 230 pm English Thursday Saturday 1030 am November March German Friday 230 pm and Saturday 1030 am Meeting Point Neckarmnzplatz, in front of the Tourist Information Note During the Old Town festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023 there will be no Walking Tour of the Old Town at 230 pm City and Castle Sightseeing Tour Open view on the Heidelberg scenery Act one The fo cus shifts to the romantic castle, which is majestically en throned above the Karlsplatz Enjoy a comfortable city tour by bus, accompanied by a tour guide, that leads you along the Ne ckar River Act two takes you up to Heidelberg Castle In addition to an exterior tour, you will visit the in ner courtyard and the famous Great Barrel Take advan tage of your castle ticket with a visit of the German Phar macy Museum and the funicular ride back to the Old Town Kornmarkt station Price 30 , 27 discount price , 28 with H DC AR D including castle ticket funicular castle courtyard admission Duration 2 hours Times April October German Friday, Saturday and Whitsunday 130 pm November March German Saturday 130 pm Meeting Point Neckarmnzplatz, information board bus stop Minimum number of participants 5 people Note During the Old Town festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023 there will be no tour Information and booking 49 6221 5840 223225 , guideheidelberg-marketing.de Information and booking 49 6221 5840 223225 , guideheidelberg-marketing.de Eligibility criteria for the reduced price school and university students up to 28 years old, people with disabilities and a disabled persons pass and owners of the H DC AR D One accompanying person of a severely disabled person with the characteristic B in the severely disabled persons identity card as well as of children and young people up to 18 years of age with a disability is free of charge Children aged 6 to 14 pay a reduced price Tickets are available at our Tourist Information.including castle ticket Public City Tours 39 The University in the Old Town Founded by Prince Elector Ruprecht I in 1386 , the Ruperto Carola is Germanys oldest university and one of the most venerable education facilities in Europe The tour conveys not only the universitys history but also provides an insight into student life It takes you from the university library via the Peterskirche St Peters Church, the oldest church in Heidelberg, to the Alte Aula Old Auditorium and the histor ical Studentenkarzer Student Prison Visit the place that once made people suffer From 1778 to 1914 , students were punished here for trivial offenses Price 15 , discount price 13 including admission to the Alte Aula Old Auditorium and the Studentenkarzer Student Prison Duration 1 5 2 hours Times April October German Saturday 300 pm Meeting Point Neckarmnzplatz, in front of the Tourist Information Minimum number of participants 5 people Note During the Old Town festival Heidelberger Herbst Heidelberg Autumn on September 30 , 2023 there will be no tour Christmas Market Tour Take a walk through the Old Town and immediately per ceive the Christmas spirit with the scent of roasted almonds and mulled wine Lovingly arranged booths spread over various historical squares The unique backdrop with Hei delberg Castle towering above the Old Town creates an outstanding atmosphere and makes the Heidelberg Christ mas Market one of the most fairy tale like events in Germa ny The Advent themed guided tour provides in teresting information on the regions Christmas and pre Christmas traditions The walk starts at the Neck armnzplatz and pas ses the most beautiful corners of the Old Town ending at the Universittsplatz University Square Price 14 , 12 discount price Duration 1 5 hours Times Advent Saturdays December 2 , 9 , and 16 , 2023 German 430 pm Meeting Point Neckarmnzplatz, in front of the Tourist Information Minimum number of participants 5 people Information and booking 49 6221 5840 223225 , guideheidelberg marketing.de Castle ticket including funicular railway Two Heidelberg attractions go hand in hand with the castle ticket you have the option of reaching the romantic Heidelberg Castle com fortably, eventful and in an environmentally- friendly way by taking the funicular railway The funicular railway also combines two special features the lower funicular railway to the Molkenkur station is the most modern, while the upper one leading to the Knigstuhl Kings Seat is the oldest cableway in Germany 9 per person Castle ticket including funicular railway 41 Gltig ab Valid from Gltig ab Valid from Gltig ab Valid from Family Gltig ab Valid from Your benefit The castle ticket includes admission to the castle court yard, the barrel cellar as well as the German Pharmacy Museum and the fare for the funicular railway trip to the castle with a continuation of the trip to the Molkenkur station return trip including one stop Price per person valid for using the lower railway 9 adult, 4 50 discount price pupils and univer sity students up to 28 years of age and severely disabled people with appropriate I D You can get the castle ticket including the funicular railway at the following places in Heidelberg Tourist Information at the Hauptbahnhof main station Tourist Information at the Neckarmnzplatz Old Town Tourist Information in the Rathaus town hall at the Marktplatz Market Square, Old Town Checkout counter at the Kornmarkt and castle funicular railway station Heidelberg Castle checkout counter Group order For groups of 15 people or more, you can order castle tickets via our reservation department dispatch by post within Germany Please let us know the final number of participants two weeks before traveling refund for excess ordered tickets is not possible Phone 49 6221 58 40 228 gruppenheidelberg-marketing.de Opening hours Heidelberg Castle Castle courtyard, Great Barrel All year 900 am 600 pm, last admission 530 pm German Pharmacy Museum April October 1000 am 600 pm, last admission 540 pm November March 1000 am 530 pm, last admission 510 pm Note Information are subject to change Heidelberg CA RD The Heidelberg CA RD makes your journey even easier Benefit from the following included services castle ticket including funicular railway free use of public transport in Heidelberg combo ticket one time free entrance to the University Museum, the Student Prison and the special exhibition numerous discounts on tours, museums, leisure activities, restaurants and shops 1 day 24 Valid from midnight to midnight on the day of validity 2 days 26 Valid all day on the first day until midnight the following day 4 days 28 Valid all day on the first day until midnight of the fourth day Family 2 days 57 Valid all day on the first day until midnight the following day for a family 2 adults up to 3 children or 1 adult up to 4 children under the age of 16  Funicular railway departure times Annual funicular railway maintenance expected March 6 March 19 , 2023 Summer timetable valid from April 1 November 1 , 2023 Kornmarkt Castle 900 am every 10 minutes, last trip at 800 pm Castle Kornmarkt 903 am every 10 minutes, last trip at 803 pm Winter timetable valid from November 2 , 2023 March 31 , 2024 Kornmarkt Castle 900 am every 10 minutes, last trip at 510 pm Castle Kornmarkt 903 am every 10 minutes, last trip at 543 pm Note The lower funicular railway from the Kornmarkt station via the castle to the Molkenkur station is equipped to facilitate disabled access Strollers must be carried on the steps The conductors are happy to assist you We recommend that families with small children use a buggy 42 Events 2023 Further events can be found on our website Dates and information are subject to change Heidelberger Frhling Heidelberg Spring March 17 April 15 , 2023 One of the largest music festivals in Germany that be longs to the top league of international festivals for classi cal music and once again invites visitors to well over 100 events this year Heidelberger Schlossfestspiele Heidelberg Castle Festival June 11 July 30 , 2023 The one of a kind Renaissance buildings, the sleepy nooks and crannies, as well as the sprawling gardens and parks offer fascinating opportunities for the artistic work of the Heidelberg Theater Squares and surfaces become stages, walls and corners backdrops Heidelberg Castle Illuminations with Fireworks June 3 , July 8 and September 2 , 2023 Bengali lights blaze three times every summer on the walls of the ruin in a picturesque and eternal manner The illumi-nation of the beautiful facade of the castle is com plemented by festive fireworks which bathe the Old Town in an impres sive brilliance Summer at the River August 19 and 20 , 2023 Stroll, relax, enjoy the river The city is moving closer to the water and invites you to linger and stroll along the banks of the Neckar River, with musical entertainment and a varied range of information and gastronomic offerings marketing.com Heidelberger Herbst Heidelberg Autumn September 30 October 1 , 2023 One of the biggest Old Town festivals in the region features an artisan market, a giant flea market, numerous regional specialties and many bands providing entertainment in various different squares in the Old Town marketing.com Heidelberg Wine Village October 2 15 , 2023 Enjoy delicious local and regional wines in the heart of the Old Town Enjoy Jazz Beginning of October Mid November 2023 The festival offers diverse events for about seven weeks, with an emphasis not only on jazz, but also on other genres in exclusive venues, e.g in Heidelberg Heidelberg Christmas Market November 27 December 22 , 2023 Nestled in the Old Town, illuminated by the world famous castle above, the Heidelberg Christmas Market invites you to take some time out and visit one of its six historic locations Winterwldchen Winter Forest and Heidelberg Ice Rink November 27 , 2023 Beginning of January 2024 At the Kornmarkt, the Winter Forest beckons visitors with its magical atmosphere, while one of Germanys prettiest ice rinks offers ice skating pleasure on the Karlsplatz marketing.com Events 2023 A great program all year round The German National Garden Show 2023 B UG A right here in our backyardHeidelberg Marketing Gmb H Phone 49 6221 58 44444 infoheidelberg-marketing.de start to blossom Get your B UG A Bonus Further events can be found on our website Dates and information are subject to change 44 Good to know Good to know The central location of Heidelberg makes traveling easy with all modes of transport Rail Heidelberg is well connected to the European long distance network, as well as to the German Rail Network I CE and I C E C, Within the region and across its borders, the Rhine Neckar Trans port Associa- tion ensures the best connections with the S Bahn Car Bus The A5 A6 highways Autobahn access large sections of the entire Rhine Neckar region with inter connected exits and connecting federal roads The A5 A656 highways have direct exits to Heidelberg Airports City Airport Mannheim approx 18 km Frankfurt Airport approx 80 km airport.comen Baden Airpark Flughafen Karlsruhe Baden Baden approx 90 km Flughafen Stuttgart approx 120 km Frankfurt Hahn Airport approx 150 km Environmental zone for cars and buses Heidelberg established an environmental zone in 2010 , where only vehicles with certain exhaust standards are admitted Vehicles that are used in the environmental zone must have a sticker that marks the pollutant category Vehicles with a green pollutant sticker may drive in the en vironmental zone You can get more information on the low emission zone at Shuttle Service Frankfurt Airport Simply and conveniently book your T LS transfer that will pick you up directly at your terminal at Frankfurt Airport and take you to your hotel in Heidelberg Prices on request H LS Heidelberg Limousine Service Book with us an exclusive limousine service, V IP first class service, cab service, airport transfer or shuttle service Our trained chauffeurs will bring you safely to your destination Price on request R V Camping R V site Harbigweg 1 3 , 69124 Heidelberg 48 parking spaces open all year round Camping Heidelberg Schlierbacher Landstrasse 151 , 69118 Heidelberg quiet location, right beside the Neckar River open April October Camping Haide Ziegelhuser Landstrasse 91 , 69151 Neckargemnd 200 parking spaces, right beside the Neckar River open April November haide.de Nette Toilette Nice restrooms You can use a total of 30 nice restrooms in the Old Town area without being forced to purchase or consume any thing Some restaurants and stores bear the Nette Toilet te symbol on the entrance door here Good to know 45 Public passenger transport You can get information on the Heidelberg public trans portation lines and fares around the clock from the service hotline 49 621 1077077 , online at or at Rhein- Neckar Verkehr Gmb Hs R NV Customer Service Center at the Hauptbahnhof main station, Kurfrsten Anlage 62  Bicycle Heidelberg is a very bicycle friendly town, where you can easily rent a bike Radolino bike rental radhof B ER GH EI M Bike im Bahnhof D B bike rental system in front of the main station Pedelec rental E-bike, several locations hd.org Swapfiets Joyrides E bike rental V RNnextbike V RNnextbike has numerous bike stations in the metro politan area Rhine Neckar and operates around the clock The stations are located at busy transport hubs near bus and tram stops For further information Phone 49 30 69205046 , Stadtmobil Rhein Neckar Phone 49 621 12855585 e Carsharing Rhein Neckar Phone 49 6221 3574974 Any Move Phone 49 30 83795645 Taxi Taxizentrale Phone 49 6221 302030 Taxi HDirekt Phone 49 6221 739090 e-Scooter At many locations throughout the city of Heidelberg, numerous e-scooters from various providers are avail able for rent To rent an e-scooter, register via the app of the respective rental company of transportation in Heidelberg Mobile in Heidelberg Request Form Please complete this form and send it to Heidelberg Marketing Gmb H, Neuenheimer Landstrasse 5 , 69120 HeidelbergGermany Phone 49 6221 5840 226 , Fax 49 6221 5840 222 , reservationheidelberg-marketing.de Point of contact Last name First name Title Street Zip code, city Country Phone Fax email Dates Alternative dates Desired hotel category Comfort First class Desired number of rooms Double rooms Single rooms Desired package Lose your heart in Heidelb erg Crisscross through Heid elberg Heidelberg Castle Illumi nation B UG A flower pot German Na tional Garden Show Summer Special stay 3 nig hts, pay for 2 City Quest Heidelberg s interactive quiz A HO Y welcome aboard for a culinary delight Delectable Heidelberg Strolling around Heidelbe rg Winter Dream stay 3 nigh ts, pay for 2 Advent experience Theater and Orchestra H eidelberg Heidelberger Schlossfestspiele Heidelberg Castle Festival Heidelberger Frhling Heidelberg Spring Music festival B OD Y W OR LD S Museum K RP ER WE LT EN The Anatomy of Happines s Hotel stay without package Other comments requests Place Date Signature 46 Request Form Note I c o n fi r m t h a t I h a v e r e a d t h e f o r m w i t h t r a v e l e r s i n f o r m a t i o n f o r a p a c k a g e t r i p i n a c c o r d a n c e w i t h 6 5 1 a G e r m a n C i v i l C o d e b e f o r e b o o k i n g a n d acknowledge it Note Please note that you have no statutory withdrawal right from contracts on domestic tourism sights, in particular contracts on accommodations, guest tours and cultural events, after conclusion of the contract according to the law, but only a cancellation right usually subject to fees according to the agreed terms and conditions or the statutory provisions Note Please note that the detailed data protection provisions of Heidelberg Marketing Gmb H can be found on our website about usprivacy statement With your signature, you confirm that you have understood and accepted the data protection provisions Form sheet for information of the traveler on a package tour according to 651a of the German Civil Code The most important rights according to policy E U 2015 2302 Travelers receive all essential information concerning the package tour before the conclusion of the package tour contract At least one entrepreneur at a time shall be liable for proper rendering of all travel services included in a contract The travelers will receive an emergency phone number or information on a point of contact through which they can contact the tour operator or the travel agency The travelers can transfer the package tour to another person within an appropriate period of time and potentially subject to additional fees The price for the package tour must only be increased if certain costs e.g fuel prices increase and if this is expressly stipulated in the contract, and in any case no later than 20 days before the start of the package tour If the price increase exceeds 8% of the price for the package tour, the traveler may withdraw from the contract If a tour operator reserves the right to increase the price, the traveler shall have the right to reduce the price if the corresponding costs reduce The travelers may withdraw from the contract without paying a revocation fee and shall be reimbursed in full for all payments if one of the essential parts of the package tour, except for the price, is changed considerably If the entrepreneur responsible for the package tour cancels the package tour before it commences, the travelers shall have a claim to reimbursement for costs and, potentially, compensation The travelers may withdraw from the contract without incurring any revocation fee if any extraordinary circumstances occur before commencement of the package tour, e g if there are any security risks at the destination that are expected to impair the package tour The travelers may also withdraw at any time before commencement of the package tour against payment of an appropriate and reasonable revocation fee If essential parts of the package tour cannot be performed according to the agreement after commencement of the package tour, the traveler shall be offered appropriate other provisions without additional costs The traveler may withdraw from the contract without paying any revocation fee in the Federal Republic of Germany, this right is called termination, if services are not rendered according to the contract and this has essential effects on rendering of the contractual package tour services and the tour operator does not remedy this The traveler shall have a claim to price reduction and or damages if the travel services are not rendered or not rendered properly The tour operator shall support the traveler if heshe is in trouble In case of insolvency of the tour operator or in some Member States the travel agent, payments shall be reimbursed If the tour operator or, if relevant, the travel agent, becomes insolvent after commencement of the package tour and if transport is part of the package tour, return transport of the travelers is ensured Heidelberg Marketing Gmb H has taken out insolvency insurance with RV Allgemeine Versicherung A G The travelers may contact RV Allgemeine Versicherung A G RV Allgemeine Versicherung A G, Abt Kredit Schaden, Raiffeisenplatz 1 , 65189 Wiesbaden, Phone 49 611 5335859 , Fax 49 611 5334500 , Email inforuv.de, if they are denied any services by Heidelberg Marketing Gmb H due to insolvency Website on which the policy E U 2015 2302 in the form transposed into national law can be found richtlinie eu2015- 2302 .de The combination of travel services offered to you is a package tour within the meaning of directive E U 20152302  Therefore, you may claim all E U rights that apply to package tours The company Heidelberg Marketing Gmb H, Neuenheimer Landstrasse 5 , D- 69120 Heidelberg bears full responsibility for proper execution of the entire package tour The company Heidelberg Marketing Gmb H also has the legally required security for repayment of your claims and, if transport is included in the package tour, to ensure your return transport in case of its insolvency Form sheet 47 RI VE RB OA T H EI DE LB ER GBoat trips on historic sloops Exclusive Romantic Classy Trips starting from 249 49 172 57 38 08 9 inforiverboat heidelberg.de W WW R IV ER BO AT H EI DE LB ER G D E riverboat.heidelberg Outdoor Funfor Kids, Families and Friends M AR N OV daily 10 a.m till 6 p.m., Sundays and holidays until 7 p.m Knigstuhl 5 , 69117 Heidelber g, free parking Take the funicular to the most famous ruin in the world More at bergbahn heidelberg.de Just get in and enjoy the magnif_i cent view From Kornmarkt to Molkenkur and back Includes entry to the Castle courtyard, the wine cellar and the German Pharmacy Museum 9 Euro incl admission to the Castle courtyard CA ST LE T IC KE T historicgermany.com Crowned by a Castle The romantic charm of Heidelberg, an ancient city along the Neckar River, lies in the flower filled balconies, bronze fountains and cozy cafs, where people exchange chocolate treats known as kisses Heidelberg Castle, perched on a granite rock, rises high above the bumpy stone streets of the Old Town The harmonic mixture between old and new, past and future is evident everywhere History is what makes a destination so interesting Each place has its own fascinating past If you are an absolute lover of history and flair then youll adore the Historic Highlights of Germany cities Great cities for History Buffs Crowned by a Castle Photo credits Heidelberg MarketingTobias Schwerdt historicgermany historicgermany Rostock Lbeck Potsdam Osnabrck Mnster Erfurt Aachen Bonn Koblenz Wiesbaden Trier Wrzburg Heidelberg Freiburg Regensburg Augsburg Tbingen H Ho G_ AD_ HD2018_190x136_1008.indd 3 10 08 18 1944experience a journey back in time Nuremberg Bayreuth Coburg Rothenburg o.d T Neckargemnd Schwetzingen Cobur g Schwetzingen Abenberg Lichtenau Wolframs- Eschenbach Roth Nuremberg Forchheim Ebermannstadt Aufse Heiligenstadt Memmelsdorf Bamber g Pottenstein Gweinstein Waischenfeld Muggen- dorf Kulmbac h Bayreuth Rabenstein Castle Eglof fstein Kronac h Mannheim Neckar steinach Neckargemnd Hirsch- horn Weinsberg Bad Wimpfen Guttenberg Castle Heilbronn Schwbisc h Hall Walden- burg hringen Rothenbur g ob der Tauber Colmberg Ansbac h Selach Heidel- berg Sinsheim Neckar bischofsheim Ebern Pfarrweisach Maroldsweisach Rentweinsdorf Stein Cadolzburg Langenzenn Eberbach Streitberg Mosbach Hotel Restaurant Hornberg Castle Heldburg Lauf a.d Pegnitz GundelsheimSchlosshotel Horneck Kirchberg an der Ja gst Biohotel Schloss Kirchberg Die Bur gen strae e V Allee 1 2 74072 Heil bronn Phone 49 0 71 3 1 9 73501 - 0 infoburgenstrasse.de is situated along on the Castle Road which runs through southern Germany from Mannheim to Bayreuth With idyllic landscapes, about 70 castles and palaces as well as countless other places of interest, the holiday route is the ideal destination for anyone who want to immerse themselves in bygone times G ER MA NY Munich Cologne Berlin Hamburg Frankfurt a M M AN NH EI MN UR EM BE RG BA YR EU TH H EI DE LB ER GHere Germany begins being Italy Emperor Joseph I I on his way to Francfort Tourismus Service Bergstrasse e V  Tel 49 62 52 13 11 70 active nature experience enjoyment cities culture Hessen Agentur Blofield EX PE RI EN CE B ER GS TR AS SE The holiday Route Bergstrasse with its connected hiking-and biking trails and its milde climate is situated between the rivers Rhein, Main and Neckar More than 30 castles and palaces aswell as the U NE SC O World Heritage Abbey of Lorsch invite for a stay The fanciers of excellent wine and food wont miss anything The destination between the cities Darmstadt and Heidelberg has a broad cultural choice The wine growing growing district Bergstrasse has much to experience such as the rarity Red Riesling discover the town and beautiful landscape alongside the river Neckar and the Bergstrae in a completely new and innovative way For more information infostadtsafari.com Fon 0621 43 715 418 Discover Heidelberg with your Segway Airport Health Wedding Limousine Sprinter Be it in an elegant limousine, a comfortable Sprinter or a classic Mercedes Benz vintage car T LS is your trusted automotive partner for all travel connections to Heidelberg and in the Rhine Neckar region Our health shuttles are happy to drive patients with and without medical equipment to clinical treatments and check up appointments Barrier free and e-wheelchair accessible rides can be provided on request More information about T LS Heidelberg is available online at or by phone 49 6221 770077 We look forward to meeting youT LS Travel well with us Heidelberger Frhling Musikfestival March 17 April 15 2023 heidelberger fruehling.de Heidelberg Marketing.indd 2 Heidelberg Marketing.indd 2 20 09 2022 16424620 09 2022 164246 MU SE UM H EI DE LB ER G D AI LY 10 A M 6 P MWhat the B OD Y are all about Heidelberger Frhling Musikfestival March 17 April 15 2023 heidelberger fruehling.de Heidelberg Marketing.indd 2 Heidelberg Marketing.indd 2 20 09 2022 16424620 09 2022 164246 Two ideal destinations technik museum.de Wi Fi Heidelberg4 You 1  Choose network Heidelberg4you 2  Connect Accept terms and conditions 3  Lets go Join more than 200 hotspots Opening 2023 Alre A dy book A ble The North German based A TL An Ti C Hotels Group offers exceptional 4 to 5 star comfort and individual full service hotels and they are coming to Heidelberg soon right next to the new Heidelberg Congress Center and the railway station, the A TL A n Ti C Hotel Heidelberg, opening mid- 2023 , will be providing 310 stylish rooms and suites, a spa and wellness area, a restaurant, and a rooftop bar with a terrace overlooking Heidelberg and near surroundings State of the art function rooms and a 350 sqm ballroom with flexible partition walls located on the 13th floor complete the offer reservations and general enquiries heidelbergatlantic hotels.de Find out more about our 19 locations atlantic hotels.deen A Tl AN TI C Hotels Management Gmb H ludwig r oselius Allee 2 , 28329 b remen, Germany Heidelberg Trio 1 bottle of Riesling Winery Clauer 1 bottle of Pinot Noir Winery Hans Winter 1 bottle of Pinot Noir Ros Winery Bauer Available at the Tourist Information Neckarmnzplatz Heidelberg Marketing Gmb H special p rice 19 99 instead o f 24 99 General Terms and Conditions Heidelberg Marketing Gmb H, Package Travel Terms and Conditions Dear guests, We ask you to carefully read the following Package Travel Terms and Conditions To the extent effectively agreed, these Terms and Conditions become part of the contract for package travel concluded between the customer traveler hereinafter referred to as the Traveler and Heidel berg Marketing Gmb H hereinafter referred to as H DM They supplement the statutory provisions contained in section 651a y of the German Civil Code Brgerliches Gesetzbuch B GB and Articles 250 and 252 of the Introductory Act to the German Civil C ode Einfhrungsgesetz zum B GB E GB GB and expand upon them These Terms and Conditions apply exclusively to package travel arrangements from H DM They do not apply to package travel arrangements for groups of 15 persons or more, to the arrangement of t hird party services e.g guided tours and tickets, nor to contracts for accommodation services or making arrangements for such contracts 1  Conclusion of the travel contract obligations of the Traveler 1 1 The following applies to all booking channels a The trip description and supplemental information provided by H DM for the respective trip, to the extent available to the Traveler at the time of booking, comprise the basis for an offer by H DM and the boo king made by the Traveler b Travel agents and booking offices are not authorized by H DM to make agreements, to provide information, or make warranties that amend the agreed contents of the travel contract, extend beyond, or conflict with what is included in the trip description and or services contractually agreed by H DM c Information contained in hotel guides and similar listings that are not published by H DM is not binding upon H DM and its duty of performance to the extent not included within H DMs duty of performance by express agreement with the Traveler d If the contents of the travel confirmation from H DM deviate from the contents of the booking, this comprises a new offer from H DM which is binding upon it for a period of ten days A contract i s concluded on the basis of this new offer in the event that H DM has provided notice of the change in regard to the new offer, has satisfied its pre contractual information obligations and the Traveler accepts such offer during the ten day period referred to above by express statement to the travel agency or by making advance payment e The pre contractualinformation provided by the travel agent with regard to the essential features of the trip, the trip price and all additional costs, payment arrangements, minimum num ber of participants, and cancel ation fees pursuant to Article 250 se ction 3 nos 1 , 3 , to 5 and 7 E GB GB do not become part of the package travel contract only if expressly agreed between the parties f The Traveler is liable for all contractual obligations of other travelers for whom the Traveler makes a booking to the e xtent of the Travelers own liability, provided the Traveler has made such an undertaking by express and separate agreement 1 2 The following applies to bookings made verbally, by telephone, in writing, by email, or fax a When making a booking, the Trav eler is making a binding offer to conclude a package travel contract with H DM The Traveler is bound by the booking for three business days b The contract is concluded upon receipt of the trip confirmation acceptance notice from H DM Upon or immediatel y after conclusion of the contract, H DM will provide the Traveler with a travel confirmation in compliance with legal requirements on a durable medium which permits the Traveler to save or store the confirmation unedited such that it will be accessible to the Traveler for a reasonable period, e.g on paper or by email, provided that the Traveler does not have a right to a trip confirmation in paper form pursuant to Article 250 section 6 subsection 1 second sentence E GB GB whilst the contract was conclude d in the simultaneous physical presence of both parties or outside of the business premises 1 3 H DM notes that there is no right to cance lation under applicable law section 312 subsection 7 , section 312g subsection 2 first sentence no 9 B GB in the case of package travel contracts under sections 651a and 651c B GB concluded via distance sales letters, catalogues, telephone calls, fax, email, or messages S MS sent via mobile network as well as radio, telemedia, and online services but rather only th e statutory revocation and termination rights, in particular revocation pursuant to section 651h B GB see also section 3 are availa ble However, there is a cancel ation right if the contract for travel services under section 651a B GB is concluded outside o f business premises, unless the verbal negotiations upon which the formation of the contract is based were conducted on the basis of a preceding order by a consumer there is like wise no right of cance lation in the latter case 2  Payment 2 1 H DM and the travel agency may only request or accept payments for the trip prior to the end of the package travel if an effective customer funds insurance contract is in place and the Traveler is provided a risk coverage certificate with the name and cont act information of the customer funds insurer in clear, understandable and highlighted fashion Following conclusion of the contract, the trip price is due for payment four weeks prior to the commencement of travel in exchange for provision of the risk coverage certificate, provided the booking confirmation invoice does not reflect any other agreement The entire trip price is immediately due for payment in the case of bookings made less than four weeks prior to the commencement of travel 2 2 The provisi ons of section 2 1 notwithstanding, a risk coverage certificate needs not be provided as a prerequisite for payment falling due if the package travel offer does not include transportation to and or from the location at which the package travel services w ill be provided and, contrary to section 3 1 , it has been agreed and noted in the trip confirmation, that the entire trip price is due for payment at the end of the package travel upon the end of travel without prior advance payment 2 3 If the Traveler do es not make an advance payment and or the final payment in accordance with the agreed payment terms, even though H DM is ready and able to provide the contractually agreed services, has satisfied its statutory information obligations, and the Traveler has no statutory or contractual right of offsetting or retention and if the Traveler is responsible for the default of payment, H DM is entitled to revoke the package travel contract following a warning and grant of an appropriate grace period and the expirati on of this period , and charge the Traveler cance lation fees in accordance with section 3 , unless the Traveler has a set -offor retention right at the time the payment falls due, or the Traveler is not at fault for the payment delay 3  Cancel ation by the Traveler rebooking 3 1 The Traveler may cancel the package travel contract at an y time before departure Cancel ation must be communicated to H DM at the address set out below if the trip has been booked via a travel agency, notice of cancel ation may also be provided to the travel agency It is advisable for the custo mer to provide notice of cancel ation in writing 3 2 If the customer cancels prior to the commencement of travel or fails to begin travel, the tour operator loses the right to receive the trip price Instead, the tour operator may demand an appropriate compensation as far as i t is not at fault for the cance lation H DM may not demand compensation if unavoidable, exceptional events that significantly impair the ability to provide the package trip or to transport persons to the destination occur at the or in its immediate vicinity circumstances are deemed unavoidable and exceptional if they are not subject to the c ontrol of the party invoking such circumstances and their results could not have been prevented even if all reasonable precautions had been taken 3 3 H DM has defined the following fixed compensation levels under consideration of the period between notice of cance lation and the commencement of travel as well as under consideration of expected savings and the expected profits that may be earned by other use of the travel services Compensation is comput ed based on the relevant cancel ation tier based on the t ime at which notice of the cancel ation is received a Between the 27th and the 21st day prior to the commencement of travel, 20% of the trip price b Between the 20th and 12th day prior to the commencement of travel, 40% of the trip price c Between the 11th and the 3rd day prior to the commencement of travel, 60% of the trip price d From the 2nd day prior to the commencement of travel and in the case of a no -show, 90% of the trip price 3 4 We strongly encourage the purchase of a travel c ancel ation insurance as well as an insurance to cover return related expenses in the event of an accident or illness 3 5 In any event, the Traveler is free to prove to H DM that H DM incurred no damages at all or damages that are significa ntly less than the fixed cancel ation fees demanded by H DM 3 6 A fixed compensation fee in accordance with section 3 3 shall not be deemed to have been fixed or agreed upon to the extent H DM proves that H DM has incurred expenses that are significantly in excess of the applicable fixed compensation fee under section 3 3  In such cases, H DM is obliged to specifically quantify and substantiate the amount of compensation demanded subject to consideration of saved expenses and the purchase of any other use of the travel serv ices 3 7 If the tour operator is obliged to refund the trip price following cance lation, section 651h 2 B GB shall remain unaffected 3 8 The foregoing is without prejudice to the customers statutory right to demand that a third party take over the rights and duties under the package travel contract in lieu of General Terms and Conditions 5758 General Terms and Conditions the Traveler pursuant to section 651e B GB by providing notice to the tour operato r on a durable medium Such a declaration is timely in any event if received by the tour operator 7 days before the commencement of travel 3 9 If any changes are made with regard to travel dates, accommodations, meal arrangements, or other services booki ng changes at the request of the Traveler after the conclusion of the contract, H DM may, up to the 31st day prior to the commencement of travel, impose a fee of 15 without the Travelers having a legal right to any such booking changes and only to the e xtent such changes are possible Later booking changes are only possible subject to termination of the travel contract and rebooking in acco rdance with the terms of cancel ation set out above The foregoing does not apply to requests for booking changes tha t result in only minor expenses, or if booking changes are necessary because H DM provided the Traveler no, insufficient or incorrect pre contractual information as required under Article 250 section 3 E GB GB 4  Duties of the Traveler 4 1 Travel documents The customer is required to notify H DM or the travel agent from whom he booked the package travel if he does not receive the required travel documents e.g hotel or other vouchers by the deadline indicated by H DM 4 2 Notice of defects demand for rel ief a The Traveler may demand relief if the package travel is not provided free of defects b In the event that H DM could not provide relief due to a failure to provide notice of the defect for which the Traveler was at fault, the Traveler may not deman d a reduction in price under section 651m B GB or compensation for damages under section 651n B GB c The Traveler is obliged to provide notice of defects immediately to H DMs local representative If a local representative of H DM is neither available, nor contractually required, any defects in the package travel are to be reported to H DM at the contact office indicated by H DM The booking confirmation will provide information regarding the availability of a local representative of H DM and or its contact office However, the Traveler may also notify the travel agent from whom he booked the package travel of the defect d The representative of H DM is commissioned to provide relief to the extent possible However, such representative is not authorized to re cognize any claims 4 3 Deadlines prior to termination If the Traveler desires to terminate a package travel contract pursuant to section 651l B GB due to a major defect in the travel package of the type described in section 651i subsection 2 B GB, the Tr aveler is first required to provide H DM a reasonable period to provide relief The foregoing does not apply only in cases where H DM refuses to provide relief or if immediate relief is required 5  Limitation of liability 5 1 Contractual liability for da mages on the part of H DM that do not result from injury to life, limb or health, and are not the result of fault on the part of H DM, is limited to three times the trip price 5 2 H DM is not liable for interruptions in performance, personal and material dam ages related to services t hat are only provided as third -partyservices e.g excursions offered by third parties, sporting events, thea ter visits, or exhibitions if these services are explicitly labeled in the travel description and booking confirmation, are clearly labeled as third -partyservices, including the ide ntity and address of the third -partycontract partner, such that it is apparent to the Traveler that they are not part of the H DM package travel and may be selected separately This is without prejudice to sections 651b, 651c, 651w and 651y B GB 5 3 Nonetheless, H DM is liable if, and to the extent that, damages result from a violation of notice, information or organizational duties on the part of H DM 6  Unused services The Traveler has no right to a proportionate refund if the Traveler does not make use of specific elements of the package travel due to premature return related to an illness or other reasons for which H DM is not at fault However, to the extent that very small sums are not involved, H DM will endeavour to obtain a refund from the service provider and repay the corresponding amounts to the Traveler as soon as, and to the extent that, such amounts are actually refunded to H DM from the individual service provi ders 7  Special regulations in connection with pandemics in particular the Corona virus 7 1 The parties agree that the agreed travel services shall always be provided by the respective service providers in compliance with and in accordance with the o fficial requirements and conditions applicable at the time of travel 7 2 The Traveler agrees to comply with reasonable regulations or restrictions on use of the service providers when using travel services and to notify the tour guide and the service prov ider immediately in the event of typical symptoms of illness 8  Choice of law and jurisdiction information regarding consumer dispute settlement 8 1 In relation to Travelers who are not citizens of a member state of the European Union or Switzerland, t he parties agree to the exclusive application of German law to the either legal and contractual relationship between H DM and the Traveler Such Travelers may only lodge suit against H DM at its place of domicile 8 2 In the case of lawsuits lodged by H DM ag ainst Travelers and or contractual partners to the package travel contract who are merchants, legal persons under public or private law and who maintain their residence or habitual place of abode outside of Germany, or whose residence or habitual place o f abode is unknown at the time a lawsuit is lodged, the location of H DMs domicile is the agreed place of jurisdiction 8 3 With reference to the Act on Consumer Dispute Resolution Gesetz ber Verbraucherstreitbeilegung, H DM indicates that it will not pa rticipate in voluntary consumer dispute resolution H DM will provide appropriate notice to the consumer should consumer dispute resolution become mandatory for H DM after these Terms and Conditions have been printed H DM notes the European online dispute re solution platform with regard to all travel contracts concluded electronically protection Noll Htten Dukic Rechtsanwlte, Stuttgart Mnchen, 2017 2023 Tourism agency Heidelberg Marketing Gmb H Managing director Mathias Schiemer Neuenheimer Landstrae 5 69120 Heidelberg, Germany Phone 49 6221 5840 200 Telefax 49 6221 5840 222 infoheidelberg -marketing.de Commercial register number H RB 337405 Register court A G Mannheim V AT I D D E226325597 General Terms and Conditions Terms and Conditions for Guest Accommodations and Agency Services Dear guests, Heidelberg Marketing Gmb H, referred to hereinafter as H DM, arranges accommodations at lodging establishments and from private renters, referred to hereinafter as Hosts, in Heidelberg and the vicinity based on current availability To the extent validly agreed, the following Terms and Conditions shall become part of the lodging agreement concluded between the guest and the Host in the event of a booking, and provide terms that supplement the statutory rules applicable to the contractual relationship between the guest and the Host and to the contractual relationship between the Host and H DM related to placement services Accordingly, we request that you read these Terms and Conditions carefully 1  Status of H DM 1 1  H DM is the operator of the respective websites and or publisher of the respective directory of accommodat ions, catalogues, flyers or other printed media and websites to the extent it is expressly listed as publisher operator in such media 1 2  To the extent that H DM arranges further services provided by the hosts, which are not a material part of the overall value of the hosts services, and represent neither a material feature of such combination of services by the host nor of H DM itself nor have been advertised as such, H DM is deemed merely to be an agent arranging acco mmodation services 1 3  As an agent, H DM is deemed to be the provider of related travel services provided that the requirements for offering related travel services have been satisfied pursuant to the provisions of section 651w B GB 1 4  Without prejudice to the obligations of H DM as the provider of related travel services in particular providing the legally required information sheet and obtaining a guarantee for customer funds in the event of collection activities by H DM and the legal consequences of t he failure to comply with such statutory obligations, H DM is neither the tour operator nor a party of the contract with regard to any accommodation contract formed as the result of a booking provided that the requirements of 1 2 or 1 3 have been satisfied Accordingly, it is not liable for information provided by the host regarding prices and services, the provision of services itself as well as for any associated defects 2  Contract formation 2 1 The following applies to all booking types a The base s for the Hosts offer and the g uests booking consist of the description of the accommodations and supplemental information as contained in other materials on which the booking was made e.g., description of the city, explanation of classification, etc t o the extent available to the guest at the time of booking b H DM expressly notes that, in accordance with the statutory provisions section 312g paragraph 2 sentence 1 n o 9 of the German Civil Code Brgerliches Gesetzbuch B GB, there is no right of withdrawal in the case of contracts for lodging services that were concluded via distance selling letters, catalogues, telephone calls, facsimile, emails, via messages sent on a cellular network as well as via radio and telemedia In such cases, th e statutory provisions applica ble to unused rental premises section 537 B GB see also section 6 of these Terms and Conditions for Guest Accommodations apply exclusively However, there is a right of withdrawal if the lodging agreement was concluded off -premises 2 2 The following applies to bookings made verbally, by telephone, in writing, by email or by fax a By making a booking, the guest makes a binding offer to conclude a lodging agreement with the Host b The contract is formed upon receipt of th e booking confirmation from the Host by the guest There is no form requirement applicable to the booking confirmation such that confirmations made verbally or by telephone are binding for the guest Generally, the Host or H DM sends an additional, written booking confirmation to the guest However, bookings made by a guest verbally or by telephone shall also result in a binding contract if confirmed verbally or by telephone, even if the guest is not sent a corresponding written booking confirmation 2 3 Section 2 2 notwithstanding, the following applies to bookings made online a The process for making an online booking shall be explained to the guest at the relevant online portal The guest has the ability to correct or delete information he she has ente red, or to reset the entire online reservation form by means of a correction function, the use of which is explained to the guest Contract languages for making an online booking are indicated b To the extent the contract text is stored by the Host or in the online booking system, the guest shall be informed of this and informed of the ability to access the contract text at a later time c By clicking the make binding reservation button, the guest makes a binding offer to conclude a lodging agreement w ith the Host The guest shall receive immediate electronic confirmation of his her booking d The transmission of an offer to conclude a contract by clicking on the make binding reservation button does not confer upon the guest any right to the format ion of a lodging agreement in accordance with his her booking information Instead, the Host is free to decide whether or not to accept the guests offer e The contract is concluded when the guest receives the booking confirmation from the Host or H DM as its agent 2 4 If the booking confirmation is provided in the form of a message on the screen real time booking immediately after the guest makes the booking by clicking on the make binding reservation button, the lodging agreement is concluded upon receipt and pres entation of this booking confirmation on the guests screen without the requirement of an intervening notice that the booking has been received In such cases, the customer is provided the option to save and print the booking confirmation However, the bin ding nature of the lodging agreement does not depend on the circumstance that the guest has the option to save or print the booking confirmation The Host or H DM generally sends an additional, written booking confirmation to the guest by email, email attac hment, postal mail or fax However, receipt of such additional booking confirmation is not a requirement for the lodging agreement to be binding 3  Reservations 3 1 Non bindingreservations that entitle the guest to a right of withdrawal without charge are only permitted in the event of an express agreement to such effect with H DM or the Host 3 2 If no reservation has been expressly agreed, a booking generally results in a legally binding contract concluded by and between the Host and the guest client in accordance with section 2 c ontract formation 3 3 If a non bindingreservation has been agreed with individual guests, the guest is required to notify H DM by the agreed date as to whether the reservation is to be considered a binding booking If the guest fails to do so, the reservation shall be voided without any additional duty to provide notice on the part of H DM or the Host If timely notice is given, the booking becomes binding, regardless of a booking confirmation subsequently issued by H DM or t he Host 4  Pricing and services price increases 4 1 The prices indicated in the booking basis host directory, host offer, internet are final prices and include statutory sales tax and all ancillary costs unless otherwise stipulated with respect to ancillary costs Visitors tax or fees for consumption based goods and services e.g electricity, gas, water, firewood and for optional and additional goods and services may be incurred and listed separately 4 2 Goods and services the Host is required to provide shall be based exclusively on the booking confi rmation together with the applicable brochure and or property description and any supplemental and express agreements made with the guest client The guest client is advised to obtain any supplemental agreements in writing such as by fax, email or ot her informal written form 4 3 Moreover, the Host may adjust its prices if the customer subsequently wishes to make changes to the number of rooms reserved, the Hosts services or the length of the stay, and the Host consents to such changes 4 4 The Host may charge a re bookingfee in the amount of 15 00 for each change in the case of re -bookingschanges with regard to arrival or departure dates, length of stay, meal plan, booked additional goods and services or other supplemental goods and services fo r which there is no legal right The foregoing shall not apply in the event that such change is only minor 5  Payment 5 1 The due date for payment is based on th e terms agreed with the guest client and noted in the booking confirmation In the event that no special agreement has been made, the full price for the accommodations, including fees for ancillary costs and supplemental goods and services, shall be payable to the Host at the end of t he stay 5 2 Payments may not be made in foreign currency or by collection onlycheck Credit card payments are only permitted if this has been agreed or the Host offers this form of General Terms and Conditions 5960 General Terms and Conditions payment in general as indicated on a posted notice Payments at the end o f the stay may not be made via bank transfer 5 3 If the guest does not make an agreed down payment and or the remaining payment or does not make it in full within the specified period despite a reminder from the Host setting a reasonable deadline, altho ugh the Host is willing and able to properly provide the contractual services, no legal or contractual right of set -offor retention of the guest exists, and if the guest is responsible for the delay in payment, the Host shall be entitled to withdraw from the contract with the guest and to demand cancelation costs from him her in accordance with section 6 of these terms and conditions 6  Cance lation and no -show The followin g applies in the event of cance lation and no -showunless otherwise agreed in ind ividual cases and noted in the booking confirmation 6 1 Cancelation at no charge is possible up to 2 days prior to arrival in the case of bookings for up to 3 rooms 6 2 For booki ngs of more than 3 rooms, cance lation of the reservation, in whole or in part, is only possible without incurring a charge up to 7 business days prior to arrival 6 3 The following provisions shall apply in the case of a no -showor in the event it is no longer possible to cancel without incurring a charge a In the event of cance lation, the Hosts claim to payment of the agreed price for accommodations, including the meals component and fees for additional services, shall remain unaffected b The Host shall undertake to make efforts to rent the cancel ed rooms to other guests within the scope of its normal business operations and without obligation make any special efforts as well as under consideration of the particular nature of the accommodations booked e.g., non smokingroom, family room c The Host is required to provide c redit for other rental of the room and, where this is not possible, for expenses saved d In accordance with percentage rates approved by applicable jurisprudence for calculating saved expenses, the guest and or client shall undertake to pay the followi ng amounts to the lodging establishment, in each case based on the full price for lodging services including all ancillary expenses in the case of accommodations without meals, 90% in the case of overnight stays breakfast, 80% in the case of half-board, 70% in the case of full board, 60% e The guest client remains expressly entitled to provide evidence to the Host that the expenses saved by the latter are materially higher than the deductions provided for above, or that the lodging services or other goods and services were subject to othe r use In the event of such proof, the guest client is only obligated to pay the lowest relevant amount 6 4 Purchase of a travel cancel ation insurance is highly recommended 6 5 For administ rative reasons, notice of cance lation must be addressed to H DM not the lodging establishment and should be provided in writing, such as by fax, email or other informal written form, in the interest of the guest 7  Arrival and departure 7 1 Guests are required to arrive at the agreed time or by 600 pm at the latest if no specific time has been agreed 7 2 The following applies to later arrivals a The guest shall undertake to inform the Host not later than the agreed date of arrival if the guest will be arriving late or intends to arrive a day late in the case of accommodations booked for multiple days b The Host is authorized to rent the accommodations to another guest if timely notice is not provided The provisions of s ection 6 3 shall apply mutatis mutandis for the period during which a room is vacant c If the guest provides notice of late arrival, he she shall undertake to pay the agreed charges less expenses saved by the Host in accordance with s ection 6 3 , including for periods of non -occupancyfor which a room was reserved, unless the Host is contractually or legally required to accept responsibility for the reasons for delayed occupancy 7 3 Guests are required to vacate the accommodations at the agreed time, but not later than 1200 pm on the date of departure if no specific time has been agreed The Host may demand additional compensation as appropriate in the event that the accommodations are not vacated in a timely manner The foregoing is without prejudice to the Hosts abi lity to claim additional damages 8  Duties of the customer termination by the Host 8 1 Unless otherwise agreed, the accommodation may only be occupied by the guest for whom it was booked Occupation by another person, in particular subletting in the case of commercial customers or, in particular, the transfer of blocks of rooms, is prohibited 8 2 The guest shall undertake to treat the room and all furnishings, as well as all furnishings and fixtures at the lodging establishment itself, only as intended and with care and, if posted e.g., in the case of pools and saunas, only pursuant to the rules for use 8 3 The guest shall undertake to immediately report any defects and malfunctions to the Host and request they be remedied Notice of defects provided solely to H DM is insufficient The guests right to assert claims may lapse, in whole or in part, if the guest is at fault for a failure to provide notice 8 4 The guest may only terminate the contract in the event of substantial defects or malfunctions Prior to termination, he she must provide the Host a reasonable period to remedy said defect as part of the notification of the same unless it is impossible for said defect to be remedied, the Host refuses to remedy the defect, if termination of the cont ract without notice is justified by a legitimate interest of the guest or that this would make it objectively unreasonable to expect the guest to continue with their stay 8 5 Bringing and housing pets at the accommodations is only permitted if expressly agreed and only if the Host provides for such an option in the description In the case of such an agreement, the guest is required to provide truthful information about the type and size of the pet A breach of this obligation may provide the Host with rea sons to terminate the lodging agreement 8 6 The Host may terminate the lodging agreement without observing a notice period if the guest, despite receiving a warning from the Host, continuously disrupts the Hosts operations, other guests or the completion of the stay, or if the guest acts in a manner that contravenes the contract to such a degree that the immediate dissolution of the contract is justified In the event that the Host terminates the contract, the foregoing provisions regarding the obligation to pay upon withdrawal by the guest shall apply mutatis mutandis with regard to the Hosts right to payment 9  Limitation of liability 9 1 The Host is liable without limitation, as far as the damage results from violation of an essential obligation, the performance of which was required for proper execution of the contract or the violation of which endangers achievement of the purpose of the contract or the damage results from violation of life, body or health Apart from this, the Hosts liability shall be limited to damage caused by the Host or its servants willfully or grossly negligently 9 2 The potential Hosts innkeepers liability for items brought by the guest in accordance with sections 701 et seq B GB shall remain unaffected by this provision 9 3 The Host is not liable for disruptions in connection with goods and services that were merely arranged for the guest client during the stay where it is clear that such goods and or services are third -partyservices e.g., sporting events, theater tickets, exhibitions, etc. The foregoing applies mutatis mutandis to third -partygoods and services arranged in combination with the reservation if and insofar as the same were expressly indicated as third -partyservices in the description and or book ing confirmation 10  Special regulations in connection with pandemics in particular the Corona virus 10 1 The parties agree that the agreed travel services shall always be provided by the respective service providers in compliance with and in accordan ce with the official requirements and conditions applicable at the time of travel 10 2 The guest agrees to comply with reasonable regulations or restrictions on use of H DM and the Hosts when using services and to notify the Host immediately in the event of typical symptoms of illness 11  Alternative dispute resolution 11 1 In respect of the German Consumer Dispute Resolution Act Gesetz ber Verbraucherstreitbeilegung, H DM and the Host s advi se that neither H DM nor the Hosts currently participate in voluntary consumer dispute resolution 11 2 H DM will provide the guest appropriate notice in the event consumer dispute resolution were to become obligatory for H DM or the Hosts following the publication of these Terms and Conditions for Guest Accommodations and Agency Services 11 3 Please refer to the European online dispute resolution platform for all guest accommodation and agency services contracts concluded by electronic means 12  Applicable law and place of jurisdiction 12 1 The contractual relationship between the guest and or client and the Host and or H DM is exclusively governed by German law The foregoing shall apply in like manner to all other aspects of the legal relationship 12 2 The gu est and or client may only file suit against the Host and or H DM at the location of their respective registered office 12 3 The place of residence of the guest is determinative for suits brought by the Host and or H DM against the guest and or clie nt The Parties agree that the place of jurisdiction shall be the location of the Hosts registered office for suits filed against guests and or clients that are merchants, legal entities under public law or private law or are persons whose residence or habitual place of abode is located outside of Germany or whose residence or habitual place of abode is unknown at the time a suit is filed 12 4 The foregoing provisions shall not apply if and insofar as relevant and non waivable provisions of European Uni on law or other international laws are applicable to the contract payment in general as indicated on a posted notice Payments at the end o f the stay may not be made via bank transfer 5 3 If the guest does not make an agreed down payment and or the remaining payment or does not make it in full within the specified period despite a reminder from the Host setting a reasonable deadline, altho ugh the Host is willing and able to properly provide the contractual services, no legal or contractual right of set -offor retention of the guest exists, and if the guest is responsible for the delay in payment, the Host shall be entitled to withdraw from the contract with the guest and to demand cancelation costs from him her in accordance with section 6 of these terms and conditions 6  Cance lation and no -show The followin g applies in the event of cance lation and no -showunless otherwise agreed in ind ividual cases and noted in the booking confirmation 6 1 Cancelation at no charge is possible up to 2 days prior to arrival in the case of bookings for up to 3 rooms 6 2 For booki ngs of more than 3 rooms, cance lation of the reservation, in whole or in part, is only possible without incurring a charge up to 7 business days prior to arrival 6 3 The following provisions shall apply in the case of a no -showor in the event it is no longer possible to cancel without incurring a charge a In the event of cance lation, the Hosts claim to payment of the agreed price for accommodations, including the meals component and fees for additional services, shall remain unaffected b The Host shall undertake to make efforts to rent the cancel ed rooms to other guests within the scope of its normal business operations and without obligation make any special efforts as well as under consideration of the particular nature of the accommodations booked e.g., non smokingroom, family room c The Host is required to provide c redit for other rental of the room and, where this is not possible, for expenses saved d In accordance with percentage rates approved by applicable jurisprudence for calculating saved expenses, the guest and or client shall undertake to pay the followi ng amounts to the lodging establishment, in each case based on the full price for lodging services including all ancillary expenses in the case of accommodations without meals, 90% in the case of overnight stays breakfast, 80% in the case of half-board, 70% in the case of full board, 60% e The guest client remains expressly entitled to provide evidence to the Host that the expenses saved by the latter are materially higher than the deductions provided for above, or that the lodging services or other goods and services were subject to othe r use In the event of such proof, the guest client is only obligated to pay the lowest relevant amount 6 4 Purchase of a travel cancel ation insurance is highly recommended 6 5 For administ rative reasons, notice of cance lation must be addressed to H DM not the lodging establishment and should be provided in writing, such as by fax, email or other informal written form, in the interest of the guest 7  Arrival and departure 7 1 Guests are required to arrive at the agreed time or by 600 pm at the latest if no specific time has been agreed 7 2 The following applies to later arrivals a The guest shall undertake to inform the Host not later than the agreed date of arrival if the guest will be arriving late or intends to arrive a day late in the case of accommodations booked for multiple days b The Host is authorized to rent the accommodations to another guest if timely notice is not provided The provisions of s ection 6 3 shall apply mutatis mutandis for the period during which a room is vacant c If the guest provides notice of late arrival, he she shall undertake to pay the agreed charges less expenses saved by the Host in accordance with s ection 6 3 , including for periods of non -occupancyfor which a room was reserved, unless the Host is contractually or legally required to accept responsibility for the reasons for delayed occupancy 7 3 Guests are required to vacate the accommodations at the agreed time, but not later than 1200 pm on the date of departure if no specific time has been agreed The Host may demand additional compensation as appropriate in the event that the accommodations are not vacated in a timely manner The foregoing is without prejudice to the Hosts abi lity to claim additional damages 8  Duties of the customer termination by the Host 8 1 Unless otherwise agreed, the accommodation may only be occupied by the guest for whom it was booked Occupation by another person, in particular subletting in the case of commercial customers or, in particular, the transfer of blocks of rooms, is prohibited 8 2 The guest shall undertake to treat the room and all furnishings, as well as all furnishings and fixtures at the lodging establishment itself, only as intended and with care and, if posted e.g., in the case of pools and saunas, only pursuant to the rules for use 8 3 The guest shall undertake to immediately report any defects and malfunctions to the Host and request they be remedied Notice of defects provided solely to H DM is insufficient The guests right to assert claims may lapse, in whole or in part, if the guest is at fault for a failure to provide notice 8 4 The guest may only terminate the contract in the event of substantial defects or malfunctions Prior to termination, he she must provide the Host a reasonable period to remedy said defect as part of the notification of the same unless it is impossible for said defect to be remedied, the Host refuses to remedy the defect, if termination of the cont ract without notice is justified by a legitimate interest of the guest or that this would make it objectively unreasonable to expect the guest to continue with their stay 8 5 Bringing and housing pets at the accommodations is only permitted if expressly agreed and only if the Host provides for such an option in the description In the case of such an agreement, the guest is required to provide truthful information about the type and size of the pet A breach of this obligation may provide the Host with rea sons to terminate the lodging agreement 8 6 The Host may terminate the lodging agreement without observing a notice period if the guest, despite receiving a warning from the Host, continuously disrupts the Hosts operations, other guests or the completion of the stay, or if the guest acts in a manner that contravenes the contract to such a degree that the immediate dissolution of the contract is justified In the event that the Host terminates the contract, the foregoing provisions regarding the obligation to pay upon withdrawal by the guest shall apply mutatis mutandis with regard to the Hosts right to payment 9  Limitation of liability 9 1 The Host is liable without limitation, as far as the damage results from violation of an essential obligation, the performance of which was required for proper execution of the contract or the violation of which endangers achievement of the purpose of the contract or the damage results from violation of life, body or health Apart from this, the Hosts liability shall be limited to damage caused by the Host or its servants willfully or grossly negligently 9 2 The potential Hosts innkeepers liability for items brought by the guest in accordance with sections 701 et seq B GB shall remain unaffected by this provision 9 3 The Host is not liable for disruptions in connection with goods and services that were merely arranged for the guest client during the stay where it is clear that such goods and or services are third -partyservices e.g., sporting events, theater tickets, exhibitions, etc. The foregoing applies mutatis mutandis to third -partygoods and services arranged in combination with the reservation if and insofar as the same were expressly indicated as third -partyservices in the description and or book ing confirmation 10  Special regulations in connection with pandemics in particular the Corona virus 10 1 The parties agree that the agreed travel services shall always be provided by the respective service providers in compliance with and in accordan ce with the official requirements and conditions applicable at the time of travel 10 2 The guest agrees to comply with reasonable regulations or restrictions on use of H DM and the Hosts when using services and to notify the Host immediately in the event of typical symptoms of illness 11  Alternative dispute resolution 11 1 In respect of the German Consumer Dispute Resolution Act Gesetz ber Verbraucherstreitbeilegung, H DM and the Host s advi se that neither H DM nor the Hosts currently participate in voluntary consumer dispute resolution 11 2 H DM will provide the guest appropriate notice in the event consumer dispute resolution were to become obligatory for H DM or the Hosts following the publication of these Terms and Conditions for Guest Accommodations and Agency Services 11 3 Please refer to the European online dispute resolution platform for all guest accommodation and agency services contracts concluded by electronic means 12  Applicable law and place of jurisdiction 12 1 The contractual relationship between the guest and or client and the Host and or H DM is exclusively governed by German law The foregoing shall apply in like manner to all other aspects of the legal relationship 12 2 The gu est and or client may only file suit against the Host and or H DM at the location of their respective registered office 12 3 The place of residence of the guest is determinative for suits brought by the Host and or H DM against the guest and or clie nt The Parties agree that the place of jurisdiction shall be the location of the Hosts registered office for suits filed against guests and or clients that are merchants, legal entities under public law or private law or are persons whose residence or habitual place of abode is located outside of Germany or whose residence or habitual place of abode is unknown at the time a suit is filed 12 4 The foregoing provisions shall not apply if and insofar as relevant and non waivable provisions of European Uni on law or other international laws are applicable to the contract General Terms and Conditions 61 protection Noll Htten Dukic Rechtsanwlte, Mnchen Stuttgart, 2020 2023 Tourism agency Heidelberg Marketing Gmb H Managing director Mathias Schiemer Neuenheimer Landstrae 5 69120 Heidelberg, Germany Phone 49 6221 5840 200 Telefax 49 6221 5840 222 infoheidelberg -marketing.de Commercial register number H RB 337405 Register court A G Mannheim V AT I D D E226325597 62 General Terms and Conditions Imprint Heidelberg Marketing Gmb H Neuenheimer Landstrasse 5 69120 Heidelberg Germany Phone 49 6221 58 40 200 Fax 49 6221 58 40 222 infoheidelberg-marketing.de The Heidelberg Marketing Gmb H is a subsidiary of the City of Heidelberg Content Heidelberg Marketing Gmb H Layout a B Grafik Artem Bathauer Photos Cover page, pages 3 , 4 , 5 , 6 below, 7 , 8 , 9 , 10 , 11 on the top, 12 , 13 , 14 on the top, 15 , 16 , 17 , 21 , 22 , 23 , 38 , 39 on the right, 40 , 43 , 45 , 55 Tobias Schwerdt Pages 6 on the top, 56 Christoph Dpper Page 11 below T MB W Stefan Kuhn Page 14 below Heidelberg Marketing Gmb H Page 18 Theater und Orchester Heidelberg Waechter Waechter Page 19 Theater und Orchester Heidelberg Susanne Reichardt Page 20 on the top Heidelberger Frhling Susanne Reichardt Page 20 below studio visuell for Heidelberger Frhling Pages 24 to 35 Photos of the respective accommodations Page 39 on the left Achim Mende 2023  All contents, in particular texts, photographs and graphics, are protected by  Unless expressly stated other wise, Heidelberg Marketing Gmb H owns the  Heidelberg Marketing Gmb H Neuenheimer Landstrasse 5 69120 Heidelberg Germany Phone 49 6221 58 44444 Fax 49 6221 58 40222 infoheidelberg marketing.de\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def correct_split_hyphenated_words(text):\n",
    "    text = re.sub(r'\\s+-\\s+', '-', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def nlp_handle_hyphenated_words(text):\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if '-' in token.text:\n",
    "            if token.pos_ in ['PROPN', 'NOUN']: \n",
    "                new_text.append(token.text)\n",
    "            else:\n",
    "                parts = token.text.split('-')\n",
    "                if len(parts) > 1:\n",
    "                    sub_doc = nlp(' '.join(parts))\n",
    "                    sub_parts = [sub_token.text_with_ws for sub_token in sub_doc]\n",
    "                    new_text.extend(sub_parts)\n",
    "                else:\n",
    "                    new_text.append(token.text_with_ws)\n",
    "        else:\n",
    "            new_text.append(token.text_with_ws)\n",
    "    return ''.join(new_text)\n",
    "\n",
    "\n",
    "def preserve_hyphenated_words(text):\n",
    "    hyphenated_words = re.findall(r'\\w+-\\w+', text)\n",
    "    \n",
    "    for word in hyphenated_words:\n",
    "        if word.lower() == 'e-mail':\n",
    "            text = text.replace(word, 'email')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions = {\n",
    "        \"doesn't\": \"does not\", \"isn't\": \"is not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"wouldn't\": \"would not\",\n",
    "        \"aren't\": \"are not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\", \"won't\": \"will not\", \"didn't\": \"did not\", \"don't\": \"do not\", \"you're\": \"you are\", \"you've\": \"you would have\", \"you'll\": \"you will\", \"you'd\": \"you would\", \"we're\": \"we are\", \"we've\": \"we have\"\n",
    "    }\n",
    "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions.keys()) + r')\\b', re.IGNORECASE)\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0).lower()]\n",
    "    return contractions_pattern.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean_and_refine_text(text):\n",
    "    text = re.sub(r'\\.{2,}', ': ', text)\n",
    "\n",
    "    text = correct_split_hyphenated_words(text)\n",
    "    text = nlp_handle_hyphenated_words(text)\n",
    "    text = preserve_hyphenated_words(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'Page \\d+ of \\d+|\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'Copyright|All rights reserved|Confidential', '', text, flags=re.I)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)\n",
    "    text = re.sub(r'(\\w)([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'[^\\w\\s,.%-]', '', text)\n",
    "    text = re.sub(r'(\\n|^)(\\d+\\s+)?([A-Z][\\w\\s]+):', r'\\n\\n\\3:\\n', text)\n",
    "    text = re.sub(r'(\\d{1,2}) %', r'\\1%', text)\n",
    "    text = re.sub(r'\\b(\\d+)\\b(?!\\%)', r' \\1 ', text)\n",
    "    text = re.sub(r'\\.([A-Z])', r'. \\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\.\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+(\\d+)', r' \\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "cleaned_and_refined_texts = [clean_and_refine_text(text) for text in pdf_as_text]\n",
    "\n",
    "for text in cleaned_and_refined_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_text(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(tokens):\n",
    "#     return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_words(tokens):\n",
    "#     return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# def tag_parts_of_speech(tokens):\n",
    "#     return nltk.pos_tag(tokens)\n",
    "\n",
    "# from nltk.chunk import ne_chunk\n",
    "\n",
    "# def extract_named_entities(tagged_tokens):\n",
    "#     return ne_chunk(tagged_tokens)\n",
    "\n",
    "# def normalize_text(tokens):\n",
    "#     return [token.lower() for token in tokens]\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     tokens = tokenize_text(text)\n",
    "#     tokens = normalize_text(tokens)\n",
    "#     tokens = remove_stop_words(tokens)\n",
    "#     tokens = lemmatize_words(tokens)\n",
    "#     tagged_tokens = tag_parts_of_speech(tokens)\n",
    "#     named_entities = extract_named_entities(tagged_tokens)\n",
    "#     return named_entities\n",
    "\n",
    "\n",
    "# text = preprocess_text(text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import accelerate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.2.2\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, bitsandbytes, optimum, torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am your personal chatbot.\n",
      "\n",
      "If you have any questions, please don't hesitate to contact me. I will be happy to\n"
     ]
    }
   ],
   "source": [
    "def setup_model(model_name='gpt2'):\n",
    "    # Load tokenizer and model from the GPT-2 model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()  # Move model to GPU if available\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_response(tokenizer, model, text, max_length=200):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "\n",
    "    # Generate a response using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,  # Lower temperature\n",
    "        top_k=50,\n",
    "        top_p=0.85,  # Adjust top_p to control diversity\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer, model = setup_model()\n",
    "\n",
    "# Define a text prompt\n",
    "text = \"Hello, I am your personal chatbot\"\n",
    "\n",
    "# Generate a response from the model\n",
    "response = generate_response(tokenizer, model, text, max_length=30)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When is the registration deadline?\n",
      " Response: When is the registration deadline?\n",
      "\n",
      "The deadline for registration is July 1st, 2018. Registration is open to the general public and is subject to change at any time. If you have any questions, please contact us.\n",
      "\n",
      "Query: How can I apply for financial aid?\n",
      " Response: How can I apply for financial aid?\n",
      "\n",
      "You can apply online or by phone at 1-800-273-TALK (8255) or online at www.gofundme.com.\n",
      "\n",
      "Query: What courses are required for a Computer Science major?\n",
      " Response: What courses are required for a Computer Science major?\n",
      "\n",
      "Computer Science majors must take at least one of the following courses:\n",
      "\n",
      "Query: Tell me more about campus facilities.\n",
      " Response: Tell me more about campus facilities.\n",
      "\n",
      "\"I think it's important for us to be able to provide a safe environment for all of our students and faculty,\" he said. \"We need to make sure that we have the facilities that are right for our campus, and that's what we're trying to do. We're going to continue to work with the university to ensure that that is the case.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"When is the registration deadline?\",\n",
    "    \"How can I apply for financial aid?\",\n",
    "    \"What courses are required for a Computer Science major?\",\n",
    "    \"Tell me more about campus facilities.\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for query in test_queries:\n",
    "    response = generate_response(tokenizer, model, query)\n",
    "    responses.append(response)\n",
    "    print(f\"Query: {query}\\n Response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the registration deadline for new students?\n",
      " Response: What is the registration deadline for new students?\n",
      "\n",
      "If you are a new student, you will be required to register at the end of the first year of your degree program. If you do not have a valid student ID card, your registration will not be accepted until you have completed your first semester of college. You will also need to complete an online application to become a registered student in order to be eligible to apply for a certificate of enrollment in the College of Arts and Sciences (CAS).\n",
      "\n",
      "Query: How can a freshman apply for financial aid?\n",
      " Response: How can a freshman apply for financial aid?\n",
      "\n",
      "If you are applying for a scholarship, you will need to submit a letter of application to the Office of Financial Aid (OFA), which will be sent to you by the end of the year. The letter must include the following information: your name, address, phone number, and the date you applied for the scholarship. If you do not receive the letter within the first six months of your scholarship application, your application will not be accepted.\n",
      "\n",
      "Query: Which core courses are required for a Computer Science major?\n",
      " Response: Which core courses are required for a Computer Science major?\n",
      "\n",
      "Yes, you will need to complete a minimum of three core computer science courses in order to be considered for admission to the College of Computing and Information Sciences.\n",
      "... You will be required to have completed at least one of the following core coursework courses: Computer Information Systems (CIS), Computer Systems Analysis (CSAT), Computational Analysis and Applications (CEA), and Computer Programming (CP). You may also choose to\n",
      "\n",
      "Query: Describe the campus facilities available to all students.\n",
      " Response: Describe the campus facilities available to all students.\n",
      "\n",
      "Faculty and staff are required to report to the Office of Student Affairs at least 30 days in advance of the start of classes. Students must be present at the beginning of each semester to participate in the program. If you are not present, you will not be able to attend classes until you have completed the coursework required for your degree. You will be asked to provide your name, address, phone number, and any other information necessary to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is the registration deadline for new students?\",\n",
    "    \"How can a freshman apply for financial aid?\",\n",
    "    \"Which core courses are required for a Computer Science major?\",\n",
    "    \"Describe the campus facilities available to all students.\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for query in test_queries:\n",
    "    response = generate_response(tokenizer, model, query, max_length=100)  # Slightly increase max_length for more detailed responses\n",
    "    responses.append(response)\n",
    "    print(f\"Query: {query}\\n Response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model as well as the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Assuming you have already initialized and potentially trained/fine-tuned your model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # Or your fine-tuned model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def save_model_and_tokenizer(model, tokenizer, save_directory):\n",
    "    \"\"\"\n",
    "    Saves the model and tokenizer to the specified directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "save_directory = 'C:\\\\Users\\\\asha4\\\\OneDrive\\\\Desktop\\\\mini\\\\mini'\n",
    "save_model_and_tokenizer(model, tokenizer, save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
